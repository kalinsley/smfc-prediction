{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jlab-sensing/MFC_Modeling/blob/main/SNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PSKbPI5wRzN"
   },
   "source": [
    "#  SNN Models\n",
    "###  In order to run the code in this notebook, you must download the `ucscMFCDataset` directory and `stanfordMFCDataset.zip`, which expands into the directory `rocket4`, from [Hugging Face](https://huggingface.co/datasets/adunlop621/Soil_MFC/tree/main), and store them in the same directory as this notebook. You can also find several pretrained models in the at this link, with the naming conventions described in the [README](https://github.com/jlab-sensing/MFC_Modeling#:~:text=Repository%20files%20navigation-,README,-MFC_Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "zcF-Dv23N_ON",
    "outputId": "923b5bd8-f434-4946-b038-e908b048a14b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hepml in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (0.0.12)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (1.3.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (1.26.4)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (0.13.2)\n",
      "Requirement already satisfied: black in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (24.10.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (4.66.4)\n",
      "Requirement already satisfied: wget in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (3.2)\n",
      "Requirement already satisfied: nbdev in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (2.3.31)\n",
      "Requirement already satisfied: sklearn-pandas in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (2.2.0)\n",
      "Requirement already satisfied: graphviz in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (0.20.3)\n",
      "Requirement already satisfied: gdown in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (5.2.0)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (17.0.0)\n",
      "Requirement already satisfied: numba in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (0.60.0)\n",
      "Requirement already satisfied: Cython in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (3.0.11)\n",
      "Requirement already satisfied: fastprogress in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (1.0.3)\n",
      "Requirement already satisfied: giotto-tda in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (0.6.2)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (10.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from black->hepml) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from black->hepml) (1.0.0)\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from black->hepml) (24.1)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from black->hepml) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from black->hepml) (4.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from gdown->hepml) (4.12.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from gdown->hepml) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from gdown->hepml) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (1.13.1)\n",
      "Requirement already satisfied: joblib>=0.16.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (1.4.2)\n",
      "Requirement already satisfied: giotto-ph>=0.2.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (0.2.4)\n",
      "Requirement already satisfied: pyflagser>=0.4.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (0.4.7)\n",
      "Requirement already satisfied: igraph>=0.9.8 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (0.11.6)\n",
      "Requirement already satisfied: plotly>=4.8.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (5.24.1)\n",
      "Requirement already satisfied: ipywidgets>=7.5.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (8.1.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from scikit-learn->hepml) (3.5.0)\n",
      "Requirement already satisfied: fastcore>=1.5.27 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (1.7.19)\n",
      "Requirement already satisfied: execnb>=0.1.4 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (0.1.6)\n",
      "Requirement already satisfied: astunparse in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (1.6.3)\n",
      "Requirement already satisfied: ghapi>=1.0.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (1.0.6)\n",
      "Requirement already satisfied: watchdog in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (5.0.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (2.4.1)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (6.0.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from numba->hepml) (0.43.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas->hepml) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas->hepml) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas->hepml) (2024.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from seaborn->hepml) (3.9.0)\n",
      "Requirement already satisfied: ipython in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from execnb>=0.1.4->nbdev->hepml) (8.29.0)\n",
      "Requirement already satisfied: texttable>=1.6.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from igraph>=0.9.8->giotto-tda->hepml) (1.7.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (3.0.13)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (3.1.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from plotly>=4.8.2->giotto-tda->hepml) (9.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->hepml) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from astunparse->nbdev->hepml) (0.43.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from beautifulsoup4->gdown->hepml) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests[socks]->gdown->hepml) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests[socks]->gdown->hepml) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests[socks]->gdown->hepml) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests[socks]->gdown->hepml) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests[socks]->gdown->hepml) (1.7.1)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from jedi>=0.16->ipython->execnb>=0.1.4->nbdev->hepml) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pexpect>4.3->ipython->execnb>=0.1.4->nbdev->hepml) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->execnb>=0.1.4->nbdev->hepml) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from stack-data->ipython->execnb>=0.1.4->nbdev->hepml) (2.1.0)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from stack-data->ipython->execnb>=0.1.4->nbdev->hepml) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: arrow in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from arrow) (2.9.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from arrow) (2.9.0.20241003)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.7.0->arrow) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: keras_lr_finder in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (0.1)\n",
      "Requirement already satisfied: keras>=2.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras_lr_finder) (3.3.3)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras_lr_finder) (3.9.0)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (0.0.8)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (3.11.0)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (0.3.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->keras_lr_finder) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from optree->keras>=2.0.0->keras_lr_finder) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from rich->keras>=2.0.0->keras_lr_finder) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from rich->keras>=2.0.0->keras_lr_finder) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=2.0.0->keras_lr_finder) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade hepml\n",
    "%pip install arrow\n",
    "%pip install keras_lr_finder\n",
    "%pip install pandas\n",
    "%pip install snntorch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LJsRk5_qTljC"
   },
   "outputs": [],
   "source": [
    "# reload modules before executing user code\n",
    "#%load_ext autoreload\n",
    "# reload all modules every time before executing Python code\n",
    "#%autoreload 2\n",
    "# render plots in notebook\n",
    "\n",
    "# Misc imports\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from hepml.core import plot_regression_tree\n",
    "sns.set(color_codes=True)\n",
    "sns.set_palette(sns.color_palette(\"muted\"))\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# snnTorch imports\n",
    "import snntorch as snn\n",
    "from snntorch import functional as SF\n",
    "import snntorch.spikeplot as splt\n",
    "\n",
    "# # keras imports\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import LSTM\n",
    "# from keras import backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N35aqIRugIHl"
   },
   "source": [
    "##  Load and Format Dataset 1\n",
    "\n",
    "### Remember to download `stanfordMFCDataset.zip`, which expands into the directory `rocket4`, from [Hugging Face](https://huggingface.co/datasets/adunlop621/Soil_MFC/tree/main), and store it in the same directory as this notebook before executing the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7PoS8fwOS-Q"
   },
   "outputs": [],
   "source": [
    "#Load teros data\n",
    "import glob\n",
    "teros_files = glob.glob(\"rocket4/TEROSoutput*.csv\")\n",
    "X = pd.DataFrame()\n",
    "for f in teros_files:\n",
    "  try:\n",
    "    csv = pd.read_csv(f, index_col=False).dropna()\n",
    "    X = pd.concat([X, csv])\n",
    "  except:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zaaHwFRvXtN4"
   },
   "outputs": [],
   "source": [
    "#Load power data\n",
    "power_files = glob.glob(\"rocket4/soil*.csv\")\n",
    "y = pd.DataFrame()\n",
    "for f in sorted(power_files, key=lambda x: int(x.split('.')[0].split('_')[-1])):\n",
    "#in power_files:\n",
    "  try:\n",
    "    csv = pd.read_csv(f, on_bad_lines='skip', skiprows=10).dropna(how='all')\n",
    "    csv = csv.rename({'Unnamed: 0': 'timestamp'}, axis='columns')\n",
    "    y = pd.concat([y,csv])\n",
    "  except:\n",
    "    continue\n",
    "y[\"timestamp\"] = y[\"timestamp\"].round(decimals = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ViS0TjC4n6lk"
   },
   "outputs": [],
   "source": [
    "#Convert current to amps, voltage to volts\n",
    "y[\"I1L [10pA]\"] = np.abs(y[\"I1L [10pA]\"] * 1E-11)\n",
    "y[\"V1 [10nV]\"] = np.abs(y[\"V1 [10nV]\"] * 1E-8)\n",
    "y[\"I1H [nA]\"] = np.abs(y[\"I1H [nA]\"] * 1E-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0aqozsZpSOB",
    "outputId": "641b06d1-6a3b-4fd8-e542-a6859314ffff"
   },
   "outputs": [],
   "source": [
    "#Sort data by timestamp, convert to datetime\n",
    "X = X.sort_values(['timestamp'])\n",
    "y = y.sort_values(['timestamp'])\n",
    "X['timestamp'] = pd.to_datetime(X['timestamp'], unit='s')\n",
    "y['timestamp'] = pd.to_datetime(y['timestamp'], unit='s')\n",
    "\n",
    "#Merge data by timestamp\n",
    "uncut_df = pd.merge_asof(left=X,right=y,direction='nearest',tolerance=pd.Timedelta('1 sec'), on = 'timestamp').dropna(how='all')\n",
    "\n",
    "#Isolate data from cell0\n",
    "df = uncut_df.loc[uncut_df['sensorID'] == 0]\n",
    "\n",
    "#Localize timestamp\n",
    "df.timestamp = df.timestamp.dt.tz_localize('UTC').dt.tz_convert('US/Pacific')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBAfOHB61Jwc"
   },
   "outputs": [],
   "source": [
    "#Use only data from after deployment date\n",
    "#df = df.loc[(df['timestamp'] > '2021-09-24') & (df['timestamp'] < '2021-10-15')] #Future of Clean Computing Graph\n",
    "#df = df.loc[(df['timestamp'] > '2021-06-24') & (df['timestamp'] < '2021-07-02')]\n",
    "#df = df.loc[(df['timestamp'] > '2021-06-18')] #Two weeks after deployment\n",
    "df = df.loc[(df['timestamp'] > '2021-06-04')] #Deployment date\n",
    "#df = df.loc[(df['timestamp'] > '2021-06-25') & (df['timestamp'] < '2021-06-26')] #Small training set\n",
    "\n",
    "#Power drop\n",
    "#df = df.loc[(df['timestamp'] > '2021-11-01') & (df['timestamp'] < '2021-11-22')]\n",
    "\n",
    "#Drop data outages\n",
    "df = df.drop(df[(df.timestamp > '2021-11-11') & (df.timestamp < '2021-11-22 01:00:00')].index)\n",
    "df = df.drop(df[(df.timestamp > '2022-01-27')].index)\n",
    "#df = df.set_index('timestamp')\n",
    "df = df[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8fFsoLNfrFA"
   },
   "outputs": [],
   "source": [
    "df = df.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kNxDOBwk7IN"
   },
   "outputs": [],
   "source": [
    "#Get time since deployement\n",
    "df['tsd'] = (df.index - df.index[0]).days\n",
    "df['hour'] = (df.index).hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Pfyv0fM3te1"
   },
   "outputs": [],
   "source": [
    "#Calculate power\n",
    "df[\"power\"] = np.abs(np.multiply(df.iloc[:, 7], df.iloc[:, 8]))\n",
    "#df[\"power\"] = np.abs(np.multiply(df[\"I1L [10pA]\"], df[\"V1 [10nV]\"]))\n",
    "\n",
    "#Convert to nW\n",
    "df['power'] = df['power']*1E9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jA-WVzVh2-lf"
   },
   "outputs": [],
   "source": [
    "#Convert to 10 nanoamps, 10 microvolts\n",
    "df[\"I1L [10pA]\"] = np.abs(df[\"I1L [10pA]\"] * 1E8)\n",
    "df[\"V1 [10nV]\"] = np.abs(df[\"V1 [10nV]\"] * 1E5)\n",
    "df[\"I1H [nA]\"] = np.abs(df[\"I1H [nA]\"] * 1E8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxAlm4FXh57m"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sHtXZCVrsQJ"
   },
   "outputs": [],
   "source": [
    "#Add power time series\n",
    "df['power - 1h'] = df['power'].shift(1).dropna()\n",
    "df['power - 2h'] = df['power'].shift(2).dropna()\n",
    "df['power - 3h'] = df['power'].shift(3).dropna()\n",
    "#df['power - 2h'] = df['power'].shift(2).dropna()\n",
    "#df['previous_power - 3'] = df['power'].shift(3).dropna()\n",
    "#df['previous_power - 4'] = df['power'].shift(4).dropna()\n",
    "\n",
    "#Add teros time series\n",
    "df['EC - 1h'] = df['EC'].shift(1).dropna()\n",
    "df['EC - 2h'] = df['EC'].shift(2).dropna()\n",
    "df['EC - 3h'] = df['EC'].shift(3).dropna()\n",
    "\n",
    "df['temp - 1h'] = df['temp'].shift(1).dropna()\n",
    "df['temp - 2h'] = df['temp'].shift(2).dropna()\n",
    "df['temp - 3h'] = df['temp'].shift(3).dropna()\n",
    "\n",
    "df['raw_VWC - 1h'] = df['raw_VWC'].shift(1).dropna()\n",
    "df['raw_VWC - 2h'] = df['raw_VWC'].shift(2).dropna()\n",
    "df['raw_VWC - 3h'] = df['raw_VWC'].shift(3).dropna()\n",
    "\n",
    "#Add voltage and current time series\n",
    "df['V1 - 1h'] = df['V1 [10nV]'].shift(1).dropna()\n",
    "df['V1 - 2h'] = df['V1 [10nV]'].shift(2).dropna()\n",
    "df['V1 - 3h'] = df['V1 [10nV]'].shift(3).dropna()\n",
    "\n",
    "df['I1L - 1h'] = df['I1L [10pA]'].shift(1).dropna()\n",
    "df['I1L - 2h'] = df['I1L [10pA]'].shift(2).dropna()\n",
    "df['I1L - 3h'] = df['I1L [10pA]'].shift(3).dropna()\n",
    "\n",
    "df['I1H - 1h'] = df['I1H [nA]'].shift(1).dropna()\n",
    "df['I1H - 2h'] = df['I1H [nA]'].shift(2).dropna()\n",
    "df['I1H - 3h'] = df['I1H [nA]'].shift(3).dropna()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4SM1_EvGS6y"
   },
   "outputs": [],
   "source": [
    "#df = df.rename(columns={'power': 'power [Î¼W]'})\n",
    "df = df.rename(columns={'I1L [10pA]': 'Current (uA)', 'V1 [10nV]' : 'Voltage (mV)', 'power' : 'Power (uW)'})\n",
    "df = df.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSkfrkLBZ42n"
   },
   "source": [
    "## Specify Device so we can use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8NhMTInXpyS"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMJdiecpR7VB"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import LSTM\n",
    "# from keras import backend as K\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.9\n",
    "\n",
    "# old design network\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(200, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'))\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(3))\n",
    "# model.compile(loss=quantile_loss, metrics=['mape'], optimizer='adam')\n",
    "\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_steps):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        num_hidden1 = 200\n",
    "\n",
    "        # layer 1\n",
    "        self.slstm1 = snn.SLSTM(num_inputs, num_hidden1, threshold = 0.25)\n",
    "\n",
    "        # layer 2\n",
    "        self.fc1 = torch.nn.Linear(in_features=num_hidden1, out_features=100)\n",
    "        self.lif1 = snn.Leaky(beta=beta, threshold = 0.5)\n",
    "\n",
    "        # randomly initialize decay rate for output neuron\n",
    "        beta_out = random.uniform(0.5, 1)\n",
    "\n",
    "        # layer 2\n",
    "        self.fc2 = torch.nn.Linear(in_features=100, out_features=3)\n",
    "        self.lif2 = snn.Leaky(beta=beta_out, learn_beta=True, reset_mechanism=\"none\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states and outputs at t=0\n",
    "        syn1, mem1 = self.slstm1.reset_mem()\n",
    "        mem2 = self.lif1.reset_mem()\n",
    "        mem3 = self.lif2.reset_mem()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk1_rec = []\n",
    "        spk2_rec = []\n",
    "        spk3_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            spk1, syn1, mem1 = self.slstm1(x.flatten(1), syn1, mem1)\n",
    "            spk2, mem2 = self.lif1(self.fc1(spk1), mem2)\n",
    "            spk3, mem3 = self.lif2(self.fc2(spk2), mem3)\n",
    "\n",
    "            # Append the Spike and Membrane History\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "            spk3_rec.append(spk3)\n",
    "            mem_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spk1_rec), torch.stack(spk2_rec), torch.stack(spk3_rec), torch.stack(mem_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import LSTM\n",
    "# from keras import backend as K\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(969652, 20)\n",
      "X_train_teacher shape: (339378, 20)\n",
      "X_train_student shape: (339378, 20)\n",
      "X_valid shape: (145448, 20)\n",
      "X_test shape: (145448, 20)\n"
     ]
    }
   ],
   "source": [
    "# Combine features into X and targets into y\n",
    "X = pd.concat([\n",
    "    df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"],\n",
    "    df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"],\n",
    "    df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"],\n",
    "    df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"],\n",
    "    df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"],\n",
    "    df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"],\n",
    "    df[\"tsd\"], df[\"hour\"]\n",
    "], axis=1)\n",
    "\n",
    "\n",
    "y = pd.concat([\n",
    "    df['Power (uW)'], df['Voltage (mV)'], df['Current (uA)']\n",
    "], axis=1)\n",
    "\n",
    "# Split into training and testing sets (70% training, 30% testing)\n",
    "X_train, X_test = train_test_split(X, test_size=0.3, shuffle=False)\n",
    "y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Split the training set into teacher and student subsets (50/50)\n",
    "X_train_teacher, X_train_student = train_test_split(X_train, test_size=0.5, shuffle=False)\n",
    "y_train_teacher, y_train_student = train_test_split(y_train, test_size=0.5, shuffle=False)\n",
    "print(X.shape)\n",
    "\n",
    "# Split the testing set into validation and final test sets (50/50)\n",
    "X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Print the shapes for verification\n",
    "print(f\"X_train_teacher shape: {X_train_teacher.shape}\")\n",
    "print(f\"X_train_student shape: {X_train_student.shape}\")\n",
    "print(f\"X_valid shape: {X_valid.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cbZHNDVyAzb"
   },
   "source": [
    "# Loading Teacher Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m time_frame \u001b[38;5;241m=\u001b[39m time_frame_list[j]\n\u001b[1;32m     19\u001b[0m time_frame_seconds \u001b[38;5;241m=\u001b[39m time_frame_seconds_list[j]\n\u001b[0;32m---> 21\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mconcat([df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpower - 1h\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpower - 2h\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpower - 3h\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m     22\u001b[0m                df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV1 - 1h\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV1 - 2h\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV1 - 3h\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m     23\u001b[0m                df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI1L - 1h\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI1L - 2h\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI1L - 3h\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m     24\u001b[0m                df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEC - 1h\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEC - 2h\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEC - 3h\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m     25\u001b[0m                df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_VWC - 1h\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_VWC - 2h\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_VWC - 3h\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m     26\u001b[0m                df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp - 1h\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp - 2h\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp - 3h\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m     27\u001b[0m                df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtsd\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m\"\u001b[39m]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPower (uW)\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVoltage (mV)\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent (uA)\u001b[39m\u001b[38;5;124m'\u001b[39m]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Normalize Data\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Set parameters\n",
    "batchsize_list = [300, 150, 50, 20, 8]\n",
    "time_frame_list = ['3min', '5min', '15min', '30min', '60min']\n",
    "time_frame_seconds_list = [180, 300, 900, 1800, 3600]\n",
    "n = 0\n",
    "\n",
    "snn_power_mape_list = []\n",
    "snn_volt_mape_list = []\n",
    "snn_curr_mape_list = []\n",
    "\n",
    "# Dictionary to store mv variables\n",
    "mv_dict = {}\n",
    "\n",
    "for j in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[j]\n",
    "    time_frame = time_frame_list[j]\n",
    "    time_frame_seconds = time_frame_seconds_list[j]\n",
    "\n",
    "    X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], \n",
    "                   df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], \n",
    "                   df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"], \n",
    "                   df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"], \n",
    "                   df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"], \n",
    "                   df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"], \n",
    "                   df[\"tsd\"], df[\"hour\"]], axis=1)\n",
    "    y = pd.concat([df[\"Power (uW)\"], df['Voltage (mV)'], df['Current (uA)']], axis=1)\n",
    "\n",
    "    # Normalize Data\n",
    "    X_normalized = ((X - X.min()) / (X.max() - X.min()))\n",
    "\n",
    "    # Split train and test sets\n",
    "    X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
    "    y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
    "\n",
    "    X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "    y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "    # Calculate actual energy generated in test set\n",
    "    E_actual = 0\n",
    "    for i in range(len(y_test) - 1):\n",
    "        t = (y_test.index[i+1] - y_test.index[i]).total_seconds()\n",
    "        if t < 180:\n",
    "            E_actual += y_test['Power (uW)'][i] * t\n",
    "\n",
    "    # Resample data\n",
    "    X_valid = X_valid.resample(time_frame).mean().dropna()\n",
    "    y_valid = y_valid.resample(time_frame).mean().dropna()\n",
    "\n",
    "    X_test = X_test.resample(time_frame).mean().dropna()\n",
    "    y_test = y_test.resample(time_frame).mean().dropna()\n",
    "\n",
    "    # Define mv variable for the current time frame\n",
    "    mv_dict[time_frame] = y_test\n",
    "\n",
    "    # Reshape data\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
    "    X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "    # Convert to tensor\n",
    "    X_train = torch.tensor(X_train)\n",
    "    y_train = torch.tensor(y_train.values)\n",
    "    X_valid = torch.tensor(X_valid)\n",
    "    y_valid = torch.tensor(y_valid.values)\n",
    "    X_test = torch.tensor(X_test)\n",
    "    y_test = torch.tensor(y_test.values)\n",
    "\n",
    "    # Make datasets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=False)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batchsize, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
    "\n",
    "    num_steps = 50\n",
    "    num_inputs = X_train.shape[2]\n",
    "\n",
    "    # Create new instance of the SNN Class\n",
    "    model = Net(num_inputs, num_steps).to(device)\n",
    "\n",
    "    file = 'trained_models/snn_' + time_frame + '_quant50.pth'\n",
    "    print(file)\n",
    "\n",
    "    checkpoint = torch.load(file, map_location=torch.device('cpu'), weights_only=True)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            # Prepare data\n",
    "            data = data.to(device).float()\n",
    "            targets = targets.to(device).float()\n",
    "\n",
    "            _, _, _, output = model(data)\n",
    "\n",
    "            output = output.cpu().squeeze(1).detach()\n",
    "            actuals.append(targets)\n",
    "            predictions.append(output[-1])\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    actuals = torch.cat(actuals, dim=0)\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "\n",
    "    mv = mv_dict[time_frame]\n",
    "    mv[\"power_pred_med_\" + time_frame] = predictions[:, 0].numpy()\n",
    "    mv[\"voltage_pred_med_\" + time_frame] = predictions[:, 1].numpy()\n",
    "    mv[\"current_pred_med_\" + time_frame] = predictions[:, 2].numpy()\n",
    "\n",
    "    print(f'Voltage overestimation rate for {time_frame}: %.3f%%' % (\n",
    "        (mv['Voltage (mV)'].values <= mv[\"voltage_pred_med_\" + time_frame]).mean() * 100))\n",
    "    print(f\"Test MAPE power ({time_frame}): %3f\" % MAPE(mv['Power (uW)'].values.ravel(), mv[\"power_pred_med_\" + time_frame]))\n",
    "    print(f\"Test MAPE voltage ({time_frame}): %3f\" % MAPE(mv['Voltage (mV)'], mv[\"voltage_pred_med_\" + time_frame]))\n",
    "    print(f\"Test MAPE current ({time_frame}): %3f\" % MAPE(mv['Current (uA)'], mv[\"current_pred_med_\" + time_frame]))\n",
    "\n",
    "    E_pred = 0\n",
    "    for i in range(len(mv) - 1):\n",
    "        t = (mv.index[i+1] - mv.index[i]).total_seconds()\n",
    "        if t <= time_frame_seconds + 50:\n",
    "            E_pred += mv[\"power_pred_med_\" + time_frame][i] * t\n",
    "\n",
    "    print(f'Predicted vs. Actual Total Energy Percent Difference ({time_frame}): %.3f%%' % (\n",
    "        (E_pred - E_actual) * 100 / E_actual))\n",
    "\n",
    "    V_actual = mv['Voltage (mV)'].mean()\n",
    "    V_pred = mv[\"voltage_pred_med_\" + time_frame].mean()\n",
    "    print(f'Predicted vs. Actual Total Voltage Percent Difference ({time_frame}): %.3f%%' % (\n",
    "        (V_pred - V_actual) * 100 / V_actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Specifically 5minute predictions to mv_Dict['5min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Npx5qknU0O-J",
    "outputId": "a55b8709-b96e-402a-dce0-b655cce94a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_models/snn_5min_quant50.pth\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'power_pred_med_5min'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'power_pred_med_5min'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[243], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictions\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39mf)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActuals: \u001b[39m\u001b[38;5;124m\"\u001b[39m, mv[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPower (uW)\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mravel(), file\u001b[38;5;241m=\u001b[39mf)\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mmv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpower_pred_med_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime_frame\u001b[49m\u001b[43m]\u001b[49m, file\u001b[38;5;241m=\u001b[39mf)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(MAPE(mv[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPower (uW)\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mravel(), mv[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpower_pred_med_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m time_frame]), file\u001b[38;5;241m=\u001b[39mf)\n\u001b[1;32m    116\u001b[0m mv \u001b[38;5;241m=\u001b[39m mv_dict[time_frame]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/core/frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3760\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3762\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'power_pred_med_5min'"
     ]
    }
   ],
   "source": [
    "# from keras.models import load_model\n",
    "\n",
    "# # Set parameters\n",
    "# batchsize_list = [300, 150, 50, 20, 8]\n",
    "# time_frame = '5min'\n",
    "# time_frame_seconds = 300\n",
    "\n",
    "# # Initialize results storage\n",
    "# snn_power_mape_list = []\n",
    "# snn_volt_mape_list = []\n",
    "# snn_curr_mape_list = []\n",
    "\n",
    "# # Dictionary to store mv variables\n",
    "# mv_dict = {}\n",
    "\n",
    "# # Process only the first batch size\n",
    "# batchsize = batchsize_list[1]  # Pick the first batch size\n",
    "\n",
    "# X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"],\n",
    "#                df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"],\n",
    "#                df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"],\n",
    "#                df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"],\n",
    "#                df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"],\n",
    "#                df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"],\n",
    "#                df[\"tsd\"], df[\"hour\"]], axis=1)\n",
    "# y = pd.concat([df[\"Power (uW)\"], df['Voltage (mV)'], df['Current (uA)']], axis=1)\n",
    "\n",
    "# # Normalize Data\n",
    "# X_normalized = ((X - X.min()) / (X.max() - X.min()))\n",
    "\n",
    "# # Split train and test sets\n",
    "# X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
    "# y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# # Split the testing set into validation and final test sets (50/50)\n",
    "# X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "# y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "# # Resample data\n",
    "# X_train = X_train.resample(time_frame).mean().dropna()\n",
    "# y_train = y_train.resample(time_frame).mean().dropna()\n",
    "# X_valid = X_valid.resample(time_frame).mean().dropna()\n",
    "# y_valid = y_valid.resample(time_frame).mean().dropna()\n",
    "\n",
    "# X_test = X_test.resample(time_frame).mean().dropna()\n",
    "# y_test = y_test.resample(time_frame).mean().dropna()\n",
    "\n",
    "# # Define mv variable for the current time frame\n",
    "# mv_dict[time_frame] = y_train\n",
    "\n",
    "# # Reshape data\n",
    "# X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "# X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
    "# X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# # Convert to tensor\n",
    "# X_train = torch.tensor(X_train)\n",
    "# y_train = torch.tensor(y_train.values)\n",
    "# X_valid = torch.tensor(X_valid)\n",
    "# y_valid = torch.tensor(y_valid.values)\n",
    "# X_test = torch.tensor(X_test)\n",
    "# y_test = torch.tensor(y_test.values)\n",
    "\n",
    "# # Make datasets\n",
    "# train_dataset = TensorDataset(X_train, y_train)\n",
    "# valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "# test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# # Create DataLoaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=False)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=batchsize, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
    "\n",
    "# num_steps = 50\n",
    "# num_inputs = X_train.shape[2]\n",
    "\n",
    "# # Create new instance of the SNN Class\n",
    "# model = Net(num_inputs, num_steps).to(device)\n",
    "\n",
    "# file = 'trained_models/snn_' + time_frame + '_quant50.pth'\n",
    "# print(file)\n",
    "\n",
    "# checkpoint = torch.load(file, map_location=torch.device('cpu'), weights_only=True)\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# model.eval()\n",
    "# actuals = []\n",
    "# predictions = []\n",
    "# with torch.no_grad():\n",
    "#     for data, targets in train_loader:\n",
    "#         # Prepare data\n",
    "#         data = data.to(device).float()\n",
    "#         targets = targets.to(device).float()\n",
    "\n",
    "#         _, _, _, output = model(data)\n",
    "\n",
    "#         output = output.cpu().squeeze(1).detach()\n",
    "#         actuals.append(targets)\n",
    "#         predictions.append(output[-1])\n",
    "#         # print(\"targets: \",targets)\n",
    "#         # print(\"prediction: \", output[-1])\n",
    "\n",
    "# # Convert lists to tensors\n",
    "# actuals = torch.cat(actuals, dim=0)\n",
    "# predictions = torch.cat(predictions, dim=0)\n",
    "\n",
    "# with open('mape_test.txt', 'w') as f:\n",
    "#     # Print the shapes of the actuals and predictions\n",
    "#     print(f\"Actual values shape: {actuals.shape}\", file=f)\n",
    "#     print(f\"Predictions shape: {predictions.shape}\", file=f)\n",
    "\n",
    "#     print(\"Actuals: \", mv['Power (uW)'].values.ravel(), file=f)\n",
    "#     print(\"predictions: \", mv[\"power_pred_med_\" + time_frame], file=f)\n",
    "#     print(MAPE(mv['Power (uW)'].values.ravel(), mv[\"power_pred_med_\" + time_frame]), file=f)\n",
    "\n",
    "# mv = mv_dict[time_frame]\n",
    "# mv[\"power_pred_med_\" + time_frame] = predictions[:, 0].numpy()\n",
    "# mv[\"voltage_pred_med_\" + time_frame] = predictions[:, 1].numpy()\n",
    "# mv[\"current_pred_med_\" + time_frame] = predictions[:, 2].numpy()\n",
    "\n",
    "# print(f'Voltage overestimation rate for {time_frame}: %.3f%%' % (\n",
    "#     (mv['Voltage (mV)'].values <= mv[\"voltage_pred_med_\" + time_frame]).mean() * 100))\n",
    "# print(f\"Test MAPE power ({time_frame}): %3f\" % MAPE(mv['Power (uW)'].values.ravel(), mv[\"power_pred_med_\" + time_frame]))\n",
    "# print(f\"Test MAPE voltage ({time_frame}): %3f\" % MAPE(mv['Voltage (mV)'], mv[\"voltage_pred_med_\" + time_frame]))\n",
    "# print(f\"Test MAPE current ({time_frame}): %3f\" % MAPE(mv['Current (uA)'], mv[\"current_pred_med_\" + time_frame]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMZgk8NYuEaQ"
   },
   "source": [
    "# Student Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(y_true, y_pred, quantile=0.5):\n",
    "    error = y_true - y_pred\n",
    "    loss = torch.mean(torch.max(quantile * error, (quantile - 1) * error))\n",
    "    return loss\n",
    "\n",
    "def distillation_loss(y_teacher, y_pred):\n",
    "  error = y_teacher - y_pred\n",
    "  loss = torch.mean((error) ** 2)\n",
    "  return loss\n",
    "\n",
    "def combined_loss(y_true, y_pred, y_teacher, quantile=0.5, alpha=1):\n",
    "  return ((alpha) * quantile_loss(y_true, y_pred, quantile)) + ((1-alpha) * distillation_loss(y_teacher, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDBlmqCZuEaR"
   },
   "source": [
    "## 2. Preparing Student Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated X Shape: Rows 10285 Columns 1\n",
      "X_train shape:  torch.Size([10285, 1, 20])\n",
      "y_train shape:  torch.Size([10285, 6])\n"
     ]
    }
   ],
   "source": [
    "# Load teacher predictions\n",
    "teacher_timeframe = '5min'\n",
    "student_timeframe = '15min'\n",
    "teacher_5min_preds = mv_dict[teacher_timeframe]\n",
    "teacher_5min_preds_df = pd.DataFrame(\n",
    "    teacher_5min_preds,\n",
    "    columns=[\n",
    "        \"power_pred_med_\" + teacher_timeframe,\n",
    "        \"voltage_pred_med_\" + teacher_timeframe,\n",
    "        \"current_pred_med_\" + teacher_timeframe\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_normalized = ((X - X.min()) / (X.max() - X.min()))\n",
    "\n",
    "# Using X, y from Teacher model\n",
    "X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
    "y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Split the testing set into validation and final test sets (50/50)\n",
    "X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Resample data to 15-minute intervals\n",
    "X_train_final = X_train.resample(student_timeframe).mean().dropna()\n",
    "y_train_final = y_train.resample(student_timeframe).mean().dropna()\n",
    "X_valid_final = X_valid.resample(student_timeframe).mean().dropna()\n",
    "y_valid_final = y_valid.resample(student_timeframe).mean().dropna()\n",
    "X_test_final = X_test.resample(student_timeframe).mean().dropna()\n",
    "y_test_final = y_test.resample(student_timeframe).mean().dropna()\n",
    "\n",
    "\n",
    "# Align predictions to data\n",
    "# Filter 5-minute predictions to keep those 5 minutes before 15-minute intervals\n",
    "valid_timestamps = y_train_final.index\n",
    "\n",
    "teacher_5min_preds_df = teacher_5min_preds_df.loc[teacher_5min_preds_df.index.isin(valid_timestamps - pd.Timedelta(minutes=5))]\n",
    "# Reassign the prediction timestamps to align with the current data timestamp\n",
    "teacher_5min_preds_df.index = teacher_5min_preds_df.index + pd.Timedelta(minutes=5)\n",
    "\n",
    "\n",
    "# X_train_student = pd.concat([X_train, teacher_5min_preds_df], axis=1)\n",
    "y_train_final = pd.concat([y_train_final, teacher_5min_preds_df], axis=1)\n",
    "\n",
    "# Reshape data for LSTM input\n",
    "X_train_final = X_train_final.values.reshape((X_train_final.shape[0], 1, X_train_final.shape[1]))\n",
    "X_valid_final = X_valid_final.values.reshape((X_valid_final.shape[0], 1, X_valid_final.shape[1]))\n",
    "X_test_final = X_test_final.values.reshape((X_test_final.shape[0], 1, X_test_final.shape[1]))\n",
    "\n",
    "\n",
    "X_train_final = torch.tensor(X_train_final).float()\n",
    "y_train_final = torch.tensor(y_train_final.values).float()\n",
    "X_valid_final = torch.tensor(X_valid_final).float()\n",
    "y_valid_final = torch.tensor(y_valid_final.values).float()\n",
    "X_test_final = torch.tensor(X_test_final).float()\n",
    "y_test_final = torch.tensor(y_test_final.values).float()\n",
    "\n",
    "\n",
    "# Clean X_train_final tensor\n",
    "nan_mask = torch.isnan(y_train_final)  # Identify NaN values\n",
    "non_nan_values = y_train_final[~nan_mask]  # Filter out NaN values for mean calculation\n",
    "mean_value = torch.mean(non_nan_values)  # Compute mean of valid values\n",
    "\n",
    "# Replace NaNs with the computed mean\n",
    "tensor_cleaned = y_train_final.clone()\n",
    "tensor_cleaned[nan_mask] = mean_value\n",
    "\n",
    "# Reassign cleaned tensor back if desired\n",
    "y_train_final = tensor_cleaned\n",
    "\n",
    "print(\"Updated X Shape: Rows \" + str(X_train_final.shape[0]) + \" Columns \" + str(X_train_final.shape[1]))\n",
    "print(\"X_train shape: \", X_train_final.shape)\n",
    "print(\"y_train shape: \", y_train_final.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Training on Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JDO5w2qMuEaR",
    "outputId": "60478509-388c-4cd5-a6ee-b899f94de651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0! Avg loss for the last 100 iterations: 708.4064447021484\n",
      "Epoch 1! Avg loss for the last 100 iterations: 633.7810388183593\n",
      "Epoch 2! Avg loss for the last 100 iterations: 573.849333190918\n",
      "Epoch 3! Avg loss for the last 100 iterations: 513.1004376220703\n",
      "Epoch 4! Avg loss for the last 100 iterations: 455.5786422729492\n",
      "Epoch 5! Avg loss for the last 100 iterations: 395.7302096557617\n",
      "Epoch 6! Avg loss for the last 100 iterations: 352.6347819519043\n",
      "Epoch 7! Avg loss for the last 100 iterations: 320.5615461730957\n",
      "Epoch 8! Avg loss for the last 100 iterations: 294.0406089782715\n",
      "Epoch 9! Avg loss for the last 100 iterations: 272.7545364379883\n",
      "Epoch 10! Avg loss for the last 100 iterations: 256.4401141357422\n",
      "Epoch 11! Avg loss for the last 100 iterations: 244.21494499206543\n",
      "Epoch 12! Avg loss for the last 100 iterations: 235.29292610168457\n",
      "Epoch 13! Avg loss for the last 100 iterations: 229.28981185913085\n",
      "Epoch 14! Avg loss for the last 100 iterations: 226.16855674743653\n",
      "Epoch 15! Avg loss for the last 100 iterations: 224.60007347106932\n",
      "Epoch 16! Avg loss for the last 100 iterations: 224.42249683380126\n",
      "Epoch 17! Avg loss for the last 100 iterations: 224.94722747802734\n",
      "Epoch 18! Avg loss for the last 100 iterations: 225.9999048233032\n",
      "Epoch 19! Avg loss for the last 100 iterations: 227.2774524307251\n",
      "Epoch 20! Avg loss for the last 100 iterations: 228.6075453186035\n",
      "Epoch 21! Avg loss for the last 100 iterations: 229.95091575622558\n",
      "Epoch 22! Avg loss for the last 100 iterations: 231.2313832092285\n",
      "Epoch 23! Avg loss for the last 100 iterations: 232.36841217041015\n",
      "Epoch 24! Avg loss for the last 100 iterations: 233.41147994995117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "power_mape = []\n",
    "voltage_mape = []\n",
    "current_mape = []\n",
    "\n",
    "E_actual_list = []\n",
    "E_pred_list = []\n",
    "\n",
    "max_act_list = []\n",
    "pred_act_list = []\n",
    "succ_act_list = []\n",
    "\n",
    "pred_act_naive_list = []\n",
    "false_act_naive_list = []\n",
    "succ_act_naive_list = []\n",
    "\n",
    "# initialize histories\n",
    "loss_hist = []\n",
    "avg_loss_hist = []\n",
    "acc_hist = []\n",
    "mape_hist = []\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "batch_size = 32  # Adjust as needed\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-2\n",
    "beta = 0.8  # For quantile loss\n",
    "\n",
    "# Create datasets\n",
    "train_dataset_final = TensorDataset(X_train_final, y_train_final)\n",
    "valid_dataset_final = TensorDataset(X_valid_final, y_valid_final)\n",
    "test_dataset_final = TensorDataset(X_test_final, y_test_final)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset_final, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset_final, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset_final, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#loss_fn = combined_loss\n",
    "#loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "# Define model, optimizer, and loss function\n",
    "num_steps = 1  # Since you're using LSTM for time series data with one step\n",
    "num_inputs = X_train_final.shape[2]\n",
    "output_size = y_train_final.shape[1]\n",
    "# num_inputs = X_train.shape[2]\n",
    "# output_size = y_train.shape[1]\n",
    "model = Net(num_inputs, num_steps).to(device)\n",
    "loss_fn = combined_loss\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# put model into train mode\n",
    "model.train()\n",
    "\n",
    "# Train Loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
    "        # move to device\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # change to floats\n",
    "        data = data.float()\n",
    "        targets = targets.float()\n",
    "\n",
    "        # run forward pass\n",
    "        _, _, _, mem = model(data)\n",
    "        # calculate loss\n",
    "        with torch.no_grad():\n",
    "            y_teacher = targets[:, [3, 4, 5]].squeeze(-1)\n",
    "            y_true = targets[:, [0, 1, 2]].squeeze(-1)\n",
    "        \n",
    "        y_pred = mem[-1]\n",
    "\n",
    "        # print(\"y_true: \", y_true)\n",
    "        # print(\"Student Predictions: \", y_pred)\n",
    "        # print(\"y_teacher: \", y_teacher)\n",
    "\n",
    "        loss_val = loss_fn(y_true, y_pred, y_teacher, alpha=1.0)\n",
    "\n",
    "        # calculate and store MAPE Loss\n",
    "        mem_numpy = mem.cpu().detach().numpy()\n",
    "        #mem_numpy = mem.detach().numpy()\n",
    "        targets_numpy = y_true.cpu().detach().numpy()\n",
    "        #targets_numpy = targets.detach().numpy()\n",
    "        mape_hist.append(MAPE(mem_numpy[-1], targets_numpy))\n",
    "        power_mape.append(MAPE(mem_numpy[-1][:,0], targets_numpy[:,0]))\n",
    "        voltage_mape.append(MAPE(mem_numpy[-1][:,1], targets_numpy[:,1]))\n",
    "        current_mape.append(MAPE(mem_numpy[-1][:,2], targets_numpy[:,2]))\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # if i%10 == 0:\n",
    "            # print(f\"Epoch {epoch}, Iteration {i} Train Loss: {loss_val.item():.2f}\")\n",
    "        if len(loss_hist) > 100:\n",
    "            avg_loss_hist.append(sum(loss_hist[-100:])/len(loss_hist[-100:]))\n",
    "        else:\n",
    "            avg_loss_hist.append(0)\n",
    "\n",
    "    if len(loss_hist) > 100:\n",
    "        print(f'Epoch {epoch}! Avg loss for the last 100 iterations: {avg_loss_hist[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAPE power Using Teacher-Student Model: 1.007\n",
      "Test MAPE voltage Using Teacher-Student Model: 1.075\n",
      "Test MAPE current Using Teacher-Student Model: 0.169\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# Define model, optimizer, and loss function\n",
    "num_steps = 1  # Since you're using LSTM for time series data with one step\n",
    "num_inputs = X_train_final.shape[2]\n",
    "output_size = y_train_final.shape[1]\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Assuming the model has already been trained\n",
    "# Now let's test the model on the test set\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "actuals = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = model(data)\n",
    "        y_pred = outputs[-1]\n",
    "        y_pred = y_pred.permute(1, 0, 2)  # Reorganize to match ground truth shape\n",
    "\n",
    "        # Collect actual and predicted values\n",
    "        actuals.append(targets.cpu().numpy()) \n",
    "        predictions.append(y_pred.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "actuals = np.concatenate(actuals, axis=0)\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "predictions = predictions.squeeze(1)\n",
    "\n",
    "# Compute MAPE for each output (e.g., power, voltage, current)\n",
    "mape_power = MAPE(actuals[:, 0], predictions[:, 0])\n",
    "mape_voltage = MAPE(actuals[:, 1], predictions[:, 1])\n",
    "mape_current = MAPE(actuals[:, 2], predictions[:, 2])\n",
    "\n",
    "# Print results\n",
    "print(f\"Test MAPE power Using Teacher-Student Model: {mape_power:.3f}\")\n",
    "print(f\"Test MAPE voltage Using Teacher-Student Model: {mape_voltage:.3f}\")\n",
    "print(f\"Test MAPE current Using Teacher-Student Model: {mape_current:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "cObaSZAexrX-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
