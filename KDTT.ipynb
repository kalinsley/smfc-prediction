{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jlab-sensing/MFC_Modeling/blob/main/SNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PSKbPI5wRzN"
   },
   "source": [
    "#  SNN Models\n",
    "###  In order to run the code in this notebook, you must download the `ucscMFCDataset` directory and `stanfordMFCDataset.zip`, which expands into the directory `rocket4`, from [Hugging Face](https://huggingface.co/datasets/adunlop621/Soil_MFC/tree/main), and store them in the same directory as this notebook. You can also find several pretrained models in the at this link, with the naming conventions described in the [README](https://github.com/jlab-sensing/MFC_Modeling#:~:text=Repository%20files%20navigation-,README,-MFC_Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "zcF-Dv23N_ON",
    "outputId": "923b5bd8-f434-4946-b038-e908b048a14b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hepml in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (0.0.12)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (1.3.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (1.26.4)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (0.13.2)\n",
      "Requirement already satisfied: black in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (24.10.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (4.66.4)\n",
      "Requirement already satisfied: wget in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (3.2)\n",
      "Requirement already satisfied: nbdev in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (2.3.31)\n",
      "Requirement already satisfied: sklearn-pandas in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (2.2.0)\n",
      "Requirement already satisfied: graphviz in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (0.20.3)\n",
      "Requirement already satisfied: gdown in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (5.2.0)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (17.0.0)\n",
      "Requirement already satisfied: numba in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (0.60.0)\n",
      "Requirement already satisfied: Cython in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (3.0.11)\n",
      "Requirement already satisfied: fastprogress in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (1.0.3)\n",
      "Requirement already satisfied: giotto-tda in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (0.6.2)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (10.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from black->hepml) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from black->hepml) (1.0.0)\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from black->hepml) (24.1)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from black->hepml) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from black->hepml) (4.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from gdown->hepml) (4.12.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from gdown->hepml) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from gdown->hepml) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (1.13.1)\n",
      "Requirement already satisfied: joblib>=0.16.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (1.4.2)\n",
      "Requirement already satisfied: giotto-ph>=0.2.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (0.2.4)\n",
      "Requirement already satisfied: pyflagser>=0.4.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (0.4.7)\n",
      "Requirement already satisfied: igraph>=0.9.8 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (0.11.6)\n",
      "Requirement already satisfied: plotly>=4.8.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (5.24.1)\n",
      "Requirement already satisfied: ipywidgets>=7.5.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (8.1.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from scikit-learn->hepml) (3.5.0)\n",
      "Requirement already satisfied: fastcore>=1.5.27 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (1.7.19)\n",
      "Requirement already satisfied: execnb>=0.1.4 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (0.1.6)\n",
      "Requirement already satisfied: astunparse in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (1.6.3)\n",
      "Requirement already satisfied: ghapi>=1.0.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (1.0.6)\n",
      "Requirement already satisfied: watchdog in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (5.0.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (2.4.1)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (6.0.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from numba->hepml) (0.43.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas->hepml) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas->hepml) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas->hepml) (2024.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from seaborn->hepml) (3.9.0)\n",
      "Requirement already satisfied: ipython in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from execnb>=0.1.4->nbdev->hepml) (8.29.0)\n",
      "Requirement already satisfied: texttable>=1.6.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from igraph>=0.9.8->giotto-tda->hepml) (1.7.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (3.0.13)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (3.1.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from plotly>=4.8.2->giotto-tda->hepml) (9.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->hepml) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from astunparse->nbdev->hepml) (0.43.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from beautifulsoup4->gdown->hepml) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests[socks]->gdown->hepml) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests[socks]->gdown->hepml) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests[socks]->gdown->hepml) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests[socks]->gdown->hepml) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests[socks]->gdown->hepml) (1.7.1)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from jedi>=0.16->ipython->execnb>=0.1.4->nbdev->hepml) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pexpect>4.3->ipython->execnb>=0.1.4->nbdev->hepml) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->execnb>=0.1.4->nbdev->hepml) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from stack-data->ipython->execnb>=0.1.4->nbdev->hepml) (2.1.0)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from stack-data->ipython->execnb>=0.1.4->nbdev->hepml) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: arrow in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from arrow) (2.9.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from arrow) (2.9.0.20241003)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.7.0->arrow) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: keras_lr_finder in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (0.1)\n",
      "Requirement already satisfied: keras>=2.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras_lr_finder) (3.3.3)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras_lr_finder) (3.9.0)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (0.0.8)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (3.11.0)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (0.3.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->keras_lr_finder) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from optree->keras>=2.0.0->keras_lr_finder) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from rich->keras>=2.0.0->keras_lr_finder) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from rich->keras>=2.0.0->keras_lr_finder) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=2.0.0->keras_lr_finder) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade hepml\n",
    "%pip install arrow\n",
    "%pip install keras_lr_finder\n",
    "%pip install pandas\n",
    "%pip install snntorch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJsRk5_qTljC"
   },
   "outputs": [],
   "source": [
    "# reload modules before executing user code\n",
    "#%load_ext autoreload\n",
    "# reload all modules every time before executing Python code\n",
    "#%autoreload 2\n",
    "# render plots in notebook\n",
    "\n",
    "# Misc imports\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from hepml.core import plot_regression_tree\n",
    "sns.set(color_codes=True)\n",
    "sns.set_palette(sns.color_palette(\"muted\"))\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# snnTorch/torch imports\n",
    "import snntorch as snn\n",
    "from snntorch import functional as SF\n",
    "import snntorch.spikeplot as splt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N35aqIRugIHl"
   },
   "source": [
    "##  Load and Format Dataset 1\n",
    "\n",
    "### Remember to download `stanfordMFCDataset.zip`, which expands into the directory `rocket4`, from [Hugging Face](https://huggingface.co/datasets/adunlop621/Soil_MFC/tree/main), and store it in the same directory as this notebook before executing the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "B7PoS8fwOS-Q"
   },
   "outputs": [],
   "source": [
    "#Load teros data\n",
    "import glob\n",
    "teros_files = glob.glob(\"rocket4/TEROSoutput*.csv\")\n",
    "X = pd.DataFrame()\n",
    "for f in teros_files:\n",
    "  try:\n",
    "    csv = pd.read_csv(f, index_col=False).dropna()\n",
    "    X = pd.concat([X, csv])\n",
    "  except:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zaaHwFRvXtN4"
   },
   "outputs": [],
   "source": [
    "#Load power data\n",
    "power_files = glob.glob(\"rocket4/soil*.csv\")\n",
    "y = pd.DataFrame()\n",
    "for f in sorted(power_files, key=lambda x: int(x.split('.')[0].split('_')[-1])):\n",
    "#in power_files:\n",
    "  try:\n",
    "    csv = pd.read_csv(f, on_bad_lines='skip', skiprows=10).dropna(how='all')\n",
    "    csv = csv.rename({'Unnamed: 0': 'timestamp'}, axis='columns')\n",
    "    y = pd.concat([y,csv])\n",
    "  except:\n",
    "    continue\n",
    "y[\"timestamp\"] = y[\"timestamp\"].round(decimals = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ViS0TjC4n6lk"
   },
   "outputs": [],
   "source": [
    "#Convert current to amps, voltage to volts\n",
    "y[\"I1L [10pA]\"] = np.abs(y[\"I1L [10pA]\"] * 1E-11)\n",
    "y[\"V1 [10nV]\"] = np.abs(y[\"V1 [10nV]\"] * 1E-8)\n",
    "y[\"I1H [nA]\"] = np.abs(y[\"I1H [nA]\"] * 1E-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0aqozsZpSOB",
    "outputId": "641b06d1-6a3b-4fd8-e542-a6859314ffff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1g/jt7x_x9n6j1f3317_dwv3k9m0000gn/T/ipykernel_40442/3455547848.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.timestamp = df.timestamp.dt.tz_localize('UTC').dt.tz_convert('US/Pacific')\n"
     ]
    }
   ],
   "source": [
    "#Sort data by timestamp, convert to datetime\n",
    "X = X.sort_values(['timestamp'])\n",
    "y = y.sort_values(['timestamp'])\n",
    "X['timestamp'] = pd.to_datetime(X['timestamp'], unit='s')\n",
    "y['timestamp'] = pd.to_datetime(y['timestamp'], unit='s')\n",
    "\n",
    "#Merge data by timestamp\n",
    "uncut_df = pd.merge_asof(left=X,right=y,direction='nearest',tolerance=pd.Timedelta('1 sec'), on = 'timestamp').dropna(how='all')\n",
    "\n",
    "#Isolate data from cell0\n",
    "df = uncut_df.loc[uncut_df['sensorID'] == 0]\n",
    "\n",
    "#Localize timestamp\n",
    "df.timestamp = df.timestamp.dt.tz_localize('UTC').dt.tz_convert('US/Pacific')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CBAfOHB61Jwc"
   },
   "outputs": [],
   "source": [
    "#Use only data from after deployment date\n",
    "#df = df.loc[(df['timestamp'] > '2021-09-24') & (df['timestamp'] < '2021-10-15')] #Future of Clean Computing Graph\n",
    "#df = df.loc[(df['timestamp'] > '2021-06-24') & (df['timestamp'] < '2021-07-02')]\n",
    "#df = df.loc[(df['timestamp'] > '2021-06-18')] #Two weeks after deployment\n",
    "df = df.loc[(df['timestamp'] > '2021-06-04')] #Deployment date\n",
    "#df = df.loc[(df['timestamp'] > '2021-06-25') & (df['timestamp'] < '2021-06-26')] #Small training set\n",
    "\n",
    "#Power drop\n",
    "#df = df.loc[(df['timestamp'] > '2021-11-01') & (df['timestamp'] < '2021-11-22')]\n",
    "\n",
    "#Drop data outages\n",
    "df = df.drop(df[(df.timestamp > '2021-11-11') & (df.timestamp < '2021-11-22 01:00:00')].index)\n",
    "df = df.drop(df[(df.timestamp > '2022-01-27')].index)\n",
    "#df = df.set_index('timestamp')\n",
    "df = df[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "p8fFsoLNfrFA"
   },
   "outputs": [],
   "source": [
    "df = df.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9kNxDOBwk7IN"
   },
   "outputs": [],
   "source": [
    "#Get time since deployement\n",
    "df['tsd'] = (df.index - df.index[0]).days\n",
    "df['hour'] = (df.index).hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-Pfyv0fM3te1"
   },
   "outputs": [],
   "source": [
    "#Calculate power\n",
    "df[\"power\"] = np.abs(np.multiply(df.iloc[:, 7], df.iloc[:, 8]))\n",
    "#df[\"power\"] = np.abs(np.multiply(df[\"I1L [10pA]\"], df[\"V1 [10nV]\"]))\n",
    "\n",
    "#Convert to nW\n",
    "df['power'] = df['power']*1E9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jA-WVzVh2-lf"
   },
   "outputs": [],
   "source": [
    "#Convert to 10 nanoamps, 10 microvolts\n",
    "df[\"I1L [10pA]\"] = np.abs(df[\"I1L [10pA]\"] * 1E8)\n",
    "df[\"V1 [10nV]\"] = np.abs(df[\"V1 [10nV]\"] * 1E5)\n",
    "df[\"I1H [nA]\"] = np.abs(df[\"I1H [nA]\"] * 1E8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XxAlm4FXh57m"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8sHtXZCVrsQJ"
   },
   "outputs": [],
   "source": [
    "#Add power time series\n",
    "df['power - 1h'] = df['power'].shift(1).dropna()\n",
    "df['power - 2h'] = df['power'].shift(2).dropna()\n",
    "df['power - 3h'] = df['power'].shift(3).dropna()\n",
    "#df['power - 2h'] = df['power'].shift(2).dropna()\n",
    "#df['previous_power - 3'] = df['power'].shift(3).dropna()\n",
    "#df['previous_power - 4'] = df['power'].shift(4).dropna()\n",
    "\n",
    "#Add teros time series\n",
    "df['EC - 1h'] = df['EC'].shift(1).dropna()\n",
    "df['EC - 2h'] = df['EC'].shift(2).dropna()\n",
    "df['EC - 3h'] = df['EC'].shift(3).dropna()\n",
    "\n",
    "df['temp - 1h'] = df['temp'].shift(1).dropna()\n",
    "df['temp - 2h'] = df['temp'].shift(2).dropna()\n",
    "df['temp - 3h'] = df['temp'].shift(3).dropna()\n",
    "\n",
    "df['raw_VWC - 1h'] = df['raw_VWC'].shift(1).dropna()\n",
    "df['raw_VWC - 2h'] = df['raw_VWC'].shift(2).dropna()\n",
    "df['raw_VWC - 3h'] = df['raw_VWC'].shift(3).dropna()\n",
    "\n",
    "#Add voltage and current time series\n",
    "df['V1 - 1h'] = df['V1 [10nV]'].shift(1).dropna()\n",
    "df['V1 - 2h'] = df['V1 [10nV]'].shift(2).dropna()\n",
    "df['V1 - 3h'] = df['V1 [10nV]'].shift(3).dropna()\n",
    "\n",
    "df['I1L - 1h'] = df['I1L [10pA]'].shift(1).dropna()\n",
    "df['I1L - 2h'] = df['I1L [10pA]'].shift(2).dropna()\n",
    "df['I1L - 3h'] = df['I1L [10pA]'].shift(3).dropna()\n",
    "\n",
    "df['I1H - 1h'] = df['I1H [nA]'].shift(1).dropna()\n",
    "df['I1H - 2h'] = df['I1H [nA]'].shift(2).dropna()\n",
    "df['I1H - 3h'] = df['I1H [nA]'].shift(3).dropna()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "u4SM1_EvGS6y"
   },
   "outputs": [],
   "source": [
    "#df = df.rename(columns={'power': 'power [μW]'})\n",
    "df = df.rename(columns={'I1L [10pA]': 'Current (uA)', 'V1 [10nV]' : 'Voltage (mV)', 'power' : 'Power (uW)'})\n",
    "df = df.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSkfrkLBZ42n"
   },
   "source": [
    "## Specify Device so we can use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "M8NhMTInXpyS"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.9\n",
    "\n",
    "# old design network\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(200, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'))\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(3))\n",
    "# model.compile(loss=quantile_loss, metrics=['mape'], optimizer='adam')\n",
    "\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_steps):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        num_hidden1 = 200\n",
    "\n",
    "        # layer 1\n",
    "        self.slstm1 = snn.SLSTM(num_inputs, num_hidden1, threshold = 0.25)\n",
    "\n",
    "        # layer 2\n",
    "        self.fc1 = torch.nn.Linear(in_features=num_hidden1, out_features=100)\n",
    "        self.lif1 = snn.Leaky(beta=beta, threshold = 0.5)\n",
    "\n",
    "        # randomly initialize decay rate for output neuron\n",
    "        beta_out = random.uniform(0.5, 1)\n",
    "\n",
    "        # layer 2\n",
    "        self.fc2 = torch.nn.Linear(in_features=100, out_features=3)\n",
    "        self.lif2 = snn.Leaky(beta=beta_out, learn_beta=True, reset_mechanism=\"none\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states and outputs at t=0\n",
    "        syn1, mem1 = self.slstm1.reset_mem()\n",
    "        mem2 = self.lif1.reset_mem()\n",
    "        mem3 = self.lif2.reset_mem()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk1_rec = []\n",
    "        spk2_rec = []\n",
    "        spk3_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            spk1, syn1, mem1 = self.slstm1(x.flatten(1), syn1, mem1)\n",
    "            spk2, mem2 = self.lif1(self.fc1(spk1), mem2)\n",
    "            spk3, mem3 = self.lif2(self.fc2(spk2), mem3)\n",
    "\n",
    "            # Append the Spike and Membrane History\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "            spk3_rec.append(spk3)\n",
    "            mem_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spk1_rec), torch.stack(spk2_rec), torch.stack(spk3_rec), torch.stack(mem_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cbZHNDVyAzb"
   },
   "source": [
    "# Loading Teacher Predictions from X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_models/snn_3min_quant50.pth\n",
      "3min\n",
      "Voltage overestimation rate for 3min: 85.231%\n",
      "Test MAPE power (3min): 0.537724\n",
      "Test MAPE voltage (3min): 0.290864\n",
      "Test MAPE current (3min): 0.316784\n",
      "Predicted vs. Actual Total Voltage Percent Difference (3min): 21.857%\n",
      "trained_models/snn_5min_quant50.pth\n",
      "5min\n",
      "Voltage overestimation rate for 5min: 73.179%\n",
      "Test MAPE power (5min): 0.394733\n",
      "Test MAPE voltage (5min): 0.214422\n",
      "Test MAPE current (5min): 0.241269\n",
      "Predicted vs. Actual Total Voltage Percent Difference (5min): 12.812%\n",
      "trained_models/snn_15min_quant50.pth\n",
      "15min\n",
      "Voltage overestimation rate for 15min: 80.689%\n",
      "Test MAPE power (15min): 0.328628\n",
      "Test MAPE voltage (15min): 0.257437\n",
      "Test MAPE current (15min): 0.227218\n",
      "Predicted vs. Actual Total Voltage Percent Difference (15min): 17.903%\n",
      "trained_models/snn_30min_quant50.pth\n",
      "30min\n",
      "Voltage overestimation rate for 30min: 82.230%\n",
      "Test MAPE power (30min): 0.317190\n",
      "Test MAPE voltage (30min): 0.264197\n",
      "Test MAPE current (30min): 0.169668\n",
      "Predicted vs. Actual Total Voltage Percent Difference (30min): 18.920%\n",
      "trained_models/snn_60min_quant50.pth\n",
      "60min\n",
      "Voltage overestimation rate for 60min: 86.775%\n",
      "Test MAPE power (60min): 0.404176\n",
      "Test MAPE voltage (60min): 0.210382\n",
      "Test MAPE current (60min): 0.183518\n",
      "Predicted vs. Actual Total Voltage Percent Difference (60min): 16.774%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Set parameters\n",
    "batchsize_list = [300, 150, 50, 20, 8]\n",
    "time_frame_list = ['3min', '5min', '15min', '30min', '60min']\n",
    "time_frame_seconds_list = [180, 300, 900, 1800, 3600]\n",
    "n = 0\n",
    "\n",
    "snn_power_mape_list = []\n",
    "snn_volt_mape_list = []\n",
    "snn_curr_mape_list = []\n",
    "\n",
    "# Dictionary to store mv variables\n",
    "mv_dict = {}\n",
    "\n",
    "for j in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[j]\n",
    "    time_frame = time_frame_list[j]\n",
    "    time_frame_seconds = time_frame_seconds_list[j]\n",
    "\n",
    "    X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], \n",
    "                   df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], \n",
    "                   df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"], \n",
    "                   df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"], \n",
    "                   df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"], \n",
    "                   df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"], \n",
    "                   df[\"tsd\"], df[\"hour\"]], axis=1)\n",
    "    y = pd.concat([df[\"Power (uW)\"], df['Voltage (mV)'], df['Current (uA)']], axis=1)\n",
    "\n",
    "    # Normalize Data\n",
    "    X_normalized = ((X - X.min()) / (X.max() - X.min()))\n",
    "\n",
    "    # Split train and test sets\n",
    "    X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
    "    y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
    "\n",
    "    X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "    y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "    # Resample data\n",
    "\n",
    "    X_train = X_train.resample(time_frame).mean().dropna()\n",
    "    y_train = y_train.resample(time_frame).mean().dropna()\n",
    "\n",
    "    X_valid = X_valid.resample(time_frame).mean().dropna()\n",
    "    y_valid = y_valid.resample(time_frame).mean().dropna()\n",
    "\n",
    "    X_test = X_test.resample(time_frame).mean().dropna()\n",
    "    y_test = y_test.resample(time_frame).mean().dropna()\n",
    "\n",
    "    # Define mv variable for the current time frame\n",
    "    mv_dict[time_frame] = y_test\n",
    "\n",
    "    # Reshape data\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
    "    X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "    # Convert to tensor\n",
    "    X_train = torch.tensor(X_train)\n",
    "    y_train = torch.tensor(y_train.values)\n",
    "    X_valid = torch.tensor(X_valid)\n",
    "    y_valid = torch.tensor(y_valid.values)\n",
    "    X_test = torch.tensor(X_test)\n",
    "    y_test = torch.tensor(y_test.values)\n",
    "\n",
    "    # Make datasets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=False)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batchsize, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
    "\n",
    "    num_steps = 50\n",
    "    num_inputs = X_train.shape[2]\n",
    "\n",
    "    # Create new instance of the SNN Class\n",
    "    model = Net(num_inputs, num_steps).to(device)\n",
    "\n",
    "    file = 'trained_models/snn_' + time_frame + '_quant50.pth'\n",
    "    print(file)\n",
    "\n",
    "    checkpoint = torch.load(file, map_location=torch.device('cpu'), weights_only=True)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            # Prepare data\n",
    "            data = data.to(device).float()\n",
    "            targets = targets.to(device).float()\n",
    "\n",
    "            _, _, _, output = model(data)\n",
    "\n",
    "            output = output.cpu().squeeze(1).detach()\n",
    "            actuals.append(targets)\n",
    "            predictions.append(output[-1])\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    actuals = torch.cat(actuals, dim=0)\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "\n",
    "    mv = mv_dict[time_frame]\n",
    "    print(time_frame)\n",
    "    mv[\"power_pred_med_\" + time_frame] = predictions[:, 0].numpy()\n",
    "    mv[\"voltage_pred_med_\" + time_frame] = predictions[:, 1].numpy()\n",
    "    mv[\"current_pred_med_\" + time_frame] = predictions[:, 2].numpy()\n",
    "\n",
    "    print(f'Voltage overestimation rate for {time_frame}: %.3f%%' % (\n",
    "        (mv['Voltage (mV)'].values <= mv[\"voltage_pred_med_\" + time_frame]).mean() * 100))\n",
    "    print(f\"Test MAPE power ({time_frame}): %3f\" % MAPE(mv['Power (uW)'].values.ravel(), mv[\"power_pred_med_\" + time_frame]))\n",
    "    print(f\"Test MAPE voltage ({time_frame}): %3f\" % MAPE(mv['Voltage (mV)'], mv[\"voltage_pred_med_\" + time_frame]))\n",
    "    print(f\"Test MAPE current ({time_frame}): %3f\" % MAPE(mv['Current (uA)'], mv[\"current_pred_med_\" + time_frame]))\n",
    "\n",
    "\n",
    "    V_actual = mv['Voltage (mV)'].mean()\n",
    "    V_pred = mv[\"voltage_pred_med_\" + time_frame].mean()\n",
    "    print(f'Predicted vs. Actual Total Voltage Percent Difference ({time_frame}): %.3f%%' % (\n",
    "        (V_pred - V_actual) * 100 / V_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Power (uW)', 'Voltage (mV)', 'Current (uA)', 'power_pred_med_15min',\n",
      "       'voltage_pred_med_15min', 'current_pred_med_15min'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(mv_dict['15min'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have populated *mv_dict* with the predictions for power, voltage, and current at each tiem step 3min, 5min, 15min, 30min, and 60min. For this implementation, we focused on using the 5min predictions to predict for 15mins. \n",
    "\n",
    "In this next section, we use *mv_dict['5min']* as the teacher prediction to assist the forecasting for 15mins. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMZgk8NYuEaQ"
   },
   "source": [
    "# Student Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(y_true, y_pred, quantile=0.5):\n",
    "    error = y_true - y_pred\n",
    "    loss = torch.mean(torch.max(quantile * error, (quantile - 1) * error))\n",
    "    return loss\n",
    "\n",
    "def distillation_loss(y_teacher, y_pred):\n",
    "  error = y_teacher - y_pred\n",
    "  loss = torch.mean((error) ** 2)\n",
    "  return loss\n",
    "\n",
    "def combined_loss(y_true, y_pred, y_teacher, quantile=0.5, alpha=1):\n",
    "  if alpha > 1 or alpha < 0:\n",
    "    raise ValueError(\"alpha must be between 0 and 1\")\n",
    "  return ((alpha) * quantile_loss(y_true, y_pred, quantile)) + ((1-alpha) * distillation_loss(y_teacher, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDBlmqCZuEaR"
   },
   "source": [
    "## 2. Preparing Student Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Change the values to alter the Student-Teacher forecasting relationship "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid teacher timeframes: 3min, 5min, 10min, 15min, 30min, 60min\n",
    "valid_timeframes = ['3min', '5min', '10min', '15min', '30min', '60min']\n",
    "teacher_timeframe = valid_timeframes[1]\n",
    "#valid student timeframes: 3min, 5min, 10min, 15min, 30min, 60min (student timeframe must be <= teacher timeframe)\n",
    "student_timeframe = valid_timeframes[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated X Shape: Rows 2576 Columns 1\n",
      "X_train shape:  torch.Size([2576, 1, 20])\n",
      "y_train shape:  torch.Size([2576, 6])\n"
     ]
    }
   ],
   "source": [
    "teacher_5min_preds = mv_dict[teacher_timeframe]\n",
    "teacher_5min_preds_df = pd.DataFrame(\n",
    "    teacher_5min_preds,\n",
    "    columns=[\n",
    "        \"power_pred_med_\" + teacher_timeframe,\n",
    "        \"voltage_pred_med_\" + teacher_timeframe,\n",
    "        \"current_pred_med_\" + teacher_timeframe\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_normalized = ((X - X.min()) / (X.max() - X.min()))\n",
    "\n",
    "# Using X, y from Teacher model\n",
    "X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
    "y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Split the testing set into validation and final test sets (50/50)\n",
    "X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Resample data to 15-minute intervals\n",
    "X_train_final = X_train.resample(student_timeframe).mean().dropna()\n",
    "y_train_final = y_train.resample(student_timeframe).mean().dropna()\n",
    "X_valid_final = X_valid.resample(student_timeframe).mean().dropna()\n",
    "y_valid_final = y_valid.resample(student_timeframe).mean().dropna()\n",
    "X_test_final = X_test.resample(student_timeframe).mean().dropna()\n",
    "y_test_final = y_test.resample(student_timeframe).mean().dropna()\n",
    "\n",
    "\n",
    "# Align predictions to data\n",
    "# Filter 5-minute predictions to keep those 5 minutes before 15-minute intervals\n",
    "valid_timestamps = y_train_final.index\n",
    "\n",
    "teacher_5min_preds_df = teacher_5min_preds_df.loc[teacher_5min_preds_df.index.isin(valid_timestamps - pd.Timedelta(minutes=5))]\n",
    "# Reassign the prediction timestamps to align with the current data timestamp\n",
    "teacher_5min_preds_df.index = teacher_5min_preds_df.index + pd.Timedelta(minutes=5)\n",
    "\n",
    "\n",
    "# X_train_student = pd.concat([X_train, teacher_5min_preds_df], axis=1)\n",
    "y_train_final = pd.concat([y_train_final, teacher_5min_preds_df], axis=1)\n",
    "\n",
    "# Reshape data for LSTM input\n",
    "X_train_final = X_train_final.values.reshape((X_train_final.shape[0], 1, X_train_final.shape[1]))\n",
    "X_valid_final = X_valid_final.values.reshape((X_valid_final.shape[0], 1, X_valid_final.shape[1]))\n",
    "X_test_final = X_test_final.values.reshape((X_test_final.shape[0], 1, X_test_final.shape[1]))\n",
    "\n",
    "\n",
    "X_train_final = torch.tensor(X_train_final).float()\n",
    "y_train_final = torch.tensor(y_train_final.values).float()\n",
    "X_valid_final = torch.tensor(X_valid_final).float()\n",
    "y_valid_final = torch.tensor(y_valid_final.values).float()\n",
    "X_test_final = torch.tensor(X_test_final).float()\n",
    "y_test_final = torch.tensor(y_test_final.values).float()\n",
    "\n",
    "\n",
    "# Clean X_train_final tensor\n",
    "nan_mask = torch.isnan(y_train_final)  # Identify NaN values\n",
    "non_nan_values = y_train_final[~nan_mask]  # Filter out NaN values for mean calculation\n",
    "mean_value = torch.mean(non_nan_values)  # Compute mean of valid values\n",
    "\n",
    "# Replace NaNs with the computed mean\n",
    "tensor_cleaned = y_train_final.clone()\n",
    "tensor_cleaned[nan_mask] = mean_value\n",
    "\n",
    "# Reassign cleaned tensor back if desired\n",
    "y_train_final = tensor_cleaned\n",
    "\n",
    "print(\"Updated X Shape: Rows \" + str(X_train_final.shape[0]) + \" Columns \" + str(X_train_final.shape[1]))\n",
    "print(\"X_train shape: \", X_train_final.shape)\n",
    "print(\"y_train shape: \", y_train_final.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Training on Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JDO5w2qMuEaR",
    "outputId": "60478509-388c-4cd5-a6ee-b899f94de651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing alpha sweep on student model\n",
      "Epoch 1! Avg loss for the last 100 iterations: 3822594.6175\n",
      "Epoch 2! Avg loss for the last 100 iterations: 3643008.33\n",
      "Epoch 3! Avg loss for the last 100 iterations: 3472663.0225\n",
      "Epoch 4! Avg loss for the last 100 iterations: 3309446.2425\n",
      "Epoch 5! Avg loss for the last 100 iterations: 3152562.255\n",
      "Epoch 6! Avg loss for the last 100 iterations: 3001566.7075\n",
      "Epoch 7! Avg loss for the last 100 iterations: 2856154.6025\n",
      "Epoch 8! Avg loss for the last 100 iterations: 2716089.975\n",
      "Epoch 9! Avg loss for the last 100 iterations: 2581175.1575\n",
      "Alpha =  0.0\n",
      "Test MAPE power Using Teacher-Student Model: 0.745\n",
      "Test MAPE voltage Using Teacher-Student Model: 0.601\n",
      "Test MAPE current Using Teacher-Student Model: 0.805\n",
      "Epoch 1! Avg loss for the last 100 iterations: 3434380.1325\n",
      "Epoch 2! Avg loss for the last 100 iterations: 3267247.9025\n",
      "Epoch 3! Avg loss for the last 100 iterations: 3108952.5475\n",
      "Epoch 4! Avg loss for the last 100 iterations: 2957491.8025\n",
      "Epoch 5! Avg loss for the last 100 iterations: 2812109.1\n",
      "Epoch 6! Avg loss for the last 100 iterations: 2672379.3625\n",
      "Epoch 7! Avg loss for the last 100 iterations: 2538009.0425\n",
      "Epoch 8! Avg loss for the last 100 iterations: 2408769.4125\n",
      "Epoch 9! Avg loss for the last 100 iterations: 2284468.3825\n",
      "Alpha =  0.1\n",
      "Test MAPE power Using Teacher-Student Model: 0.804\n",
      "Test MAPE voltage Using Teacher-Student Model: 0.588\n",
      "Test MAPE current Using Teacher-Student Model: 0.798\n",
      "Epoch 1! Avg loss for the last 100 iterations: 3070109.11\n",
      "Epoch 2! Avg loss for the last 100 iterations: 2933109.155\n",
      "Epoch 3! Avg loss for the last 100 iterations: 2803158.295\n",
      "Epoch 4! Avg loss for the last 100 iterations: 2678487.69\n",
      "Epoch 5! Avg loss for the last 100 iterations: 2558457.4775\n",
      "Epoch 6! Avg loss for the last 100 iterations: 2442721.2825\n",
      "Epoch 7! Avg loss for the last 100 iterations: 2331047.365\n",
      "Epoch 8! Avg loss for the last 100 iterations: 2223259.9675\n",
      "Epoch 9! Avg loss for the last 100 iterations: 2119214.32625\n",
      "Alpha =  0.2\n",
      "Test MAPE power Using Teacher-Student Model: 0.655\n",
      "Test MAPE voltage Using Teacher-Student Model: 0.621\n",
      "Test MAPE current Using Teacher-Student Model: 0.815\n",
      "Epoch 1! Avg loss for the last 100 iterations: 2709664.815\n",
      "Epoch 2! Avg loss for the last 100 iterations: 2607325.97\n",
      "Epoch 3! Avg loss for the last 100 iterations: 2509586.2\n",
      "Epoch 4! Avg loss for the last 100 iterations: 2415269.4175\n",
      "Epoch 5! Avg loss for the last 100 iterations: 2323949.945\n",
      "Epoch 6! Avg loss for the last 100 iterations: 2235401.2525\n",
      "Epoch 7! Avg loss for the last 100 iterations: 2149475.1375\n",
      "Epoch 8! Avg loss for the last 100 iterations: 2066061.68125\n",
      "Epoch 9! Avg loss for the last 100 iterations: 1985071.93\n",
      "Alpha =  0.30000000000000004\n",
      "Test MAPE power Using Teacher-Student Model: 0.425\n",
      "Test MAPE voltage Using Teacher-Student Model: 0.676\n",
      "Test MAPE current Using Teacher-Student Model: 0.841\n",
      "Epoch 1! Avg loss for the last 100 iterations: 2299871.0525\n",
      "Epoch 2! Avg loss for the last 100 iterations: 2195728.135\n",
      "Epoch 3! Avg loss for the last 100 iterations: 2096805.94125\n",
      "Epoch 4! Avg loss for the last 100 iterations: 2001893.56125\n",
      "Epoch 5! Avg loss for the last 100 iterations: 1910537.8275\n",
      "Epoch 6! Avg loss for the last 100 iterations: 1822487.90375\n",
      "Epoch 7! Avg loss for the last 100 iterations: 1737573.0825\n",
      "Epoch 8! Avg loss for the last 100 iterations: 1655661.54625\n",
      "Epoch 9! Avg loss for the last 100 iterations: 1576643.62875\n",
      "Alpha =  0.4\n",
      "Test MAPE power Using Teacher-Student Model: 0.688\n",
      "Test MAPE voltage Using Teacher-Student Model: 0.615\n",
      "Test MAPE current Using Teacher-Student Model: 0.812\n",
      "Epoch 1! Avg loss for the last 100 iterations: 1938370.865\n",
      "Epoch 2! Avg loss for the last 100 iterations: 1867028.68125\n",
      "Epoch 3! Avg loss for the last 100 iterations: 1798752.54625\n",
      "Epoch 4! Avg loss for the last 100 iterations: 1732785.005\n",
      "Epoch 5! Avg loss for the last 100 iterations: 1668849.88\n",
      "Epoch 6! Avg loss for the last 100 iterations: 1606799.2525\n",
      "Epoch 7! Avg loss for the last 100 iterations: 1546535.87875\n",
      "Epoch 8! Avg loss for the last 100 iterations: 1487987.2925\n",
      "Epoch 9! Avg loss for the last 100 iterations: 1431094.68625\n",
      "Alpha =  0.5\n",
      "Test MAPE power Using Teacher-Student Model: 0.398\n",
      "Test MAPE voltage Using Teacher-Student Model: 0.683\n",
      "Test MAPE current Using Teacher-Student Model: 0.845\n",
      "Epoch 1! Avg loss for the last 100 iterations: 1537034.11125\n",
      "Epoch 2! Avg loss for the last 100 iterations: 1470005.22\n",
      "Epoch 3! Avg loss for the last 100 iterations: 1406259.55125\n",
      "Epoch 4! Avg loss for the last 100 iterations: 1345019.51875\n",
      "Epoch 5! Avg loss for the last 100 iterations: 1285996.39125\n",
      "Epoch 6! Avg loss for the last 100 iterations: 1229031.89\n",
      "Epoch 7! Avg loss for the last 100 iterations: 1174019.45625\n",
      "Epoch 8! Avg loss for the last 100 iterations: 1120877.36375\n",
      "Epoch 9! Avg loss for the last 100 iterations: 1069537.649375\n",
      "Alpha =  0.6000000000000001\n",
      "Test MAPE power Using Teacher-Student Model: 0.629\n",
      "Test MAPE voltage Using Teacher-Student Model: 0.628\n",
      "Test MAPE current Using Teacher-Student Model: 0.819\n",
      "Epoch 1! Avg loss for the last 100 iterations: 1167252.225\n",
      "Epoch 2! Avg loss for the last 100 iterations: 1127177.1325\n",
      "Epoch 3! Avg loss for the last 100 iterations: 1088771.60125\n",
      "Epoch 4! Avg loss for the last 100 iterations: 1051601.444375\n",
      "Epoch 5! Avg loss for the last 100 iterations: 1015510.516875\n",
      "Epoch 6! Avg loss for the last 100 iterations: 980416.611875\n",
      "Epoch 7! Avg loss for the last 100 iterations: 946266.64625\n",
      "Epoch 8! Avg loss for the last 100 iterations: 913021.776875\n",
      "Epoch 9! Avg loss for the last 100 iterations: 880651.03125\n",
      "Alpha =  0.7000000000000001\n",
      "Test MAPE power Using Teacher-Student Model: 0.322\n",
      "Test MAPE voltage Using Teacher-Student Model: 0.704\n",
      "Test MAPE current Using Teacher-Student Model: 0.855\n",
      "Epoch 1! Avg loss for the last 100 iterations: 777080.730625\n",
      "Epoch 2! Avg loss for the last 100 iterations: 749076.671875\n",
      "Epoch 3! Avg loss for the last 100 iterations: 722287.701875\n",
      "Epoch 4! Avg loss for the last 100 iterations: 696399.43125\n",
      "Epoch 5! Avg loss for the last 100 iterations: 671298.580625\n",
      "Epoch 6! Avg loss for the last 100 iterations: 646925.264375\n",
      "Epoch 7! Avg loss for the last 100 iterations: 623240.745625\n",
      "Epoch 8! Avg loss for the last 100 iterations: 600216.636875\n",
      "Epoch 9! Avg loss for the last 100 iterations: 577830.07375\n",
      "Alpha =  0.8\n",
      "Test MAPE power Using Teacher-Student Model: 0.368\n",
      "Test MAPE voltage Using Teacher-Student Model: 0.690\n",
      "Test MAPE current Using Teacher-Student Model: 0.848\n",
      "Epoch 1! Avg loss for the last 100 iterations: 381421.2865625\n",
      "Epoch 2! Avg loss for the last 100 iterations: 362273.3740625\n",
      "Epoch 3! Avg loss for the last 100 iterations: 344144.04625\n",
      "Epoch 4! Avg loss for the last 100 iterations: 326813.5778125\n",
      "Epoch 5! Avg loss for the last 100 iterations: 310197.758125\n",
      "Epoch 6! Avg loss for the last 100 iterations: 294249.2446875\n",
      "Epoch 7! Avg loss for the last 100 iterations: 278934.3953125\n",
      "Epoch 8! Avg loss for the last 100 iterations: 264225.91390625\n",
      "Epoch 9! Avg loss for the last 100 iterations: 250101.17421875\n",
      "Alpha =  0.9\n",
      "Test MAPE power Using Teacher-Student Model: 0.858\n",
      "Test MAPE voltage Using Teacher-Student Model: 0.575\n",
      "Test MAPE current Using Teacher-Student Model: 0.792\n",
      "Epoch 1! Avg loss for the last 100 iterations: 926.27326171875\n",
      "Epoch 2! Avg loss for the last 100 iterations: 904.3412078857422\n",
      "Epoch 3! Avg loss for the last 100 iterations: 882.9003240966797\n",
      "Epoch 4! Avg loss for the last 100 iterations: 861.8451516723633\n",
      "Epoch 5! Avg loss for the last 100 iterations: 841.2770242309571\n",
      "Epoch 6! Avg loss for the last 100 iterations: 821.6495886230468\n",
      "Epoch 7! Avg loss for the last 100 iterations: 803.5562310791015\n",
      "Epoch 8! Avg loss for the last 100 iterations: 786.7850030517578\n",
      "Epoch 9! Avg loss for the last 100 iterations: 770.2560479736328\n",
      "Alpha =  1.0\n",
      "Test MAPE power Using Teacher-Student Model: 0.491\n",
      "Test MAPE voltage Using Teacher-Student Model: 0.607\n",
      "Test MAPE current Using Teacher-Student Model: 0.808\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing alpha sweep on student model\")\n",
    "for i in np.arange(0, 1.1, 0.1):\n",
    "    alpha = i\n",
    "    power_mape = []\n",
    "    voltage_mape = []\n",
    "    current_mape = []\n",
    "\n",
    "    E_actual_list = []\n",
    "    E_pred_list = []\n",
    "\n",
    "    max_act_list = []\n",
    "    pred_act_list = []\n",
    "    succ_act_list = []\n",
    "\n",
    "    pred_act_naive_list = []\n",
    "    false_act_naive_list = []\n",
    "    succ_act_naive_list = []\n",
    "\n",
    "    # initialize histories\n",
    "    loss_hist = []\n",
    "    avg_loss_hist = []\n",
    "    acc_hist = []\n",
    "    mape_hist = []\n",
    "\n",
    "\n",
    "    # Set parameters\n",
    "    batch_size = 32  # Adjust as needed\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-2\n",
    "    beta = 0.8  # For quantile loss\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset_final = TensorDataset(X_train_final, y_train_final)\n",
    "    valid_dataset_final = TensorDataset(X_valid_final, y_valid_final)\n",
    "    test_dataset_final = TensorDataset(X_test_final, y_test_final)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset_final, batch_size=batch_size, shuffle=False)\n",
    "    valid_loader = DataLoader(valid_dataset_final, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset_final, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define model, optimizer, and loss function\n",
    "    num_steps = 1\n",
    "    num_inputs = X_train_final.shape[2]\n",
    "    output_size = y_train_final.shape[1]\n",
    "    StudentModel = Net(num_inputs, num_steps).to(device)\n",
    "    loss_fn = combined_loss\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=StudentModel.parameters(), lr=learning_rate)\n",
    "\n",
    "    # put model into train mode\n",
    "    StudentModel.train()\n",
    "\n",
    "    # Train Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, targets) in enumerate(iter(train_loader)):\n",
    "            # move to device\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # change to floats\n",
    "            data = data.float()\n",
    "            targets = targets.float()\n",
    "\n",
    "            # run forward pass\n",
    "            _, _, _, mem = StudentModel(data)\n",
    "\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_teacher = targets[:, [3, 4, 5]].squeeze(-1)\n",
    "                y_true = targets[:, [0, 1, 2]].squeeze(-1)\n",
    "            \n",
    "            y_pred = mem[-1]\n",
    "\n",
    "            # Here, alter alpha from 0-1 to change between weighing quantile loss and distillation loss\n",
    "            # Ideally, we would perform an alpha sweep to identify the optimized alpha value\n",
    "            # calculate loss\n",
    "            loss_val = loss_fn(y_true, y_pred, y_teacher, alpha=alpha)\n",
    "\n",
    "            # calculate and store MAPE Loss\n",
    "            mem_numpy = mem.cpu().detach().numpy()\n",
    "            #mem_numpy = mem.detach().numpy()\n",
    "            targets_numpy = y_true.cpu().detach().numpy()\n",
    "            #targets_numpy = targets.detach().numpy()\n",
    "            mape_hist.append(MAPE(mem_numpy[-1], targets_numpy))\n",
    "            power_mape.append(MAPE(mem_numpy[-1][:,0], targets_numpy[:,0]))\n",
    "            voltage_mape.append(MAPE(mem_numpy[-1][:,1], targets_numpy[:,1]))\n",
    "            current_mape.append(MAPE(mem_numpy[-1][:,2], targets_numpy[:,2]))\n",
    "\n",
    "            # Gradient calculation + weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store loss history for future plotting\n",
    "            loss_hist.append(loss_val.item())\n",
    "\n",
    "            if len(loss_hist) > 100:\n",
    "                avg_loss_hist.append(sum(loss_hist[-100:])/len(loss_hist[-100:]))\n",
    "            else:\n",
    "                avg_loss_hist.append(0)\n",
    "\n",
    "        if len(loss_hist) > 100:\n",
    "            print(f'Epoch {epoch}! Avg loss for the last 100 iterations: {avg_loss_hist[-1]}')\n",
    "\n",
    "    # Define model, optimizer, and loss function\n",
    "    num_steps = 1 \n",
    "    num_inputs = X_train_final.shape[2]\n",
    "    output_size = y_train_final.shape[1]\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Now let's test the model on the test set\n",
    "    StudentModel.eval()  # Set model to evaluation mode\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = StudentModel(data)\n",
    "            y_pred = outputs[-1]\n",
    "            y_pred = y_pred.permute(1, 0, 2)  # Reorganize to match ground truth shape\n",
    "\n",
    "            # Collect actual and predicted values\n",
    "            actuals.append(targets.cpu().numpy()) \n",
    "            predictions.append(y_pred.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    predictions = predictions.squeeze(1)\n",
    "\n",
    "    # Compute MAPE for each output (e.g., power, voltage, current)\n",
    "    mape_power = MAPE(actuals[:, 0], predictions[:, 0])\n",
    "    mape_voltage = MAPE(actuals[:, 1], predictions[:, 1])\n",
    "    mape_current = MAPE(actuals[:, 2], predictions[:, 2])\n",
    "\n",
    "    # Print results\n",
    "    print(\"Alpha = \", alpha)\n",
    "    print(f\"Test MAPE power Using Teacher-Student Model: {mape_power:.3f}\")\n",
    "    print(f\"Test MAPE voltage Using Teacher-Student Model: {mape_voltage:.3f}\")\n",
    "    print(f\"Test MAPE current Using Teacher-Student Model: {mape_current:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAPE power Using Teacher-Student Model: 0.491\n",
      "Test MAPE voltage Using Teacher-Student Model: 0.607\n",
      "Test MAPE current Using Teacher-Student Model: 0.808\n"
     ]
    }
   ],
   "source": [
    "# Define model, optimizer, and loss function\n",
    "num_steps = 1  # Since you're using LSTM for time series data with one step\n",
    "num_inputs = X_train_final.shape[2]\n",
    "output_size = y_train_final.shape[1]\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Assuming the model has already been trained\n",
    "# Now let's test the model on the test set\n",
    "\n",
    "StudentModel.eval()\n",
    "actuals = []\n",
    "predictions = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = StudentModel(data)\n",
    "        y_pred = outputs[-1]\n",
    "        y_pred = y_pred.permute(1, 0, 2)  # Reorganize to match ground truth shape\n",
    "\n",
    "        # Collect actual and predicted values\n",
    "        actuals.append(targets.cpu().numpy()) \n",
    "        predictions.append(y_pred.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "actuals = np.concatenate(actuals, axis=0)\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "predictions = predictions.squeeze(1)\n",
    "\n",
    "# Compute MAPE for each output (e.g., power, voltage, current)\n",
    "mape_power = MAPE(actuals[:, 0], predictions[:, 0])\n",
    "mape_voltage = MAPE(actuals[:, 1], predictions[:, 1])\n",
    "mape_current = MAPE(actuals[:, 2], predictions[:, 2])\n",
    "\n",
    "# Print results\n",
    "print(f\"Test MAPE power Using Teacher-Student Model: {mape_power:.3f}\")\n",
    "print(f\"Test MAPE voltage Using Teacher-Student Model: {mape_voltage:.3f}\")\n",
    "print(f\"Test MAPE current Using Teacher-Student Model: {mape_current:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding Thoughts\n",
    "As we can see, the improvements made to the forecasting accuracy ie either currently non-existant or negligible. We we're hoping for improvements in the MAPE and percent error between the predicted and actual data, but in this iteration were not able to acheive meaningful improvements. \n",
    "\n",
    "One thing we noticed was that the components of the student model may have been suffering from overfitting, as lowering the number of epochs during the trianing process resulted in a significant improvement in the model accuracy. Training for 100 epochs led to mean absolute percent errors of 5+ in some prediciton categories, whereas training for 10 epochs minimized percent error in all categories.\n",
    "\n",
    "### Kai added postnote 12/3/2024 22:45 PST\n",
    "After some more slight alterations, and messing with alpha sweeps, Student-Teacher time relations, and hyperparamter tuning, I got much better results (not better than the original, but better than before) with regards to the MAPE between student predictions and actuals. I added an alpha sweep from 0 - 1 incrementing by 0.1, and found that each alpha value produced better results for each of power, voltage, and current, so it is difficult to determine which alpha value is most optimal. \n",
    "\n",
    "I can defintely say there is some merit to this approach, and this degradation in percent error could be due to the sporadicity of the given dataset. SMFC energy output is often not constant, and is difficult to predict as the power, voltage, and current can change drastically at any given moment. Studying the Stanford and UCSC datasets, it is not clear that the values follow any obvious pattern, and thus made this project slightly more difficult. Regardless, this has been a great learning experience and I am invested; this project will be seeing more of me this winter break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "cObaSZAexrX-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
