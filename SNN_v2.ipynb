{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jlab-sensing/MFC_Modeling/blob/main/SNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PSKbPI5wRzN"
   },
   "source": [
    "#  SNN Models\n",
    "###  In order to run the code in this notebook, you must download the `ucscMFCDataset` directory and `stanfordMFCDataset.zip`, which expands into the directory `rocket4`, from [Hugging Face](https://huggingface.co/datasets/adunlop621/Soil_MFC/tree/main), and store them in the same directory as this notebook. You can also find several pretrained models in the at this link, with the naming conventions described in the [README](https://github.com/jlab-sensing/MFC_Modeling#:~:text=Repository%20files%20navigation-,README,-MFC_Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "zcF-Dv23N_ON",
    "outputId": "923b5bd8-f434-4946-b038-e908b048a14b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hepml in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (0.0.12)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (1.3.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (1.26.4)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (0.13.2)\n",
      "Requirement already satisfied: black in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (24.10.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (4.66.4)\n",
      "Requirement already satisfied: wget in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (3.2)\n",
      "Requirement already satisfied: nbdev in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (2.3.31)\n",
      "Requirement already satisfied: sklearn-pandas in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (2.2.0)\n",
      "Requirement already satisfied: graphviz in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (0.20.3)\n",
      "Requirement already satisfied: gdown in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (5.2.0)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (17.0.0)\n",
      "Requirement already satisfied: numba in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (0.60.0)\n",
      "Requirement already satisfied: Cython in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (3.0.11)\n",
      "Requirement already satisfied: fastprogress in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (1.0.3)\n",
      "Requirement already satisfied: giotto-tda in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (0.6.2)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from hepml) (10.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from black->hepml) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from black->hepml) (1.0.0)\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from black->hepml) (24.1)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from black->hepml) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from black->hepml) (4.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from gdown->hepml) (4.12.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from gdown->hepml) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from gdown->hepml) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (1.13.1)\n",
      "Requirement already satisfied: joblib>=0.16.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (1.4.2)\n",
      "Requirement already satisfied: giotto-ph>=0.2.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (0.2.4)\n",
      "Requirement already satisfied: pyflagser>=0.4.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (0.4.7)\n",
      "Requirement already satisfied: igraph>=0.9.8 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (0.11.6)\n",
      "Requirement already satisfied: plotly>=4.8.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (5.24.1)\n",
      "Requirement already satisfied: ipywidgets>=7.5.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from giotto-tda->hepml) (8.1.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from scikit-learn->hepml) (3.5.0)\n",
      "Requirement already satisfied: fastcore>=1.5.27 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (1.7.19)\n",
      "Requirement already satisfied: execnb>=0.1.4 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (0.1.6)\n",
      "Requirement already satisfied: astunparse in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (1.6.3)\n",
      "Requirement already satisfied: ghapi>=1.0.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (1.0.6)\n",
      "Requirement already satisfied: watchdog in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (5.0.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (2.4.1)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nbdev->hepml) (6.0.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from numba->hepml) (0.43.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas->hepml) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas->hepml) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas->hepml) (2024.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from seaborn->hepml) (3.9.0)\n",
      "Requirement already satisfied: ipython in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from execnb>=0.1.4->nbdev->hepml) (8.29.0)\n",
      "Requirement already satisfied: texttable>=1.6.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from igraph>=0.9.8->giotto-tda->hepml) (1.7.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (3.0.13)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (3.1.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from plotly>=4.8.2->giotto-tda->hepml) (9.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->hepml) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from astunparse->nbdev->hepml) (0.43.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from beautifulsoup4->gdown->hepml) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests[socks]->gdown->hepml) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests[socks]->gdown->hepml) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests[socks]->gdown->hepml) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests[socks]->gdown->hepml) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests[socks]->gdown->hepml) (1.7.1)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from jedi>=0.16->ipython->execnb>=0.1.4->nbdev->hepml) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pexpect>4.3->ipython->execnb>=0.1.4->nbdev->hepml) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->execnb>=0.1.4->nbdev->hepml) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from stack-data->ipython->execnb>=0.1.4->nbdev->hepml) (2.1.0)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from stack-data->ipython->execnb>=0.1.4->nbdev->hepml) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: arrow in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from arrow) (2.9.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from arrow) (2.9.0.20241003)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.7.0->arrow) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: keras_lr_finder in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (0.1)\n",
      "Requirement already satisfied: keras>=2.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras_lr_finder) (3.3.3)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras_lr_finder) (3.9.0)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (0.0.8)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (3.11.0)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from keras>=2.0.0->keras_lr_finder) (0.3.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from matplotlib->keras_lr_finder) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->keras_lr_finder) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from optree->keras>=2.0.0->keras_lr_finder) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from rich->keras>=2.0.0->keras_lr_finder) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from rich->keras>=2.0.0->keras_lr_finder) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=2.0.0->keras_lr_finder) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade hepml\n",
    "%pip install arrow\n",
    "%pip install keras_lr_finder\n",
    "%pip install pandas\n",
    "%pip install snntorch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LJsRk5_qTljC"
   },
   "outputs": [],
   "source": [
    "# reload modules before executing user code\n",
    "#%load_ext autoreload\n",
    "# reload all modules every time before executing Python code\n",
    "#%autoreload 2\n",
    "# render plots in notebook\n",
    "\n",
    "# Misc imports\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from hepml.core import plot_regression_tree\n",
    "sns.set(color_codes=True)\n",
    "sns.set_palette(sns.color_palette(\"muted\"))\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# snnTorch imports\n",
    "import snntorch as snn\n",
    "from snntorch import functional as SF\n",
    "import snntorch.spikeplot as splt\n",
    "\n",
    "# # keras imports\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import LSTM\n",
    "# from keras import backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N35aqIRugIHl"
   },
   "source": [
    "##  Load and Format Dataset 1\n",
    "\n",
    "### Remember to download `stanfordMFCDataset.zip`, which expands into the directory `rocket4`, from [Hugging Face](https://huggingface.co/datasets/adunlop621/Soil_MFC/tree/main), and store it in the same directory as this notebook before executing the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "B7PoS8fwOS-Q"
   },
   "outputs": [],
   "source": [
    "#Load teros data\n",
    "import glob\n",
    "teros_files = glob.glob(\"rocket4/TEROSoutput*.csv\")\n",
    "X = pd.DataFrame()\n",
    "for f in teros_files:\n",
    "  try:\n",
    "    csv = pd.read_csv(f, index_col=False).dropna()\n",
    "    X = pd.concat([X, csv])\n",
    "  except:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zaaHwFRvXtN4"
   },
   "outputs": [],
   "source": [
    "#Load power data\n",
    "power_files = glob.glob(\"rocket4/soil*.csv\")\n",
    "y = pd.DataFrame()\n",
    "for f in sorted(power_files, key=lambda x: int(x.split('.')[0].split('_')[-1])):\n",
    "#in power_files:\n",
    "  try:\n",
    "    csv = pd.read_csv(f, on_bad_lines='skip', skiprows=10).dropna(how='all')\n",
    "    csv = csv.rename({'Unnamed: 0': 'timestamp'}, axis='columns')\n",
    "    y = pd.concat([y,csv])\n",
    "  except:\n",
    "    continue\n",
    "y[\"timestamp\"] = y[\"timestamp\"].round(decimals = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ViS0TjC4n6lk"
   },
   "outputs": [],
   "source": [
    "#Convert current to amps, voltage to volts\n",
    "y[\"I1L [10pA]\"] = np.abs(y[\"I1L [10pA]\"] * 1E-11)\n",
    "y[\"V1 [10nV]\"] = np.abs(y[\"V1 [10nV]\"] * 1E-8)\n",
    "y[\"I1H [nA]\"] = np.abs(y[\"I1H [nA]\"] * 1E-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0aqozsZpSOB",
    "outputId": "641b06d1-6a3b-4fd8-e542-a6859314ffff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1g/jt7x_x9n6j1f3317_dwv3k9m0000gn/T/ipykernel_23191/3455547848.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.timestamp = df.timestamp.dt.tz_localize('UTC').dt.tz_convert('US/Pacific')\n"
     ]
    }
   ],
   "source": [
    "#Sort data by timestamp, convert to datetime\n",
    "X = X.sort_values(['timestamp'])\n",
    "y = y.sort_values(['timestamp'])\n",
    "X['timestamp'] = pd.to_datetime(X['timestamp'], unit='s')\n",
    "y['timestamp'] = pd.to_datetime(y['timestamp'], unit='s')\n",
    "\n",
    "#Merge data by timestamp\n",
    "uncut_df = pd.merge_asof(left=X,right=y,direction='nearest',tolerance=pd.Timedelta('1 sec'), on = 'timestamp').dropna(how='all')\n",
    "\n",
    "#Isolate data from cell0\n",
    "df = uncut_df.loc[uncut_df['sensorID'] == 0]\n",
    "\n",
    "#Localize timestamp\n",
    "df.timestamp = df.timestamp.dt.tz_localize('UTC').dt.tz_convert('US/Pacific')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CBAfOHB61Jwc"
   },
   "outputs": [],
   "source": [
    "#Use only data from after deployment date\n",
    "#df = df.loc[(df['timestamp'] > '2021-09-24') & (df['timestamp'] < '2021-10-15')] #Future of Clean Computing Graph\n",
    "#df = df.loc[(df['timestamp'] > '2021-06-24') & (df['timestamp'] < '2021-07-02')]\n",
    "#df = df.loc[(df['timestamp'] > '2021-06-18')] #Two weeks after deployment\n",
    "df = df.loc[(df['timestamp'] > '2021-06-04')] #Deployment date\n",
    "#df = df.loc[(df['timestamp'] > '2021-06-25') & (df['timestamp'] < '2021-06-26')] #Small training set\n",
    "\n",
    "#Power drop\n",
    "#df = df.loc[(df['timestamp'] > '2021-11-01') & (df['timestamp'] < '2021-11-22')]\n",
    "\n",
    "#Drop data outages\n",
    "df = df.drop(df[(df.timestamp > '2021-11-11') & (df.timestamp < '2021-11-22 01:00:00')].index)\n",
    "df = df.drop(df[(df.timestamp > '2022-01-27')].index)\n",
    "#df = df.set_index('timestamp')\n",
    "df = df[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "p8fFsoLNfrFA"
   },
   "outputs": [],
   "source": [
    "df = df.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9kNxDOBwk7IN"
   },
   "outputs": [],
   "source": [
    "#Get time since deployement\n",
    "df['tsd'] = (df.index - df.index[0]).days\n",
    "df['hour'] = (df.index).hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-Pfyv0fM3te1"
   },
   "outputs": [],
   "source": [
    "#Calculate power\n",
    "df[\"power\"] = np.abs(np.multiply(df.iloc[:, 7], df.iloc[:, 8]))\n",
    "#df[\"power\"] = np.abs(np.multiply(df[\"I1L [10pA]\"], df[\"V1 [10nV]\"]))\n",
    "\n",
    "#Convert to nW\n",
    "df['power'] = df['power']*1E9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jA-WVzVh2-lf"
   },
   "outputs": [],
   "source": [
    "#Convert to 10 nanoamps, 10 microvolts\n",
    "df[\"I1L [10pA]\"] = np.abs(df[\"I1L [10pA]\"] * 1E8)\n",
    "df[\"V1 [10nV]\"] = np.abs(df[\"V1 [10nV]\"] * 1E5)\n",
    "df[\"I1H [nA]\"] = np.abs(df[\"I1H [nA]\"] * 1E8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XxAlm4FXh57m"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8sHtXZCVrsQJ"
   },
   "outputs": [],
   "source": [
    "#Add power time series\n",
    "df['power - 1h'] = df['power'].shift(1).dropna()\n",
    "df['power - 2h'] = df['power'].shift(2).dropna()\n",
    "df['power - 3h'] = df['power'].shift(3).dropna()\n",
    "#df['power - 2h'] = df['power'].shift(2).dropna()\n",
    "#df['previous_power - 3'] = df['power'].shift(3).dropna()\n",
    "#df['previous_power - 4'] = df['power'].shift(4).dropna()\n",
    "\n",
    "#Add teros time series\n",
    "df['EC - 1h'] = df['EC'].shift(1).dropna()\n",
    "df['EC - 2h'] = df['EC'].shift(2).dropna()\n",
    "df['EC - 3h'] = df['EC'].shift(3).dropna()\n",
    "\n",
    "df['temp - 1h'] = df['temp'].shift(1).dropna()\n",
    "df['temp - 2h'] = df['temp'].shift(2).dropna()\n",
    "df['temp - 3h'] = df['temp'].shift(3).dropna()\n",
    "\n",
    "df['raw_VWC - 1h'] = df['raw_VWC'].shift(1).dropna()\n",
    "df['raw_VWC - 2h'] = df['raw_VWC'].shift(2).dropna()\n",
    "df['raw_VWC - 3h'] = df['raw_VWC'].shift(3).dropna()\n",
    "\n",
    "#Add voltage and current time series\n",
    "df['V1 - 1h'] = df['V1 [10nV]'].shift(1).dropna()\n",
    "df['V1 - 2h'] = df['V1 [10nV]'].shift(2).dropna()\n",
    "df['V1 - 3h'] = df['V1 [10nV]'].shift(3).dropna()\n",
    "\n",
    "df['I1L - 1h'] = df['I1L [10pA]'].shift(1).dropna()\n",
    "df['I1L - 2h'] = df['I1L [10pA]'].shift(2).dropna()\n",
    "df['I1L - 3h'] = df['I1L [10pA]'].shift(3).dropna()\n",
    "\n",
    "df['I1H - 1h'] = df['I1H [nA]'].shift(1).dropna()\n",
    "df['I1H - 2h'] = df['I1H [nA]'].shift(2).dropna()\n",
    "df['I1H - 3h'] = df['I1H [nA]'].shift(3).dropna()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "u4SM1_EvGS6y"
   },
   "outputs": [],
   "source": [
    "#df = df.rename(columns={'power': 'power [Î¼W]'})\n",
    "df = df.rename(columns={'I1L [10pA]': 'Current (uA)', 'V1 [10nV]' : 'Voltage (mV)', 'power' : 'Power (uW)'})\n",
    "df = df.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "239oMsSK72B3"
   },
   "outputs": [],
   "source": [
    "#New runtime calculation\n",
    "import math\n",
    "from dateutil import parser\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def internal_R_v3(R=2000): #return internal resistance of v3 cells in ohms\n",
    "    #https://www.jstage.jst.go.jp/article/jwet/20/1/20_21-087/_pdf\n",
    "    v0_oc = 48.5e-3 #48.5 mV\n",
    "    v0_cc = 4.8e-3\n",
    "    v0_r = R*((v0_oc/v0_cc)-1)\n",
    "\n",
    "    v1_oc = 43.8e-3\n",
    "    v1_cc = 20.9e-3\n",
    "    v1_r = R*((v1_oc/v1_cc)-1)\n",
    "\n",
    "    v2_oc = 45.2e-3\n",
    "    v2_cc = 23.5e-3\n",
    "    v2_r = R*((v2_oc/v2_cc)-1)\n",
    "\n",
    "    return (v0_r+v1_r+v2_r)/3\n",
    "\n",
    "def internal_R_v0(R=2000): #return internal resistance of v0 cells in ohms\n",
    "    v3_oc = 41.7e-3 #41.7mV\n",
    "    v3_cc = 5.1e-3\n",
    "    v3_r = R*((v3_oc/v3_cc)-1)\n",
    "\n",
    "    v4_oc = 48.7e-3\n",
    "    v4_cc = 16.8e-3\n",
    "    v4_r = R*((v4_oc/v4_cc)-1)\n",
    "\n",
    "    v5_oc = 39.1e-3\n",
    "    v5_cc = 16.9e-3\n",
    "    v5_r = R*((v5_oc/v5_cc)-1)\n",
    "\n",
    "    return (v3_r+v4_r+v5_r)/3\n",
    "\n",
    "def SMFC_current(v, R):\n",
    "    return v/R\n",
    "\n",
    "#MODEL\n",
    "def cap_leakage(E_cap_tn, timestep):\n",
    "    #Spec for KEMET T491\n",
    "    return 0.01e-6 * E_cap_tn * timestep\n",
    "\n",
    "def Matrix_Power(V, R):\n",
    "    #efficiency interpolated from https://www.analog.com/media/en/technical-documentation/data-sheets/ADP5091-5092.pdf\n",
    "    #given I_in = 100 uA and SYS = 3V\n",
    "    #V is the voltage (V) of the SMFC we captured\n",
    "    #R is the resistance (ohms) of the load we used to get that voltage trace\n",
    "    #Eta = -292.25665*V**4 + 784.30311*V**3 - 770.71691*V**2 + 342.00502*V + 15.83307\n",
    "    #Eta = Eta/100\n",
    "    Eta = 0.60\n",
    "    Pmax = (V**2)/R\n",
    "    Pout = Eta*Pmax\n",
    "    #assert((Eta > 0) & (Eta < 1))\n",
    "    #assert(Pout < 12000e-6)\n",
    "    return Pout\n",
    "\n",
    "def update_capEnergy(e0, V_applied, R, C, dt):\n",
    "    # e0: initial energy stored\n",
    "    # V_applied: voltage from SMFC\n",
    "    # R: internal resistance of SMFC\n",
    "    # C: capacitance of capacitor\n",
    "    # dt: time step since last data point\n",
    "    e_cap = e0 + Matrix_Power(V_applied, R)*dt - cap_leakage(e0, dt)\n",
    "    v_cap = math.sqrt(2*e_cap/C)\n",
    "    if e_cap < 0: #Not charging if leakage is greater than energy\n",
    "        e_cap = 0\n",
    "\n",
    "    return e_cap, v_cap #output final e and v\n",
    "\n",
    "def Advanced_energy():\n",
    "    #Now representing \"Advanced\"\n",
    "    #startup time of 2500 ms\n",
    "    t = 2500e-3\n",
    "    e = 2.4 * 128e-3 * t\n",
    "    e_startup = 2.4 * 128e-3 * 5e-3\n",
    "    return e+e_startup\n",
    "\n",
    "def Minimal_energy():\n",
    "    #Now representing \"Minimal\"\n",
    "    t = 0.888e-3 #tentative time\n",
    "    e = 0.9 * 4.8e-3 * t #this uses average current\n",
    "    e_startup = 0#assume negligible, no known startup time given\n",
    "    return  e + e_startup\n",
    "\n",
    "def Analog_energy():\n",
    "    #Now representing Analog\n",
    "    t = 1e-3 #estimated operating time\n",
    "    e = 0.11 * 2.15e-6 * t\n",
    "    e_startup = 0 #analog device, no startup needed :)\n",
    "    return e + e_startup\n",
    "\n",
    "#STEP 3:\n",
    "# For each day:\n",
    "#   on_Minimal, on_Advanced, on_Analog = 0\n",
    "#   For each time step (like every 60 s given our logging freq):\n",
    "#       - Update the energy in our capacitor (put fcn in models.py) given (1) input voltage, (2) time step, (3) capacitance (prob 10 uF), this will be an integral\n",
    "#       - Check if energy is enough to turn on (1) 1 uJ load, (2) 10 uJ load, and (3) 20 uJ load (will tweak later to reflect real energy cost of each system)\n",
    "#       - If so, add to on_Minimal, on_Advanced, and on_Analog and reset capacitor energy to 0 J (might tweak this value)\n",
    "#   Append on_Minimal, on_Advanced, on_Analog to on_Minimal_list, on_Advanced_list, on_Analog_list. This will be a list of how many sensor readings we are able to take with each of these systems every day given the energy we got\n",
    "#STEP 4: Visualize the daily # of readings with 3 bar graphs, y axis is # of readings and x axis is days.\n",
    "#   - Given 3 lists of integer values, plot them on bar graphs\n",
    "\n",
    "def group_util(test_date1, test_date2, N):\n",
    "    diff = (test_date2 - test_date1) / N\n",
    "    return [test_date1 + diff * idx for idx in range(N)] + [test_date2]\n",
    "\n",
    "def oracle_simulate(v_list, C_h):\n",
    "    #Calculate maximum energy\n",
    "    total_E = 0\n",
    "    for i in range(len(v_list) - 1):\n",
    "        t = (v_list.index[i+1] - v_list.index[i]).total_seconds()\n",
    "        if t > 180:\n",
    "          print(\"Discontinuity\")\n",
    "          print(v_list.index[i+1], v_list.index[i])\n",
    "          print(v_list['Voltage (mV)'][i+1], v_list['Voltage (mV)'][i])\n",
    "          #total_E, ignore = update_capEnergy(total_E, V_applied=(v_list['V1 [mV]'][i+1] + v_list['V1 [mV]'][i])/2, R=internal_R_v0(), C=C_h[0], dt = t)\n",
    "        else:\n",
    "          total_E, ignore = update_capEnergy(total_E, V_applied=max(v_list['Voltage (mV)'][i], v_list['Voltage (mV)'][i+1]), R=internal_R_v0(), C=C_h[0], dt = t)\n",
    "    print(\"Oracle activations:\", math.floor(total_E/Minimal_energy()))\n",
    "    return(math.floor(total_E/Minimal_energy()))\n",
    "\n",
    "def naive_simulate(t_list, v_list, v_list_naive, v_list_fine, C_h):\n",
    "    # t_list: list of decimal time stamps in unit of days (e.g. 71.85893518518519 day), same length as v_list\n",
    "    # v_list: list of voltage values from SFMC\n",
    "    # C_h: capacitance of the capacitor being filled up by harvester\n",
    "\n",
    "    #assume capacitor is completely discharged at start\n",
    "    e_minimal_stored = 0\n",
    "    e_minimal_stored_theo = 0\n",
    "\n",
    "    #Initialize evaluation metrics\n",
    "    false_act = 0\n",
    "    max_act = 0\n",
    "    pred_act = 0\n",
    "    succ_act = 0\n",
    "\n",
    "    total_E = 0\n",
    "    total_E_naive = 0\n",
    "\n",
    "    #Calculate maximum energy\n",
    "    #for i in range(len(v_list_fine) - 1):\n",
    "    #    t = (v_list_fine.index[i+1] - v_list_fine.index[i]).total_seconds()\n",
    "    #    total_E, ignore = update_capEnergy(total_E, V_applied=v_list_fine['V1 [10nV]'][i], R=internal_R_v0(), C=C_h[0], dt = t)\n",
    "    #print(total_E/Minimal_energy())\n",
    "    v = v_list_naive.mean()\n",
    "    #for each voltage data point\n",
    "    for jj in range(len(v_list) - 1): #last data point was at 71.85893518518519 day\n",
    "        t = (v_list.index[jj+1] - v_list.index[jj]).total_seconds()\n",
    "        if t <= time_frame_seconds:\n",
    "          #Total predicted vs. actual energy stored\n",
    "          #Predict energy stored during scheduled sub-interval\n",
    "          total_E, ignore = update_capEnergy(total_E, V_applied=v_list[jj], R=internal_R_v0(), C=C_h[0], dt = t)\n",
    "          total_E_naive, ignore = update_capEnergy(total_E_naive, V_applied=v, R=internal_R_v0(), C=C_h[0], dt = t)\n",
    "\n",
    "          E_Minimal_pred, v_minimal_pred = update_capEnergy(e_minimal_stored, V_applied=v, R=internal_R_v0(), C=C_h[0], dt = t) #set dt as length of prediction interval, in seconds\n",
    "          pred_act += math.floor(E_Minimal_pred/Minimal_energy()) #Update number of activations predicted\n",
    "          itn = 0\n",
    "          if math.floor(E_Minimal_pred/Minimal_energy()) > 0:\n",
    "              minimal_intervals = [date for date in group_util(v_list.index[jj], v_list.index[jj] + timedelta(seconds=t), math.floor(E_Minimal_pred/Minimal_energy()))]\n",
    "              #Calculate desired interval\n",
    "              int_len = time_frame_seconds /  math.floor(E_Minimal_pred/Minimal_energy())\n",
    "              for i in range(len(minimal_intervals) - 1):\n",
    "                  #Determine actual energy stored during scheduled sub-interval\n",
    "                  start = v_list_fine.index.searchsorted(minimal_intervals[i])\n",
    "                  end =  v_list_fine.index.searchsorted(minimal_intervals[i+1])\n",
    "\n",
    "                  E_Minimal, ignore = update_capEnergy(e_minimal_stored, V_applied=v_list_fine.iloc[start:end]['Voltage (mV)'].mean(), R=internal_R_v0(), C=C_h[0], dt = int_len)\n",
    "                  if not math.isnan(v_list_fine.iloc[start:end]['Voltage (mV)'].mean()):\n",
    "                    if E_Minimal < Minimal_energy():\n",
    "                        false_act += 1\n",
    "                        e_minimal_stored = max(0, E_Minimal - Minimal_energy())\n",
    "                        itn += 1\n",
    "\n",
    "                    elif E_Minimal >= Minimal_energy():\n",
    "                        succ_act += 1\n",
    "                        e_minimal_stored = max(0, E_Minimal - Minimal_energy())\n",
    "                        itn+= 1\n",
    "\n",
    "                    else:\n",
    "                      print('Error')\n",
    "                      print(e_minimal_stored, v)\n",
    "\n",
    "                  #Unit test\n",
    "                  #else:\n",
    "                  #  print(\"?\")\n",
    "                  #  print(v_list_fine.index[start])\n",
    "                  #  print(v_list_fine.index[end])\n",
    "                  #  print(minimal_intervals[i], minimal_intervals[i+1])\n",
    "\n",
    "              #Unit test\n",
    "              #if itn != math.floor(E_Minimal_pred/Minimal_energy()):\n",
    "              #    print(\"itn not matching\")\n",
    "              #    print(itn, math.floor(E_Minimal_pred/Minimal_energy()))\n",
    "              #    continue\n",
    "\n",
    "          else:\n",
    "              e_minimal_stored, ignore = update_capEnergy(e_minimal_stored, V_applied=v_list[jj], R=internal_R_v0(), C=C_h[0], dt = t)\n",
    "              #Added this\n",
    "              #start = v_list_fine.index.searchsorted(v_list.index[jj])\n",
    "              #end =  v_list_fine.index.searchsorted(v_list.index[jj+1])\n",
    "              #for h in range(start, end):\n",
    "              #    v = v_list_fine.iloc[h]['V1 [mV]']\n",
    "              #    interval_length = ((v_list_fine.index[h+1]) - (v_list_fine.index[h])).total_seconds()\n",
    "              #    E_Minimal, ignore = update_capEnergy(e_minimal_stored, V_applied=v, R=internal_R_v0(), C=C_h[0], dt = interval_length)\n",
    "              #    e_minimal_stored = E_Minimal\n",
    "\n",
    "\n",
    "        else:\n",
    "          print(\"It's over 9000!\", v_list.index[jj], v_list.index[jj+1])\n",
    "\n",
    "    print(\"Naive total_E activations:\", total_E/Minimal_energy())\n",
    "    print(\"Naive total_E_pred activations:\", total_E_naive/Minimal_energy())\n",
    "    return pred_act, false_act, succ_act, total_E_naive\n",
    "\n",
    "def getMax(c_list, input_list):\n",
    "    max_value = max(input_list)\n",
    "    i = [index for index, item in enumerate(input_list) if item == max_value][0]\n",
    "    return i, max_value, c_list[i]\n",
    "\n",
    "\n",
    "#SMFC\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from scipy.signal import butter, lfilter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "        return butter(order, cutoff, fs=fs, btype='low', analog=False)\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def getMFC_data(y_test, test_pred):\n",
    "    unix_time = y_test.index\n",
    "    d0 = unix_time[0]\n",
    "    days = []\n",
    "    for d in unix_time:\n",
    "        day = d\n",
    "        day_from_start = day-d0\n",
    "        decimal_day = day_from_start.total_seconds()/(24 * 3600)\n",
    "        days.append(decimal_day)\n",
    "\n",
    "    return days\n",
    "\n",
    "def simulate(t_list, v_list, v_list_pred, v_list_fine, C_h):\n",
    "    # t_list: list of decimal time stamps in unit of days (e.g. 71.85893518518519 day), same length as v_list\n",
    "    # v_list: list of voltage values from SFMC\n",
    "    # C_h: capacitance of the capacitor being filled up by harvester\n",
    "\n",
    "    #assume capacitor is completely discharged at start\n",
    "    e_minimal_stored = 0\n",
    "    e_minimal_stored_theo = 0\n",
    "\n",
    "    #Initialize evaluation metrics\n",
    "    false_act = 0\n",
    "    max_act = 0\n",
    "    pred_act = 0\n",
    "    succ_act = 0\n",
    "\n",
    "    total_E = 0\n",
    "    total_E_pred = 0\n",
    "\n",
    "    #Calculate maximum energy\n",
    "    #for i in range(len(v_list_fine) - 1):\n",
    "    #    t = (v_list_fine.index[i+1] - v_list_fine.index[i]).total_seconds()\n",
    "    #    total_E, ignore = update_capEnergy(total_E, V_applied=v_list_fine['V1 [10nV]'][i], R=internal_R_v0(), C=C_h[0], dt = t)\n",
    "    #print(total_E/Minimal_energy())\n",
    "    #for each voltage data point\n",
    "    for jj in range(len(v_list) - 1): #last data point was at 71.85893518518519 day\n",
    "        t = (v_list.index[jj+1] - v_list.index[jj]).total_seconds()\n",
    "        total_E, ignore = update_capEnergy(total_E, V_applied=v_list[jj], R=internal_R_v0(), C=C_h[0], dt = t)\n",
    "        total_E_pred, ignore = update_capEnergy(total_E_pred, V_applied=v_list_pred[jj], R=internal_R_v0(), C=C_h[0], dt = t)\n",
    "        if t <= time_frame_seconds:\n",
    "          #Total predicted vs. actual energy stored\n",
    "          #Predict energy stored during scheduled sub-interval\n",
    "          E_Minimal_pred, v_minimal_pred = update_capEnergy(e_minimal_stored, V_applied=v_list_pred[jj], R=internal_R_v0(), C=C_h[0], dt = t) #set dt as length of prediction interval, in seconds\n",
    "          pred_act += math.floor(E_Minimal_pred/Minimal_energy()) #Update number of activations predicted\n",
    "          itn = 0\n",
    "          if math.floor(E_Minimal_pred/Minimal_energy()) > 0:\n",
    "              minimal_intervals = [date for date in group_util(v_list_pred.index[jj], v_list_pred.index[jj] + timedelta(seconds=t), math.floor(E_Minimal_pred/Minimal_energy()))]\n",
    "              #Calculate desired interval\n",
    "              int_len = time_frame_seconds /  math.floor(E_Minimal_pred/Minimal_energy())\n",
    "              for i in range(len(minimal_intervals) - 1):\n",
    "                  #Determine actual energy stored during scheduled sub-interval\n",
    "                  start = v_list_fine.index.searchsorted(minimal_intervals[i])\n",
    "                  end =  v_list_fine.index.searchsorted(minimal_intervals[i+1])\n",
    "                  v = v_list_fine.iloc[start:end]['Voltage (mV)'].mean()\n",
    "\n",
    "                  #interval_length = ((v_list_fine.index[end]) - (v_list_fine.index[start])).total_seconds()\n",
    "                  #if interval_length > int_len:\n",
    "                  #  print('interval_length > int_len')\n",
    "                  #  print('interval_length, int_len:', interval_length, int_len)\n",
    "                  #  print(v_list_fine.index[start], v_list_fine.index[end])\n",
    "                  #else:\n",
    "                  #  print('interval_length <= int_len')\n",
    "                  #  print('interval_length, int_len:', interval_length, int_len)\n",
    "                  #  print(v_list_fine.index[start], v_list_fine.index[end])\n",
    "\n",
    "                  E_Minimal, ignore = update_capEnergy(e_minimal_stored, V_applied=v, R=internal_R_v0(), C=C_h[0], dt = int_len)\n",
    "                  if not math.isnan(v_list_fine.iloc[start:end]['Voltage (mV)'].mean()):\n",
    "                    if E_Minimal < Minimal_energy():\n",
    "                        false_act += 1\n",
    "                        e_minimal_stored = max(0, E_Minimal - Minimal_energy())\n",
    "                        itn += 1\n",
    "\n",
    "                    elif E_Minimal >= Minimal_energy():\n",
    "                        succ_act += 1\n",
    "                        e_minimal_stored = max(0, E_Minimal - Minimal_energy())\n",
    "                        itn+= 1\n",
    "\n",
    "                    else:\n",
    "                      print('Error')\n",
    "                      print(e_minimal_stored, v)\n",
    "\n",
    "                  #Unit test\n",
    "                  #else:\n",
    "                  #  print(\"?\")\n",
    "                  #  print(v_list_fine.index[start])\n",
    "                  #  print(v_list_fine.index[end])\n",
    "                  #  print(minimal_intervals[i], minimal_intervals[i+1])\n",
    "\n",
    "              #Unit test\n",
    "              #if itn != math.floor(E_Minimal_pred/Minimal_energy()):\n",
    "              #    print(\"itn not matching\")\n",
    "              #    print(itn, math.floor(E_Minimal_pred/Minimal_energy()))\n",
    "              #    continue\n",
    "\n",
    "          else:\n",
    "              e_minimal_stored, ignore = update_capEnergy(e_minimal_stored, V_applied=v_list[jj], R=internal_R_v0(), C=C_h[0], dt = t)\n",
    "              #Added this\n",
    "              #start = v_list_fine.index.searchsorted(v_list.index[jj])\n",
    "              #end =  v_list_fine.index.searchsorted(v_list.index[jj+1])\n",
    "              #for h in range(start, end):\n",
    "              #    v = v_list_fine.iloc[h]['V1 [mV]']\n",
    "              #    interval_length = ((v_list_fine.index[h+1]) - (v_list_fine.index[h])).total_seconds()\n",
    "              #    E_Minimal, ignore = update_capEnergy(e_minimal_stored, V_applied=v, R=internal_R_v0(), C=C_h[0], dt = interval_length)\n",
    "              #    e_minimal_stored = E_Minimal\n",
    "\n",
    "\n",
    "        else:\n",
    "          print(\"It's over 9000!\", v_list.index[jj], v_list.index[jj+1])\n",
    "\n",
    "    print(\"Runtime total_E activations:\", total_E/Minimal_energy())\n",
    "    print(\"Runtime total_E_pred activations:\", total_E_pred/Minimal_energy())\n",
    "    return pred_act, false_act, succ_act, total_E, total_E_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSkfrkLBZ42n"
   },
   "source": [
    "## Specify Device so we can use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "M8NhMTInXpyS"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LX2yVusgZ_qU"
   },
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wpNnJc0hxWjZ"
   },
   "outputs": [],
   "source": [
    "beta = 0.9\n",
    "\n",
    "# old design network\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(200, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'))\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(3))\n",
    "# model.compile(loss=quantile_loss, metrics=['mape'], optimizer='adam')\n",
    "\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_steps):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        num_hidden1 = 200\n",
    "\n",
    "        # layer 1\n",
    "        self.slstm1 = snn.SLSTM(num_inputs, num_hidden1, threshold = 0.25)\n",
    "\n",
    "        # layer 2\n",
    "        self.fc1 = torch.nn.Linear(in_features=num_hidden1, out_features=100)\n",
    "        self.lif1 = snn.Leaky(beta=beta, threshold = 0.5)\n",
    "\n",
    "        # randomly initialize decay rate for output neuron\n",
    "        beta_out = random.uniform(0.5, 1)\n",
    "\n",
    "        # layer 2\n",
    "        self.fc2 = torch.nn.Linear(in_features=100, out_features=3)\n",
    "        self.lif2 = snn.Leaky(beta=beta_out, learn_beta=True, reset_mechanism=\"none\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states and outputs at t=0\n",
    "        syn1, mem1 = self.slstm1.reset_mem()\n",
    "        mem2 = self.lif1.reset_mem()\n",
    "        mem3 = self.lif2.reset_mem()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk1_rec = []\n",
    "        spk2_rec = []\n",
    "        spk3_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            spk1, syn1, mem1 = self.slstm1(x.flatten(1), syn1, mem1)\n",
    "            spk2, mem2 = self.lif1(self.fc1(spk1), mem2)\n",
    "            spk3, mem3 = self.lif2(self.fc2(spk2), mem3)\n",
    "\n",
    "            # Append the Spike and Membrane History\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "            spk3_rec.append(spk3)\n",
    "            mem_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spk1_rec), torch.stack(spk2_rec), torch.stack(spk3_rec), torch.stack(mem_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nMJdiecpR7VB"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import LSTM\n",
    "# from keras import backend as K\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vD62zunjFSGt",
    "outputId": "57ac4049-46e6-4000-f5d4-10b7644b81fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            power - 1h   power - 2h   power - 3h   V1 - 1h   \n",
      "timestamp                                                                    \n",
      "2021-09-27 12:54:04-07:00    29.157199   255.766324   722.260645  3265.925  \\\n",
      "2021-09-27 12:54:18-07:00   420.039723    29.157199   255.766324  3215.913   \n",
      "2021-09-27 12:54:32-07:00   270.391732   420.039723    29.157199  3242.947   \n",
      "2021-09-27 12:54:45-07:00  1282.425236   270.391732   420.039723  3370.497   \n",
      "2021-09-27 12:54:59-07:00   259.344092  1282.425236   270.391732  3233.608   \n",
      "...                                ...          ...          ...       ...   \n",
      "2021-12-12 01:48:42-08:00  2107.290708   700.025260   197.476456  4348.382   \n",
      "2021-12-12 01:48:55-08:00   882.158515  2107.290708   700.025260  4250.078   \n",
      "2021-12-12 01:49:09-08:00  2330.585839   882.158515  2107.290708  3994.854   \n",
      "2021-12-12 01:49:22-08:00   221.110620  2330.585839   882.158515  4202.154   \n",
      "2021-12-12 01:49:36-08:00  1232.285215   221.110620  2330.585839  4090.333   \n",
      "\n",
      "                            V1 - 2h   V1 - 3h  I1L - 1h  I1L - 2h  I1L - 3h   \n",
      "timestamp                                                                     \n",
      "2021-09-27 12:54:04-07:00  3240.120  3200.307    89.277   789.373  2256.848  \\\n",
      "2021-09-27 12:54:18-07:00  3265.925  3240.120  1306.129    89.277   789.373   \n",
      "2021-09-27 12:54:32-07:00  3215.913  3265.925   833.784  1306.129    89.277   \n",
      "2021-09-27 12:54:45-07:00  3242.947  3215.913  3804.855   833.784  1306.129   \n",
      "2021-09-27 12:54:59-07:00  3370.497  3242.947   802.027  3804.855   833.784   \n",
      "...                             ...       ...       ...       ...       ...   \n",
      "2021-12-12 01:48:42-08:00  4090.947  4152.879  4846.149  1711.157   475.517   \n",
      "2021-12-12 01:48:55-08:00  4348.382  4090.947  2075.629  4846.149  1711.157   \n",
      "2021-12-12 01:49:09-08:00  4250.078  4348.382  5833.970  2075.629  4846.149   \n",
      "2021-12-12 01:49:22-08:00  3994.854  4250.078   526.184  5833.970  2075.629   \n",
      "2021-12-12 01:49:36-08:00  4202.154  3994.854  3012.677   526.184  5833.970   \n",
      "\n",
      "                           EC - 1h  EC - 2h  EC - 3h  raw_VWC - 1h   \n",
      "timestamp                                                            \n",
      "2021-09-27 12:54:04-07:00    226.0    227.0    226.0       2559.22  \\\n",
      "2021-09-27 12:54:18-07:00    225.0    226.0    227.0       2558.87   \n",
      "2021-09-27 12:54:32-07:00    226.0    225.0    226.0       2559.30   \n",
      "2021-09-27 12:54:45-07:00    228.0    226.0    225.0       2559.62   \n",
      "2021-09-27 12:54:59-07:00    228.0    228.0    226.0       2558.68   \n",
      "...                            ...      ...      ...           ...   \n",
      "2021-12-12 01:48:42-08:00    265.0    266.0    266.0       2674.99   \n",
      "2021-12-12 01:48:55-08:00    259.0    265.0    266.0       2675.77   \n",
      "2021-12-12 01:49:09-08:00    271.0    259.0    265.0       2674.75   \n",
      "2021-12-12 01:49:22-08:00    272.0    271.0    259.0       2676.17   \n",
      "2021-12-12 01:49:36-08:00    267.0    272.0    271.0       2674.82   \n",
      "\n",
      "                           raw_VWC - 2h  raw_VWC - 3h  temp - 1h  temp - 2h   \n",
      "timestamp                                                                     \n",
      "2021-09-27 12:54:04-07:00       2558.38       2559.47       21.7       21.7  \\\n",
      "2021-09-27 12:54:18-07:00       2559.22       2558.38       21.7       21.7   \n",
      "2021-09-27 12:54:32-07:00       2558.87       2559.22       21.7       21.7   \n",
      "2021-09-27 12:54:45-07:00       2559.30       2558.87       21.7       21.7   \n",
      "2021-09-27 12:54:59-07:00       2559.62       2559.30       21.7       21.7   \n",
      "...                                 ...           ...        ...        ...   \n",
      "2021-12-12 01:48:42-08:00       2675.86       2675.36       10.9       10.9   \n",
      "2021-12-12 01:48:55-08:00       2674.99       2675.86       10.9       10.9   \n",
      "2021-12-12 01:49:09-08:00       2675.77       2674.99       10.9       10.9   \n",
      "2021-12-12 01:49:22-08:00       2674.75       2675.77       10.9       10.9   \n",
      "2021-12-12 01:49:36-08:00       2676.17       2674.75       10.9       10.9   \n",
      "\n",
      "                           temp - 3h  tsd  hour  \n",
      "timestamp                                        \n",
      "2021-09-27 12:54:04-07:00       21.7  115    12  \n",
      "2021-09-27 12:54:18-07:00       21.7  115    12  \n",
      "2021-09-27 12:54:32-07:00       21.7  115    12  \n",
      "2021-09-27 12:54:45-07:00       21.7  115    12  \n",
      "2021-09-27 12:54:59-07:00       21.7  115    12  \n",
      "...                              ...  ...   ...  \n",
      "2021-12-12 01:48:42-08:00       10.9  191     1  \n",
      "2021-12-12 01:48:55-08:00       10.9  191     1  \n",
      "2021-12-12 01:49:09-08:00       10.9  191     1  \n",
      "2021-12-12 01:49:22-08:00       10.9  191     1  \n",
      "2021-12-12 01:49:36-08:00       10.9  191     1  \n",
      "\n",
      "[339378 rows x 20 columns]\n",
      "X_train_teacher shape: (339378, 20)\n",
      "X_train_student shape: (339378, 20)\n",
      "X_valid shape: (145448, 20)\n",
      "X_test shape: (145448, 20)\n"
     ]
    }
   ],
   "source": [
    "# Combine features into X and targets into y\n",
    "X = pd.concat([\n",
    "    df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"],\n",
    "    df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"],\n",
    "    df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"],\n",
    "    df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"],\n",
    "    df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"],\n",
    "    df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"],\n",
    "    df[\"tsd\"], df[\"hour\"]\n",
    "], axis=1)\n",
    "\n",
    "\n",
    "y = pd.concat([\n",
    "    df['Power (uW)'], df['Voltage (mV)'], df['Current (uA)']\n",
    "], axis=1)\n",
    "\n",
    "# Split into training and testing sets (70% training, 30% testing)\n",
    "X_train, X_test = train_test_split(X, test_size=0.3, shuffle=False)\n",
    "y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Split the training set into teacher and student subsets (50/50)\n",
    "X_train_teacher, X_train_student = train_test_split(X_train, test_size=0.5, shuffle=False)\n",
    "y_train_teacher, y_train_student = train_test_split(y_train, test_size=0.5, shuffle=False)\n",
    "print(X_train_student)\n",
    "\n",
    "# Split the testing set into validation and final test sets (50/50)\n",
    "X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Print the shapes for verification\n",
    "print(f\"X_train_teacher shape: {X_train_teacher.shape}\")\n",
    "print(f\"X_train_student shape: {X_train_student.shape}\")\n",
    "print(f\"X_valid shape: {X_valid.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zd1y-AXaHJR"
   },
   "source": [
    "## 2. Train and Load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcteGp8Mxgg5"
   },
   "source": [
    "###  2.1 Train new SNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "FadItqzcypID",
    "outputId": "7621b3a4-d913-4897-e687-5b394b28fd51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5min\n",
      "tensor([[28544.3102, 30979.2252,  9214.1085],\n",
      "        [28838.2100, 30978.0755,  9309.3040],\n",
      "        [29093.4932, 30980.6113,  9391.0057],\n",
      "        ...,\n",
      "        [  612.2184,  3290.0938,  1851.7723],\n",
      "        [  666.4920,  3256.4468,  2043.8347],\n",
      "        [  759.2590,  3232.3066,  2368.1128]], dtype=torch.float64)\n",
      "output:  tensor([[-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510],\n",
      "        [-0.6275,  2.8781,  2.0510]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[28544.3105, 30979.2246,  9214.1084],\n",
      "        [28838.2109, 30978.0762,  9309.3037],\n",
      "        [29093.4941, 30980.6113,  9391.0059],\n",
      "        [29338.5156, 30978.7793,  9470.5723],\n",
      "        [28767.2734, 30984.8906,  9284.4736],\n",
      "        [28904.3750, 30982.8457,  9329.4316],\n",
      "        [29276.8945, 30983.7734,  9449.1602],\n",
      "        [28745.6699, 30987.2578,  9276.7002],\n",
      "        [28706.1777, 30988.5371,  9263.5654],\n",
      "        [29374.6250, 30988.4141,  9479.5566],\n",
      "        [28680.7285, 30991.6367,  9254.4883],\n",
      "        [29122.3730, 30991.0723,  9397.0605],\n",
      "        [29157.1797, 30991.9160,  9408.0518],\n",
      "        [28309.6699, 30999.9141,  9132.6182],\n",
      "        [28619.9766, 30996.0000,  9233.5371],\n",
      "        [29001.8730, 30996.5234,  9356.6660],\n",
      "        [29098.4238, 30997.6855,  9387.3789],\n",
      "        [29270.5762, 30999.4570,  9442.3662],\n",
      "        [29460.4258, 31000.0605,  9503.4443],\n",
      "        [29096.4531, 31002.8301,  9385.1875],\n",
      "        [29051.5332, 31003.4336,  9370.5986],\n",
      "        [28906.4941, 31006.3711,  9322.8271],\n",
      "        [28932.5371, 31007.7793,  9330.9180],\n",
      "        [28985.8555, 31009.5605,  9347.4854],\n",
      "        [29103.5625, 31009.7949,  9385.4883],\n",
      "        [29647.7539, 31009.8008,  9560.8232],\n",
      "        [28823.4785, 31016.6270,  9292.9746],\n",
      "        [28926.3594, 31018.7383,  9326.1133],\n",
      "        [29051.3320, 31018.4531,  9365.9248],\n",
      "        [28060.7305, 31025.0098,  9044.8574],\n",
      "        [28265.4395, 31025.4570,  9110.4648],\n",
      "        [28927.9883, 31023.2793,  9324.8457],\n",
      "        [29697.2988, 31022.9043,  9572.8320],\n",
      "        [29164.3047, 31025.8535,  9400.1016],\n",
      "        [28441.3633, 31031.5742,  9165.3857],\n",
      "        [29157.2578, 31029.6621,  9396.7373],\n",
      "        [29337.3633, 31031.8691,  9454.0049],\n",
      "        [28795.5977, 31038.7402,  9277.5088],\n",
      "        [29145.9688, 31037.6172,  9390.5869],\n",
      "        [29101.3711, 31041.4883,  9375.1494],\n",
      "        [29265.0781, 31040.7109,  9428.0635],\n",
      "        [28918.0117, 31047.2793,  9314.6924],\n",
      "        [29249.6113, 31045.9395,  9421.4775],\n",
      "        [29511.8223, 31046.8770,  9505.6484],\n",
      "        [28957.6992, 31053.3105,  9325.7959],\n",
      "        [29050.8438, 31053.2012,  9355.2559],\n",
      "        [29515.9219, 31053.2559,  9505.0332],\n",
      "        [29211.4238, 31056.8145,  9405.8721],\n",
      "        [28694.5684, 31061.8066,  9237.9365],\n",
      "        [28987.0781, 31061.5684,  9332.3193],\n",
      "        [29539.1172, 31062.6504,  9509.5762],\n",
      "        [29569.0312, 31063.8184,  9518.8564],\n",
      "        [29878.2871, 31066.5000,  9617.5742],\n",
      "        [28558.8906, 31074.2402,  9191.0771],\n",
      "        [29062.0293, 31074.7266,  9352.6309],\n",
      "        [29510.0000, 31074.3418,  9496.6240],\n",
      "        [29570.7930, 31076.1348,  9515.6445],\n",
      "        [29327.8809, 31079.9316,  9436.7842],\n",
      "        [29497.1113, 31080.4961,  9490.6143],\n",
      "        [29410.8242, 31083.4238,  9462.0264],\n",
      "        [29014.5410, 31088.1367,  9333.2295],\n",
      "        [29477.6621, 31087.4004,  9482.2285],\n",
      "        [29074.8359, 31091.2773,  9351.5137],\n",
      "        [29097.5332, 31093.4219,  9358.1455],\n",
      "        [28845.0781, 31097.6328,  9275.8457],\n",
      "        [29174.0977, 31097.3262,  9381.6445],\n",
      "        [29104.1914, 31099.0742,  9358.6230],\n",
      "        [29613.3379, 31101.0391,  9521.7998],\n",
      "        [29302.9023, 31103.2012,  9421.2148],\n",
      "        [29533.9863, 31104.2910,  9495.2266],\n",
      "        [29153.6133, 31109.9375,  9371.3125],\n",
      "        [29174.7109, 31111.0430,  9377.7549],\n",
      "        [28864.9375, 31117.0918,  9276.7881],\n",
      "        [29166.9648, 31118.1152,  9373.3438],\n",
      "        [28911.3750, 31117.8184,  9291.0918],\n",
      "        [28765.5293, 31117.4004,  9244.5732],\n",
      "        [29249.8613, 31114.0039,  9400.9297],\n",
      "        [29350.4727, 31111.5176,  9434.0264],\n",
      "        [29116.1543, 31112.5957,  9358.4902],\n",
      "        [28781.0273, 31114.6641,  9250.2295],\n",
      "        [29724.9824, 31107.0840,  9555.8896],\n",
      "        [29448.6543, 31111.1777,  9465.8818],\n",
      "        [29093.0059, 31112.2773,  9351.2627],\n",
      "        [29063.4199, 31110.8203,  9341.9570],\n",
      "        [29594.2188, 31108.4121,  9513.3574],\n",
      "        [29511.8027, 31110.0273,  9486.3555],\n",
      "        [29769.9199, 31109.6855,  9569.3701],\n",
      "        [29088.3730, 31115.1484,  9348.9336],\n",
      "        [29185.0137, 31115.5625,  9379.8389],\n",
      "        [28990.3574, 31114.6465,  9317.3496],\n",
      "        [29304.3145, 31116.6289,  9417.7363],\n",
      "        [29049.9590, 31118.7578,  9335.4150],\n",
      "        [29078.1523, 31118.4219,  9344.4541],\n",
      "        [28948.8770, 31120.0195,  9302.4062],\n",
      "        [29053.6016, 31121.2207,  9335.8457],\n",
      "        [29196.3965, 31121.6055,  9381.4404],\n",
      "        [29380.9941, 31123.0684,  9440.3271],\n",
      "        [28767.0254, 31128.9629,  9241.7305],\n",
      "        [28997.9473, 31127.3535,  9316.0957],\n",
      "        [28950.3730, 31128.4590,  9300.3496],\n",
      "        [29244.6055, 31129.3242,  9394.5977],\n",
      "        [29068.6152, 31132.5078,  9337.3242],\n",
      "        [29263.6445, 31132.3301,  9399.7949],\n",
      "        [29474.8320, 31132.7832,  9467.5410],\n",
      "        [28931.2168, 31140.1055,  9290.9268],\n",
      "        [29371.1309, 31137.2012,  9432.8711],\n",
      "        [28930.7168, 31141.9473,  9290.2080],\n",
      "        [28659.7812, 31143.5566,  9202.5312],\n",
      "        [29062.1016, 31144.2266,  9331.5000],\n",
      "        [29708.0234, 31143.1602,  9539.2305],\n",
      "        [29297.1074, 31148.2773,  9405.7432],\n",
      "        [29423.5273, 31150.3770,  9445.7354],\n",
      "        [29325.5996, 31152.2754,  9413.7412],\n",
      "        [28792.2051, 31159.5039,  9240.8447],\n",
      "        [29393.7285, 31158.0527,  9433.8770],\n",
      "        [28653.8145, 31163.5859,  9194.9736],\n",
      "        [28818.8984, 31164.3965,  9247.4541],\n",
      "        [28366.0820, 31172.5117,  9100.1045],\n",
      "        [29426.1230, 31167.5078,  9441.3408],\n",
      "        [29256.2578, 31171.8184,  9385.6240],\n",
      "        [29066.1738, 31176.2148,  9323.4570],\n",
      "        [29722.4258, 31175.1992,  9534.0625],\n",
      "        [29049.7676, 31182.8340,  9316.3057],\n",
      "        [29365.7559, 31183.2305,  9417.2109],\n",
      "        [29304.0352, 31185.5215,  9396.7422],\n",
      "        [28978.2930, 31189.9336,  9291.0303],\n",
      "        [28979.3535, 31194.0332,  9290.1406],\n",
      "        [29290.1719, 31194.9219,  9389.5176],\n",
      "        [28930.6758, 31200.1094,  9272.9092],\n",
      "        [29367.7227, 31198.6523,  9413.1963],\n",
      "        [29554.6855, 31202.8633,  9471.8691],\n",
      "        [29156.1270, 31205.0977,  9343.4512],\n",
      "        [28801.8379, 31210.6660,  9228.3984],\n",
      "        [28549.8516, 31216.6309,  9146.1133],\n",
      "        [28981.9297, 31214.0898,  9285.0156],\n",
      "        [29102.2871, 31220.5020,  9322.0547],\n",
      "        [28959.2090, 31222.1895,  9275.2754],\n",
      "        [29901.6348, 31221.8105,  9577.2656],\n",
      "        [29514.3555, 31229.7520,  9450.8643],\n",
      "        [29625.0215, 31232.5957,  9485.3711],\n",
      "        [29801.1133, 31235.5508,  9541.0039],\n",
      "        [29412.0254, 31242.6602,  9414.2334],\n",
      "        [29099.5527, 31249.2109,  9312.1670],\n",
      "        [29563.6426, 31250.5742,  9460.3086],\n",
      "        [28311.8945, 31260.9297,  9056.7930],\n",
      "        [29779.0820, 31257.9590,  9526.9639],\n",
      "        [28426.4004, 31268.8945,  9091.2773],\n",
      "        [29029.2168, 31268.9336,  9283.8984],\n",
      "        [30143.5039, 31265.4980,  9641.2773],\n",
      "        [29218.6836, 31274.4473,  9342.7285]])\n",
      "data:  tensor([[[0.3323, 0.3317, 0.3308,  ..., 0.5658, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3354, 0.3358, 0.3356,  ..., 0.5658, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3394, 0.3388, 0.3388,  ..., 0.5658, 0.0000, 0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3393, 0.3382, 0.3380,  ..., 0.5623, 0.0000, 0.5217]],\n",
      "\n",
      "        [[0.3498, 0.3497, 0.3473,  ..., 0.5623, 0.0000, 0.5217]],\n",
      "\n",
      "        [[0.3416, 0.3418, 0.3441,  ..., 0.5623, 0.0000, 0.5217]]])\n",
      "Epoch 0, Iteration 0 Train Loss: 11600.84\n",
      "output:  tensor([[-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311],\n",
      "        [-0.6059,  2.9763,  2.1311]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[29224.2773, 31279.5293,  9343.2021],\n",
      "        [30147.4492, 31277.6699,  9638.7168],\n",
      "        [29559.8984, 31283.9766,  9448.9580],\n",
      "        [30162.6543, 31286.4609,  9640.8965],\n",
      "        [29388.2949, 31293.2090,  9391.7119],\n",
      "        [29801.9727, 31291.9746,  9523.9092],\n",
      "        [30110.4277, 31293.7109,  9622.0576],\n",
      "        [29208.2188, 31303.9551,  9330.6943],\n",
      "        [29339.6348, 31307.4297,  9371.5371],\n",
      "        [17423.7930, 18603.6973,  7538.7104],\n",
      "        [   58.6022,   122.1430,  4797.8374],\n",
      "        [   60.0290,   122.1430,  4914.6475],\n",
      "        [   58.2103,   122.1430,  4765.7495],\n",
      "        [   58.9211,   122.1430,  4823.9414],\n",
      "        [   57.9446,   122.1430,  4743.9941],\n",
      "        [30347.7969, 31379.9668,  9671.1172],\n",
      "        [30301.2734, 31368.9316,  9659.7832],\n",
      "        [10841.1943, 13903.7695, 11405.9424],\n",
      "        [ 6395.7695, 14306.6123,  9618.8193],\n",
      "        [ 9688.0645, 19742.5566, 10511.6514],\n",
      "        [ 2632.6416, 12922.7842, 12767.7646],\n",
      "        [ 2271.2800, 13790.8594, 18734.5020],\n",
      "        [ 7183.7749, 25485.1992, 13086.8711],\n",
      "        [ 8293.6143, 32724.9570,  2532.0522],\n",
      "        [ 7494.0146, 32838.3750,  2279.9587],\n",
      "        [ 5237.2725, 32900.3438,  1591.7157],\n",
      "        [10946.2939, 32888.8672,  3331.6970],\n",
      "        [10579.8086, 32868.2227,  3220.3984],\n",
      "        [16357.4600, 32973.1719,  4879.8501],\n",
      "        [14311.9854, 32717.7324,  4375.4233],\n",
      "        [17018.3320, 32611.0156,  5223.6567],\n",
      "        [17876.9648, 32487.7500,  5503.7075],\n",
      "        [16868.5996, 32360.4277,  5215.3423],\n",
      "        [18900.8828, 32218.6797,  5867.9961],\n",
      "        [19114.9062, 32087.5430,  5961.0811],\n",
      "        [17969.4160, 31951.4375,  5627.8838],\n",
      "        [22422.4668, 31783.7891,  7060.0825],\n",
      "        [20561.3516, 31651.7129,  6505.1436],\n",
      "        [21539.3496, 31512.4668,  6844.7778],\n",
      "        [16255.6045, 31401.4492,  5180.9775],\n",
      "        [20799.7559, 31251.2891,  6658.9165],\n",
      "        [19204.2930, 31112.7031,  6176.6416],\n",
      "        [19444.1250, 30979.0254,  6280.9746],\n",
      "        [19370.3105, 30841.6055,  6281.5454],\n",
      "        [20964.8730, 30706.7383,  6830.5195],\n",
      "        [21066.1719, 30583.9902,  6895.8496],\n",
      "        [16814.8574, 30474.3418,  5522.5542],\n",
      "        [18893.6797, 30365.1465,  6231.7832],\n",
      "        [15716.4482, 30251.1074,  5197.0420],\n",
      "        [20633.7109, 30115.4023,  6853.7920],\n",
      "        [19685.5195, 29997.0957,  6576.9780],\n",
      "        [18493.0039, 29903.9961,  6186.4966],\n",
      "        [25307.9375, 29790.8457,  8494.5889],\n",
      "        [20273.4141, 29677.0527,  6837.7163],\n",
      "        [17348.6055, 29601.4395,  5864.8721],\n",
      "        [16178.7598, 29496.5234,  5487.7358],\n",
      "        [20970.4727, 29364.2715,  7146.3140],\n",
      "        [20634.2969, 29267.3457,  7066.4722],\n",
      "        [19968.9434, 29175.1621,  6851.5557],\n",
      "        [18658.7695, 29081.0527,  6421.6978],\n",
      "        [20684.3789, 28998.7441,  7124.8569],\n",
      "        [19821.0117, 28865.0332,  6871.2471],\n",
      "        [18272.6699, 28795.0039,  6353.2827],\n",
      "        [20308.1172, 28674.4453,  7082.8242],\n",
      "        [27543.6172, 28539.3496,  9663.1123],\n",
      "        [19088.9746, 28505.6914,  6698.4307],\n",
      "        [21815.6484, 28380.0234,  7691.5352],\n",
      "        [15895.7568, 28295.2305,  5621.0044],\n",
      "        [20550.6250, 28178.8223,  7296.4805],\n",
      "        [17639.8555, 28227.2383,  6187.9116],\n",
      "        [16338.5010, 28013.2012,  5836.2393],\n",
      "        [20989.8223, 27891.5215,  7531.5566],\n",
      "        [17521.4590, 27807.8848,  6311.6807],\n",
      "        [17567.0586, 27719.7227,  6340.6177],\n",
      "        [18369.9219, 27625.1230,  6653.4600],\n",
      "        [19429.7969, 27517.9805,  7071.4331],\n",
      "        [18763.9727, 27438.0137,  6844.6255],\n",
      "        [16504.4141, 27358.3750,  6034.6953],\n",
      "        [19285.6328, 27251.4863,  7081.8169],\n",
      "        [14245.1758, 27184.0137,  5247.8882],\n",
      "        [19886.3242, 27094.7793,  7348.3110],\n",
      "        [17799.5098, 26993.2129,  6596.7148],\n",
      "        [22377.2090, 26876.8125,  8339.8838],\n",
      "        [18725.6562, 26799.7539,  7001.5356],\n",
      "        [16212.5332, 26742.0957,  6065.3755],\n",
      "        [16982.3320, 26644.3047,  6376.3047],\n",
      "        [20250.7539, 26571.7148,  7621.9072],\n",
      "        [17964.3105, 26470.8691,  6791.2437],\n",
      "        [19305.8633, 26379.6504,  7331.6860],\n",
      "        [14534.6270, 26326.9141,  5533.7104],\n",
      "        [14911.8584, 26257.2188,  5681.4131],\n",
      "        [17820.5234, 26374.5801,  6733.2803],\n",
      "        [19131.3965, 26065.6309,  7346.6431],\n",
      "        [17964.7617, 26000.1641,  6912.8174],\n",
      "        [15276.4668, 25935.9863,  5893.3110],\n",
      "        [15055.1953, 25854.1758,  5828.0234],\n",
      "        [17111.2168, 25767.3203,  6644.1484],\n",
      "        [12332.5430, 25722.5762,  4799.1938],\n",
      "        [16501.9707, 25624.3203,  6443.4189],\n",
      "        [17320.6855, 25545.7051,  6786.3877],\n",
      "        [12944.7285, 25500.3008,  5081.1377],\n",
      "        [13611.3975, 25416.1504,  5359.4082],\n",
      "        [15482.6465, 25334.1094,  6116.5132],\n",
      "        [12992.4688, 25278.2715,  5143.4033],\n",
      "        [17174.3750, 25197.8301,  6820.3149],\n",
      "        [12136.0801, 25148.0566,  4831.3687],\n",
      "        [12340.6631, 25059.7734,  4932.0669],\n",
      "        [13347.0264, 24990.1270,  5344.9678],\n",
      "        [15495.0225, 24901.6641,  6226.4951],\n",
      "        [14222.0957, 24840.4023,  5729.0151],\n",
      "        [15137.4482, 24772.3164,  6116.5454],\n",
      "        [14723.0459, 24698.6719,  5963.6177],\n",
      "        [10952.4102, 24656.2715,  4446.7427],\n",
      "        [15313.9219, 24572.5723,  6238.1753],\n",
      "        [13841.9189, 24498.9727,  5660.7808],\n",
      "        [14612.5156, 24603.7344,  5965.5146],\n",
      "        [11043.2422, 24400.9023,  4530.7192],\n",
      "        [12387.3398, 24306.1680,  5101.8691],\n",
      "        [14919.0908, 24231.3887,  6166.2349],\n",
      "        [14061.3477, 24174.4180,  5821.5171],\n",
      "        [11449.3193, 24149.3438,  4748.7461],\n",
      "        [ 8063.4448, 24083.3574,  3352.8835],\n",
      "        [14604.5967, 23973.7637,  6100.0454],\n",
      "        [12972.3652, 23919.3496,  5434.2573],\n",
      "        [12666.6201, 23881.8652,  5311.9419],\n",
      "        [13462.3662, 23796.1680,  5663.5156],\n",
      "        [13433.0352, 23760.5879,  5653.6104],\n",
      "        [12385.8779, 23686.3906,  5233.5737],\n",
      "        [ 9862.4570, 23630.7363,  4184.4775],\n",
      "        [11196.3486, 23565.9551,  4755.5698],\n",
      "        [11636.2012, 23512.8770,  4953.0059],\n",
      "        [11583.0723, 23448.9902,  4944.6802],\n",
      "        [12504.5176, 23382.6855,  5353.7163],\n",
      "        [16178.3027, 23299.7285,  6962.7046],\n",
      "        [13160.1689, 23265.9883,  5669.9897],\n",
      "        [11408.2158, 23215.3555,  4916.5430],\n",
      "        [13093.4697, 23147.5586,  5664.7217],\n",
      "        [12362.7451, 23090.1113,  5362.0317],\n",
      "        [12807.6553, 23042.0820,  5567.2202],\n",
      "        [16190.0430, 22960.0469,  7059.3608],\n",
      "        [12323.8350, 22919.8203,  5382.7554],\n",
      "        [11894.3086, 22876.1973,  5205.0059],\n",
      "        [13095.8711, 22804.2129,  5754.6211],\n",
      "        [14922.3916, 22732.1660,  6569.9165],\n",
      "        [ 8988.2832, 22723.8262,  3960.0056],\n",
      "        [12906.5850, 22633.4688,  5709.0068],\n",
      "        [11983.1533, 22584.2891,  5313.9785],\n",
      "        [11795.6670, 22528.9590,  5244.7139],\n",
      "        [14150.0391, 22462.0117,  6310.7314],\n",
      "        [13264.4375, 22393.9082,  5934.5786]])\n",
      "data:  tensor([[[0.3382, 0.3368, 0.3354,  ..., 0.5623, 0.0000, 0.5217]],\n",
      "\n",
      "        [[0.3517, 0.3526, 0.3517,  ..., 0.5623, 0.0000, 0.5217]],\n",
      "\n",
      "        [[0.3443, 0.3434, 0.3455,  ..., 0.5623, 0.0000, 0.5217]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.1338, 0.1280, 0.1232,  ..., 0.3839, 0.0042, 0.0435]],\n",
      "\n",
      "        [[0.1700, 0.1758, 0.1821,  ..., 0.3808, 0.0042, 0.0435]],\n",
      "\n",
      "        [[0.1526, 0.1500, 0.1468,  ..., 0.3808, 0.0042, 0.0435]]])\n",
      "output:  tensor([[-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142],\n",
      "        [-0.5831,  3.0781,  2.2142]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[10321.7549, 22366.5117,  4618.5156],\n",
      "        [11263.5078, 22297.7539,  5058.7310],\n",
      "        [12094.3877, 22245.0820,  5446.1255],\n",
      "        [ 8834.3564, 22202.5273,  3981.1316],\n",
      "        [11467.7441, 22125.4531,  5190.3081],\n",
      "        [ 8782.1680, 22088.5723,  3976.8735],\n",
      "        [14045.3535, 22007.5820,  6388.4897],\n",
      "        [10927.7334, 21973.2305,  4979.8584],\n",
      "        [10776.0020, 21910.8926,  4928.7104],\n",
      "        [11118.6104, 21867.4297,  5089.4546],\n",
      "        [12673.1768, 22032.0117,  5687.0825],\n",
      "        [10956.4160, 21754.9941,  5044.9268],\n",
      "        [ 9138.0430, 21696.3965,  4222.5835],\n",
      "        [10524.3760, 21636.6484,  4873.0879],\n",
      "        [12482.1787, 21568.4961,  5791.2729],\n",
      "        [10016.6855, 21528.3242,  4659.0171],\n",
      "        [ 8752.9355, 21479.2168,  4080.1187],\n",
      "        [ 9379.1094, 21439.6836,  4380.9688],\n",
      "        [10275.1035, 21360.8945,  4817.3916],\n",
      "        [11782.1904, 21289.8750,  5539.0063],\n",
      "        [ 6399.8672, 21276.6973,  3009.9678],\n",
      "        [11578.3340, 21185.0176,  5476.4424],\n",
      "        [10201.6016, 21144.2656,  4828.3223],\n",
      "        [ 8698.7705, 21127.8105,  4117.8569],\n",
      "        [ 8982.9453, 21055.2891,  4271.3232],\n",
      "        [ 9041.7900, 21005.0410,  4309.1064],\n",
      "        [ 8537.7021, 20969.0430,  4079.3364],\n",
      "        [10216.1309, 20948.4824,  4861.0664],\n",
      "        [ 9090.5508, 20888.4961,  4356.5342],\n",
      "        [ 9145.3740, 20841.4922,  4393.5479],\n",
      "        [ 8941.0137, 20824.7480,  4297.0884],\n",
      "        [ 9059.2500, 20778.9023,  4363.1313],\n",
      "        [ 9925.0811, 20738.1289,  4787.5469],\n",
      "        [ 7464.1406, 20722.0859,  3603.4377],\n",
      "        [ 7687.4580, 20700.3418,  3715.8101],\n",
      "        [10154.0234, 20649.3477,  4922.1226],\n",
      "        [ 8243.0576, 20645.2910,  3995.0042],\n",
      "        [ 9175.0947, 20613.4883,  4455.9741],\n",
      "        [ 9290.5693, 20593.6426,  4514.4146],\n",
      "        [ 8111.5815, 20584.7832,  3944.2629],\n",
      "        [ 7782.7256, 20596.6484,  3783.0640],\n",
      "        [ 8920.5391, 20558.1074,  4340.9233],\n",
      "        [ 8457.1992, 20577.2383,  4111.3965],\n",
      "        [ 9904.4004, 20537.9824,  4823.9600],\n",
      "        [ 8229.8047, 20534.2109,  4010.9109],\n",
      "        [ 9409.8086, 20523.0254,  4586.0435],\n",
      "        [ 9707.8838, 20509.6816,  4736.4541],\n",
      "        [ 9313.0068, 20501.3711,  4545.9185],\n",
      "        [10111.6113, 20515.7129,  4925.2334],\n",
      "        [10138.3018, 20483.1172,  4953.4941],\n",
      "        [ 9535.7988, 20487.0781,  4657.6880],\n",
      "        [ 9657.7090, 20489.4121,  4715.0469],\n",
      "        [10323.4932, 20482.2969,  5043.6274],\n",
      "        [ 9957.6006, 20473.3926,  4877.2524],\n",
      "        [ 8837.5254, 20499.1465,  4314.3521],\n",
      "        [12130.4824, 20568.7305,  5820.1582],\n",
      "        [10421.9229, 20443.0859,  5100.6733],\n",
      "        [10945.2246, 20424.4863,  5360.6260],\n",
      "        [10740.7090, 20431.4238,  5260.3149],\n",
      "        [10730.2617, 20413.3145,  5259.2339],\n",
      "        [11840.0703, 20388.0801,  5811.4224],\n",
      "        [10291.8994, 20413.3711,  5042.9702],\n",
      "        [11931.3682, 20393.6484,  5854.0957],\n",
      "        [ 9915.3135, 20408.5176,  4861.3696],\n",
      "        [11228.8623, 20415.9395,  5501.3735],\n",
      "        [11324.1719, 20414.6113,  5557.6318],\n",
      "        [10744.4180, 20420.1523,  5271.5239],\n",
      "        [11201.1689, 20436.4062,  5482.5942],\n",
      "        [11805.0615, 20437.6465,  5777.1509],\n",
      "        [12435.0254, 20425.1738,  6111.3428],\n",
      "        [11089.6992, 20439.9355,  5431.4658],\n",
      "        [11768.9092, 20460.3008,  5753.1230],\n",
      "        [11311.8525, 20469.7617,  5528.9727],\n",
      "        [11693.0801, 20477.0840,  5713.4585],\n",
      "        [11515.7188, 20445.8223,  5637.2178],\n",
      "        [10712.8057, 20434.4785,  5244.4717],\n",
      "        [11754.8203, 20371.9160,  5773.1685],\n",
      "        [11840.4268, 20329.2207,  5825.2480],\n",
      "        [12893.2139, 20271.0781,  6366.6641],\n",
      "        [11933.1416, 20233.1152,  5899.3394],\n",
      "        [13817.1523, 20166.6602,  6871.0430],\n",
      "        [12973.6436, 20125.2324,  6451.6973],\n",
      "        [13102.6846, 20082.0000,  6529.7725],\n",
      "        [13029.4941, 20039.7734,  6506.3496],\n",
      "        [11038.6689, 20010.7637,  5517.4346],\n",
      "        [11282.0889, 19969.8379,  5651.4878],\n",
      "        [11800.2334, 19937.3691,  5920.2646],\n",
      "        [12238.5527, 19877.9629,  6158.9751],\n",
      "        [14025.2266, 19814.0039,  7100.6719],\n",
      "        [11841.6348, 19743.0566,  6000.5991],\n",
      "        [11577.0127, 19695.6738,  5880.6777],\n",
      "        [12656.8877, 19633.4121,  6449.0737],\n",
      "        [11384.1250, 19611.2773,  5805.6328],\n",
      "        [12948.1621, 19587.0977,  6617.3198],\n",
      "        [13801.0771, 19569.3750,  7060.7852],\n",
      "        [13280.5869, 19537.5371,  6798.9849],\n",
      "        [12020.4170, 19525.7090,  6157.2583],\n",
      "        [12212.7510, 19489.1230,  6267.3389],\n",
      "        [12744.0322, 19459.4590,  6551.9077],\n",
      "        [12922.8223, 19457.5703,  6646.5669],\n",
      "        [12438.1592, 19481.9746,  6385.4316],\n",
      "        [12021.9600, 19521.2344,  6161.6616],\n",
      "        [10671.2803, 19616.9141,  5441.2764],\n",
      "        [10797.7559, 19673.4277,  5489.3057],\n",
      "        [12413.1914, 19637.9141,  6321.6719],\n",
      "        [11389.5225, 19577.8047,  5818.8340],\n",
      "        [10635.1748, 19515.7715,  5450.3213],\n",
      "        [12151.3154, 19463.7598,  6244.1709],\n",
      "        [11131.0410, 19445.5566,  5724.8921],\n",
      "        [11390.0850, 19431.8105,  5862.5991],\n",
      "        [10998.6484, 19452.2812,  5655.3145],\n",
      "        [11172.2959, 19463.5254,  5740.8501],\n",
      "        [12051.3438, 19440.3164,  6199.7158],\n",
      "        [11657.2607, 19428.8613,  6000.4478],\n",
      "        [11438.1279, 19417.2598,  5892.3584],\n",
      "        [10866.9756, 19386.0488,  5606.2700],\n",
      "        [11329.5352, 19352.4062,  5855.0029],\n",
      "        [11365.9697, 19332.7793,  5879.6719],\n",
      "        [10962.2666, 19333.8027,  5670.6606],\n",
      "        [12040.4150, 19314.4648,  6234.6460],\n",
      "        [11262.9512, 19300.6680,  5836.7949],\n",
      "        [10648.2832, 19283.0410,  5522.7202],\n",
      "        [10425.3145, 19252.5547,  5415.4424],\n",
      "        [10645.4980, 19226.3770,  5537.9189],\n",
      "        [11171.2607, 19185.7910,  5823.0596],\n",
      "        [11559.2236, 19154.5020,  6035.8975],\n",
      "        [ 9720.6377, 19127.0215,  5082.5112],\n",
      "        [ 9150.0039, 19080.0801,  4796.0342],\n",
      "        [ 8981.8408, 19062.6270,  4712.2812],\n",
      "        [ 8338.7900, 19054.3262,  4377.0000],\n",
      "        [ 8261.5127, 19044.8418,  4338.9609],\n",
      "        [ 7500.3696, 19041.7305,  3940.1699],\n",
      "        [ 8295.5947, 19010.4180,  4364.0386],\n",
      "        [ 7107.1421, 18942.0352,  3753.0474],\n",
      "        [ 6003.0771, 18841.4863,  3186.7898],\n",
      "        [ 6883.5454, 18677.2949,  3698.4863],\n",
      "        [ 3303.5864, 18466.0781,  1785.7379],\n",
      "        [ 3548.6333, 18047.9102,  1974.6786],\n",
      "        [ 6683.1587, 17578.9180,  3807.0842],\n",
      "        [10217.2070, 16997.1895,  6014.5234],\n",
      "        [15439.5137, 16622.0352,  9262.6201],\n",
      "        [16659.2910, 15622.3105, 10667.9023],\n",
      "        [19504.3008, 14891.8594, 13104.4121],\n",
      "        [21803.6055, 14189.6377, 15374.0391],\n",
      "        [23240.1113, 13431.4854, 17324.1719],\n",
      "        [24674.0488, 12585.6689, 19599.0254],\n",
      "        [26346.2773, 11686.3037, 22562.3887],\n",
      "        [27325.6309, 10636.6172, 25720.1035],\n",
      "        [27112.4746,  9472.8633, 28625.8281],\n",
      "        [25977.5371,  8332.4795, 31206.4648]])\n",
      "data:  tensor([[[0.1193, 0.1199, 0.1200,  ..., 0.3808, 0.0042, 0.0435]],\n",
      "\n",
      "        [[0.1356, 0.1379, 0.1299,  ..., 0.3774, 0.0042, 0.0435]],\n",
      "\n",
      "        [[0.1381, 0.1378, 0.1468,  ..., 0.3772, 0.0042, 0.0435]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3179, 0.3174, 0.3165,  ..., 0.8827, 0.0042, 0.5652]],\n",
      "\n",
      "        [[0.3193, 0.3205, 0.3203,  ..., 0.8795, 0.0042, 0.5652]],\n",
      "\n",
      "        [[0.3006, 0.3006, 0.3015,  ..., 0.8767, 0.0042, 0.5652]]])\n",
      "output:  tensor([[-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005],\n",
      "        [-0.5592,  3.1836,  2.3005]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[23934.3418,  7210.4438, 33233.4883],\n",
      "        [20547.9902,  6023.1528, 34104.6328],\n",
      "        [16839.9414,  4922.2075, 34179.0195],\n",
      "        [13717.9990,  3880.9548, 35326.2500],\n",
      "        [10173.0664,  2929.9189, 34690.3086],\n",
      "        [ 6709.6074,  2018.3221, 33044.3438],\n",
      "        [ 4073.2166,  1281.0039, 31752.4355],\n",
      "        [ 2255.7515,   733.5085, 30593.3711],\n",
      "        [  705.5434,   246.9452, 27957.1992],\n",
      "        [  364.8591,   138.9720, 26549.1602],\n",
      "        [ 1130.6140,   454.5802, 25066.6113],\n",
      "        [ 1521.6539,   627.9482, 23422.0195],\n",
      "        [ 2013.3790,   854.9315, 23608.2617],\n",
      "        [ 2304.6929,  1020.1615, 22691.3887],\n",
      "        [ 2558.8418,  1150.0353, 22459.1309],\n",
      "        [ 2896.1096,  1353.9727, 21429.7871],\n",
      "        [ 3020.2566,  1446.2618, 20931.7715],\n",
      "        [ 3105.9397,  1514.2820, 20627.8105],\n",
      "        [ 3135.4058,  1563.2052, 20103.1602],\n",
      "        [ 3106.5049,  1613.7261, 19289.8535],\n",
      "        [ 2989.9277,  1675.2390, 18007.2754],\n",
      "        [ 3104.7952,  1670.1619, 18653.4844],\n",
      "        [ 2935.7144,  1696.1678, 17455.6270],\n",
      "        [ 2984.5791,  1727.6532, 17445.1406],\n",
      "        [ 2828.7222,  1727.9200, 16399.5840],\n",
      "        [ 2772.3000,  1713.1421, 16216.3057],\n",
      "        [ 2855.8516,  1721.8052, 16610.6582],\n",
      "        [ 2768.6899,  1736.7352, 16033.2080],\n",
      "        [ 2733.8037,  1756.8151, 15608.5361],\n",
      "        [ 2803.8931,  1808.8607, 15554.8779],\n",
      "        [ 2707.3467,  1835.6487, 14777.6475],\n",
      "        [ 2765.0581,  1890.0737, 15062.4170],\n",
      "        [ 2582.1650,  1878.4056, 13878.9766],\n",
      "        [ 2736.9646,  1870.5524, 14682.1895],\n",
      "        [ 2755.3074,  1878.2380, 14718.6152],\n",
      "        [ 2720.5798,  1861.3027, 14659.9121],\n",
      "        [ 2539.9426,  1863.3695, 13648.5840],\n",
      "        [ 2340.7380,  1875.0042, 12534.3320],\n",
      "        [ 2317.9365,  1885.6893, 12372.0098],\n",
      "        [ 2425.7405,  1879.2491, 12930.5176],\n",
      "        [ 2344.5393,  1890.5931, 12450.7285],\n",
      "        [ 2416.9014,  1876.0151, 12916.2012],\n",
      "        [ 2132.9397,  1903.5571, 11305.7676],\n",
      "        [ 1955.1516,  1907.5842, 10302.5859],\n",
      "        [ 2315.6221,  1868.8712, 12439.2793],\n",
      "        [ 1991.2500,  1890.7104, 10613.2998],\n",
      "        [ 1784.5533,  1897.6923,  9446.4883],\n",
      "        [ 2225.1143,  1850.2827, 12139.5518],\n",
      "        [ 1760.5759,  1885.7393,  9407.6973],\n",
      "        [ 1776.5089,  1874.8311,  9518.8301],\n",
      "        [ 1831.0704,  1862.0514,  9883.4609],\n",
      "        [ 1447.0519,  1873.6914,  7815.7305],\n",
      "        [ 1604.0457,  1848.5625,  8763.7881],\n",
      "        [ 1814.0239,  1809.8661, 10070.4375],\n",
      "        [ 1277.8530,  1831.4037,  7024.0801],\n",
      "        [ 1291.9319,  1823.1316,  7172.1870],\n",
      "        [ 1299.0229,  1789.5797,  7351.9438],\n",
      "        [ 1186.0519,  1777.3650,  6704.1416],\n",
      "        [ 1264.2979,  1755.8934,  7265.6958],\n",
      "        [ 1148.3043,  1771.4156,  6401.8257],\n",
      "        [ 1014.5975,  1756.8541,  5789.7446],\n",
      "        [ 1155.4219,  1700.4855,  6842.9790],\n",
      "        [  957.4274,  1703.6860,  5671.2671],\n",
      "        [  908.2721,  1689.7614,  5486.0586],\n",
      "        [  976.6717,  1641.9215,  6043.7905],\n",
      "        [ 1102.9418,  1638.0900,  6806.2778],\n",
      "        [  766.9327,  1648.2611,  4736.7720],\n",
      "        [  945.8137,  1625.6567,  5898.3164],\n",
      "        [  688.2073,  1651.0594,  4198.5386],\n",
      "        [  796.6044,  1621.3112,  4954.6382],\n",
      "        [  695.9985,  1624.0815,  4346.9810],\n",
      "        [  680.5754,  1614.8263,  4278.3062],\n",
      "        [  765.7690,  1598.4719,  4839.1772],\n",
      "        [  663.1564,  1608.7214,  4186.5112],\n",
      "        [  511.3296,  1623.7966,  3127.6243],\n",
      "        [  633.4509,  1591.0491,  4032.4724],\n",
      "        [  608.9503,  1563.9984,  3937.9753],\n",
      "        [  801.5586,  1531.8539,  5444.9448],\n",
      "        [  565.7123,  1562.0435,  3667.4724],\n",
      "        [  414.1397,  1556.7819,  2691.9509],\n",
      "        [  512.0435,  1519.1804,  3408.7080],\n",
      "        [  535.2282,  1495.1293,  3616.4373],\n",
      "        [  501.2202,  1488.8289,  3399.4773],\n",
      "        [  492.8836,  1474.4852,  3372.8435],\n",
      "        [  476.1036,  1460.7729,  3310.0466],\n",
      "        [  411.5475,  1435.9119,  2904.1311],\n",
      "        [  477.5529,  1412.5366,  3412.1465],\n",
      "        [  327.2164,  1413.2795,  2329.8435],\n",
      "        [  468.8143,  1390.9487,  3401.3718],\n",
      "        [  317.3785,  1374.8066,  2326.1548],\n",
      "        [  298.2184,  1366.5627,  2190.0913],\n",
      "        [  355.5295,  1350.0405,  2633.4714],\n",
      "        [  386.4480,  1337.7524,  2906.1953],\n",
      "        [  256.5274,  1308.8700,  1989.0674],\n",
      "        [  260.9951,  1302.2119,  2010.9744],\n",
      "        [  248.5903,  1289.9071,  1918.9081],\n",
      "        [  241.4454,  1271.2852,  1889.1682],\n",
      "        [  327.7557,  1249.6693,  2629.1399],\n",
      "        [  360.9168,  1225.7802,  2982.1365],\n",
      "        [  320.7725,  1217.6869,  2651.9724],\n",
      "        [  224.8593,  1202.7845,  1874.4709],\n",
      "        [  291.2271,  1180.5153,  2483.8865],\n",
      "        [  124.1981,  1181.0404,  1048.3735],\n",
      "        [  245.6565,  1152.7889,  2174.3826],\n",
      "        [  296.0341,  1158.5643,  2575.1677],\n",
      "        [  177.2176,  1139.3503,  1564.4635],\n",
      "        [  288.4403,  1167.6296,  2381.2314],\n",
      "        [  323.4507,  1139.1938,  2820.7500],\n",
      "        [  248.3835,  1115.6565,  2271.6584],\n",
      "        [  192.1323,  1117.0474,  1707.0675],\n",
      "        [  210.4069,  1071.6542,  2013.1307],\n",
      "        [  297.0492,  1097.7104,  2672.9297],\n",
      "        [  228.1425,  1039.4928,  2209.0234],\n",
      "        [  324.8185,  1030.2377,  3132.7571],\n",
      "        [  346.3778,  1000.7965,  3464.0793],\n",
      "        [  279.0264,   980.4432,  2837.8411],\n",
      "        [  283.7248,  1002.0533,  2708.8662],\n",
      "        [  246.9375,   940.2498,  2642.5203],\n",
      "        [  229.7531,   958.2127,  2326.0107],\n",
      "        [  203.0313,   931.7208,  2145.9990],\n",
      "        [  192.8784,   926.5208,  2056.3826],\n",
      "        [  190.0554,   887.2939,  2171.4175],\n",
      "        [  201.6580,   879.1501,  2286.8918],\n",
      "        [  303.7778,   893.2982,  3112.6489],\n",
      "        [  278.7621,   860.7404,  3166.3848],\n",
      "        [  266.2017,   851.6278,  2973.9097],\n",
      "        [  171.2701,   815.6376,  2135.3232],\n",
      "        [  213.0321,   803.6567,  2647.1990],\n",
      "        [  305.4015,   836.0916,  3182.6792],\n",
      "        [  232.9336,   808.8902,  2712.8550],\n",
      "        [  158.4844,   779.7565,  2005.7061],\n",
      "        [  211.2722,   761.7654,  2772.6528],\n",
      "        [  253.5212,   766.9991,  3183.4207],\n",
      "        [  172.8940,   741.7192,  2344.2578],\n",
      "        [  207.0522,   737.0720,  2694.3967],\n",
      "        [  278.3545,   729.7383,  3723.6658],\n",
      "        [  229.9519,   713.1326,  3170.3870],\n",
      "        [  230.6843,   721.9243,  3128.9307],\n",
      "        [  290.0847,   694.7731,  4141.1938],\n",
      "        [  247.1703,   720.2318,  3158.2312],\n",
      "        [  288.8132,   708.6921,  3627.2500],\n",
      "        [  254.1734,   691.9189,  3571.4998],\n",
      "        [  265.7983,   697.5322,  3740.5288],\n",
      "        [  215.7728,   697.9904,  2969.3018],\n",
      "        [  268.1684,   686.7635,  3794.8494],\n",
      "        [  419.3148,   903.5811,  3455.1838],\n",
      "        [  264.8953,   664.4607,  4058.4875],\n",
      "        [  181.7591,   657.0990,  2711.9246],\n",
      "        [  198.8330,   639.2031,  3059.5730],\n",
      "        [  139.2858,   646.3245,  2171.7129]])\n",
      "data:  tensor([[[0.2794, 0.2810, 0.2834,  ..., 0.8735, 0.0042, 0.5652]],\n",
      "\n",
      "        [[0.2410, 0.2430, 0.2454,  ..., 0.8678, 0.0042, 0.5652]],\n",
      "\n",
      "        [[0.1991, 0.2015, 0.2025,  ..., 0.8609, 0.0042, 0.6087]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0024, 0.0023, 0.0021,  ..., 0.3915, 0.0084, 0.0870]],\n",
      "\n",
      "        [[0.0022, 0.0022, 0.0024,  ..., 0.3889, 0.0084, 0.0870]],\n",
      "\n",
      "        [[0.0015, 0.0016, 0.0017,  ..., 0.3879, 0.0084, 0.0870]]])\n",
      "output:  tensor([[-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902],\n",
      "        [-0.5340,  3.2931,  2.3902]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 227.6256,  668.6887, 3264.0488],\n",
      "        [ 279.2218,  682.4348, 3948.3816],\n",
      "        [ 188.7696,  672.2301, 2680.2524],\n",
      "        [ 219.7940,  674.5815, 3202.5193],\n",
      "        [ 209.1878,  669.6272, 3087.0459],\n",
      "        [ 291.4392,  717.7742, 3821.6875],\n",
      "        [ 180.9014,  703.0117, 2554.2615],\n",
      "        [ 188.7229,  713.8644, 2579.0312],\n",
      "        [ 211.2771,  735.1394, 2790.6475],\n",
      "        [ 232.8598,  764.4241, 2981.9517],\n",
      "        [ 561.3384,  983.1278, 3937.5684],\n",
      "        [ 206.0641,  797.5462, 2516.8513],\n",
      "        [ 313.2757,  847.2348, 3638.9797],\n",
      "        [ 159.5587,  833.8351, 1884.9253],\n",
      "        [ 330.5457,  884.3391, 3672.2966],\n",
      "        [ 200.0006,  879.9657, 2252.2419],\n",
      "        [ 222.1669,  905.1562, 2427.1648],\n",
      "        [ 267.5963,  935.2563, 2803.5125],\n",
      "        [ 276.8850,  943.1710, 3014.2273],\n",
      "        [ 403.7371, 1015.1512, 3679.0977],\n",
      "        [ 261.1854, 1027.5734, 2490.9453],\n",
      "        [ 357.7547, 1078.9934, 3265.2070],\n",
      "        [ 271.5805, 1118.5443, 2337.3159],\n",
      "        [ 278.0756, 1182.1631, 2261.0322],\n",
      "        [ 341.7876, 1231.3545, 2748.4526],\n",
      "        [ 393.2745, 1298.2910, 2949.7888],\n",
      "        [ 330.6352, 1338.9590, 2442.4031],\n",
      "        [ 250.8279, 1362.4850, 1831.4117],\n",
      "        [ 419.4665, 1435.7443, 2519.7280],\n",
      "        [ 399.2803, 1462.7557, 2680.6658],\n",
      "        [ 356.1461, 1474.8652, 2384.0188],\n",
      "        [ 449.9175, 1514.1981, 2780.2925],\n",
      "        [ 424.0599, 1517.9851, 2699.9446],\n",
      "        [ 376.7069, 1512.4442, 2458.3438],\n",
      "        [ 415.4519, 1532.8033, 2618.8665],\n",
      "        [ 509.9132, 1562.9539, 3099.2566],\n",
      "        [ 291.3839, 1537.0540, 1871.7385],\n",
      "        [ 662.7100, 1662.9677, 3047.4089],\n",
      "        [ 457.8657, 1543.1088, 2797.3328],\n",
      "        [ 379.1768, 1530.1392, 2450.1582],\n",
      "        [ 376.5940, 1524.8330, 2449.3513],\n",
      "        [ 407.9814, 1525.4081, 2590.8162],\n",
      "        [ 455.5283, 1493.6436, 2985.6082],\n",
      "        [ 358.9013, 1433.6498, 2490.7764],\n",
      "        [ 323.9717, 1376.8087, 2283.4651],\n",
      "        [ 424.8648, 1333.0718, 3156.3416],\n",
      "        [ 412.4379, 1311.5565, 3083.1726],\n",
      "        [ 311.5563, 1270.3413, 2433.0903],\n",
      "        [ 249.1301, 1231.8795, 1995.2979],\n",
      "        [ 362.5895, 1248.1053, 2871.3389],\n",
      "        [ 414.3540, 1305.1555, 3105.2346],\n",
      "        [ 270.7429, 1344.5332, 1992.6251],\n",
      "        [ 397.8034, 1407.8336, 2679.1951],\n",
      "        [ 332.5427, 1457.5558, 2261.3540],\n",
      "        [ 480.1623, 1536.7915, 3055.2996],\n",
      "        [ 305.0968, 1529.3514, 2036.1735],\n",
      "        [ 449.8712, 1599.3601, 2793.1274],\n",
      "        [ 464.3752, 1639.5309, 2793.7407],\n",
      "        [ 454.6129, 1662.9900, 2710.0156],\n",
      "        [ 461.8097, 1686.0916, 2706.3462],\n",
      "        [ 418.5677, 1731.1273, 2400.1135],\n",
      "        [ 517.1687, 1769.8909, 2848.6897],\n",
      "        [ 420.1800, 1796.5446, 2324.6836],\n",
      "        [ 574.5952, 1941.5831, 2601.9993],\n",
      "        [ 393.5851, 1817.4512, 2181.9121],\n",
      "        [ 641.2299, 2035.7100, 2689.3499],\n",
      "        [ 481.1331, 1993.8019, 2403.4712],\n",
      "        [ 503.4975, 2044.3170, 2456.1641],\n",
      "        [ 573.9230, 2093.1118, 2600.2961],\n",
      "        [ 526.7170, 2145.0347, 2427.4109],\n",
      "        [ 619.1876, 2208.7148, 2817.3142],\n",
      "        [ 453.4203, 2274.4338, 1989.7964],\n",
      "        [ 458.5978, 2302.3220, 1973.6971],\n",
      "        [ 607.4702, 2350.4634, 2526.7678],\n",
      "        [ 794.5782, 2417.2659, 3206.3154],\n",
      "        [ 652.7328, 2475.6287, 2632.1194],\n",
      "        [ 744.3758, 2542.1184, 2891.6941],\n",
      "        [ 605.7510, 2568.3982, 2349.2776],\n",
      "        [ 696.8215, 2619.3733, 2560.8962],\n",
      "        [ 770.4760, 2656.4868, 2891.9849],\n",
      "        [ 902.9447, 2699.1599, 3255.3110],\n",
      "        [ 798.8065, 2684.3081, 3097.2659],\n",
      "        [ 639.6683, 2651.9011, 2390.6270],\n",
      "        [ 510.5808, 2592.0862, 1963.6060],\n",
      "        [ 744.1153, 2516.8215, 2754.8718],\n",
      "        [ 529.8868, 2390.9080, 2265.4326],\n",
      "        [ 621.9615, 2321.7427, 2640.8872],\n",
      "        [ 601.3554, 2247.5618, 2536.0120],\n",
      "        [ 376.4883, 2143.0686, 1740.1265],\n",
      "        [ 431.3145, 2081.8684, 2111.2632],\n",
      "        [ 482.9099, 2095.1282, 2261.8687],\n",
      "        [ 391.7947, 2043.2446, 1911.7856],\n",
      "        [ 536.2543, 2061.3359, 2513.0042],\n",
      "        [ 369.5116, 2066.1953, 1771.8740],\n",
      "        [ 539.0225, 2151.8433, 2368.8772],\n",
      "        [ 467.8209, 2165.8630, 2149.6372],\n",
      "        [ 631.6086, 2263.1902, 2619.2202],\n",
      "        [ 313.2616, 2294.6589, 1355.9368],\n",
      "        [ 528.1748, 2317.2129, 2318.1289],\n",
      "        [ 460.2050, 2338.3540, 2620.3196],\n",
      "        [ 338.1976, 2463.2625, 1358.9689],\n",
      "        [ 376.3438, 2518.6482, 1486.8158],\n",
      "        [ 422.3608, 2528.8474, 1645.2452],\n",
      "        [ 433.1840, 2562.1033, 1677.5902],\n",
      "        [ 259.1748, 2561.1482, 1007.7234],\n",
      "        [ 291.2964, 2533.8965, 1148.1184],\n",
      "        [ 357.9246, 2536.1809, 1367.7144],\n",
      "        [ 345.8246, 2513.2693, 1352.4070],\n",
      "        [ 300.8313, 2532.4778, 1177.6903],\n",
      "        [ 313.8294, 2554.8030, 1226.1245],\n",
      "        [ 460.8985, 2563.7144, 1766.1643],\n",
      "        [ 322.0425, 2547.1062, 1245.1434],\n",
      "        [ 210.1325, 2539.7556,  823.9985],\n",
      "        [ 399.7501, 2531.3552, 1584.6384],\n",
      "        [ 253.8963, 2521.2007, 1003.5236],\n",
      "        [ 189.3268, 2514.3418,  752.7382],\n",
      "        [ 244.7249, 2521.9211,  955.9698],\n",
      "        [ 282.4550, 2555.5125, 1099.8754],\n",
      "        [ 357.3506, 2524.8479, 1452.5460],\n",
      "        [ 276.3286, 2551.4907, 1076.5818],\n",
      "        [ 480.0854, 2584.8027, 1706.6309],\n",
      "        [ 417.9610, 2558.8469, 1561.5710],\n",
      "        [ 172.4535, 2507.9294,  685.5760],\n",
      "        [ 242.7142, 2489.6313,  974.0583],\n",
      "        [ 330.3999, 2493.0833, 1305.0037],\n",
      "        [ 312.7772, 2482.6106, 1251.4719],\n",
      "        [ 244.7187, 2529.7463,  961.0866],\n",
      "        [ 206.0891, 2557.6907,  805.6050],\n",
      "        [ 386.2253, 2597.9900, 1469.1344],\n",
      "        [ 321.6574, 2591.2261, 1235.6450],\n",
      "        [ 306.0541, 2618.9412, 1163.0730],\n",
      "        [ 258.5513, 2599.5762,  994.3032],\n",
      "        [ 382.1863, 2614.8862, 1398.2192],\n",
      "        [ 333.9722, 2591.2092, 1291.2991],\n",
      "        [ 277.5824, 2557.0146, 1066.1449],\n",
      "        [ 354.0042, 2580.8872, 1300.7129],\n",
      "        [ 359.3085, 2615.3386, 1370.7727],\n",
      "        [ 355.4902, 2540.6438, 2260.2332],\n",
      "        [ 247.3143, 2656.5649,  929.2847],\n",
      "        [ 298.2333, 2669.7578, 1114.3990],\n",
      "        [ 288.3492, 2686.5200, 1073.7155],\n",
      "        [ 394.5736, 2709.9775, 1508.3467],\n",
      "        [ 377.6380, 2749.9490, 1362.5729],\n",
      "        [ 328.0895, 2775.1282, 1178.9156],\n",
      "        [ 489.9216, 2810.4006, 1708.4718],\n",
      "        [ 292.1251, 2841.3723, 1030.5288],\n",
      "        [ 275.4411, 2833.7537,  968.6871],\n",
      "        [ 261.9061, 2809.8813,  932.8154],\n",
      "        [ 262.3041, 2781.5012,  945.3872],\n",
      "        [ 422.4785, 2781.3003, 1466.7629]])\n",
      "data:  tensor([[[0.0028, 0.0024, 0.0023,  ..., 0.3879, 0.0084, 0.0870]],\n",
      "\n",
      "        [[0.0031, 0.0032, 0.0033,  ..., 0.3879, 0.0084, 0.0870]],\n",
      "\n",
      "        [[0.0024, 0.0027, 0.0027,  ..., 0.3863, 0.0084, 0.0870]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0031, 0.0029, 0.0029,  ..., 0.7981, 0.0084, 0.6087]],\n",
      "\n",
      "        [[0.0032, 0.0033, 0.0031,  ..., 0.7860, 0.0084, 0.6087]],\n",
      "\n",
      "        [[0.0049, 0.0046, 0.0048,  ..., 0.7747, 0.0084, 0.6087]]])\n",
      "output:  tensor([[-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834],\n",
      "        [-0.5074,  3.4070,  2.4834]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 426.4997, 2763.8623, 1535.7543],\n",
      "        [ 410.0856, 2741.5037, 1466.1024],\n",
      "        [ 453.0068, 2658.7656, 1710.9208],\n",
      "        [ 541.6937, 2611.1216, 2047.1807],\n",
      "        [ 325.0953, 2568.0183, 1259.9441],\n",
      "        [ 537.3612, 2542.2412, 2126.7417],\n",
      "        [ 421.4873, 2512.1914, 1680.6367],\n",
      "        [ 483.9198, 2440.4009, 2020.3252],\n",
      "        [ 360.2346, 2460.5369, 1458.5376],\n",
      "        [ 512.6509, 2413.1213, 2109.0007],\n",
      "        [ 509.3871, 2388.0371, 2132.8713],\n",
      "        [ 470.5367, 2311.4431, 2086.6086],\n",
      "        [ 407.0295, 2341.6943, 1751.6292],\n",
      "        [ 426.3822, 2319.2236, 1850.0850],\n",
      "        [ 390.3301, 2318.1458, 1689.7024],\n",
      "        [ 660.0864, 2309.4155, 2897.9922],\n",
      "        [ 308.4709, 2327.6130, 1324.3748],\n",
      "        [ 467.2715, 2334.7068, 2001.3397],\n",
      "        [ 496.3052, 2306.3660, 2263.6458],\n",
      "        [ 560.4520, 2325.4629, 2383.9614],\n",
      "        [ 399.5633, 2321.0894, 1734.2352],\n",
      "        [ 695.5617, 2329.4172, 2994.1433],\n",
      "        [ 616.6361, 2308.9856, 2682.0242],\n",
      "        [ 379.0071, 2299.1284, 1648.8005],\n",
      "        [ 429.7636, 2303.5510, 1863.7147],\n",
      "        [ 385.0750, 2294.0779, 1678.5270],\n",
      "        [ 345.9695, 2301.7524, 1479.6206],\n",
      "        [ 426.9611, 2280.7173, 1883.2794],\n",
      "        [ 475.2807, 2271.7693, 2106.8862],\n",
      "        [ 459.0711, 2259.4980, 2038.6909],\n",
      "        [ 498.2963, 2246.7241, 2219.4905],\n",
      "        [ 448.7490, 2238.1448, 2003.6078],\n",
      "        [ 523.3410, 2205.3076, 2392.8225],\n",
      "        [ 528.5146, 2218.0425, 2365.1714],\n",
      "        [ 475.8644, 2200.2417, 2165.8655],\n",
      "        [ 504.2092, 2197.9238, 2268.6426],\n",
      "        [ 448.5071, 2177.4417, 2067.5483],\n",
      "        [ 368.1091, 2173.4983, 1699.7864],\n",
      "        [ 382.3116, 2165.9355, 1755.1079],\n",
      "        [ 448.1299, 2143.8337, 2118.7085],\n",
      "        [ 522.9902, 2157.4065, 2416.0134],\n",
      "        [ 513.9102, 2143.1917, 2393.6892],\n",
      "        [ 552.9411, 2007.7544, 2740.0710],\n",
      "        [ 488.1675, 2099.4404, 2351.6738],\n",
      "        [ 349.7720, 2127.0159, 1633.8850],\n",
      "        [ 265.5566, 2104.0930, 1269.1979],\n",
      "        [ 625.6881, 2089.8220, 3007.9314],\n",
      "        [ 292.9550, 2080.4998, 1422.5216],\n",
      "        [ 462.7338, 2070.9653, 2239.4463],\n",
      "        [ 294.6494, 2060.2803, 1438.6925],\n",
      "        [ 536.4023, 2064.9497, 2540.8286],\n",
      "        [ 441.7417, 2008.1454, 2292.2866],\n",
      "        [ 390.4654, 2028.1583, 1920.8831],\n",
      "        [ 395.2273, 2024.1758, 1931.2498],\n",
      "        [ 393.3116, 1993.3997, 1976.7682],\n",
      "        [ 562.2357, 1988.3002, 2808.6228],\n",
      "        [ 318.2269, 1985.3319, 1565.4250],\n",
      "        [ 401.6379, 1953.1843, 2052.9834],\n",
      "        [ 521.5839, 1979.8381, 2521.1531],\n",
      "        [ 271.3302, 1884.8010, 1753.8279],\n",
      "        [ 378.5094, 1912.3375, 1985.4984],\n",
      "        [ 168.3199, 1901.4011,  889.4489],\n",
      "        [ 517.3989, 1900.3230, 2710.3853],\n",
      "        [ 310.4091, 1860.7164, 1721.6921],\n",
      "        [ 427.6528, 1872.2561, 2271.7917],\n",
      "        [ 255.5037, 1839.9829, 1406.4176],\n",
      "        [ 408.0757, 1814.8651, 2296.4260],\n",
      "        [ 431.2165, 1830.2754, 2367.0759],\n",
      "        [ 480.8992, 1807.1794, 2666.3071],\n",
      "        [ 309.5808, 1823.6344, 1663.4100],\n",
      "        [ 393.6174, 1813.0442, 2133.3406],\n",
      "        [ 431.6087, 1770.9855, 2442.2661],\n",
      "        [ 329.0543, 1770.0862, 1859.8772],\n",
      "        [ 378.0990, 1752.4193, 2168.9365],\n",
      "        [ 525.0351, 1742.6838, 3004.6885],\n",
      "        [ 324.8986, 1736.2660, 1851.1541],\n",
      "        [ 350.5637, 1745.5884, 1859.1525],\n",
      "        [ 640.8446, 1880.6901, 2936.7295],\n",
      "        [ 349.9626, 1694.8777, 2062.2307],\n",
      "        [ 602.4242, 1709.2715, 3498.2026],\n",
      "        [ 387.3787, 1684.3265, 2271.8521],\n",
      "        [ 352.2547, 1668.0393, 2084.1694],\n",
      "        [ 410.3886, 1651.1711, 2495.5811],\n",
      "        [ 645.8765, 1635.5038, 3961.4929],\n",
      "        [ 542.8635, 1625.7627, 3335.7979],\n",
      "        [ 523.8206, 1593.0820, 3331.4731],\n",
      "        [ 376.1440, 1600.3486, 2355.4004],\n",
      "        [ 407.1245, 1617.6302, 2506.2410],\n",
      "        [ 477.4563, 1598.5613, 2968.7715],\n",
      "        [ 526.9799, 1553.5256, 3433.3083],\n",
      "        [ 430.4568, 1595.4076, 2663.5129],\n",
      "        [ 276.0304, 1572.3766, 1734.7206],\n",
      "        [ 431.4297, 1564.8585, 2753.7368],\n",
      "        [ 527.4066, 1528.6033, 3342.0603],\n",
      "        [ 309.5137, 1546.2030, 1971.1207],\n",
      "        [ 306.8643, 1532.5408, 1926.7567],\n",
      "        [ 363.7442, 1486.4718, 2512.5039],\n",
      "        [ 494.0344, 1676.5349, 2490.6729],\n",
      "        [ 338.2237, 1470.3967, 2354.1758],\n",
      "        [ 440.8976, 1477.3171, 3021.6704],\n",
      "        [ 402.5381, 1470.6592, 2770.5842],\n",
      "        [ 422.5988, 1454.4055, 2946.8804],\n",
      "        [ 356.4712, 1465.0737, 2420.0859],\n",
      "        [ 486.4231, 1487.1141, 3121.6558],\n",
      "        [ 447.2971, 1447.7419, 3089.0208],\n",
      "        [ 315.8636, 1426.9528, 2250.3977],\n",
      "        [ 247.4603, 1439.2632, 1710.4054],\n",
      "        [ 352.2503, 1449.2834, 2400.8862],\n",
      "        [ 317.8632, 1404.0076, 2261.8813],\n",
      "        [ 328.8598, 1427.0868, 2273.4661],\n",
      "        [ 395.5344, 1421.0265, 2749.5320],\n",
      "        [ 451.7839, 1380.7831, 3298.2451],\n",
      "        [ 356.1158, 1386.8713, 2522.1179],\n",
      "        [ 312.1911, 1341.1595, 2403.1772],\n",
      "        [ 271.7311, 1370.0590, 1986.3414],\n",
      "        [ 281.0062, 1370.9751, 2022.1892],\n",
      "        [ 205.8321, 1349.8618, 1540.5308],\n",
      "        [ 247.3331, 1359.2566, 1808.6454],\n",
      "        [ 341.3678, 1365.2052, 2367.8152],\n",
      "        [ 304.0268, 1321.1748, 2308.3301],\n",
      "        [ 303.4510, 1331.7816, 2256.3032],\n",
      "        [ 354.7246, 1307.6466, 2719.3428],\n",
      "        [ 391.0894, 1342.3885, 2654.5974],\n",
      "        [ 311.8571, 1285.7571, 2470.0383],\n",
      "        [ 455.4820, 1319.9515, 3223.4980],\n",
      "        [ 225.1535, 1275.6602, 1774.7354],\n",
      "        [ 250.7590, 1270.4360, 1965.6311],\n",
      "        [ 323.0028, 1266.9061, 2564.7278],\n",
      "        [ 450.1653, 1281.6965, 3447.6396],\n",
      "        [ 307.6318, 1235.7614, 2468.5183],\n",
      "        [ 452.9334, 1270.6094, 3529.0913],\n",
      "        [ 277.3730, 1202.5277, 2400.2607],\n",
      "        [ 229.7009, 1221.2502, 1887.1725],\n",
      "        [ 262.2824, 1196.8640, 2266.4668],\n",
      "        [ 246.1863, 1211.6880, 2046.4669],\n",
      "        [ 368.1114, 1217.6141, 2963.8328],\n",
      "        [ 277.9745, 1219.9098, 2242.1318],\n",
      "        [ 413.3661, 1252.5570, 3102.8894],\n",
      "        [ 320.7453, 1195.4845, 2690.7178],\n",
      "        [ 324.2370, 1190.0051, 2710.2012],\n",
      "        [ 529.6573, 1369.4836, 3221.7151],\n",
      "        [ 233.7086, 1163.9879, 2017.3809],\n",
      "        [ 273.8528, 1206.4320, 2156.4343],\n",
      "        [ 291.4131, 1170.6458, 2512.3806],\n",
      "        [ 216.3521, 1213.7769, 1698.0081],\n",
      "        [ 276.7381, 1214.4305, 2207.5298],\n",
      "        [ 213.6851, 1208.0686, 1756.6567],\n",
      "        [ 313.5376, 1246.8597, 2482.8843],\n",
      "        [ 222.8980, 1243.2235, 1798.8220],\n",
      "        [ 253.0765, 1270.7377, 2002.4095]])\n",
      "data:  tensor([[[0.0048, 0.0051, 0.0051,  ..., 0.7643, 0.0084, 0.6087]],\n",
      "\n",
      "        [[0.0049, 0.0047, 0.0046,  ..., 0.7548, 0.0084, 0.6087]],\n",
      "\n",
      "        [[0.0052, 0.0054, 0.0055,  ..., 0.7452, 0.0084, 0.6522]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0037, 0.0033, 0.0033,  ..., 0.3594, 0.0127, 0.1304]],\n",
      "\n",
      "        [[0.0026, 0.0030, 0.0030,  ..., 0.3607, 0.0127, 0.1304]],\n",
      "\n",
      "        [[0.0026, 0.0025, 0.0024,  ..., 0.3636, 0.0127, 0.1304]]])\n",
      "output:  tensor([[-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804],\n",
      "        [-0.4794,  3.5253,  2.5804]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 208.1051, 1301.4133, 1589.1227],\n",
      "        [ 188.2022, 1307.1160, 1467.5380],\n",
      "        [ 254.4131, 1347.8679, 1867.5585],\n",
      "        [ 212.6872, 1358.6591, 1577.8917],\n",
      "        [ 230.7897, 1381.8164, 1673.7400],\n",
      "        [ 253.0274, 1425.0256, 1662.6285],\n",
      "        [ 285.0873, 1389.1836, 2174.1179],\n",
      "        [ 335.2603, 1636.7716, 1361.9076],\n",
      "        [ 265.4456, 1504.2560, 1591.4329],\n",
      "        [ 246.9994, 1475.3287, 1725.2603],\n",
      "        [ 247.0539, 1541.6912, 1606.8407],\n",
      "        [ 244.6362, 1561.5406, 1589.7306],\n",
      "        [ 206.7326, 1602.4377, 1301.8951],\n",
      "        [ 255.3884, 1642.4355, 1546.9335],\n",
      "        [ 230.0507, 1656.2148, 1398.8494],\n",
      "        [ 250.4313, 1672.8093, 1578.6167],\n",
      "        [ 255.4819, 1657.0470, 1606.7340],\n",
      "        [ 240.6788, 1671.3962, 1456.9188],\n",
      "        [ 233.7083, 1619.3394, 1494.6270],\n",
      "        [ 227.2372, 1575.8955, 1452.4034],\n",
      "        [ 194.1771, 1527.4749, 1265.8290],\n",
      "        [ 561.0597, 2550.9771, 1833.2256],\n",
      "        [ 587.9987, 3710.8879, 1582.9694],\n",
      "        [ 458.7786, 3408.4392, 1321.1444],\n",
      "        [ 539.5883, 3237.7129, 1646.5603],\n",
      "        [ 393.8638, 3321.8584, 1185.9857],\n",
      "        [ 735.0292, 3356.4382, 2179.4116],\n",
      "        [ 558.4412, 3387.8621, 1591.8160],\n",
      "        [ 567.0204, 3457.9993, 1641.1312],\n",
      "        [ 523.1660, 3476.1575, 1510.4116],\n",
      "        [ 863.1973, 3610.1426, 2373.7556],\n",
      "        [ 758.4655, 3718.9365, 2069.6682],\n",
      "        [ 773.2936, 3824.2734, 2026.8250],\n",
      "        [ 745.5090, 3900.6772, 1913.3676],\n",
      "        [ 858.9102, 3961.3579, 2158.4910],\n",
      "        [ 759.7748, 4004.5002, 1907.3696],\n",
      "        [ 862.6804, 4019.5254, 2209.2307],\n",
      "        [ 772.1133, 4094.5386, 1893.8922],\n",
      "        [ 817.1414, 4154.2642, 1967.1066],\n",
      "        [ 821.0186, 4189.6704, 1965.4550],\n",
      "        [1052.0551, 4263.8569, 2430.2695],\n",
      "        [ 971.6746, 4325.6436, 2214.8318],\n",
      "        [1111.5510, 4306.8037, 2625.2476],\n",
      "        [ 914.6533, 4351.0352, 2109.7415],\n",
      "        [1184.4359, 4377.7231, 2682.3928],\n",
      "        [ 904.5081, 4362.2329, 2078.3928],\n",
      "        [1070.5001, 4385.4922, 2450.2913],\n",
      "        [ 950.9942, 4419.0552, 2157.5876],\n",
      "        [ 962.9461, 4437.1416, 2174.7649],\n",
      "        [ 905.3675, 4464.6162, 2031.9186],\n",
      "        [1069.9397, 4486.2212, 2390.0010],\n",
      "        [1133.8010, 4498.5205, 2524.0188],\n",
      "        [1182.8838, 4522.9790, 2623.9490],\n",
      "        [1161.8195, 4566.1831, 2549.0786],\n",
      "        [ 920.2903, 4622.8086, 1995.5306],\n",
      "        [1181.0321, 4671.8379, 2539.4558],\n",
      "        [1230.8678, 4738.5229, 2600.8535],\n",
      "        [ 851.9036, 4825.6177, 1769.0143],\n",
      "        [1093.4305, 4882.8018, 2244.9758],\n",
      "        [1281.7783, 4961.2783, 2589.3430],\n",
      "        [1334.9185, 5057.4380, 2641.8042],\n",
      "        [1323.1405, 5148.2969, 2578.2234],\n",
      "        [1363.2272, 5232.7832, 2613.8040],\n",
      "        [1419.6007, 5322.2739, 2672.4197],\n",
      "        [1142.0046, 5436.3521, 2106.0713],\n",
      "        [1640.9796, 5520.0728, 2976.3115],\n",
      "        [1396.4442, 5624.3149, 2483.6436],\n",
      "        [1537.1260, 5729.1714, 2685.1023],\n",
      "        [1601.4449, 5827.8726, 2752.1013],\n",
      "        [1841.7759, 5889.6426, 3133.1521],\n",
      "        [1874.0597, 5960.5337, 3147.4985],\n",
      "        [1958.2516, 6005.8208, 3262.2585],\n",
      "        [1681.0496, 6027.2134, 2792.4829],\n",
      "        [1616.2233, 6044.5674, 2677.1750],\n",
      "        [1877.4467, 6069.1211, 3096.0837],\n",
      "        [1858.9211, 6102.4663, 3048.0615],\n",
      "        [1793.8833, 6128.4058, 2929.3057],\n",
      "        [1981.8567, 6140.3193, 3232.2947],\n",
      "        [2006.9725, 6142.3950, 3273.4607],\n",
      "        [2180.0999, 6115.2520, 3566.7812],\n",
      "        [1533.4160, 6077.7900, 2527.4810],\n",
      "        [1692.9325, 6035.5415, 2806.0696],\n",
      "        [1953.0657, 5993.1025, 3266.8345],\n",
      "        [1686.5925, 5952.5239, 2836.5642],\n",
      "        [1845.7008, 5881.6216, 3140.1514],\n",
      "        [1658.5594, 5813.1494, 2858.8125],\n",
      "        [1398.2346, 5756.2275, 2432.7549],\n",
      "        [1793.8191, 5701.8193, 3146.7363],\n",
      "        [1687.8309, 5630.4590, 3001.3154],\n",
      "        [1671.1379, 5554.7144, 3010.4202],\n",
      "        [1784.2820, 5443.8257, 3283.1123],\n",
      "        [1694.3997, 5343.7222, 3174.3823],\n",
      "        [1666.7378, 5247.8584, 3178.1106],\n",
      "        [1556.9967, 5125.4580, 3042.3855],\n",
      "        [1419.8750, 4988.0605, 2849.7405],\n",
      "        [1393.2740, 4820.6074, 2893.3413],\n",
      "        [1260.2722, 4598.9644, 2742.2603],\n",
      "        [1097.7285, 4359.0620, 2525.7371],\n",
      "        [1043.8976, 4057.6128, 2575.6919],\n",
      "        [ 894.9913, 3722.3101, 2402.6921],\n",
      "        [ 929.7124, 3347.2109, 2780.2002],\n",
      "        [ 495.0257, 3019.6667, 1639.6161],\n",
      "        [ 713.9366, 2727.7075, 2614.6460],\n",
      "        [ 579.2252, 2507.5720, 2315.0522],\n",
      "        [ 550.6082, 2331.5508, 2377.2834],\n",
      "        [ 454.4688, 2189.6907, 2092.8982],\n",
      "        [ 551.3901, 2090.5872, 2661.7009],\n",
      "        [ 336.5704, 1992.9585, 1697.2806],\n",
      "        [ 455.9756, 1907.5453, 2398.6714],\n",
      "        [ 565.3004, 1835.7045, 3091.1636],\n",
      "        [ 451.8530, 1808.3016, 2522.5583],\n",
      "        [ 296.9945, 1790.2610, 1669.7123],\n",
      "        [ 357.3788, 1781.3074, 2022.0913],\n",
      "        [ 460.8813, 1774.6720, 2623.1421],\n",
      "        [ 490.2664, 1735.5511, 2841.5989],\n",
      "        [ 393.3266, 1729.6025, 2299.4795],\n",
      "        [ 399.8181, 1717.6273, 2336.9104],\n",
      "        [ 359.1205, 1718.5376, 2104.1746],\n",
      "        [ 397.0646, 1683.9971, 2370.0833],\n",
      "        [ 423.2278, 1638.4810, 2604.7007],\n",
      "        [ 405.7576, 1628.6840, 2514.1746],\n",
      "        [ 443.9394, 1602.4657, 2782.3362],\n",
      "        [ 440.3103, 1592.8307, 2786.7766],\n",
      "        [ 353.9708, 1610.5422, 2214.3181],\n",
      "        [ 397.1602, 1629.5162, 2457.1719],\n",
      "        [ 393.8999, 1662.6158, 2392.8352],\n",
      "        [ 313.3251, 1694.0342, 1857.5924],\n",
      "        [ 332.9121, 1730.5187, 1939.4232],\n",
      "        [ 382.6121, 1761.1439, 2192.7864],\n",
      "        [ 384.1392, 1823.5226, 2115.7053],\n",
      "        [ 447.9802, 1872.7365, 2414.7285],\n",
      "        [ 454.1744, 1907.4110, 2393.8455],\n",
      "        [ 363.3467, 1937.8409, 1892.7028],\n",
      "        [ 333.3322, 1975.8781, 1691.4973],\n",
      "        [ 349.0537, 1993.9025, 1760.9539],\n",
      "        [ 237.1847, 2022.1482, 1176.5687],\n",
      "        [ 383.0522, 2046.8250, 1875.4667],\n",
      "        [ 355.0080, 2063.2629, 1720.1196],\n",
      "        [ 499.0742, 2072.5627, 2413.1477],\n",
      "        [ 489.1799, 2067.7761, 2374.9915],\n",
      "        [ 354.5820, 2088.0237, 1705.6024],\n",
      "        [ 336.4594, 2084.5325, 1617.3499],\n",
      "        [ 422.8350, 2077.1597, 2048.9390],\n",
      "        [ 269.3826, 2091.3489, 1299.8876],\n",
      "        [ 444.5850, 2073.8811, 2171.9048],\n",
      "        [ 362.6137, 2100.1997, 1726.1732],\n",
      "        [ 438.1334, 2108.0251, 2075.6460],\n",
      "        [ 395.8074, 2092.8381, 1897.1449],\n",
      "        [ 396.2103, 2098.1499, 1888.9276],\n",
      "        [ 429.9342, 2075.7354, 2089.3254]])\n",
      "data:  tensor([[[0.0027, 0.0029, 0.0029,  ..., 0.3682, 0.0127, 0.1304]],\n",
      "\n",
      "        [[0.0021, 0.0019, 0.0019,  ..., 0.3722, 0.0127, 0.1304]],\n",
      "\n",
      "        [[0.0029, 0.0031, 0.0030,  ..., 0.3777, 0.0127, 0.1304]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0048, 0.0045, 0.0044,  ..., 0.5854, 0.0127, 0.6522]],\n",
      "\n",
      "        [[0.0049, 0.0051, 0.0052,  ..., 0.5810, 0.0127, 0.6522]],\n",
      "\n",
      "        [[0.0047, 0.0047, 0.0048,  ..., 0.5775, 0.0127, 0.6522]]])\n",
      "output:  tensor([[-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814],\n",
      "        [-0.4499,  3.6484,  2.6814]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 436.5048, 2067.6589, 2113.4077],\n",
      "        [ 333.7480, 2063.2129, 1624.5715],\n",
      "        [ 383.6506, 2064.1846, 1854.8529],\n",
      "        [ 471.0416, 2042.8704, 2322.9517],\n",
      "        [ 442.9582, 2017.9480, 2203.5474],\n",
      "        [ 392.9851, 1999.7617, 1977.8093],\n",
      "        [ 353.0316, 1997.3766, 1770.2147],\n",
      "        [ 322.2607, 1993.2264, 1607.2646],\n",
      "        [ 296.6172, 1955.8931, 1542.5138],\n",
      "        [ 426.7887, 1974.1913, 2150.0427],\n",
      "        [ 379.4011, 1956.1055, 1947.2032],\n",
      "        [ 484.5418, 1931.0043, 2547.8269],\n",
      "        [ 345.9306, 1951.3298, 1785.0076],\n",
      "        [ 350.0022, 1957.6917, 1788.1392],\n",
      "        [ 325.7279, 1942.3651, 1684.7720],\n",
      "        [ 308.1135, 1930.9037, 1631.2092],\n",
      "        [ 536.2619, 1953.9773, 2732.3892],\n",
      "        [ 349.3802, 1970.7395, 1753.2812],\n",
      "        [ 419.6730, 1951.7375, 2149.6819],\n",
      "        [ 408.0730, 1929.5687, 2134.5457],\n",
      "        [ 542.5363, 1950.5925, 2771.9871],\n",
      "        [ 452.4362, 1940.5498, 2344.6895],\n",
      "        [ 407.9617, 1936.0255, 2100.3589],\n",
      "        [ 308.2311, 1924.8545, 1613.1143],\n",
      "        [ 433.3288, 1903.3448, 2296.5623],\n",
      "        [ 522.9620, 1937.1259, 2652.2935],\n",
      "        [ 467.0318, 1911.9310, 2442.4775],\n",
      "        [ 343.8941, 1894.2629, 1832.0698],\n",
      "        [ 429.1471, 1891.3807, 2288.9695],\n",
      "        [ 297.4710, 1909.7904, 1532.7158],\n",
      "        [ 423.9019, 1896.8881, 2231.0273],\n",
      "        [ 360.2573, 1904.8921, 1873.0201],\n",
      "        [ 548.5426, 1904.7915, 2870.4788],\n",
      "        [ 317.4928, 1881.1538, 1702.7577],\n",
      "        [ 313.2608, 1884.9183, 1646.4816],\n",
      "        [ 366.6112, 1877.0205, 1961.4177],\n",
      "        [ 342.5772, 1873.0435, 1842.2780],\n",
      "        [ 349.5145, 1862.5038, 1893.0166],\n",
      "        [ 450.3936, 1865.3132, 2416.9424],\n",
      "        [ 459.1154, 1850.9919, 2496.4097],\n",
      "        [ 355.1819, 1856.9238, 1919.5258],\n",
      "        [ 315.4510, 1851.8522, 1713.3300],\n",
      "        [ 422.0179, 1829.5605, 2331.6143],\n",
      "        [ 335.3856, 1851.0925, 1800.3157],\n",
      "        [ 398.8323, 1830.0745, 2194.6785],\n",
      "        [ 350.5704, 1861.9231, 1839.0299],\n",
      "        [ 376.1576, 1831.8896, 2044.7533],\n",
      "        [ 417.3527, 1832.8392, 2247.0640],\n",
      "        [ 314.2709, 1811.3239, 1731.1252],\n",
      "        [ 381.9001, 1794.4111, 2144.6445],\n",
      "        [ 350.6790, 1801.1808, 1937.3799],\n",
      "        [ 229.9172, 1783.6031, 1311.2367],\n",
      "        [ 289.1241, 1798.8962, 1580.6426],\n",
      "        [ 452.9275, 1792.4059, 2519.7422],\n",
      "        [ 501.0102, 1776.8446, 2826.3792],\n",
      "        [ 278.5803, 1753.2291, 1633.4659],\n",
      "        [ 290.7078, 1775.3142, 1657.3820],\n",
      "        [ 317.7527, 1775.2975, 1779.5634],\n",
      "        [ 385.4949, 1780.2015, 2141.7202],\n",
      "        [ 448.7602, 1783.3573, 2499.8574],\n",
      "        [ 458.9299, 1784.5857, 2545.5200],\n",
      "        [ 266.4445, 1762.4844, 1506.7773],\n",
      "        [ 465.3674, 1766.1317, 2609.8997],\n",
      "        [ 363.2200, 1721.5707, 2166.2319],\n",
      "        [ 471.6000, 1744.2253, 2739.9453],\n",
      "        [ 515.9240, 1743.4043, 2959.9460],\n",
      "        [ 437.7140, 1706.6964, 2577.5049],\n",
      "        [ 403.1253, 1717.2921, 2354.8738],\n",
      "        [ 382.3468, 1702.5745, 2260.2419],\n",
      "        [ 329.2103, 1702.9374, 1929.6206],\n",
      "        [ 395.3884, 1650.2383, 2461.9946],\n",
      "        [ 405.9729, 1686.0748, 2408.6677],\n",
      "        [ 505.5742, 1667.9778, 3069.8042],\n",
      "        [ 390.1904, 1654.7347, 2379.5967],\n",
      "        [ 334.4113, 1653.2657, 2047.2778],\n",
      "        [ 425.8252, 1658.4769, 2560.1047],\n",
      "        [ 270.1185, 1640.8156, 1666.0876],\n",
      "        [ 319.9206, 1654.0924, 1901.6722],\n",
      "        [ 319.6592, 1636.8444, 1942.0563],\n",
      "        [ 428.3679, 1635.4814, 2601.9348],\n",
      "        [ 299.7021, 1624.2490, 1862.0123],\n",
      "        [ 427.3091, 1621.2328, 2625.8499],\n",
      "        [ 503.9235, 1609.1514, 3148.1296],\n",
      "        [ 444.2765, 1599.8461, 2780.6392],\n",
      "        [ 501.5841, 1585.0444, 3191.0925],\n",
      "        [ 373.9420, 1586.5248, 2374.6138],\n",
      "        [ 494.1099, 1600.6281, 3073.5913],\n",
      "        [ 387.8577, 1582.7935, 2458.9475],\n",
      "        [ 391.2773, 1568.9751, 2534.4944],\n",
      "        [ 552.3980, 1582.3745, 3470.4792],\n",
      "        [ 438.3456, 1560.4684, 2816.3494],\n",
      "        [ 429.1620, 1550.5374, 2764.4158],\n",
      "        [ 429.6716, 1535.5179, 2842.9753],\n",
      "        [ 427.8024, 1522.3306, 2870.6912],\n",
      "        [ 554.1786, 1571.9353, 3492.1091],\n",
      "        [ 425.6769, 1550.3418, 2706.1702],\n",
      "        [ 464.6398, 1524.9502, 3039.9854],\n",
      "        [ 315.8526, 1513.6340, 2084.1782],\n",
      "        [ 277.7357, 1505.2837, 1844.8159],\n",
      "        [ 413.7395, 1504.2727, 2740.2512],\n",
      "        [ 301.3372, 1475.8427, 2079.0750],\n",
      "        [ 233.4873, 1489.1193, 1567.4612],\n",
      "        [ 353.8708, 1455.5282, 2476.8052],\n",
      "        [ 303.4395, 1441.9666, 2145.8044],\n",
      "        [ 309.1292, 1474.1448, 2080.9248],\n",
      "        [ 419.8116, 1408.5374, 3054.6042],\n",
      "        [ 395.6686, 1407.2416, 2899.8770],\n",
      "        [ 315.8292, 1458.3320, 2114.3455],\n",
      "        [ 416.0830, 1430.2482, 2949.1650],\n",
      "        [ 427.2671, 1438.7772, 2955.4585],\n",
      "        [ 272.6145, 1424.0204, 1945.1176],\n",
      "        [ 249.9213, 1420.6914, 1748.0435],\n",
      "        [ 373.1326, 1407.4929, 2653.5896],\n",
      "        [ 405.3572, 1400.8741, 2896.0913],\n",
      "        [ 297.1691, 1389.4685, 2154.3511],\n",
      "        [ 270.1822, 1372.6395, 2020.2344],\n",
      "        [ 197.9055, 1384.3970, 1410.1465],\n",
      "        [ 429.4447, 1379.2527, 3108.2148],\n",
      "        [ 400.9579, 1381.0121, 2854.7632],\n",
      "        [ 291.1811, 1330.9382, 2227.1135],\n",
      "        [ 288.8566, 1335.2501, 2192.7720],\n",
      "        [ 271.9646, 1319.1975, 2094.9951],\n",
      "        [ 293.5159, 1327.9946, 2217.0320],\n",
      "        [ 290.5625, 1297.8329, 2303.6538],\n",
      "        [ 415.9445, 1308.6241, 3232.7471],\n",
      "        [ 421.7377, 1312.6456, 3217.5010],\n",
      "        [ 219.9413, 1319.7953, 1658.2216],\n",
      "        [ 429.9433, 1287.1759, 3369.9707],\n",
      "        [ 347.7003, 1319.4990, 2595.2693],\n",
      "        [ 385.8381, 1298.0955, 2989.7261],\n",
      "        [ 284.8946, 1282.4448, 2245.4148],\n",
      "        [ 370.5182, 1278.7908, 2931.8296],\n",
      "        [ 270.2019, 1294.3643, 2134.3911],\n",
      "        [ 264.6689, 1324.5093, 1958.9561],\n",
      "        [ 323.9150, 1343.6898, 2358.2070],\n",
      "        [ 378.5812, 1317.3151, 2908.5078],\n",
      "        [ 231.6635, 1341.3663, 1749.5197],\n",
      "        [ 256.4456, 1381.9561, 1848.9578],\n",
      "        [ 262.8275, 1384.7545, 1905.3615],\n",
      "        [ 240.2827, 1406.2417, 1746.2677],\n",
      "        [ 189.5176, 1417.2620, 1333.5768],\n",
      "        [ 219.4782, 1411.4865, 1564.8125],\n",
      "        [ 255.6555, 1431.6670, 1790.4901],\n",
      "        [ 163.6198, 1432.0411, 1145.8806],\n",
      "        [ 299.0634, 1417.6808, 2132.4219],\n",
      "        [ 122.8274, 1452.0819,  849.4526],\n",
      "        [ 255.7261, 1459.6781, 1763.0911],\n",
      "        [ 256.3222, 1473.9435, 1751.5828],\n",
      "        [ 275.4835, 1487.1309, 1867.4460],\n",
      "        [ 222.6847, 1515.1086, 1494.6653]])\n",
      "data:  tensor([[[0.0052, 0.0052, 0.0049,  ..., 0.5744, 0.0127, 0.6522]],\n",
      "\n",
      "        [[0.0041, 0.0042, 0.0042,  ..., 0.5712, 0.0127, 0.6522]],\n",
      "\n",
      "        [[0.0043, 0.0038, 0.0040,  ..., 0.5683, 0.0127, 0.6957]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0028, 0.0029, 0.0030,  ..., 0.4034, 0.0169, 0.1739]],\n",
      "\n",
      "        [[0.0032, 0.0031, 0.0031,  ..., 0.4080, 0.0169, 0.1739]],\n",
      "\n",
      "        [[0.0028, 0.0028, 0.0029,  ..., 0.4144, 0.0169, 0.1739]]])\n",
      "output:  tensor([[-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867],\n",
      "        [-0.4187,  3.7766,  2.7867]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 218.8493, 1552.4531, 1402.2224],\n",
      "        [ 172.1786, 1556.1173, 1111.5009],\n",
      "        [ 170.0005, 1594.4950, 1064.7667],\n",
      "        [ 202.9805, 1598.8073, 1276.8473],\n",
      "        [ 193.6377, 1595.5452, 1221.8950],\n",
      "        [ 197.7595, 1612.7485, 1230.9736],\n",
      "        [ 183.0828, 1608.4478, 1146.6206],\n",
      "        [ 291.3569, 1587.6976, 1848.4940],\n",
      "        [ 235.5804, 1564.8921, 1504.7792],\n",
      "        [ 248.3245, 1500.1451, 1672.1962],\n",
      "        [ 195.7455, 1451.0151, 1347.9751],\n",
      "        [ 140.2490, 1378.4539, 1006.1014],\n",
      "        [ 143.5441, 1273.5304, 1126.9324],\n",
      "        [ 133.3865, 1162.4464, 1139.2239],\n",
      "        [  99.9299, 1047.9717,  956.8118],\n",
      "        [  76.0377,  952.3872,  792.3099],\n",
      "        [ 112.4079,  826.3553, 1375.1653],\n",
      "        [  95.0691,  745.7184, 1270.8104],\n",
      "        [  64.7397,  651.0052, 1004.2056],\n",
      "        [  42.7174,  607.1088,  712.4420],\n",
      "        [  51.0454,  590.0676,  857.1332],\n",
      "        [  48.6450,  593.2847,  811.1910],\n",
      "        [  59.3948,  572.5348, 1045.1479],\n",
      "        [  53.9978,  563.9610,  953.5193],\n",
      "        [  34.8153,  572.7357,  611.3795],\n",
      "        [  54.6374,  597.7197,  915.1196],\n",
      "        [  53.3505,  623.2006,  849.9968],\n",
      "        [  63.0091,  613.9734, 1046.8352],\n",
      "        [  59.3995,  639.5717,  949.8643],\n",
      "        [  53.2518,  639.1696,  841.1311],\n",
      "        [  60.4178,  646.3860,  941.2805],\n",
      "        [  54.7818,  638.2088,  866.5844],\n",
      "        [  82.2213,  662.6007, 1253.5670],\n",
      "        [  57.9292,  677.3686,  895.7166],\n",
      "        [  66.5306,  691.6229,  992.8263],\n",
      "        [  55.0716,  736.2397,  739.8273],\n",
      "        [  61.4719,  753.2587,  823.0457],\n",
      "        [  59.4014,  790.7654,  755.0024],\n",
      "        [  71.3009,  805.1815,  901.1356],\n",
      "        [  72.1030,  859.1541,  832.9144],\n",
      "        [  75.4387,  892.1588,  846.3649],\n",
      "        [  55.5043,  933.5808,  590.9097],\n",
      "        [ 101.7426,  974.7067, 1053.2323],\n",
      "        [  64.9683, 1028.9419,  635.1017],\n",
      "        [  88.9987, 1097.5819,  800.4753],\n",
      "        [  78.8237, 1143.1652,  685.6454],\n",
      "        [  95.7908, 1177.3705,  819.6783],\n",
      "        [  96.5770, 1222.2111,  787.4391],\n",
      "        [  84.4906, 1257.5560,  671.0596],\n",
      "        [  84.2151, 1279.9985,  656.4986],\n",
      "        [ 118.2662, 1307.4688,  899.9312],\n",
      "        [  96.0113, 1329.0280,  727.8209],\n",
      "        [ 127.5985, 1366.2609,  935.5296],\n",
      "        [ 107.4257, 1392.2446,  788.6142],\n",
      "        [ 111.9598, 1433.2812,  781.4899],\n",
      "        [  98.5332, 1448.1832,  675.4946],\n",
      "        [ 135.0253, 1450.3446,  936.9205],\n",
      "        [  92.7518, 1498.9777,  625.8051],\n",
      "        [  98.6061, 1577.6158,  631.5902],\n",
      "        [ 139.1586, 1650.7802,  844.8282],\n",
      "        [ 172.0933, 1885.9406,  921.9478],\n",
      "        [ 148.5106, 1841.6698,  809.5825],\n",
      "        [ 180.5290, 1839.7876,  983.8315],\n",
      "        [ 141.8399, 1733.9592,  821.5402],\n",
      "        [ 111.5258, 1777.1853,  626.9665],\n",
      "        [ 183.6980, 2000.0800,  928.4301],\n",
      "        [ 191.6428, 2110.3989,  911.4998],\n",
      "        [ 169.7653, 1983.4799,  864.1508],\n",
      "        [ 125.7733, 1820.4003,  689.3354],\n",
      "        [ 140.3507, 1821.7240,  772.1700],\n",
      "        [ 153.9301, 1936.4110,  799.7910],\n",
      "        [ 153.1563, 1824.5728,  838.5930],\n",
      "        [ 126.2881, 1867.8491,  670.6638],\n",
      "        [ 149.4593, 2052.4104,  731.2164],\n",
      "        [ 175.5337, 1938.7402,  906.4889],\n",
      "        [ 161.4865, 1896.8379,  852.9158],\n",
      "        [ 123.9733, 1893.3524,  656.9529],\n",
      "        [ 173.2563, 1901.7195,  914.5732],\n",
      "        [ 165.3129, 1925.1619,  861.1577],\n",
      "        [ 180.5847, 1939.6115,  935.4139],\n",
      "        [ 149.9013, 1980.8099,  757.8035],\n",
      "        [ 138.1664, 1993.9750,  693.5616],\n",
      "        [ 126.4965, 2016.6465,  625.9907],\n",
      "        [ 148.0120, 2014.5483,  740.7194],\n",
      "        [ 173.0988, 2046.7748,  845.4073],\n",
      "        [ 154.9369, 2048.6345,  757.7054],\n",
      "        [ 186.9707, 2019.2941,  924.5352],\n",
      "        [ 189.6357, 2091.3245,  905.0613],\n",
      "        [ 276.6708, 2085.4763, 1376.1643],\n",
      "        [ 245.2090, 2150.7598, 1124.8867],\n",
      "        [ 222.7741, 2157.1104, 1033.3927],\n",
      "        [ 284.6036, 2086.0967, 1328.4814],\n",
      "        [ 214.4552, 2154.7031, 1020.7836],\n",
      "        [ 235.5974, 2176.4194, 1049.6642],\n",
      "        [ 240.3698, 2173.7607, 1107.5385],\n",
      "        [ 242.1749, 2155.8650, 1134.4042],\n",
      "        [ 265.7229, 2178.2961, 1194.4923],\n",
      "        [ 226.8518, 2147.2520, 1058.5396],\n",
      "        [ 213.2467, 2055.6724, 1018.4963],\n",
      "        [ 241.8179, 2120.1792, 1141.3325],\n",
      "        [ 160.3770, 2116.0347,  746.8911],\n",
      "        [ 195.7038, 2098.8203,  938.0175],\n",
      "        [ 319.8934, 2051.9133, 1566.3141],\n",
      "        [ 172.4705, 2024.0919,  846.9354],\n",
      "        [ 472.2391, 2030.3757, 2170.1721],\n",
      "        [ 230.8979, 2007.6539, 1152.1310],\n",
      "        [ 241.4867, 1997.7396, 1166.8433],\n",
      "        [ 194.4645, 1956.3623,  994.0412],\n",
      "        [ 437.9867, 1911.8237, 2340.2134],\n",
      "        [ 268.2694, 1909.1370, 1413.6350],\n",
      "        [ 239.1263, 1931.3337, 1236.5327],\n",
      "        [ 252.0727, 1948.6935, 1290.8372],\n",
      "        [ 206.2920, 1971.6218, 1038.4498],\n",
      "        [ 326.9610, 1980.5029, 1685.2476],\n",
      "        [ 238.8123, 2016.3673, 1192.8241],\n",
      "        [ 369.6925, 2101.8931, 1659.5391],\n",
      "        [ 255.7678, 2133.8469, 1229.2410],\n",
      "        [ 290.2949, 2200.4094, 1321.5068],\n",
      "        [ 348.5243, 2244.6965, 1552.4939],\n",
      "        [ 369.5853, 2250.4441, 1631.6091],\n",
      "        [ 325.1192, 2294.9604, 1413.1078],\n",
      "        [ 255.3989, 2288.6152, 1114.9240],\n",
      "        [ 246.7938, 2327.0881, 1050.8057],\n",
      "        [ 266.9677, 2351.3962, 1134.6400],\n",
      "        [ 358.6862, 2349.5867, 1530.2805],\n",
      "        [ 378.1967, 2369.0405, 1604.2135],\n",
      "        [ 447.5153, 2405.5979, 1837.6837],\n",
      "        [ 323.2526, 2393.3545, 1348.7990],\n",
      "        [ 710.4690, 2388.5620, 2869.4460],\n",
      "        [ 475.5421, 2335.9077, 2060.0552],\n",
      "        [ 403.7069, 2318.1567, 1738.0575],\n",
      "        [ 391.2264, 2343.9785, 1624.4138],\n",
      "        [ 314.2368, 2334.6340, 1340.7338],\n",
      "        [ 423.7135, 2300.1438, 1843.0737],\n",
      "        [ 358.8849, 2282.6052, 1586.8284],\n",
      "        [ 419.1283, 2284.2585, 1845.1067],\n",
      "        [ 442.0051, 2273.8582, 1938.7637],\n",
      "        [ 541.2080, 2231.6711, 2480.4551],\n",
      "        [ 449.5110, 2239.8872, 2002.2462],\n",
      "        [ 306.2926, 2203.9167, 1402.4875],\n",
      "        [ 243.8342, 2197.7449, 1116.3962],\n",
      "        [ 435.0563, 2187.3335, 1996.9183],\n",
      "        [ 415.6808, 2142.0632, 1969.4199],\n",
      "        [ 438.1710, 2139.2480, 2050.3999],\n",
      "        [ 556.2273, 2122.0281, 2663.3965],\n",
      "        [ 257.6844, 2094.0615, 1231.6826],\n",
      "        [ 520.1278, 2120.7153, 2389.9395],\n",
      "        [ 560.5740, 2075.8306, 2647.9304],\n",
      "        [ 606.4122, 2021.9806, 3001.6541],\n",
      "        [ 377.1109, 2015.4514, 1892.6334]])\n",
      "data:  tensor([[[0.0023, 0.0025, 0.0024,  ..., 0.4215, 0.0169, 0.1739]],\n",
      "\n",
      "        [[0.0022, 0.0020, 0.0021,  ..., 0.4288, 0.0169, 0.1739]],\n",
      "\n",
      "        [[0.0020, 0.0019, 0.0020,  ..., 0.4368, 0.0169, 0.1739]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0065, 0.0068, 0.0070,  ..., 0.5606, 0.0169, 0.6957]],\n",
      "\n",
      "        [[0.0074, 0.0070, 0.0065,  ..., 0.5561, 0.0169, 0.6957]],\n",
      "\n",
      "        [[0.0046, 0.0050, 0.0050,  ..., 0.5519, 0.0169, 0.6957]]])\n",
      "output:  tensor([[-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966],\n",
      "        [-0.3858,  3.9103,  2.8966]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 413.4023, 2037.1788, 1994.4686],\n",
      "        [ 382.4987, 2007.7992, 1895.5172],\n",
      "        [ 389.2236, 1977.5425, 1971.3661],\n",
      "        [ 437.9699, 1971.5826, 2202.1628],\n",
      "        [ 401.1689, 1971.6943, 2031.2177],\n",
      "        [ 399.3888, 1967.3824, 2013.2686],\n",
      "        [ 485.4603, 1984.5188, 2375.3062],\n",
      "        [ 375.2876, 1952.8937, 1927.7278],\n",
      "        [ 584.0803, 1921.0845, 3062.0352],\n",
      "        [ 382.0593, 1933.4841, 1997.6840],\n",
      "        [ 461.9735, 1968.8180, 2333.1223],\n",
      "        [ 463.9279, 1974.4089, 2253.3843],\n",
      "        [ 396.3037, 1962.5344, 2023.1318],\n",
      "        [ 491.3352, 1935.1263, 2579.2083],\n",
      "        [ 513.8444, 1952.5474, 2644.3455],\n",
      "        [ 355.6786, 1951.7432, 1840.2870],\n",
      "        [ 556.1927, 1946.3699, 2882.2878],\n",
      "        [ 529.3385, 1948.9448, 2722.8669],\n",
      "        [ 447.0322, 1943.0968, 2309.5586],\n",
      "        [ 419.0868, 1903.4733, 2239.0369],\n",
      "        [ 392.7006, 1930.9987, 2019.0776],\n",
      "        [ 375.8728, 1911.1422, 1965.2527],\n",
      "        [ 522.7406, 1899.8427, 2760.3838],\n",
      "        [ 488.7697, 1919.2021, 2515.5425],\n",
      "        [ 286.8722, 1890.9395, 1542.3671],\n",
      "        [ 493.3697, 1863.1685, 2684.7542],\n",
      "        [ 615.7570, 1854.9913, 3444.4055],\n",
      "        [ 523.5018, 1893.0564, 2754.4666],\n",
      "        [ 528.0432, 1885.1473, 2722.5015],\n",
      "        [ 273.9282, 1828.7227, 1532.6920],\n",
      "        [ 320.9811, 1833.3196, 1763.1995],\n",
      "        [ 499.2929, 1838.5186, 2720.3291],\n",
      "        [ 334.6925, 1892.5817, 1719.1772],\n",
      "        [ 359.3697, 1809.0172, 2006.8063],\n",
      "        [ 370.0471, 1802.4988, 2055.2412],\n",
      "        [ 443.0340, 1800.1584, 2466.7737],\n",
      "        [ 493.6157, 1798.4382, 2739.0317],\n",
      "        [ 383.3200, 1775.8002, 2164.1926],\n",
      "        [ 553.6082, 1791.0150, 3080.6528],\n",
      "        [ 547.8640, 1758.1835, 3129.2358],\n",
      "        [ 415.1109, 1768.6453, 2332.6611],\n",
      "        [ 284.7878, 1738.1763, 1641.0708],\n",
      "        [ 420.8377, 1711.0253, 2512.4092],\n",
      "        [ 372.0952, 1739.3997, 2145.2898],\n",
      "        [ 492.1394, 1744.6274, 2822.6875],\n",
      "        [ 251.3473, 1718.1523, 1488.4585],\n",
      "        [ 470.9842, 1713.4214, 2769.8582],\n",
      "        [ 454.7614, 1704.0602, 2691.1792],\n",
      "        [ 495.2021, 1692.8668, 2962.3215],\n",
      "        [ 339.8008, 1700.7646, 1973.1975],\n",
      "        [ 306.0953, 1703.8870, 1781.3812],\n",
      "        [ 339.8971, 1685.0135, 2032.5400],\n",
      "        [ 220.3845, 1685.5778, 1289.4286],\n",
      "        [ 310.3895, 1691.8894, 1798.1785],\n",
      "        [ 427.7949, 1642.4410, 2664.4397],\n",
      "        [ 297.3241, 1655.8295, 1804.6096],\n",
      "        [ 322.8176, 1642.6141, 1992.7375],\n",
      "        [ 453.4205, 1656.6283, 2730.8237],\n",
      "        [ 485.0882, 1654.8130, 2914.6157],\n",
      "        [ 423.4515, 1601.8234, 2671.8965],\n",
      "        [ 408.4597, 1644.9377, 2457.2546],\n",
      "        [ 439.2180, 1619.8478, 2700.4575],\n",
      "        [ 319.4611, 1615.1223, 2004.8210],\n",
      "        [ 331.0759, 1609.4083, 2077.5518],\n",
      "        [ 313.5773, 1608.2522, 1976.5012],\n",
      "        [ 486.2124, 1590.5627, 3083.6912],\n",
      "        [ 309.6973, 1606.5765, 1921.1222],\n",
      "        [ 308.3221, 1599.6616, 1932.0710],\n",
      "        [ 467.2265, 1569.3435, 3012.2422],\n",
      "        [ 204.6050, 1591.4398, 1272.5535],\n",
      "        [ 374.2396, 1563.4175, 2434.7329],\n",
      "        [ 307.9381, 1566.0315, 1986.0427],\n",
      "        [ 323.3648, 1584.3073, 2001.2903],\n",
      "        [ 397.2654, 1549.8615, 2612.5771],\n",
      "        [ 414.3512, 1537.2717, 2740.9084],\n",
      "        [ 378.7245, 1534.6969, 2513.0879],\n",
      "        [ 260.5787, 1541.9022, 1689.9143],\n",
      "        [ 384.9184, 1541.7793, 2477.2266],\n",
      "        [ 271.0000, 1526.2850, 1762.9056],\n",
      "        [ 463.3303, 1539.6121, 2987.2014],\n",
      "        [ 429.0401, 1497.0507, 2892.7917],\n",
      "        [ 201.5398, 1488.6278, 1368.0521],\n",
      "        [ 319.1408, 1504.9205, 2080.3521],\n",
      "        [ 360.5160, 1456.2318, 2529.7290],\n",
      "        [ 212.4023, 1464.0740, 1431.5690],\n",
      "        [ 396.1871, 1450.1884, 2753.6516],\n",
      "        [ 271.9644, 1425.0259, 1968.8574],\n",
      "        [ 367.5694, 1430.2705, 2586.4824],\n",
      "        [ 361.5229, 1406.6049, 2600.0420],\n",
      "        [ 265.6487, 1426.0869, 1830.0535],\n",
      "        [ 348.5043, 1422.1827, 2455.9314],\n",
      "        [ 325.0453, 1399.3550, 2340.8816],\n",
      "        [ 380.1064, 1391.8369, 2777.2766],\n",
      "        [ 462.4357, 1439.6542, 3143.2527],\n",
      "        [ 283.2005, 1423.8584, 1934.8077],\n",
      "        [ 345.1662, 1382.7716, 2488.3132],\n",
      "        [ 248.8759, 1351.0068, 1896.7936],\n",
      "        [ 487.6072, 1403.1194, 3419.5081],\n",
      "        [ 236.2979, 1336.6241, 1802.3591],\n",
      "        [ 272.0965, 1345.9856, 2016.9785],\n",
      "        [ 241.0806, 1343.7679, 1770.8809],\n",
      "        [ 369.2434, 1328.3760, 2797.1975],\n",
      "        [ 370.5993, 1323.1018, 2802.8975],\n",
      "        [ 313.4501, 1310.9143, 2381.3948],\n",
      "        [ 332.0400, 1314.3995, 2502.3892],\n",
      "        [ 356.8082, 1281.9591, 2872.6250],\n",
      "        [ 245.8545, 1285.8130, 1919.5343],\n",
      "        [ 284.6649, 1286.9357, 2215.2542],\n",
      "        [ 431.7951, 1260.4995, 3496.6809],\n",
      "        [ 289.8777, 1267.6433, 2271.2554],\n",
      "        [ 349.5185, 1245.3517, 2827.0659],\n",
      "        [ 255.1284, 1234.4434, 2127.8125],\n",
      "        [ 303.3827, 1245.7372, 2439.6887],\n",
      "        [ 317.4964, 1239.7942, 2565.9634],\n",
      "        [ 315.2555, 1232.3655, 2548.4229],\n",
      "        [ 291.2132, 1216.3632, 2444.4026],\n",
      "        [ 266.3657, 1207.0520, 2268.0227],\n",
      "        [ 253.5137, 1233.2870, 2023.5994],\n",
      "        [ 266.2168, 1240.4030, 2105.3936],\n",
      "        [ 321.1606, 1228.3940, 2636.5002],\n",
      "        [ 319.4795, 1223.1327, 2631.3093],\n",
      "        [ 247.3699, 1225.3556, 2004.6306],\n",
      "        [ 277.7188, 1231.6896, 2232.2854],\n",
      "        [ 257.6085, 1251.9369, 2057.8445],\n",
      "        [ 291.6074, 1258.5726, 2345.4226],\n",
      "        [ 169.6975, 1284.0536, 1313.2257],\n",
      "        [ 190.1616, 1304.5187, 1446.3585],\n",
      "        [ 252.6890, 1285.1204, 2011.0348],\n",
      "        [ 214.6924, 1309.9143, 1659.5488],\n",
      "        [ 210.4599, 1337.2498, 1576.9827],\n",
      "        [ 107.4151, 1352.3082,  799.9361],\n",
      "        [ 163.1769, 1367.8192, 1202.2014],\n",
      "        [ 184.3651, 1382.8442, 1330.6697],\n",
      "        [ 221.5789, 1389.7814, 1586.6029],\n",
      "        [ 226.7769, 1392.6187, 1643.4742],\n",
      "        [ 193.5214, 1403.1362, 1390.0242],\n",
      "        [ 169.9836, 1406.3059, 1234.1957],\n",
      "        [ 204.0350, 1435.1689, 1422.9390],\n",
      "        [ 169.3904, 1458.6057, 1164.8988],\n",
      "        [ 239.5459, 1440.5533, 1682.0323],\n",
      "        [ 218.4207, 1454.5786, 1525.3939],\n",
      "        [ 133.5395, 1466.7773,  915.0248],\n",
      "        [ 192.6763, 1472.7706, 1308.3169],\n",
      "        [ 248.5902, 1459.2480, 1714.4062],\n",
      "        [ 155.6584, 1437.4255, 1084.6381],\n",
      "        [ 209.8415, 1402.7174, 1495.7566],\n",
      "        [ 118.4655, 1352.7440,  881.1437],\n",
      "        [ 162.1423, 1294.1187, 1256.7424],\n",
      "        [ 136.6960, 1206.4600, 1141.8033],\n",
      "        [  89.3483, 1111.1268,  809.1476]])\n",
      "data:  tensor([[[0.0043, 0.0039, 0.0044,  ..., 0.5477, 0.0169, 0.6957]],\n",
      "\n",
      "        [[0.0045, 0.0049, 0.0049,  ..., 0.5435, 0.0169, 0.6957]],\n",
      "\n",
      "        [[0.0045, 0.0040, 0.0039,  ..., 0.5395, 0.0169, 0.7391]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0019, 0.0017, 0.0018,  ..., 0.4945, 0.0211, 0.2174]],\n",
      "\n",
      "        [[0.0018, 0.0019, 0.0017,  ..., 0.5037, 0.0211, 0.2174]],\n",
      "\n",
      "        [[0.0010, 0.0011, 0.0012,  ..., 0.5123, 0.0211, 0.2174]]])\n",
      "output:  tensor([[-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113],\n",
      "        [-0.3511,  4.0497,  3.0113]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[  104.9516,  1002.8689,  1052.2094],\n",
      "        [   79.6031,   904.7708,   870.6122],\n",
      "        [   69.4951,   797.5405,   878.4990],\n",
      "        [   88.8273,   664.5613,  1369.7101],\n",
      "        [   22.0560,   572.4006,   382.1761],\n",
      "        [   41.3929,   481.5470,   856.0020],\n",
      "        [   29.0293,   394.1844,   767.2930],\n",
      "        [   43.5401,   342.7867,  1263.0359],\n",
      "        [   25.7871,   282.0835,   953.7894],\n",
      "        [   22.0954,   277.2577,   790.1182],\n",
      "        [   20.0761,   278.9556,   708.8881],\n",
      "        [   36.3229,   287.8366,  1239.2825],\n",
      "        [   24.1779,   278.6317,   915.5172],\n",
      "        [   31.3029,   292.3383,  1090.7125],\n",
      "        [   28.2188,   305.1293,   925.5613],\n",
      "        [   39.4577,   341.6473,  1156.5754],\n",
      "        [   36.2479,   366.5418,   984.3198],\n",
      "        [   38.2027,   380.1089,  1014.4343],\n",
      "        [   23.7681,   406.9360,   583.0835],\n",
      "        [   44.1256,   417.3977,  1078.6530],\n",
      "        [   33.8512,   441.1360,   794.2408],\n",
      "        [   34.8759,   449.0292,   790.3391],\n",
      "        [   30.1840,   478.3186,   651.2393],\n",
      "        [   50.5592,   512.5465,   989.8516],\n",
      "        [   48.3583,   560.3248,   875.0233],\n",
      "        [   52.5881,   777.4105,   670.8152],\n",
      "        [   64.9704,   763.3517,   852.3620],\n",
      "        [   36.7649,   642.4482,   592.2410],\n",
      "        [   43.3470,   607.0641,   710.2695],\n",
      "        [   55.0478,   592.9274,   956.2158],\n",
      "        [   36.3898,   620.9497,   587.0287],\n",
      "        [   67.5370,   659.3163,  1029.8134],\n",
      "        [   55.8074,   686.4898,   827.4598],\n",
      "        [   66.3912,   731.3190,   912.9888],\n",
      "        [   47.0420,   782.0576,   604.8120],\n",
      "        [   66.1341,   818.5197,   797.8823],\n",
      "        [   89.1860,   864.2426,  1034.6746],\n",
      "        [   77.0977,   894.9348,   864.7737],\n",
      "        [   39.7506,   915.7072,   436.6677],\n",
      "        [   85.9384,   935.3346,   917.0229],\n",
      "        [   65.2278,   988.8659,   662.8161],\n",
      "        [   72.7588,  1004.4551,   715.1935],\n",
      "        [   82.6450,  1032.5892,   802.3635],\n",
      "        [   92.4989,  1015.9724,   915.7937],\n",
      "        [   58.2342,  1119.2872,   512.0083],\n",
      "        [  113.9017,  1219.5970,   933.4854],\n",
      "        [   98.3176,  1241.2463,   787.5930],\n",
      "        [   92.6785,  1378.8673,   671.8676],\n",
      "        [  134.1461,  1376.5885,   993.4413],\n",
      "        [  115.8953,  1319.4768,   885.6241],\n",
      "        [   80.1121,  1340.1542,   594.1442],\n",
      "        [  131.6794,  1411.8049,   925.6170],\n",
      "        [  152.2219,  1523.7269,  1026.0176],\n",
      "        [  190.2564,  1502.1725,  1279.4100],\n",
      "        [  161.1931,  1385.2640,  1138.7662],\n",
      "        [  103.6170,  1375.5271,   742.7183],\n",
      "        [  138.6973,  1466.4534,   907.7190],\n",
      "        [  149.8274,  1402.3430,  1124.4581],\n",
      "        [  174.0408,  1480.9958,  1186.5604],\n",
      "        [  291.9122,  2216.7383,  1300.4552],\n",
      "        [ 1859.8849,  6573.3179,  2145.7456],\n",
      "        [ 1316.1573,  6631.7534,  1495.0524],\n",
      "        [ 1019.6735,  6272.5562,  1368.3890],\n",
      "        [ 1500.6825,  6396.9336,  1804.5325],\n",
      "        [ 1209.0365,  7228.2109,  1493.2192],\n",
      "        [ 1144.6063,  7137.0112,  1371.9365],\n",
      "        [ 1126.8193,  7020.9224,  1534.9226],\n",
      "        [ 1692.4985, 11280.1885,  1503.5493],\n",
      "        [ 1471.0048, 11135.6807,  1322.1522],\n",
      "        [ 1812.8738, 10970.9648,  1653.2297],\n",
      "        [ 2027.4661, 10815.8164,  1877.6105],\n",
      "        [ 1838.2429, 10669.0635,  1723.1643],\n",
      "        [ 1886.3295, 10522.5615,  1796.2426],\n",
      "        [ 1754.1637, 10384.5771,  1692.9620],\n",
      "        [ 1317.6904, 10256.4131,  1286.8483],\n",
      "        [ 1519.0443, 10119.8193,  1503.6208],\n",
      "        [ 1520.2164,  9993.2744,  1522.9017],\n",
      "        [ 1438.3549,  9873.0244,  1459.2091],\n",
      "        [ 1257.3616,  9754.5010,  1289.1459],\n",
      "        [ 1487.1582,  9626.5537,  1547.0442],\n",
      "        [ 1336.6343,  9511.0566,  1407.8523],\n",
      "        [ 2092.6655,  9371.5820,  2246.6104],\n",
      "        [ 1693.8544,  9306.8906,  1805.6235],\n",
      "        [ 1771.0168,  9180.5127,  1928.8304],\n",
      "        [ 1561.8528,  9064.7256,  1726.6967],\n",
      "        [ 1570.6102,  8981.9658,  1741.6958],\n",
      "        [ 1833.8031,  8853.5215,  2073.1011],\n",
      "        [ 1682.0951,  8751.1006,  1926.2500],\n",
      "        [ 1584.0519,  8648.1709,  1832.6331],\n",
      "        [ 1900.7532,  8508.7295,  2257.8823],\n",
      "        [ 1838.5851,  8444.7588,  2184.7578],\n",
      "        [ 1621.6483,  8336.7412,  1949.8545],\n",
      "        [ 1739.0607,  8253.8857,  2113.1807],\n",
      "        [ 1602.6609,  8155.5312,  1968.0282],\n",
      "        [ 1804.3202,  8045.2065,  2251.4070],\n",
      "        [ 1487.8859,  7953.2856,  1870.6047],\n",
      "        [ 2146.0781,  7772.9248,  2863.9548],\n",
      "        [ 1898.0491,  7763.8540,  2445.0908],\n",
      "        [ 1596.0104,  7680.8257,  2078.8391],\n",
      "        [ 2269.3738,  7557.8555,  3013.0762],\n",
      "        [ 1308.6460,  7366.4019,  1760.7340],\n",
      "        [ 1398.4395,  7422.1284,  1879.1949],\n",
      "        [ 1509.3274,  7316.1045,  2068.7512],\n",
      "        [ 1802.8237,  7227.7588,  2499.9204],\n",
      "        [ 2035.7343,  7150.0200,  2855.8779],\n",
      "        [ 2105.7549,  7071.7949,  2958.4788],\n",
      "        [ 1896.3433,  7005.7632,  2707.7405],\n",
      "        [ 1396.2606,  6923.1387,  2015.3357],\n",
      "        [ 1829.1042,  6842.6333,  2675.4065],\n",
      "        [ 1428.5607,  6764.9673,  2118.0020],\n",
      "        [ 1759.9407,  6409.8418,  2832.1077],\n",
      "        [ 1838.8811,  6633.2949,  2774.0479],\n",
      "        [ 1406.2202,  6535.9453,  2161.0540],\n",
      "        [ 1723.2821,  6509.3750,  2640.0483],\n",
      "        [ 1325.6615,  6425.1797,  2065.3337],\n",
      "        [ 1131.1072,  6385.8184,  1763.7365],\n",
      "        [ 1016.0054,  6320.5020,  1605.9287],\n",
      "        [ 1250.8793,  6267.7246,  1992.5400],\n",
      "        [ 1225.1675,  6226.1572,  1951.0844],\n",
      "        [ 1042.7881,  6139.4204,  1700.3087],\n",
      "        [ 1885.5636,  6074.1479,  3113.3955],\n",
      "        [ 1621.0101,  6027.9561,  2681.8425],\n",
      "        [ 1294.3499,  5980.3345,  2158.5540],\n",
      "        [ 1098.2344,  5941.7554,  1842.1169],\n",
      "        [ 1378.4377,  5878.0806,  2348.4585],\n",
      "        [ 1599.0472,  5820.7847,  2754.6106],\n",
      "        [ 1497.9080,  5562.0981,  3066.7808],\n",
      "        [ 2352.5608,  5725.4570,  4109.5698],\n",
      "        [ 1199.8298,  5692.6646,  2121.0850],\n",
      "        [ 1110.0099,  5654.9795,  1979.0449],\n",
      "        [ 1317.6053,  5605.8496,  2360.6462],\n",
      "        [ 1412.0710,  5552.6309,  2570.3567],\n",
      "        [ 1584.9215,  5544.5376,  2869.4094],\n",
      "        [ 1291.1403,  5509.5054,  2343.4622],\n",
      "        [ 1916.4110,  5481.1812,  3467.4880],\n",
      "        [ 1648.9619,  5459.7554,  3008.1714],\n",
      "        [ 1339.1212,  5364.9751,  2551.3442],\n",
      "        [ 1423.8889,  5361.6016,  2659.7693],\n",
      "        [ 1847.6061,  5332.8975,  3452.7866],\n",
      "        [ 1436.5593,  5284.7002,  2714.0576],\n",
      "        [ 1126.8381,  5231.0820,  2175.6797],\n",
      "        [  962.9646,  5196.1592,  1877.2439],\n",
      "        [  981.5562,  5193.4448,  1895.4360],\n",
      "        [ 1097.6759,  5140.2988,  2140.5913],\n",
      "        [ 1820.1477,  5140.5000,  3540.7312],\n",
      "        [ 1119.1702,  5106.9536,  2187.1614],\n",
      "        [ 1145.3236,  5041.6646,  2281.4316],\n",
      "        [ 1679.4716,  4988.1499,  3388.5068],\n",
      "        [ 1498.9366,  5063.5205,  2926.7180],\n",
      "        [ 1464.9377,  4946.8564,  2976.4102]])\n",
      "data:  tensor([[[1.2352e-03, 1.2227e-03, 1.1272e-03,  ..., 5.2071e-01,\n",
      "          2.1097e-02, 2.1739e-01]],\n",
      "\n",
      "        [[9.1997e-04, 9.3334e-04, 9.9310e-04,  ..., 5.2976e-01,\n",
      "          2.1097e-02, 2.1739e-01]],\n",
      "\n",
      "        [[7.2623e-04, 6.8670e-04, 6.1874e-04,  ..., 5.3866e-01,\n",
      "          2.1097e-02, 2.1739e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.9399e-02, 1.9261e-02, 2.0834e-02,  ..., 7.8761e-01,\n",
      "          7.1730e-02, 7.3913e-01]],\n",
      "\n",
      "        [[1.7628e-02, 1.8905e-02, 1.9085e-02,  ..., 7.8648e-01,\n",
      "          7.1730e-02, 7.3913e-01]],\n",
      "\n",
      "        [[1.6997e-02, 1.5392e-02, 1.5290e-02,  ..., 7.8632e-01,\n",
      "          7.1730e-02, 7.3913e-01]]])\n",
      "Epoch 0, Iteration 10 Train Loss: 1215.61\n",
      "output:  tensor([[-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312],\n",
      "        [-0.3144,  4.1952,  3.1312]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[1164.3560, 4976.8672, 2338.2407],\n",
      "        [1352.9121, 4942.0640, 2733.3101],\n",
      "        [1341.0687, 4931.9937, 2669.5645],\n",
      "        [1072.4435, 4913.8242, 2154.1382],\n",
      "        [1325.2394, 4861.7896, 2724.4514],\n",
      "        [1272.6875, 4851.5234, 2605.7842],\n",
      "        [ 958.8511, 4841.9834, 1971.3837],\n",
      "        [1473.2905, 4753.6880, 3138.1812],\n",
      "        [1343.9558, 4757.2959, 2870.5759],\n",
      "        [1238.8729, 4787.8486, 2572.9292],\n",
      "        [1168.6873, 4717.5107, 2499.5635],\n",
      "        [ 957.8416, 4733.0381, 2028.3418],\n",
      "        [ 982.0760, 4708.9536, 2080.6094],\n",
      "        [2142.3750, 4606.5215, 4678.2563],\n",
      "        [1098.5074, 4643.6147, 2375.6631],\n",
      "        [1136.1627, 4609.0459, 2475.5559],\n",
      "        [1127.7566, 4595.7192, 2467.2358],\n",
      "        [1203.9614, 4535.8647, 2677.3518],\n",
      "        [1117.7115, 4552.7500, 2460.2063],\n",
      "        [ 823.5560, 4537.1831, 1824.0652],\n",
      "        [1282.0400, 4474.8711, 2885.0852],\n",
      "        [ 885.1002, 4468.4146, 1992.7097],\n",
      "        [1003.6593, 4467.6714, 2238.5356],\n",
      "        [1127.7283, 4271.6577, 2889.8386],\n",
      "        [ 894.8042, 4452.2334, 1995.0972],\n",
      "        [ 944.5485, 4275.9385, 2384.9526],\n",
      "        [ 807.0474, 4403.9355, 1826.9888],\n",
      "        [1093.2767, 4404.0977, 2471.1042],\n",
      "        [1506.4368, 4319.0864, 3512.9380],\n",
      "        [1161.2797, 4331.0059, 2695.2585],\n",
      "        [1008.9034, 4306.4072, 2335.0144],\n",
      "        [1370.5348, 4272.4531, 3212.7993],\n",
      "        [ 765.3552, 4273.8774, 1793.1747],\n",
      "        [ 796.3812, 4259.3608, 1858.8273],\n",
      "        [ 984.1115, 4206.0024, 2362.1216],\n",
      "        [1002.8073, 4218.0112, 2361.9631],\n",
      "        [1054.6729, 4180.4658, 2488.3308],\n",
      "        [1064.0120, 4136.9712, 2573.7490],\n",
      "        [ 908.5411, 3966.0776, 2386.4551],\n",
      "        [ 965.1699, 4077.4243, 2378.2751],\n",
      "        [1109.6953, 4060.6736, 2740.2048],\n",
      "        [1160.8879, 4058.4282, 2855.1799],\n",
      "        [ 784.9405, 4025.6304, 1931.5255],\n",
      "        [1134.2070, 3995.6531, 2861.0950],\n",
      "        [1021.7772, 3998.1440, 2549.5271],\n",
      "        [1065.9795, 3980.3599, 2678.2441],\n",
      "        [ 909.4009, 3955.8564, 2315.2595],\n",
      "        [ 847.7672, 3933.0173, 2178.8547],\n",
      "        [ 803.7856, 3933.9836, 2013.0837],\n",
      "        [1247.4557, 3948.1875, 3117.3691],\n",
      "        [1256.6282, 3882.2732, 3240.6545],\n",
      "        [1011.0890, 3822.2180, 2665.6316],\n",
      "        [ 895.2274, 3861.2661, 2315.8696],\n",
      "        [ 727.2578, 3823.7092, 1919.5488],\n",
      "        [1005.0692, 3799.9038, 2653.3970],\n",
      "        [ 746.4194, 3789.9338, 1991.1992],\n",
      "        [1051.3645, 3807.2153, 2732.2617],\n",
      "        [ 939.0699, 3764.8494, 2507.9280],\n",
      "        [ 770.8252, 3761.2747, 2049.9148],\n",
      "        [ 949.8798, 3717.0151, 2584.2527],\n",
      "        [1300.8340, 3684.1389, 3555.1016],\n",
      "        [ 841.7840, 3732.1965, 2273.0859],\n",
      "        [1240.5920, 3645.2080, 3436.8557],\n",
      "        [1161.6910, 3661.4060, 3254.6372],\n",
      "        [ 958.2247, 3669.4268, 2624.1602],\n",
      "        [ 971.7202, 3691.5620, 2587.7568],\n",
      "        [ 915.9814, 3690.4226, 2452.4805],\n",
      "        [ 737.6047, 3624.7539, 2042.8295],\n",
      "        [1134.8838, 3520.7241, 3390.6985],\n",
      "        [ 763.7932, 3588.2639, 2153.7102],\n",
      "        [ 729.9537, 3612.1140, 2026.3597],\n",
      "        [ 881.0223, 3602.7251, 2444.4839],\n",
      "        [ 866.9044, 3581.8630, 2431.7439],\n",
      "        [ 897.5334, 3575.4678, 2516.2202],\n",
      "        [1078.1854, 3563.2578, 2988.0029],\n",
      "        [ 599.9622, 3538.3521, 1704.4250],\n",
      "        [1058.3732, 3508.2073, 3040.5698],\n",
      "        [ 885.1530, 3495.9749, 2564.5764],\n",
      "        [ 840.6046, 3531.4263, 2372.9839],\n",
      "        [ 989.0432, 3489.3953, 2851.0525],\n",
      "        [ 963.0638, 3521.0034, 2743.3174],\n",
      "        [ 766.4210, 3479.6765, 2215.7583],\n",
      "        [1117.5847, 3508.4138, 3180.8525],\n",
      "        [ 900.0422, 3481.8550, 2618.9373],\n",
      "        [1158.6868, 3507.5872, 3262.0737],\n",
      "        [ 871.2192, 3472.2424, 2557.4226],\n",
      "        [ 931.9124, 3464.8359, 2724.2371],\n",
      "        [ 684.3782, 3477.3474, 1995.4470],\n",
      "        [ 615.2266, 3479.0176, 1771.8129],\n",
      "        [ 607.4490, 3469.6228, 1744.7709],\n",
      "        [ 931.1202, 3486.9761, 2658.5703],\n",
      "        [ 858.5907, 3445.7336, 2508.2451],\n",
      "        [ 838.8357, 3469.0864, 2427.2007],\n",
      "        [ 663.8378, 3455.4409, 1933.1165],\n",
      "        [ 800.7885, 3503.7053, 2265.7290],\n",
      "        [ 955.9576, 3433.6575, 2814.1738],\n",
      "        [ 640.2128, 3434.0432, 1890.7328],\n",
      "        [ 801.7670, 3410.5449, 2375.4080],\n",
      "        [ 664.5833, 3310.8213, 1997.4329],\n",
      "        [ 641.2142, 3466.6008, 1818.3134],\n",
      "        [ 600.7436, 3445.3313, 1746.9847],\n",
      "        [ 434.8427, 3445.5659, 1260.7854],\n",
      "        [ 506.9842, 3289.1721, 1542.7614],\n",
      "        [ 707.2809, 3449.5540, 2021.8044],\n",
      "        [ 581.5953, 3405.4175, 1709.7538],\n",
      "        [ 651.8667, 3402.8257, 1925.8055],\n",
      "        [ 681.3129, 3396.2683, 2015.6091],\n",
      "        [ 534.2997, 3410.8689, 1562.6639],\n",
      "        [ 501.0187, 3420.1128, 1457.9950],\n",
      "        [ 493.5580, 3252.5034, 1982.2983],\n",
      "        [ 638.5846, 3409.7739, 1866.2789],\n",
      "        [ 540.1090, 3385.3711, 1612.4291],\n",
      "        [ 623.2076, 3395.5757, 1838.9886],\n",
      "        [ 708.3500, 3372.4688, 2113.2715],\n",
      "        [ 517.0800, 3361.0854, 1611.2988],\n",
      "        [ 700.5265, 3403.1384, 2059.8528],\n",
      "        [ 629.1379, 3372.6306, 1873.2855],\n",
      "        [ 687.0472, 3345.9934, 2073.8999],\n",
      "        [ 735.7665, 3314.5972, 2242.9841],\n",
      "        [ 433.6214, 3313.4299, 1354.5841],\n",
      "        [ 564.3477, 3365.4478, 1670.4802],\n",
      "        [ 470.8014, 3326.0979, 1425.5145],\n",
      "        [ 576.3476, 3342.9939, 1731.9991],\n",
      "        [ 364.1018, 3339.1331, 1098.2751],\n",
      "        [ 500.4982, 3354.9917, 1491.2435],\n",
      "        [ 552.0225, 3364.4143, 1624.1255],\n",
      "        [ 459.8277, 3332.8174, 1381.6682],\n",
      "        [ 462.9474, 3340.1677, 1383.9180],\n",
      "        [ 419.0187, 3324.1208, 1265.0067],\n",
      "        [ 369.2064, 3315.7200, 1114.3274],\n",
      "        [ 517.3302, 3312.5811, 1546.0953],\n",
      "        [ 523.9192, 3317.6301, 1531.6947],\n",
      "        [ 348.3197, 3185.4497, 1164.0846],\n",
      "        [ 328.2891, 3308.2634,  995.4855],\n",
      "        [ 391.4316, 3301.1584, 1196.4688],\n",
      "        [ 424.3529, 3302.9517, 1291.3604],\n",
      "        [ 445.5858, 3325.9023, 1339.1056],\n",
      "        [ 417.8595, 3313.8655, 1267.0597],\n",
      "        [ 422.5243, 3229.9492, 1384.9688],\n",
      "        [ 391.6490, 3321.9590, 1176.6047],\n",
      "        [ 310.8252, 3315.4241,  941.3554],\n",
      "        [ 312.2691, 3333.8506,  935.4499],\n",
      "        [ 245.9099, 3331.9624,  741.0740],\n",
      "        [ 438.3537, 3332.4319, 1316.3199],\n",
      "        [ 291.4768, 3358.6389,  865.9846],\n",
      "        [ 368.8593, 3360.9792, 1098.0217],\n",
      "        [ 425.8509, 3353.9470, 1274.4875],\n",
      "        [ 328.2569, 3381.8804,  968.5287],\n",
      "        [ 296.9371, 3381.5786,  879.4470],\n",
      "        [ 353.7559, 3390.8503, 1043.6973]])\n",
      "data:  tensor([[[0.0127, 0.0142, 0.0142,  ..., 0.7829, 0.0717, 0.7391]],\n",
      "\n",
      "        [[0.0167, 0.0164, 0.0163,  ..., 0.7829, 0.0717, 0.7391]],\n",
      "\n",
      "        [[0.0152, 0.0145, 0.0133,  ..., 0.7807, 0.0717, 0.7826]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0039, 0.0041, 0.0041,  ..., 0.7303, 0.0759, 0.2609]],\n",
      "\n",
      "        [[0.0035, 0.0037, 0.0033,  ..., 0.7331, 0.0759, 0.2609]],\n",
      "\n",
      "        [[0.0043, 0.0040, 0.0043,  ..., 0.7346, 0.0759, 0.2609]]])\n",
      "output:  tensor([[-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565],\n",
      "        [-0.2756,  4.3473,  3.2565]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 379.9185, 3378.1545, 1156.8583],\n",
      "        [ 396.2985, 3408.7966, 1157.7848],\n",
      "        [ 222.9678, 3423.7378,  656.3936],\n",
      "        [ 359.9637, 3428.6921, 1050.2010],\n",
      "        [ 305.9028, 3435.4060,  905.2111],\n",
      "        [ 349.9709, 3458.6694, 1014.8717],\n",
      "        [ 317.2002, 3466.0981,  915.7447],\n",
      "        [ 266.8351, 3479.8718,  767.5638],\n",
      "        [ 462.3827, 3463.0540, 1368.9866],\n",
      "        [ 308.4073, 3507.1125,  880.6890],\n",
      "        [ 250.4465, 3500.0522,  716.2045],\n",
      "        [ 360.6132, 3516.8647, 1025.0327],\n",
      "        [ 464.8698, 3507.3191, 1344.6699],\n",
      "        [ 205.1131, 3546.6130,  578.6926],\n",
      "        [ 385.3621, 3569.3237, 1083.3102],\n",
      "        [ 290.0655, 3580.6958,  814.4111],\n",
      "        [ 445.7018, 3646.9060, 1203.4976],\n",
      "        [ 363.8372, 3650.7603,  997.6667],\n",
      "        [ 296.6589, 3674.8560,  806.6916],\n",
      "        [ 605.2651, 3705.5928, 1538.3346],\n",
      "        [ 371.8388, 3693.1763, 1008.1782],\n",
      "        [ 385.6131, 3693.9248, 1047.9171],\n",
      "        [ 537.5973, 3719.4226, 1406.1093],\n",
      "        [ 407.7227, 3701.8171, 1102.9249],\n",
      "        [ 578.6180, 3728.1470, 1465.0071],\n",
      "        [ 493.2757, 3713.5354, 1303.8956],\n",
      "        [ 497.9202, 3708.4861, 1296.8597],\n",
      "        [ 362.0905, 3676.9561,  984.4662],\n",
      "        [ 218.9243, 3674.0068,  597.5012],\n",
      "        [ 445.6391, 3662.9197, 1222.1014],\n",
      "        [ 423.9177, 3692.3218, 1133.5535],\n",
      "        [ 588.0449, 3660.0266, 1739.9774],\n",
      "        [ 456.3939, 3709.7708, 1219.3912],\n",
      "        [ 331.5108, 3706.7825,  897.3651],\n",
      "        [ 565.7755, 3701.8730, 1546.8826],\n",
      "        [ 476.2315, 3730.3701, 1275.2803],\n",
      "        [ 394.6949, 3731.5486, 1062.1075],\n",
      "        [ 593.2568, 3698.0635, 1626.1282],\n",
      "        [ 524.8639, 3738.5308, 1401.0640],\n",
      "        [ 438.5629, 3734.2239, 1177.8137],\n",
      "        [ 375.0424, 3738.5249, 1006.3716],\n",
      "        [ 539.8141, 3765.1455, 1396.0117],\n",
      "        [ 530.2564, 3760.5764, 1389.0936],\n",
      "        [ 347.2916, 3765.0559,  921.7310],\n",
      "        [ 415.2919, 3777.7405, 1090.0035],\n",
      "        [ 400.0115, 3784.2141, 1054.4185],\n",
      "        [ 324.7340, 3790.7603,  859.1367],\n",
      "        [ 359.6577, 3785.4932,  954.3375],\n",
      "        [ 653.8171, 3794.2178, 1699.5536],\n",
      "        [ 327.4860, 3792.7488,  865.4643],\n",
      "        [ 332.5952, 3805.1768,  876.5536],\n",
      "        [ 339.1735, 3777.5115,  900.6997],\n",
      "        [ 390.8324, 3813.3928, 1019.8713],\n",
      "        [ 385.1195, 3817.9507, 1010.0219],\n",
      "        [ 457.0893, 3793.5310, 1211.1317],\n",
      "        [ 565.2521, 3772.6130, 1536.1183],\n",
      "        [ 313.7002, 3795.2288,  828.9633],\n",
      "        [ 396.9075, 3786.4875, 1050.1667],\n",
      "        [ 607.4633, 3762.4475, 1628.6440],\n",
      "        [ 319.9546, 3762.6487,  854.1337],\n",
      "        [ 589.0815, 3741.0049, 1589.1178],\n",
      "        [ 471.2816, 3729.9402, 1266.3132],\n",
      "        [ 399.7474, 3699.2700, 1106.5162],\n",
      "        [ 493.7068, 3729.6606, 1286.6738],\n",
      "        [ 354.7929, 3706.9446,  953.6771],\n",
      "        [ 411.0360, 3678.1289, 1118.9653],\n",
      "        [ 516.0591, 3671.4097, 1401.3079],\n",
      "        [ 412.2448, 3642.3650, 1133.3940],\n",
      "        [ 302.8453, 3649.0508,  827.8201],\n",
      "        [ 503.7326, 3654.0298, 1356.6012],\n",
      "        [ 406.7852, 3628.2283, 1120.2767],\n",
      "        [ 299.2041, 3598.4741,  836.0660],\n",
      "        [ 347.6092, 3583.5386,  971.7982],\n",
      "        [ 342.3465, 3566.1455,  961.0754],\n",
      "        [ 485.5478, 3407.0649, 1911.8755],\n",
      "        [ 634.1480, 3554.9299, 1758.1624],\n",
      "        [ 678.8491, 3485.6306, 1959.0284],\n",
      "        [ 538.1227, 3445.3425, 1613.7635],\n",
      "        [ 374.5005, 3475.4707, 1082.1080],\n",
      "        [ 721.6486, 3452.2351, 2073.7598],\n",
      "        [ 689.3112, 3443.9685, 1999.7120],\n",
      "        [ 815.7479, 3382.0757, 2433.9651],\n",
      "        [ 673.6091, 3403.4624, 1984.7712],\n",
      "        [ 796.4279, 3402.3120, 2330.2695],\n",
      "        [ 422.1273, 3372.8262, 1256.5242],\n",
      "        [ 587.2713, 3368.5698, 1747.0532],\n",
      "        [ 496.2831, 3350.8022, 1495.2946],\n",
      "        [ 563.9611, 3331.6553, 1695.6868],\n",
      "        [ 713.9931, 3300.0862, 2187.9460],\n",
      "        [ 373.1031, 3318.2000, 1118.0676],\n",
      "        [ 767.2065, 3284.1062, 2340.8267],\n",
      "        [ 672.1545, 3266.6179, 2055.7253],\n",
      "        [ 579.3453, 3262.2893, 1769.9598],\n",
      "        [ 681.2363, 3246.4265, 2092.4512],\n",
      "        [ 264.9062, 3224.9446,  820.1627],\n",
      "        [ 886.3356, 3212.3494, 2729.2917],\n",
      "        [ 810.2726, 3039.2996, 2920.9250],\n",
      "        [ 693.0262, 3161.7781, 2204.2986],\n",
      "        [ 816.8571, 3116.2622, 2672.5417],\n",
      "        [ 525.6208, 3150.5850, 1664.3979],\n",
      "        [ 857.7185, 3108.4202, 2772.1311],\n",
      "        [ 685.4402, 3113.7639, 2215.2822],\n",
      "        [ 649.8586, 3098.0032, 2113.0581],\n",
      "        [ 521.5021, 3051.5933, 1965.5139],\n",
      "        [ 821.2986, 3119.3118, 2601.1714],\n",
      "        [ 496.8946, 3059.5134, 1646.1742],\n",
      "        [ 705.1281, 3016.5388, 2389.4146],\n",
      "        [ 756.8615, 3035.9875, 2500.9185],\n",
      "        [ 801.1463, 3031.1726, 2660.9849],\n",
      "        [ 650.3776, 3016.6672, 2168.9741],\n",
      "        [ 943.1569, 2974.9661, 3190.3635],\n",
      "        [ 550.2134, 3023.0793, 1772.2645],\n",
      "        [ 813.3705, 2957.3716, 2791.7913],\n",
      "        [ 800.1190, 2905.6277, 2794.9358],\n",
      "        [ 755.4059, 2959.1589, 2542.7148],\n",
      "        [ 742.6161, 2956.3215, 2485.2400],\n",
      "        [ 937.8772, 2927.1150, 3141.9072],\n",
      "        [ 780.6661, 2959.6897, 2596.5847],\n",
      "        [ 584.3793, 2899.4109, 2016.0560],\n",
      "        [ 471.7563, 2875.0303, 1656.3241],\n",
      "        [ 543.3879, 2885.6428, 1881.4845],\n",
      "        [ 653.6487, 2879.2754, 2251.6191],\n",
      "        [ 587.4030, 2853.7832, 2063.3494],\n",
      "        [ 698.0266, 2872.6062, 2403.0491],\n",
      "        [ 568.3260, 2852.1858, 1976.9841],\n",
      "        [ 563.2871, 2839.9983, 1946.1024],\n",
      "        [ 726.2502, 2825.7158, 2572.6924],\n",
      "        [ 728.1848, 2804.7314, 2589.0022],\n",
      "        [ 788.6370, 2765.9568, 2863.8831],\n",
      "        [ 388.9394, 2777.6306, 1401.1796],\n",
      "        [ 683.8505, 2811.4563, 2395.7781],\n",
      "        [ 838.3471, 2756.7297, 3056.9717],\n",
      "        [ 766.1271, 2745.7651, 2791.8975],\n",
      "        [ 653.6398, 2769.2705, 2339.7480],\n",
      "        [ 429.6255, 2721.4014, 1590.4158],\n",
      "        [ 481.7912, 2752.1104, 1727.4440],\n",
      "        [ 444.4252, 2722.4236, 1634.8832],\n",
      "        [ 657.8399, 2667.0378, 2437.4392],\n",
      "        [ 523.2720, 2731.2207, 1890.3853],\n",
      "        [ 630.1691, 2723.8813, 2301.3374],\n",
      "        [ 505.5381, 2685.6768, 1887.2037],\n",
      "        [ 404.3504, 2676.4941, 1511.0497],\n",
      "        [ 692.2762, 2667.7808, 2599.5615],\n",
      "        [ 711.2760, 2649.2593, 2693.3889],\n",
      "        [ 501.0466, 2648.9966, 1902.2927],\n",
      "        [ 687.8552, 2682.9565, 2507.2615],\n",
      "        [ 751.9519, 2689.1284, 2680.9487],\n",
      "        [ 707.2447, 2642.0037, 2672.6836],\n",
      "        [ 778.7170, 2551.8425, 3108.7305],\n",
      "        [ 712.1293, 2639.7305, 2704.9529]])\n",
      "data:  tensor([[[0.0045, 0.0045, 0.0046,  ..., 0.7367, 0.0759, 0.2609]],\n",
      "\n",
      "        [[0.0046, 0.0049, 0.0047,  ..., 0.7401, 0.0759, 0.2609]],\n",
      "\n",
      "        [[0.0027, 0.0023, 0.0024,  ..., 0.7410, 0.0759, 0.2609]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0080, 0.0073, 0.0080,  ..., 0.7367, 0.0759, 0.7826]],\n",
      "\n",
      "        [[0.0090, 0.0097, 0.0095,  ..., 0.7367, 0.0759, 0.7826]],\n",
      "\n",
      "        [[0.0078, 0.0078, 0.0073,  ..., 0.7367, 0.0759, 0.7826]]])\n",
      "output:  tensor([[-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877],\n",
      "        [-0.2346,  4.5064,  3.3877]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 640.9943, 2637.1108, 2404.9641],\n",
      "        [ 473.8809, 2600.9949, 1849.0875],\n",
      "        [ 785.6793, 2647.8909, 2897.9841],\n",
      "        [ 387.9674, 2616.0088, 1479.2079],\n",
      "        [ 378.9526, 2600.3193, 1498.8627],\n",
      "        [ 606.8535, 2579.4463, 2378.9697],\n",
      "        [ 848.9084, 2580.8928, 3306.6165],\n",
      "        [ 724.3044, 2650.1809, 2684.3916],\n",
      "        [ 336.4122, 2607.8149, 1270.6519],\n",
      "        [ 622.1313, 2636.2898, 2316.6702],\n",
      "        [ 552.9870, 2595.2754, 2124.0198],\n",
      "        [ 679.1548, 2583.9536, 2567.7844],\n",
      "        [ 671.6015, 2557.5625, 2632.4119],\n",
      "        [ 805.7626, 2593.0247, 3085.4827],\n",
      "        [ 636.8863, 2521.2510, 2564.0203],\n",
      "        [ 657.5468, 2507.5107, 2856.5312],\n",
      "        [ 886.2413, 2580.9409, 3427.4407],\n",
      "        [ 718.3137, 2541.0068, 2833.5107],\n",
      "        [ 512.6305, 2499.4285, 2072.7917],\n",
      "        [ 632.2344, 2534.5947, 2486.1187],\n",
      "        [ 519.8812, 2534.8350, 2031.5657],\n",
      "        [ 477.7145, 2539.0239, 1785.7244],\n",
      "        [ 507.0749, 2515.9558, 1996.7279],\n",
      "        [ 265.7421, 2503.7683, 1053.3700],\n",
      "        [ 316.9601, 2480.0913, 1283.2585],\n",
      "        [ 323.6413, 2492.8655, 1311.6166],\n",
      "        [ 730.8192, 2526.1213, 2816.0461],\n",
      "        [ 418.0386, 2457.6492, 1741.5022],\n",
      "        [ 486.5302, 2482.3479, 1970.5997],\n",
      "        [ 585.2716, 2319.0674, 2650.0334],\n",
      "        [ 475.0755, 2463.5305, 1938.5796],\n",
      "        [ 359.4445, 2425.4541, 1506.3320],\n",
      "        [ 460.2459, 2419.7515, 1926.9089],\n",
      "        [ 538.4412, 2453.6497, 2124.3257],\n",
      "        [ 756.1581, 2406.6646, 3094.8474],\n",
      "        [ 660.2908, 2436.6812, 2700.4368],\n",
      "        [ 388.9387, 2415.9980, 1615.8243],\n",
      "        [ 555.7043, 2425.2698, 2267.3525],\n",
      "        [ 525.2964, 2407.8767, 2174.2471],\n",
      "        [ 676.6097, 2375.0898, 2885.2937],\n",
      "        [ 663.4064, 2423.0691, 2708.4167],\n",
      "        [ 619.4053, 2411.5017, 2571.3557],\n",
      "        [ 576.3595, 2401.5037, 2391.3186],\n",
      "        [ 448.5749, 2371.4536, 1903.8055],\n",
      "        [ 511.9689, 2391.6843, 2115.5562],\n",
      "        [ 494.1907, 2373.3806, 2016.8916],\n",
      "        [ 584.9196, 2368.3538, 2446.9167],\n",
      "        [ 407.4796, 2351.0051, 1734.1438],\n",
      "        [ 587.2014, 2340.6384, 2544.2844],\n",
      "        [ 400.4554, 2351.9155, 1694.8217],\n",
      "        [ 370.6671, 2343.4182, 1597.7054],\n",
      "        [ 502.8261, 2363.9746, 2107.5923],\n",
      "        [ 405.2109, 2334.0923, 1712.0880],\n",
      "        [ 468.6003, 2333.4612, 2000.6473],\n",
      "        [ 534.4611, 2341.7834, 2267.7739],\n",
      "        [ 666.7581, 2331.1377, 2843.0781],\n",
      "        [ 446.9566, 2324.8428, 1915.3234],\n",
      "        [ 497.5421, 2300.0154, 2182.1790],\n",
      "        [ 536.1943, 2315.8279, 2334.0361],\n",
      "        [ 752.5109, 2303.5898, 3297.9209],\n",
      "        [ 679.8115, 2317.5256, 2933.0891],\n",
      "        [ 399.8191, 2318.0676, 1731.3092],\n",
      "        [ 622.5206, 2329.5681, 2655.0850],\n",
      "        [ 492.9579, 2352.1558, 2049.5994],\n",
      "        [ 570.1235, 2315.5876, 2447.7898],\n",
      "        [ 572.0682, 2338.7898, 2418.0920],\n",
      "        [ 394.7018, 2339.1863, 1572.4426],\n",
      "        [ 731.6036, 2331.6179, 3087.7732],\n",
      "        [ 549.0125, 2266.4465, 2475.3989],\n",
      "        [ 457.8840, 2302.2214, 2008.5372],\n",
      "        [ 521.7515, 2274.7410, 2315.2910],\n",
      "        [ 251.3484, 2294.9099, 1104.1350],\n",
      "        [ 469.4030, 2319.3242, 1993.8322],\n",
      "        [ 766.7624, 2291.9219, 3377.6064],\n",
      "        [ 594.4998, 2255.1470, 2803.5195],\n",
      "        [ 398.5623, 2335.1926, 1676.6376],\n",
      "        [ 555.3019, 2289.3750, 2436.0505],\n",
      "        [ 445.4243, 2262.7488, 1985.4061],\n",
      "        [ 306.7271, 2298.2334, 1316.9453],\n",
      "        [ 708.6058, 2314.9563, 2967.0999],\n",
      "        [ 468.2996, 2251.6782, 2119.5530],\n",
      "        [ 338.6594, 2249.5894, 1510.1096],\n",
      "        [ 447.7719, 2259.0066, 1993.6146],\n",
      "        [ 644.6688, 2256.9861, 2841.5486],\n",
      "        [ 319.3111, 2219.5396, 1471.8757],\n",
      "        [ 463.4292, 2255.7112, 2067.2297],\n",
      "        [ 478.2518, 2272.7581, 2087.2412],\n",
      "        [ 435.8674, 2280.0806, 1866.1301],\n",
      "        [ 418.8178, 2283.0186, 1820.9280],\n",
      "        [ 283.5004, 2242.9929, 1282.3154],\n",
      "        [ 391.3742, 2250.2932, 1742.2559],\n",
      "        [ 622.4244, 2946.7703, 1997.0521],\n",
      "        [ 838.6615, 4543.3047, 1837.4448],\n",
      "        [1324.1628, 4591.1836, 2902.0566],\n",
      "        [1491.5435, 5213.5859, 2883.4893],\n",
      "        [2231.8833, 5600.4536, 4001.9629],\n",
      "        [1517.4194, 5341.6445, 2856.1877],\n",
      "        [1482.7734, 5455.8062, 2712.9448],\n",
      "        [2890.5637, 6363.7837, 4500.2837],\n",
      "        [4089.2693, 7185.4429, 5688.6958],\n",
      "        [3291.0447, 7532.4248, 4378.0449],\n",
      "        [3100.3745, 7633.9971, 4075.2583],\n",
      "        [1928.9010, 7766.2334, 2489.7703],\n",
      "        [2248.9133, 7784.4146, 2898.2949],\n",
      "        [2019.8444, 7955.3691, 2546.4304],\n",
      "        [1705.9044, 8102.8931, 2112.8406],\n",
      "        [2108.1748, 8167.2715, 2586.3611],\n",
      "        [2167.4719, 8236.9395, 2635.3984],\n",
      "        [2511.3057, 8291.2979, 3023.9856],\n",
      "        [2165.8254, 8375.4033, 2589.1216],\n",
      "        [3027.3115, 9129.9199, 3330.4478],\n",
      "        [3562.1877, 9797.4922, 3638.7463],\n",
      "        [3104.8689, 9486.1182, 3282.2361],\n",
      "        [3138.4956, 8632.2139, 3643.1350],\n",
      "        [3116.0801, 8036.5767, 3870.5391],\n",
      "        [2415.7534, 7519.7681, 3210.0852],\n",
      "        [2027.9143, 7433.6323, 2733.0872],\n",
      "        [1370.4755, 7579.9072, 1810.6075],\n",
      "        [1733.6747, 7646.7656, 2273.0334],\n",
      "        [1357.4730, 7662.2148, 1775.9331],\n",
      "        [1510.5543, 7631.5615, 1983.8047],\n",
      "        [1433.1483, 7590.8550, 1893.0160],\n",
      "        [1269.1023, 7563.7202, 1684.4939],\n",
      "        [1302.5527, 7504.2739, 1738.9187],\n",
      "        [1331.7618, 7366.5972, 1913.0092],\n",
      "        [1154.9149, 7382.2812, 1567.6660],\n",
      "        [1326.2540, 7316.3667, 1816.0629],\n",
      "        [1097.6443, 7268.2480, 1514.6106],\n",
      "        [1223.3472, 7190.0454, 1708.8368],\n",
      "        [1433.7614, 7150.6958, 1994.9666],\n",
      "        [1123.6285, 7071.1807, 1591.8035],\n",
      "        [ 946.7555, 7007.6401, 1352.3949],\n",
      "        [1416.9208, 6943.0049, 2046.3156],\n",
      "        [1126.1089, 6884.3906, 1638.0969],\n",
      "        [1234.6385, 6822.4697, 1823.0024],\n",
      "        [ 788.3813, 6811.4438, 1141.4448],\n",
      "        [ 898.4453, 6727.1313, 1339.6487],\n",
      "        [ 988.6475, 6653.4863, 1496.0776],\n",
      "        [1177.0024, 6602.5244, 1804.8964],\n",
      "        [1249.0342, 6589.7949, 1876.9397],\n",
      "        [ 805.1346, 6226.3247, 1482.9482],\n",
      "        [ 971.9565, 6478.2305, 1504.1117],\n",
      "        [1151.9236, 6413.8408, 1799.4060],\n",
      "        [1042.9160, 6401.3799, 1617.0314],\n",
      "        [1010.4655, 6330.2036, 1599.9083],\n",
      "        [ 785.5135, 6321.7920, 1240.2831],\n",
      "        [ 835.8861, 6288.5361, 1311.1514],\n",
      "        [ 824.5922, 6217.4272, 1328.3977],\n",
      "        [ 820.2388, 6203.2036, 1316.4789],\n",
      "        [ 698.9800, 6141.9614, 1139.9584]])\n",
      "data:  tensor([[[0.0083, 0.0079, 0.0081,  ..., 0.7339, 0.0759, 0.7826]],\n",
      "\n",
      "        [[0.0047, 0.0051, 0.0056,  ..., 0.7331, 0.0759, 0.7826]],\n",
      "\n",
      "        [[0.0096, 0.0096, 0.0095,  ..., 0.7331, 0.0759, 0.8261]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0095, 0.0095, 0.0096,  ..., 0.6940, 0.0802, 0.3043]],\n",
      "\n",
      "        [[0.0083, 0.0083, 0.0083,  ..., 0.6975, 0.0802, 0.3043]],\n",
      "\n",
      "        [[0.0087, 0.0088, 0.0088,  ..., 0.6991, 0.0802, 0.3043]]])\n",
      "output:  tensor([[-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252],\n",
      "        [-0.1911,  4.6728,  3.5252]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 948.4692, 6102.8242, 1557.0413],\n",
      "        [1013.7214, 6032.6704, 1690.0983],\n",
      "        [ 669.7474, 6059.4751, 1105.5038],\n",
      "        [1044.4850, 6050.2310, 1675.7533],\n",
      "        [1009.5121, 5980.3286, 1693.3256],\n",
      "        [ 754.0761, 5963.5776, 1268.2173],\n",
      "        [ 734.6345, 5658.4814, 1445.8718],\n",
      "        [ 786.5319, 5906.6953, 1337.2875],\n",
      "        [ 737.6443, 5879.8457, 1255.7196],\n",
      "        [ 856.0717, 5863.7764, 1457.1610],\n",
      "        [ 771.5880, 5631.6826, 1758.0222],\n",
      "        [1090.3784, 5806.4805, 1874.9059],\n",
      "        [ 965.4918, 5776.8101, 1690.9714],\n",
      "        [ 642.7960, 5770.8672, 1115.1143],\n",
      "        [ 660.2341, 5744.7383, 1152.1310],\n",
      "        [ 693.5576, 5719.2905, 1217.7932],\n",
      "        [ 780.1199, 5716.0791, 1363.9773],\n",
      "        [ 922.9200, 5664.8101, 1634.5288],\n",
      "        [ 823.6470, 5677.8965, 1443.7089],\n",
      "        [ 788.3597, 5638.5635, 1400.0920],\n",
      "        [ 689.9089, 5633.5029, 1226.1842],\n",
      "        [ 741.0546, 5605.3354, 1324.0703],\n",
      "        [ 844.0415, 5591.2603, 1502.6864],\n",
      "        [ 715.6218, 5559.2607, 1288.9482],\n",
      "        [ 840.3677, 5553.3960, 1517.3278],\n",
      "        [ 760.5532, 5560.2104, 1351.9871],\n",
      "        [ 555.5016, 5524.2622, 1005.9900],\n",
      "        [1048.6906, 5480.9131, 1936.5706],\n",
      "        [ 714.0638, 5476.8862, 1306.1930],\n",
      "        [ 638.6293, 5457.4038, 1172.9934],\n",
      "        [ 638.2554, 5446.9482, 1173.0831],\n",
      "        [ 919.1111, 5385.0605, 1814.2010],\n",
      "        [ 951.5074, 5429.2139, 1703.8643],\n",
      "        [ 721.4817, 5393.5952, 1333.0215],\n",
      "        [ 776.1063, 5363.5229, 1450.8851],\n",
      "        [ 683.3063, 5356.0493, 1278.7505],\n",
      "        [ 737.2557, 5326.6250, 1386.3844],\n",
      "        [ 855.1888, 5294.5977, 1619.1497],\n",
      "        [ 991.1639, 5288.7051, 1874.6892],\n",
      "        [ 847.6326, 5261.4146, 1630.1169],\n",
      "        [ 652.7491, 5258.9067, 1243.6458],\n",
      "        [1052.9990, 5233.9448, 2016.7388],\n",
      "        [ 551.2288, 5228.0303, 1055.1378],\n",
      "        [ 582.6721, 5204.0962, 1122.4052],\n",
      "        [ 665.1066, 5194.0703, 1272.5248],\n",
      "        [1056.0984, 5134.3501, 2063.2266],\n",
      "        [ 601.6447, 5137.4668, 1173.4307],\n",
      "        [ 915.7194, 5104.0098, 1809.8308],\n",
      "        [ 603.8218, 5106.2383, 1185.2864],\n",
      "        [ 612.4179, 5099.0054, 1201.4126],\n",
      "        [ 661.0898, 5062.4146, 1308.5311],\n",
      "        [ 618.0423, 5041.5752, 1229.4297],\n",
      "        [ 773.7012, 5041.0781, 1529.4640],\n",
      "        [ 609.4173, 5010.3691, 1214.9445],\n",
      "        [ 887.5655, 5008.2578, 1752.3867],\n",
      "        [ 643.6102, 4964.4565, 1299.8629],\n",
      "        [ 922.3422, 4961.3955, 1862.2241],\n",
      "        [ 951.4528, 4922.5933, 1939.1740],\n",
      "        [ 641.6176, 4909.9253, 1310.5365],\n",
      "        [ 727.5084, 4869.4639, 1516.0048],\n",
      "        [ 878.5244, 4885.3994, 1775.5275],\n",
      "        [ 773.3172, 4842.2871, 1603.6251],\n",
      "        [1183.8813, 4841.2349, 2375.2024],\n",
      "        [1117.7406, 4810.7271, 2333.1982],\n",
      "        [ 855.5206, 4759.7256, 1807.8257],\n",
      "        [ 716.6069, 4753.2856, 1505.1449],\n",
      "        [ 793.2031, 4710.1211, 1693.2626],\n",
      "        [ 873.0391, 4701.3296, 1860.9996],\n",
      "        [ 826.9609, 4688.3042, 1761.2118],\n",
      "        [1196.6130, 4685.5840, 2487.6562],\n",
      "        [1021.4137, 4650.4790, 2178.6309],\n",
      "        [1697.0471, 4567.4790, 3706.7898],\n",
      "        [ 727.7405, 4412.2803, 1593.6382],\n",
      "        [1237.3306, 4537.9932, 2743.1772],\n",
      "        [ 819.6763, 4520.3315, 1815.2671],\n",
      "        [ 984.9774, 4492.9907, 2213.5930],\n",
      "        [ 875.2439, 4469.8052, 1973.2734],\n",
      "        [ 835.9897, 4462.8291, 1886.1893],\n",
      "        [ 879.1295, 4414.1680, 1996.4243],\n",
      "        [1281.9321, 4437.5713, 2814.5991],\n",
      "        [1135.3478, 4375.5498, 2599.0007],\n",
      "        [1158.7360, 4162.4414, 2756.7673],\n",
      "        [ 989.9554, 4314.5732, 2290.3484],\n",
      "        [ 960.0511, 4283.3730, 2247.1045],\n",
      "        [1240.7719, 4263.8682, 2907.4817],\n",
      "        [1330.0198, 4262.7119, 3124.2122],\n",
      "        [ 681.8108, 4231.4727, 1618.9911],\n",
      "        [1427.6375, 4201.9194, 3376.5334],\n",
      "        [ 950.7988, 4188.2183, 2269.0798],\n",
      "        [ 892.0614, 4146.4165, 2164.3669],\n",
      "        [1110.8612, 4126.3979, 2720.1958],\n",
      "        [1298.3143, 4138.9937, 3133.4858],\n",
      "        [1240.8159, 4105.3071, 3023.2271],\n",
      "        [ 811.3185, 3968.4058, 2121.6755],\n",
      "        [ 773.5914, 4043.9954, 1917.7946],\n",
      "        [1032.8202, 3843.0740, 3177.6865],\n",
      "        [1429.1842, 4030.2271, 3441.7214],\n",
      "        [ 763.6697, 4038.1587, 1787.0931],\n",
      "        [1243.2845, 3989.7490, 3121.2725],\n",
      "        [ 936.7897, 3961.6206, 2369.6675],\n",
      "        [1069.4573, 3944.4954, 2687.0396],\n",
      "        [1165.1467, 3969.8201, 2918.1870],\n",
      "        [ 839.7786, 3914.6521, 2149.8301],\n",
      "        [1033.0236, 3896.5161, 2670.5029],\n",
      "        [ 712.5042, 3872.6047, 1833.0361],\n",
      "        [ 618.2762, 3882.2676, 1564.3424],\n",
      "        [1065.2892, 3869.2644, 2716.5344],\n",
      "        [ 781.3602, 3815.6104, 2067.3069],\n",
      "        [1152.8588, 3808.9802, 3038.7520],\n",
      "        [1157.9451, 3680.7542, 3137.6284],\n",
      "        [1079.8636, 3776.1318, 2862.3147],\n",
      "        [ 888.5933, 3765.2793, 2365.5847],\n",
      "        [1064.3615, 3717.5625, 2902.5347],\n",
      "        [ 870.6384, 3698.7842, 2381.3684],\n",
      "        [1022.6323, 3726.9070, 2745.7583],\n",
      "        [ 723.1842, 3695.6897, 1970.7185],\n",
      "        [ 738.5133, 3695.9858, 1996.9247],\n",
      "        [ 942.3111, 3653.1116, 2593.9932],\n",
      "        [1085.8007, 3646.1521, 2987.7249],\n",
      "        [1095.8619, 3668.9463, 2963.6838],\n",
      "        [ 798.7788, 3643.3481, 2173.2273],\n",
      "        [ 845.1013, 3609.9412, 2322.7830],\n",
      "        [1231.1881, 3659.7527, 3257.1968],\n",
      "        [1336.4016, 3625.0000, 3671.9924],\n",
      "        [ 735.2743, 3583.1589, 2082.6448],\n",
      "        [ 975.9915, 3546.7302, 2783.0771],\n",
      "        [1398.9171, 3602.4021, 3816.3130],\n",
      "        [ 586.0765, 3536.2185, 1683.4735],\n",
      "        [ 825.0540, 3550.4504, 2324.2607],\n",
      "        [1029.5038, 3530.0408, 2925.2158],\n",
      "        [ 884.7813, 3484.7202, 2544.0088],\n",
      "        [ 971.3751, 3476.6270, 2789.9592],\n",
      "        [ 948.1367, 3475.5266, 2715.2996],\n",
      "        [ 976.2413, 3435.2495, 2873.7329],\n",
      "        [ 773.3141, 3428.2788, 2258.4885],\n",
      "        [ 996.0059, 3397.3018, 2914.4978],\n",
      "        [ 697.6292, 3398.7595, 2045.3396],\n",
      "        [ 666.4881, 3344.6306, 2035.6758],\n",
      "        [ 697.8735, 3370.3406, 2069.2976],\n",
      "        [ 716.2420, 3326.5503, 2175.1553],\n",
      "        [1240.0510, 3359.1694, 3658.2185],\n",
      "        [ 800.2692, 3365.4365, 2352.3213],\n",
      "        [ 896.6913, 3322.5735, 2710.5813],\n",
      "        [ 959.6859, 3357.5442, 2832.3052],\n",
      "        [ 932.9473, 3274.8174, 2858.9258],\n",
      "        [ 996.7949, 3295.0537, 3027.2415],\n",
      "        [ 693.9491, 3266.5454, 2121.7964],\n",
      "        [ 682.4201, 3242.3770, 2108.4668],\n",
      "        [ 852.7750, 3247.2363, 2521.4583],\n",
      "        [1258.4456, 3184.3938, 3985.1172]])\n",
      "data:  tensor([[[0.0112, 0.0108, 0.0103,  ..., 0.7022, 0.0802, 0.3043]],\n",
      "\n",
      "        [[0.0117, 0.0114, 0.0122,  ..., 0.7048, 0.0802, 0.3043]],\n",
      "\n",
      "        [[0.0083, 0.0090, 0.0087,  ..., 0.7082, 0.0802, 0.3043]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0083, 0.0071, 0.0070,  ..., 0.7011, 0.0802, 0.8261]],\n",
      "\n",
      "        [[0.0097, 0.0099, 0.0094,  ..., 0.7011, 0.0802, 0.8261]],\n",
      "\n",
      "        [[0.0150, 0.0147, 0.0146,  ..., 0.6975, 0.0802, 0.8261]]])\n",
      "output:  tensor([[-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693],\n",
      "        [-0.1450,  4.8472,  3.6693]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 578.3563, 3207.2163, 1810.9524],\n",
      "        [ 920.7225, 3184.3828, 2911.6863],\n",
      "        [ 741.8238, 3182.2268, 2301.9541],\n",
      "        [1097.6252, 3219.5378, 3388.1055],\n",
      "        [ 638.8102, 3180.1321, 2010.8203],\n",
      "        [ 647.5681, 3187.7339, 2013.6653],\n",
      "        [1171.8053, 3133.2363, 3772.4392],\n",
      "        [ 863.3950, 3151.9534, 2740.2878],\n",
      "        [ 666.0927, 3131.4883, 2167.8276],\n",
      "        [ 767.1190, 3167.3247, 2413.5520],\n",
      "        [ 831.0456, 3204.5784, 2511.1472],\n",
      "        [ 682.7971, 3181.8860, 2146.7717],\n",
      "        [1163.4011, 3106.5713, 3769.5889],\n",
      "        [ 548.9843, 3172.7314, 1706.2343],\n",
      "        [ 988.1938, 3176.5686, 3073.3108],\n",
      "        [ 977.7855, 3200.3462, 3019.4519],\n",
      "        [ 509.7722, 3165.6267, 1623.6307],\n",
      "        [ 898.8432, 3165.7720, 2855.4165],\n",
      "        [ 722.6843, 3090.7197, 2298.5745],\n",
      "        [ 771.5855, 3123.2273, 2500.6301],\n",
      "        [ 689.8069, 3175.7922, 2163.3210],\n",
      "        [ 776.7367, 3134.5938, 2502.6990],\n",
      "        [ 638.5998, 3069.8188, 2520.6641],\n",
      "        [ 549.0420, 3141.8157, 1772.8915],\n",
      "        [1081.9875, 3136.1353, 3458.6184],\n",
      "        [1001.2775, 3120.4792, 3218.7056],\n",
      "        [1121.3048, 3109.8611, 3628.3447],\n",
      "        [ 811.4416, 3156.2151, 2557.0847],\n",
      "        [ 566.3873, 3151.6631, 1788.4425],\n",
      "        [ 638.1309, 3135.1523, 2026.0945],\n",
      "        [ 790.5533, 3098.6902, 2557.3462],\n",
      "        [ 807.5485, 3108.8447, 2615.5811],\n",
      "        [ 800.1251, 3128.0195, 2562.4150],\n",
      "        [ 916.3090, 3100.7065, 2969.8027],\n",
      "        [ 610.1158, 3104.1584, 1941.1312],\n",
      "        [1026.8220, 3057.6929, 3377.9316],\n",
      "        [ 630.9524, 3069.5562, 2045.1956],\n",
      "        [ 911.5475, 3035.2056, 3010.7454],\n",
      "        [ 567.9973, 3027.2295, 1870.9814],\n",
      "        [ 679.1226, 3058.6421, 2191.9668],\n",
      "        [ 624.5942, 3025.7717, 2080.6929],\n",
      "        [ 820.4204, 3014.9358, 2706.1567],\n",
      "        [ 687.2692, 2923.8140, 2584.3943],\n",
      "        [ 937.6361, 3017.9519, 3106.8840],\n",
      "        [ 796.7607, 2976.8481, 2711.0759],\n",
      "        [ 682.6859, 3012.4670, 2248.7185],\n",
      "        [ 858.0491, 3017.5330, 2857.3198],\n",
      "        [ 700.3036, 3047.8845, 2261.2009],\n",
      "        [ 691.6413, 2998.6931, 2334.7769],\n",
      "        [ 627.8647, 3018.0859, 2071.0950],\n",
      "        [ 739.9752, 2970.2407, 2502.7993],\n",
      "        [ 796.2485, 2973.7092, 2633.3877],\n",
      "        [ 830.4138, 2898.5898, 2892.3767],\n",
      "        [ 575.5778, 2970.1401, 1907.1300],\n",
      "        [ 883.7549, 2938.7439, 2979.8062],\n",
      "        [1053.8392, 2909.4873, 3633.1970],\n",
      "        [ 486.7655, 2931.2708, 1660.7117],\n",
      "        [ 527.0380, 2918.4575, 1826.8979],\n",
      "        [ 654.8163, 2929.8464, 2240.2954],\n",
      "        [ 894.5535, 2902.7288, 3079.9756],\n",
      "        [ 701.9055, 2921.0212, 2396.8711],\n",
      "        [ 580.1652, 2920.7419, 1971.7334],\n",
      "        [ 748.5530, 2913.8215, 2562.6501],\n",
      "        [ 727.4518, 2879.5601, 2546.7317],\n",
      "        [ 792.5825, 2883.0007, 2760.7368],\n",
      "        [ 612.5449, 2927.6848, 2065.7336],\n",
      "        [ 588.8555, 2855.3862, 2104.5769],\n",
      "        [ 790.0403, 2907.3816, 2687.1831],\n",
      "        [ 738.8066, 2859.1843, 2618.9351],\n",
      "        [ 863.1935, 2876.3821, 2968.0208],\n",
      "        [ 913.0806, 2832.2009, 3242.9180],\n",
      "        [ 581.1368, 2821.0913, 2150.3176],\n",
      "        [ 506.4937, 2845.9524, 1777.5232],\n",
      "        [ 630.0262, 2784.3333, 2287.9099],\n",
      "        [ 565.4150, 2805.5190, 2053.2932],\n",
      "        [ 649.4926, 2830.9775, 2282.8481],\n",
      "        [ 778.2682, 2758.0254, 2865.3242],\n",
      "        [ 614.2072, 2782.5767, 2216.8948],\n",
      "        [ 573.0477, 2772.7937, 2063.3660],\n",
      "        [ 608.9504, 2724.5574, 2282.7739],\n",
      "        [ 562.2575, 2771.3916, 2044.4243],\n",
      "        [ 677.7180, 2802.9216, 2399.5176],\n",
      "        [ 377.4591, 2803.9717, 1327.0443],\n",
      "        [ 551.0009, 2774.9773, 1986.9771],\n",
      "        [ 799.5775, 2773.2124, 2837.9456],\n",
      "        [ 579.2151, 2722.8145, 2124.2834],\n",
      "        [ 457.7662, 2739.7720, 1681.2946],\n",
      "        [ 522.4133, 2738.7444, 1909.9244],\n",
      "        [ 570.1857, 2733.0305, 2074.1924],\n",
      "        [ 653.9029, 2723.8198, 2396.0061],\n",
      "        [ 483.7816, 2711.0068, 1772.5927],\n",
      "        [ 660.8533, 2724.7471, 2411.1030],\n",
      "        [ 583.0035, 2664.9097, 2205.2722],\n",
      "        [ 319.0746, 2680.6609, 1184.3981],\n",
      "        [ 406.2482, 2693.8147, 1477.7328],\n",
      "        [ 463.2886, 2703.1035, 1709.1730],\n",
      "        [ 689.9350, 2678.4209, 2588.5791],\n",
      "        [ 467.8193, 2673.0701, 1741.4666],\n",
      "        [ 570.9699, 2668.7358, 2077.7063],\n",
      "        [ 482.7857, 2639.7917, 1836.1375],\n",
      "        [ 433.3657, 2646.5056, 1651.5171],\n",
      "        [ 406.9297, 2680.4319, 1515.1110],\n",
      "        [ 656.8367, 2700.5898, 2377.4028],\n",
      "        [ 401.7299, 2658.7043, 1527.8531],\n",
      "        [ 391.7679, 2691.8877, 1460.6670],\n",
      "        [ 516.6269, 2719.1113, 1864.2889],\n",
      "        [ 332.5041, 2715.2908, 1218.6105],\n",
      "        [ 372.5540, 2720.7256, 1374.3447],\n",
      "        [ 351.0380, 2724.1719, 1270.4351],\n",
      "        [ 391.7162, 2712.0066, 1452.7795],\n",
      "        [ 311.9858, 2734.2761, 1143.6667],\n",
      "        [ 551.0588, 2777.0879, 1932.6226],\n",
      "        [ 445.9484, 2788.9636, 1587.0625],\n",
      "        [ 374.6997, 2790.3933, 1335.4409],\n",
      "        [ 308.4435, 2754.8418, 1123.0345],\n",
      "        [ 242.2012, 2786.4390,  872.5218],\n",
      "        [ 418.8698, 2834.6416, 1453.5737],\n",
      "        [ 309.9391, 2863.1555, 1076.8041],\n",
      "        [ 275.0909, 2875.2258,  959.7999],\n",
      "        [ 243.4731, 2885.5535,  846.6406],\n",
      "        [ 267.6446, 2829.2629,  930.5529],\n",
      "        [ 302.6718, 2915.5139, 1038.7517],\n",
      "        [ 255.9939, 2945.2512,  872.8094],\n",
      "        [ 369.7830, 2979.8979, 1243.7373],\n",
      "        [ 247.5272, 3013.0535,  821.3417],\n",
      "        [ 368.8276, 3026.4026, 1213.1583],\n",
      "        [ 291.5261, 3037.0320,  962.7842],\n",
      "        [ 303.4973, 3043.2151,  998.9874],\n",
      "        [ 407.1306, 3082.1182, 1273.7493],\n",
      "        [ 342.6841, 3081.7886, 1103.2007],\n",
      "        [ 226.8050, 3078.4763,  736.9999],\n",
      "        [ 369.9144, 3091.0662, 1177.4386],\n",
      "        [ 282.6472, 3056.9163,  927.1130],\n",
      "        [ 317.3216, 3053.8220, 1016.6784],\n",
      "        [ 604.9047, 3051.7051, 1909.0706],\n",
      "        [ 319.8835, 3049.5994, 1053.3024],\n",
      "        [ 372.5045, 3079.0850, 1178.0967],\n",
      "        [ 277.6931, 3071.2542,  906.7949],\n",
      "        [ 375.1652, 3086.8264, 1202.4086],\n",
      "        [ 313.4843, 3080.5986, 1016.6553],\n",
      "        [ 327.5959, 3101.1423, 1035.9220],\n",
      "        [ 353.3113, 3110.5203, 1129.1686],\n",
      "        [ 310.6725, 3140.7490,  984.6544],\n",
      "        [ 289.1815, 3121.1160,  930.9297],\n",
      "        [ 466.0562, 3131.7454, 1424.2567],\n",
      "        [ 209.0814, 3111.9780,  673.0880],\n",
      "        [ 270.1467, 3126.8857,  868.3721],\n",
      "        [ 391.2302, 3143.8936, 1244.5316],\n",
      "        [ 335.7877, 3156.0642, 1068.2170],\n",
      "        [ 422.3502, 3146.8650, 1345.1256]])\n",
      "data:  tensor([[[0.0068, 0.0079, 0.0086,  ..., 0.6975, 0.0802, 0.8261]],\n",
      "\n",
      "        [[0.0096, 0.0097, 0.0096,  ..., 0.6975, 0.0802, 0.8261]],\n",
      "\n",
      "        [[0.0098, 0.0095, 0.0096,  ..., 0.6973, 0.0802, 0.8696]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0045, 0.0046, 0.0047,  ..., 0.7367, 0.0844, 0.3478]],\n",
      "\n",
      "        [[0.0037, 0.0036, 0.0035,  ..., 0.7399, 0.0844, 0.3478]],\n",
      "\n",
      "        [[0.0049, 0.0048, 0.0048,  ..., 0.7402, 0.0844, 0.3478]]])\n",
      "output:  tensor([[-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803],\n",
      "        [-0.0951,  4.9771,  3.7803]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 356.0892, 3185.3044, 1117.4612],\n",
      "        [ 407.5421, 3172.5862, 1286.5582],\n",
      "        [ 534.2097, 3178.2722, 1726.3988],\n",
      "        [ 343.9102, 3207.6296, 1065.3514],\n",
      "        [ 308.4812, 3192.6550,  967.3179],\n",
      "        [ 427.8322, 3204.9541, 1314.3068],\n",
      "        [ 343.4317, 3188.6611, 1078.7070],\n",
      "        [ 388.9456, 3220.1299, 1203.6926],\n",
      "        [ 455.0764, 3207.1604, 1417.3323],\n",
      "        [ 428.0531, 3208.0317, 1310.6669],\n",
      "        [ 314.8066, 3220.9734,  978.3099],\n",
      "        [ 385.7951, 3240.0254, 1169.3192],\n",
      "        [ 338.1345, 3268.0366, 1031.2297],\n",
      "        [ 355.0047, 3287.0496, 1089.1169],\n",
      "        [ 451.9141, 3340.7932, 1331.9055],\n",
      "        [ 379.6096, 3339.2014, 1140.4116],\n",
      "        [ 287.1930, 3364.4255,  855.6792],\n",
      "        [ 379.2560, 3385.3096, 1121.2836],\n",
      "        [ 295.4233, 3391.9229,  872.6016],\n",
      "        [ 512.4078, 3425.1008, 1392.6100],\n",
      "        [ 424.9835, 3404.6411, 1242.2167],\n",
      "        [ 346.4135, 3380.7407, 1022.9766],\n",
      "        [ 390.4565, 3357.4714, 1166.3326],\n",
      "        [ 375.5028, 3344.8037, 1131.3754],\n",
      "        [ 432.3781, 3366.9111, 1273.3484],\n",
      "        [ 270.0321, 3354.7505,  804.7271],\n",
      "        [ 500.7685, 3395.0898, 1375.0219],\n",
      "        [ 577.4672, 3383.7234, 1661.6583],\n",
      "        [ 353.2055, 3376.1772, 1046.3190],\n",
      "        [ 471.4260, 3376.5459, 1386.9355],\n",
      "        [ 425.9447, 3371.8096, 1268.7758],\n",
      "        [ 416.0177, 3345.9373, 1248.0912],\n",
      "        [ 428.4394, 3346.5630, 1298.6776],\n",
      "        [ 522.4545, 3391.9229, 1465.7391],\n",
      "        [ 310.1243, 3386.0803,  914.0187],\n",
      "        [ 392.1809, 3371.5974, 1166.3613],\n",
      "        [ 670.2589, 3360.6943, 1996.3303],\n",
      "        [ 381.0540, 3368.4639, 1131.8439],\n",
      "        [ 377.0215, 3357.1084, 1127.9282],\n",
      "        [ 441.2904, 3369.9160, 1287.0929],\n",
      "        [ 401.9536, 3351.8582, 1201.2653],\n",
      "        [ 365.1256, 3357.3708, 1085.4537],\n",
      "        [ 341.5420, 3347.4399, 1022.3115],\n",
      "        [ 478.4443, 3366.0173, 1410.0007],\n",
      "        [ 441.3593, 3339.0840, 1327.0140],\n",
      "        [ 509.2870, 3336.9670, 1542.6221],\n",
      "        [ 445.4446, 3341.6311, 1333.1449],\n",
      "        [ 514.7146, 3348.2053, 1532.6354],\n",
      "        [ 448.1687, 3324.7239, 1353.5986],\n",
      "        [ 416.8877, 3334.0627, 1256.5759],\n",
      "        [ 379.1127, 3318.7139, 1148.7542],\n",
      "        [ 476.7073, 3348.4453, 1415.9470],\n",
      "        [ 539.4401, 3313.7317, 1640.5521],\n",
      "        [ 450.7588, 3331.3425, 1352.8625],\n",
      "        [ 752.5336, 3298.6228, 2288.1362],\n",
      "        [ 505.2788, 3277.5098, 1549.7416],\n",
      "        [ 597.1589, 3264.9702, 1840.8773],\n",
      "        [ 685.7842, 3256.4226, 2110.8210],\n",
      "        [ 739.2231, 3223.9001, 2310.4285],\n",
      "        [ 629.8360, 3228.7593, 1950.0728],\n",
      "        [ 714.4938, 3191.4988, 2261.8223],\n",
      "        [ 661.2318, 3225.1960, 2035.1665],\n",
      "        [ 432.4293, 3179.1658, 1373.6716],\n",
      "        [ 393.9851, 3187.4382, 1221.8615],\n",
      "        [ 605.8083, 3149.5349, 1925.9194],\n",
      "        [ 652.0601, 3143.3015, 2074.8289],\n",
      "        [ 563.9056, 3115.9939, 1832.3302],\n",
      "        [ 752.1746, 3106.9680, 2438.7773],\n",
      "        [ 856.4527, 3073.2537, 2799.8152],\n",
      "        [ 645.2706, 3065.9983, 2111.9746],\n",
      "        [ 742.3655, 3031.8152, 2467.5034],\n",
      "        [ 664.6435, 3041.6343, 2188.4192],\n",
      "        [ 431.9482, 3037.4172, 1434.4432],\n",
      "        [ 772.9093, 3023.5320, 2572.4688],\n",
      "        [ 511.8942, 2993.0183, 1748.7405],\n",
      "        [ 758.7203, 3006.5295, 2526.9568],\n",
      "        [ 468.5314, 3001.2456, 1571.0134],\n",
      "        [ 573.4767, 3002.0051, 1911.8468],\n",
      "        [ 688.7787, 2961.3093, 2346.2319],\n",
      "        [ 676.6215, 2922.3340, 2338.4797],\n",
      "        [ 643.5696, 2907.9290, 2229.8142],\n",
      "        [ 738.1454, 2887.8604, 2559.4849],\n",
      "        [ 923.5734, 2861.0608, 3234.6599],\n",
      "        [ 624.6104, 2863.6528, 2185.4080],\n",
      "        [ 771.3051, 2824.7107, 2754.1072],\n",
      "        [ 679.8393, 2794.4932, 2466.7051],\n",
      "        [ 715.1790, 2811.5344, 2537.1936],\n",
      "        [ 846.7791, 2812.8584, 3005.6228],\n",
      "        [ 587.4055, 2810.9761, 2109.8396],\n",
      "        [ 556.2642, 2836.7495, 1951.7236],\n",
      "        [ 802.7621, 2796.2861, 2887.7100],\n",
      "        [ 697.4542, 2804.0723, 2499.7483],\n",
      "        [ 429.4545, 2815.8464, 1504.6964],\n",
      "        [ 854.8419, 2797.8948, 3052.8652],\n",
      "        [ 544.5282, 2792.4490, 1926.5837],\n",
      "        [ 887.3619, 2720.0776, 3282.6670],\n",
      "        [ 549.5337, 2699.5286, 2072.5654],\n",
      "        [ 553.8691, 2747.8655, 2000.1870],\n",
      "        [ 633.1362, 2715.4141, 2351.1455],\n",
      "        [ 683.1844, 2693.9150, 2560.3271],\n",
      "        [ 825.9607, 2673.9358, 3093.7686],\n",
      "        [ 794.0345, 2669.4844, 3003.8313],\n",
      "        [ 592.6618, 2690.8267, 2204.0652],\n",
      "        [ 745.3611, 2654.7554, 2815.8650],\n",
      "        [ 507.5890, 2635.8987, 1936.8303],\n",
      "        [ 502.4827, 2630.7769, 1930.8092],\n",
      "        [ 642.9374, 2627.2078, 2459.3411],\n",
      "        [ 470.6179, 2639.5293, 1762.9509],\n",
      "        [ 703.2553, 2604.9944, 2722.7871],\n",
      "        [ 490.0886, 2606.6365, 1892.6063],\n",
      "        [ 614.7623, 2596.3198, 2371.4558],\n",
      "        [ 599.0977, 2565.4321, 2363.6472],\n",
      "        [ 540.5219, 2516.7490, 2185.3777],\n",
      "        [ 642.7130, 2528.6294, 2557.4751],\n",
      "        [ 461.6317, 2510.8730, 1852.7664],\n",
      "        [ 495.6361, 2509.7505, 1986.7468],\n",
      "        [ 621.1382, 2475.4553, 2535.4846],\n",
      "        [ 461.5305, 2478.4214, 1879.4340],\n",
      "        [ 725.1262, 2456.8560, 2970.6082],\n",
      "        [ 604.7332, 2460.2517, 2468.5232],\n",
      "        [ 637.8479, 2428.6602, 2629.9956],\n",
      "        [ 466.5823, 2409.2229, 1952.1194],\n",
      "        [ 524.6945, 2406.4971, 2184.6416],\n",
      "        [ 835.6641, 2400.3474, 3511.2856],\n",
      "        [ 566.9235, 2365.2874, 2447.0537],\n",
      "        [ 627.0543, 2383.7754, 2655.9734],\n",
      "        [ 669.8020, 2379.9602, 2868.6536],\n",
      "        [ 495.7405, 2415.5344, 2064.2725],\n",
      "        [ 683.7172, 2422.8345, 2844.6516],\n",
      "        [ 649.4787, 2445.9026, 2640.9421],\n",
      "        [ 746.2086, 2409.2898, 3148.9241],\n",
      "        [ 620.2607, 2464.1672, 2499.0242],\n",
      "        [ 479.5620, 2452.0635, 1924.7810],\n",
      "        [ 542.8319, 2430.6096, 2206.5708],\n",
      "        [ 578.1060, 2407.8320, 2412.9973],\n",
      "        [ 516.2414, 2418.1260, 2139.2051],\n",
      "        [ 422.5822, 2408.9993, 1763.9443],\n",
      "        [ 665.9792, 2414.5291, 2771.5615],\n",
      "        [ 510.9901, 2426.5713, 2092.0959],\n",
      "        [ 670.6906, 2393.5891, 2813.6567],\n",
      "        [ 727.6320, 2403.8606, 3020.8115],\n",
      "        [ 797.1578, 2389.4780, 3334.4263],\n",
      "        [ 431.6201, 2355.2834, 1866.0082],\n",
      "        [ 567.1279, 2337.9128, 2449.5967],\n",
      "        [ 602.5115, 2371.9397, 2544.8840],\n",
      "        [ 658.7740, 2341.3367, 2824.3276],\n",
      "        [ 554.7060, 2320.5979, 2415.1707],\n",
      "        [ 581.8459, 2319.5254, 2531.1462],\n",
      "        [ 448.5954, 2331.5623, 1939.1309],\n",
      "        [ 643.1815, 2310.4043, 2816.0603]])\n",
      "data:  tensor([[[0.0041, 0.0043, 0.0039,  ..., 0.7434, 0.0844, 0.3478]],\n",
      "\n",
      "        [[0.0049, 0.0049, 0.0053,  ..., 0.7444, 0.0844, 0.3478]],\n",
      "\n",
      "        [[0.0062, 0.0063, 0.0063,  ..., 0.7473, 0.0844, 0.3478]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0067, 0.0067, 0.0060,  ..., 0.6975, 0.0844, 0.8696]],\n",
      "\n",
      "        [[0.0059, 0.0053, 0.0053,  ..., 0.6975, 0.0844, 0.8696]],\n",
      "\n",
      "        [[0.0068, 0.0073, 0.0080,  ..., 0.6975, 0.0844, 0.8696]]])\n",
      "output:  tensor([[-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328],\n",
      "        [-0.0426,  5.0296,  3.8328]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 394.9777, 2351.4019, 1661.7682],\n",
      "        [ 540.9342, 2320.1287, 2343.9319],\n",
      "        [ 517.2675, 2358.1602, 2170.4016],\n",
      "        [ 462.2929, 2316.6321, 2029.5192],\n",
      "        [ 436.1526, 2310.2366, 1925.3347],\n",
      "        [ 568.1221, 2337.2090, 2431.5598],\n",
      "        [ 328.0522, 2322.2476, 1407.9917],\n",
      "        [ 434.9555, 2314.5264, 1889.5791],\n",
      "        [ 618.0995, 2307.6003, 2687.1975],\n",
      "        [ 523.5753, 2320.2290, 2258.2041],\n",
      "        [ 628.3666, 2321.4915, 2669.1951],\n",
      "        [ 506.9014, 2301.8306, 2174.4854],\n",
      "        [ 471.5170, 2281.1978, 2061.6995],\n",
      "        [ 617.7324, 2261.5925, 2746.5295],\n",
      "        [ 708.8655, 2302.9084, 3038.0859],\n",
      "        [ 448.9204, 2235.2236, 2042.6591],\n",
      "        [ 407.3607, 2222.2595, 1864.0637],\n",
      "        [ 561.0488, 2230.8557, 2510.3833],\n",
      "        [ 484.5666, 2199.2476, 2222.5664],\n",
      "        [ 393.7249, 2231.0122, 1722.2792],\n",
      "        [ 589.9824, 2203.3025, 2679.7800],\n",
      "        [ 473.6478, 2186.5405, 2195.9395],\n",
      "        [ 602.0677, 2180.4243, 2781.5347],\n",
      "        [ 467.5150, 2170.4600, 2172.4912],\n",
      "        [ 326.4451, 2181.8989, 1477.8130],\n",
      "        [ 595.6136, 2158.9929, 2774.7568],\n",
      "        [ 592.7755, 2142.4319, 2788.1238],\n",
      "        [ 233.1994, 2149.5979, 1096.0046],\n",
      "        [ 572.5771, 2114.5825, 2735.5066],\n",
      "        [ 588.4356, 2115.4707, 2783.2012],\n",
      "        [ 543.8716, 2092.1121, 2660.2751],\n",
      "        [ 376.3146, 2112.0410, 1792.6027],\n",
      "        [ 406.0276, 2117.1516, 1920.8688],\n",
      "        [ 594.0302, 2125.6697, 2767.5220],\n",
      "        [ 403.6778, 2105.3218, 1903.9362],\n",
      "        [ 460.0688, 2110.8181, 2140.3462],\n",
      "        [ 465.0972, 2081.2820, 2260.3413],\n",
      "        [ 292.8744, 2078.9248, 1425.8737],\n",
      "        [ 461.5551, 2077.8857, 2230.8635],\n",
      "        [ 491.8045, 2046.6493, 2453.9763],\n",
      "        [ 554.4011, 2104.6013, 2603.7417],\n",
      "        [ 447.0007, 2060.1072, 2208.4348],\n",
      "        [ 458.2340, 2033.0902, 2304.8552],\n",
      "        [ 568.8558, 2104.7297, 2690.4397],\n",
      "        [ 412.6641, 2081.7288, 1996.0651],\n",
      "        [ 390.0614, 2077.0256, 1908.3798],\n",
      "        [ 509.4583, 2065.6926, 2503.8638],\n",
      "        [ 505.6707, 2112.7952, 2355.1113],\n",
      "        [ 542.9310, 2075.5791, 2638.6438],\n",
      "        [ 578.3618, 2091.9502, 2762.2505],\n",
      "        [ 364.6317, 2064.3745, 1785.3350],\n",
      "        [ 504.5276, 2077.7629, 2410.5190],\n",
      "        [ 431.2151, 2072.9316, 2074.6145],\n",
      "        [ 323.0247, 2070.6919, 1564.4166],\n",
      "        [ 380.2500, 2061.1238, 1853.3583],\n",
      "        [ 400.8901, 2081.5947, 1905.9319],\n",
      "        [ 684.8541, 2111.8232, 3204.4170],\n",
      "        [ 310.7690, 2048.6570, 1518.3676],\n",
      "        [ 455.6089, 2024.5388, 2285.2188],\n",
      "        [ 436.4076, 2045.7134, 2144.9124],\n",
      "        [ 433.4053, 2059.2861, 2105.9153],\n",
      "        [ 336.1347, 2054.9294, 1647.0614],\n",
      "        [ 538.7339, 2054.8682, 2648.8425],\n",
      "        [ 364.8192, 2055.2480, 1787.1808],\n",
      "        [ 403.4254, 2076.8918, 1950.7347],\n",
      "        [ 357.9636, 2067.8823, 1749.8407],\n",
      "        [ 369.1305, 2087.4370, 1758.6436],\n",
      "        [ 353.2932, 2077.3218, 1718.1886],\n",
      "        [ 527.1890, 2082.3767, 2536.8135],\n",
      "        [ 303.0594, 2082.3933, 1451.6156],\n",
      "        [ 329.9655, 2092.7991, 1585.4374],\n",
      "        [ 509.4115, 2113.0186, 2402.1821],\n",
      "        [ 369.9232, 2104.8804, 1758.9614],\n",
      "        [ 424.2401, 2102.5427, 2022.7024],\n",
      "        [ 370.8333, 2113.2642, 1734.8217],\n",
      "        [ 407.7790, 2100.8535, 1938.9653],\n",
      "        [ 453.4026, 2087.8000, 2192.3721],\n",
      "        [ 409.2930, 2125.6753, 1900.5847],\n",
      "        [ 213.7250, 2098.9097, 1012.5767],\n",
      "        [ 262.2243, 2097.4407, 1263.2350],\n",
      "        [ 282.7146, 2128.4458, 1310.8096],\n",
      "        [ 281.8995, 2119.5481, 1318.7848],\n",
      "        [ 374.1690, 2139.2424, 1733.4482],\n",
      "        [ 400.7398, 2076.0759, 1945.9254],\n",
      "        [ 264.1661, 2088.1465, 1269.7651],\n",
      "        [ 198.9731, 2107.0867,  945.1842],\n",
      "        [ 245.5358, 2123.3350, 1158.5990],\n",
      "        [ 215.3301, 2125.5300, 1020.6123],\n",
      "        [ 204.6073, 2133.1208,  966.3461],\n",
      "        [ 235.1680, 2128.2949, 1116.6224],\n",
      "        [ 235.4792, 2167.8403, 1087.9976],\n",
      "        [ 218.1289, 2179.2625, 1004.4755],\n",
      "        [ 270.1903, 2185.3452, 1239.8339],\n",
      "        [ 165.4057, 2193.7009,  756.5527],\n",
      "        [ 291.9399, 2211.8762, 1325.5490],\n",
      "        [ 131.7399, 2210.7087,  600.2057],\n",
      "        [ 171.6180, 2219.4502,  773.7131],\n",
      "        [ 193.0786, 2219.4446,  870.7405],\n",
      "        [ 222.9371, 2222.1870, 1006.9125],\n",
      "        [ 245.8298, 2233.1960, 1103.0741],\n",
      "        [ 234.0764, 2250.9077, 1039.9738],\n",
      "        [ 204.5273, 2248.9023,  912.6787],\n",
      "        [ 277.0906, 2256.2976, 1230.6239],\n",
      "        [ 176.1473, 2268.0830,  779.5819],\n",
      "        [ 187.1430, 2285.1130,  824.3131],\n",
      "        [ 282.2065, 2302.5344, 1227.0852],\n",
      "        [ 240.3620, 2305.2727, 1047.1627],\n",
      "        [ 214.6580, 2309.6279,  932.4705],\n",
      "        [ 216.1337, 2301.2104,  941.6254],\n",
      "        [ 175.1672, 2312.1748,  760.8943],\n",
      "        [ 259.1046, 2326.1721, 1117.5457],\n",
      "        [ 228.4673, 2352.0776,  967.3522],\n",
      "        [ 208.4667, 2355.3506,  887.5551],\n",
      "        [ 293.2434, 2078.5210, 1466.8511],\n",
      "        [ 276.6190, 1685.9744, 1650.9523],\n",
      "        [ 260.1445, 1515.9744, 1781.5192],\n",
      "        [ 259.0877, 1659.2394, 1605.8589],\n",
      "        [ 270.9785, 1714.5609, 1628.5476],\n",
      "        [ 272.9558, 1815.3370, 1554.7000],\n",
      "        [ 284.5726, 1806.6460, 1548.8328],\n",
      "        [ 376.5337, 2192.8521, 1680.7554],\n",
      "        [ 320.9390, 2214.4399, 1449.9124],\n",
      "        [ 362.7505, 2227.8059, 1670.5782],\n",
      "        [ 320.0671, 2312.1133, 1388.5759],\n",
      "        [ 379.5493, 2361.9790, 1573.2433],\n",
      "        [ 364.5633, 2403.1960, 1513.0548],\n",
      "        [ 435.7648, 2496.7698, 1802.3217],\n",
      "        [ 468.8051, 2703.2207, 1598.1445],\n",
      "        [ 317.3277, 2794.2195, 1140.6570],\n",
      "        [ 533.5684, 2881.3811, 1769.9049],\n",
      "        [ 334.2784, 2916.8823, 1147.7479],\n",
      "        [ 362.1420, 2951.4456, 1231.6650],\n",
      "        [ 369.1045, 2985.9414, 1220.3584],\n",
      "        [ 339.9291, 3016.3208, 1127.7961],\n",
      "        [ 291.6501, 3015.0920,  972.9429],\n",
      "        [ 453.6089, 3017.2202, 1506.7047],\n",
      "        [ 375.7120, 3042.7627, 1237.7473],\n",
      "        [ 434.2469, 3033.0664, 1453.8397],\n",
      "        [ 260.5944, 3045.9241,  855.3167],\n",
      "        [ 377.9503, 3033.3286, 1253.3831],\n",
      "        [ 279.8414, 3042.8408,  924.1098],\n",
      "        [ 409.0595, 3059.9436, 1326.7535],\n",
      "        [ 426.5866, 3061.3455, 1365.7578],\n",
      "        [ 375.7699, 3037.5571, 1243.6418],\n",
      "        [ 335.2344, 3039.8247, 1109.1547],\n",
      "        [ 379.6110, 3033.8484, 1255.3341],\n",
      "        [ 446.9024, 3039.9866, 1448.9022],\n",
      "        [ 353.1396, 3045.6560, 1134.4441],\n",
      "        [ 291.9613, 3025.2297,  967.5832],\n",
      "        [ 523.2277, 3042.3157, 1603.7593]])\n",
      "data:  tensor([[[0.0050, 0.0052, 0.0052,  ..., 0.6975, 0.0844, 0.8696]],\n",
      "\n",
      "        [[0.0062, 0.0061, 0.0054,  ..., 0.6975, 0.0844, 0.8696]],\n",
      "\n",
      "        [[0.0065, 0.0063, 0.0071,  ..., 0.6956, 0.0844, 0.9130]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0041, 0.0041, 0.0040,  ..., 0.6627, 0.0886, 0.3913]],\n",
      "\n",
      "        [[0.0034, 0.0033, 0.0036,  ..., 0.6668, 0.0886, 0.3913]],\n",
      "\n",
      "        [[0.0061, 0.0061, 0.0061,  ..., 0.6700, 0.0886, 0.3913]]])\n",
      "output:  tensor([[0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855],\n",
      "        [0.0101, 5.0823, 3.8855]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 395.8026, 3013.0757, 1316.4003],\n",
      "        [ 265.3014, 2997.8833,  887.4802],\n",
      "        [ 197.6395, 2875.0806, 1287.7500],\n",
      "        [ 337.9272, 3008.7583, 1113.0916],\n",
      "        [ 305.3935, 2980.4790, 1027.1320],\n",
      "        [ 450.2816, 2987.5332, 1458.7781],\n",
      "        [ 280.6082, 2969.2268,  946.6832],\n",
      "        [ 457.2784, 2939.0513, 1562.8813],\n",
      "        [ 480.9445, 2986.8296, 1513.8969],\n",
      "        [ 253.6224, 2944.3518,  869.3400],\n",
      "        [ 307.0082, 2951.5349, 1036.1650],\n",
      "        [ 491.6572, 2954.0093, 1576.6472],\n",
      "        [ 315.7527, 2927.2156, 1078.8386],\n",
      "        [ 430.4851, 2897.5566, 1539.8713],\n",
      "        [ 373.9254, 2922.7417, 1268.5464],\n",
      "        [ 288.8830, 2907.0017,  998.6122],\n",
      "        [ 582.1630, 2912.4028, 1899.4130],\n",
      "        [ 404.1345, 2892.0549, 1396.6403],\n",
      "        [ 362.4222, 2897.0317, 1223.2104],\n",
      "        [ 303.2492, 2870.9194, 1060.8934],\n",
      "        [ 354.3931, 2856.7993, 1246.5288],\n",
      "        [ 362.2503, 2841.8469, 1286.7965],\n",
      "        [ 359.7008, 2854.1519, 1255.3021],\n",
      "        [ 242.5765, 2841.5789,  856.0610],\n",
      "        [ 320.9268, 2850.8562, 1107.5536],\n",
      "        [ 307.9547, 2792.5271, 1122.3971],\n",
      "        [ 356.9486, 2818.4214, 1271.2318],\n",
      "        [ 445.4936, 2796.6772, 1601.5748],\n",
      "        [ 304.9361, 2809.2725, 1085.6649],\n",
      "        [ 425.4701, 2788.8518, 1530.9298],\n",
      "        [ 227.7404, 2745.7263,  825.1058],\n",
      "        [ 377.2289, 2768.5989, 1366.6176],\n",
      "        [ 409.2166, 2766.5544, 1478.1381],\n",
      "        [ 455.0360, 2725.1885, 1682.6545],\n",
      "        [ 366.9807, 2733.8291, 1356.9008],\n",
      "        [ 494.0581, 2751.4680, 1744.2729],\n",
      "        [ 327.2571, 2724.7917, 1205.6993],\n",
      "        [ 548.6302, 2702.6064, 2038.4392],\n",
      "        [ 353.3077, 2702.9500, 1314.3965],\n",
      "        [ 417.8028, 2689.8435, 1554.2410],\n",
      "        [ 761.4586, 2687.3188, 2802.8630],\n",
      "        [ 586.4727, 2649.7341, 2222.5249],\n",
      "        [ 673.2647, 2668.4343, 2450.9358],\n",
      "        [ 499.6471, 2655.7886, 1874.8501],\n",
      "        [ 222.9963, 2630.6147,  853.2535],\n",
      "        [ 452.1985, 2636.8984, 1674.5621],\n",
      "        [ 409.5620, 2552.2949, 1845.5067],\n",
      "        [ 368.6745, 2585.2942, 1442.8860],\n",
      "        [ 447.5377, 2589.0029, 1729.2778],\n",
      "        [ 529.4659, 2606.2063, 2009.4550],\n",
      "        [ 457.7795, 2551.9934, 1804.7291],\n",
      "        [ 430.7672, 2545.2964, 1694.9791],\n",
      "        [ 648.5283, 2534.9019, 2531.2227],\n",
      "        [ 525.3735, 2552.4292, 1921.6823],\n",
      "        [ 397.8343, 2503.8801, 1612.9366],\n",
      "        [ 534.6509, 2502.3105, 2130.6509],\n",
      "        [ 579.2975, 2479.2649, 2352.3821],\n",
      "        [ 729.2930, 2511.0015, 2838.6074],\n",
      "        [ 442.1333, 2508.0078, 1734.9546],\n",
      "        [ 620.2151, 2477.2708, 2496.3354],\n",
      "        [ 503.4971, 2406.4468, 2088.4937],\n",
      "        [ 564.1219, 2446.1987, 2326.2188],\n",
      "        [ 361.6127, 2430.7493, 1512.4485],\n",
      "        [ 601.6282, 2460.4138, 2406.4744],\n",
      "        [ 597.1334, 2474.9417, 2305.3713],\n",
      "        [ 756.2982, 2414.4004, 3136.9602],\n",
      "        [ 438.7178, 2399.3755, 1828.4260],\n",
      "        [ 582.8745, 2397.2141, 2444.6528],\n",
      "        [ 481.3812, 2389.9585, 2037.5000],\n",
      "        [ 444.7061, 2384.9148, 1859.9187],\n",
      "        [ 484.1909, 2384.5364, 2000.8859],\n",
      "        [ 513.8727, 2346.4084, 2214.1809],\n",
      "        [ 577.5771, 2375.9165, 2355.1057],\n",
      "        [ 788.6716, 2350.4912, 3256.9944],\n",
      "        [ 438.4064, 2309.0859, 1915.5441],\n",
      "        [ 646.3790, 2316.2578, 2775.2307],\n",
      "        [ 492.6492, 2303.2493, 2141.2549],\n",
      "        [ 594.7726, 2271.8364, 2668.0503],\n",
      "        [ 496.6292, 2322.1562, 2097.0256],\n",
      "        [ 496.5819, 2251.7397, 2232.6741],\n",
      "        [ 527.6085, 2238.4407, 2388.5056],\n",
      "        [ 571.1393, 2255.4485, 2527.0015],\n",
      "        [ 568.9589, 2237.5137, 2533.5815],\n",
      "        [ 361.2447, 2232.3806, 1598.7271],\n",
      "        [ 445.7879, 2213.2390, 2008.8241],\n",
      "        [ 594.7680, 2204.9949, 2675.0750],\n",
      "        [ 600.8149, 2178.4470, 2778.1338],\n",
      "        [ 595.1639, 2203.6040, 2677.9756],\n",
      "        [ 742.1048, 2207.6873, 3283.6143],\n",
      "        [ 636.6678, 2164.1873, 2971.5435],\n",
      "        [ 644.6511, 2118.0288, 3122.8364],\n",
      "        [ 544.2660, 2147.3525, 2540.0327],\n",
      "        [ 456.4741, 2165.0588, 2109.4819],\n",
      "        [ 668.0446, 2139.1699, 3142.5852],\n",
      "        [ 290.3856, 2173.4871, 1331.0076],\n",
      "        [ 413.6039, 2162.0200, 1914.7642],\n",
      "        [ 514.8753, 2203.5818, 2205.3152],\n",
      "        [ 440.1861, 2160.7466, 2034.5031],\n",
      "        [ 649.5283, 2082.0247, 3274.1135],\n",
      "        [ 432.7106, 2132.7354, 2042.9019],\n",
      "        [ 501.3223, 2160.4395, 2230.6787],\n",
      "        [ 590.2960, 2162.8242, 2669.1128],\n",
      "        [ 626.0291, 2122.4414, 3006.6091],\n",
      "        [ 443.4703, 2119.7659, 2098.3823],\n",
      "        [ 451.4483, 2114.2339, 2141.8806],\n",
      "        [ 428.4807, 2113.5381, 2035.0518],\n",
      "        [ 621.6995, 2107.6621, 2945.6829],\n",
      "        [ 567.5129, 2156.7698, 2574.8179],\n",
      "        [ 732.0240, 2148.5872, 3265.9526],\n",
      "        [ 414.3266, 2085.4094, 2005.1069],\n",
      "        [ 677.3300, 2118.4309, 2997.1665],\n",
      "        [ 362.2136, 2074.2666, 1773.2205],\n",
      "        [ 529.5034, 2077.9976, 2554.1738],\n",
      "        [ 363.5056, 2109.3042, 1689.0865],\n",
      "        [ 496.0186, 2109.9690, 2276.0042],\n",
      "        [ 462.4776, 2085.9011, 2179.0505],\n",
      "        [ 506.9262, 2052.1313, 2506.8479],\n",
      "        [ 427.1088, 2077.2268, 2054.8484],\n",
      "        [ 529.5940, 2082.9016, 2513.3406],\n",
      "        [ 466.0087, 2177.4583, 1866.4249],\n",
      "        [ 508.7847, 2070.3232, 2442.1902],\n",
      "        [ 495.7385, 2086.1411, 2319.3853],\n",
      "        [ 672.2087, 2025.0415, 3348.9314],\n",
      "        [ 510.3947, 2034.9950, 2546.3884],\n",
      "        [ 559.7852, 2041.1389, 2745.3809],\n",
      "        [ 623.9829, 2104.1543, 2930.5310],\n",
      "        [ 390.1637, 2042.3844, 1909.5182],\n",
      "        [ 377.3430, 2059.8728, 1828.9319],\n",
      "        [ 386.9880, 2070.4460, 1815.7418],\n",
      "        [ 381.2210, 2038.9438, 1860.1489],\n",
      "        [ 370.3110, 2029.9232, 1819.8566],\n",
      "        [ 417.7970, 2043.2056, 2034.6885],\n",
      "        [ 503.4441, 1960.1101, 2521.5986],\n",
      "        [ 709.5626, 2031.3140, 3492.7886],\n",
      "        [ 340.2134, 2033.8665, 1663.8904],\n",
      "        [ 405.9694, 2015.5349, 2024.4988],\n",
      "        [ 738.7126, 2075.3220, 3320.7207],\n",
      "        [ 390.6080, 2010.4816, 1938.7502],\n",
      "        [ 538.6132, 2028.2979, 2621.5010],\n",
      "        [ 443.4266, 2021.7405, 2187.6074],\n",
      "        [ 536.4929, 2001.1860, 2694.1624],\n",
      "        [ 339.9265, 1992.0480, 1718.1759],\n",
      "        [ 470.5758, 2091.8384, 2109.3042],\n",
      "        [ 600.5494, 1972.6440, 3092.0295],\n",
      "        [ 486.6235, 1999.6666, 2468.7900],\n",
      "        [ 415.7628, 2013.3734, 2045.1859],\n",
      "        [ 569.3053, 2033.0790, 2672.9495],\n",
      "        [ 279.7103, 2004.0289, 1434.6416],\n",
      "        [ 260.3010, 2018.7188, 1311.2469],\n",
      "        [ 587.4702, 2033.5984, 2866.5298]])\n",
      "data:  tensor([[[0.0044, 0.0040, 0.0041,  ..., 0.6737, 0.0886, 0.3913]],\n",
      "\n",
      "        [[0.0034, 0.0035, 0.0035,  ..., 0.6768, 0.0886, 0.3913]],\n",
      "\n",
      "        [[0.0024, 0.0025, 0.0026,  ..., 0.6804, 0.0886, 0.3913]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0036, 0.0036, 0.0042,  ..., 0.6726, 0.0886, 0.9130]],\n",
      "\n",
      "        [[0.0031, 0.0034, 0.0032,  ..., 0.6711, 0.0886, 0.9130]],\n",
      "\n",
      "        [[0.0069, 0.0066, 0.0063,  ..., 0.6690, 0.0886, 0.9130]]])\n",
      "output:  tensor([[0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384],\n",
      "        [0.0629, 5.1352, 3.9384]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 358.3312, 2043.0491, 1720.5929],\n",
      "        [ 332.9318, 2020.2994, 1640.3513],\n",
      "        [ 384.2368, 2022.0532, 1886.2843],\n",
      "        [ 288.9631, 1979.8940, 1515.8701],\n",
      "        [ 419.0473, 1985.4739, 2125.3613],\n",
      "        [ 390.0340, 1988.8531, 1966.6868],\n",
      "        [ 521.6967, 1939.6953, 2704.2830],\n",
      "        [ 334.6994, 1971.3929, 1696.0955],\n",
      "        [ 328.8258, 1949.5703, 1709.8541],\n",
      "        [ 407.8800, 2056.9961, 1573.7699],\n",
      "        [ 450.1258, 1962.0762, 2274.1545],\n",
      "        [ 423.3753, 1939.6561, 2178.2932],\n",
      "        [ 398.3000, 1910.1704, 2121.9900],\n",
      "        [ 481.2257, 1945.9567, 2403.6655],\n",
      "        [ 326.3514, 1910.8462, 1716.7427],\n",
      "        [ 411.0659, 1916.2306, 2156.9417],\n",
      "        [ 523.5701, 1947.3307, 2572.9148],\n",
      "        [ 365.8853, 1917.7722, 1911.7035],\n",
      "        [ 375.4513, 1935.9305, 1922.3561],\n",
      "        [ 445.6223, 1902.7809, 2372.4443],\n",
      "        [ 432.0131, 1955.5021, 2156.1377],\n",
      "        [ 445.7896, 1898.1157, 2369.7747],\n",
      "        [ 426.0934, 1904.6519, 2245.1934],\n",
      "        [ 406.3590, 1951.3186, 1953.8035],\n",
      "        [ 473.7381, 1924.0447, 2444.9983],\n",
      "        [ 308.8146, 1898.6139, 1627.9987],\n",
      "        [ 515.8408, 1930.1161, 2627.8733],\n",
      "        [ 432.9750, 1893.4418, 2269.5266],\n",
      "        [ 361.6933, 1904.3782, 1885.9639],\n",
      "        [ 400.2327, 1893.4196, 2110.4346],\n",
      "        [ 285.0012, 1914.9514, 1470.5851],\n",
      "        [ 481.7587, 1865.4919, 2643.2852],\n",
      "        [ 465.7593, 1917.0182, 2394.9822],\n",
      "        [ 537.7135, 1938.1090, 2610.3894],\n",
      "        [ 545.6513, 1892.3358, 2887.0120],\n",
      "        [ 343.6728, 1889.9006, 1777.1399],\n",
      "        [ 379.2388, 1882.5947, 2002.5482],\n",
      "        [ 360.7507, 1889.8280, 1872.6100],\n",
      "        [ 495.9534, 1961.0094, 2118.1501],\n",
      "        [ 469.8964, 1868.2009, 2528.5125],\n",
      "        [ 295.5013, 1885.7003, 1564.2015],\n",
      "        [ 344.7332, 1893.2910, 1815.4335],\n",
      "        [ 281.8098, 1883.9352, 1481.2235],\n",
      "        [ 357.9636, 1869.9547, 1926.7495],\n",
      "        [ 356.4169, 1885.3206, 1899.6559],\n",
      "        [ 436.8580, 1913.1976, 2273.3091],\n",
      "        [ 271.9077, 1912.1421, 1405.0178],\n",
      "        [ 181.5791, 1913.8009,  946.5400],\n",
      "        [ 407.5169, 1942.8678, 2080.6826],\n",
      "        [ 205.4501, 1913.0133, 1077.8978],\n",
      "        [ 279.7592, 1915.6106, 1465.7733],\n",
      "        [ 361.3351, 1921.4865, 1885.9656],\n",
      "        [ 258.9101, 1923.0784, 1365.5491],\n",
      "        [ 253.9596, 1935.8804, 1308.1655],\n",
      "        [ 252.4616, 1935.0034, 1313.3772],\n",
      "        [ 285.0366, 1940.7145, 1468.2379],\n",
      "        [ 311.2945, 1961.7747, 1539.9446],\n",
      "        [ 183.4877, 1929.5576,  957.1824],\n",
      "        [ 200.0132, 1959.3618, 1013.1071],\n",
      "        [ 169.5914, 1957.5240,  865.7886],\n",
      "        [ 195.4539, 1972.0854,  986.3519],\n",
      "        [ 190.2857, 1966.2152,  971.4214],\n",
      "        [ 223.3979, 1984.3400, 1113.7991],\n",
      "        [ 204.2145, 1980.1063, 1030.1735],\n",
      "        [ 395.4151, 2017.1772, 1808.8438],\n",
      "        [ 191.2165, 1977.3972,  970.4367],\n",
      "        [ 255.3898, 1996.8403, 1111.1573],\n",
      "        [ 204.7936, 1995.1704, 1005.5941],\n",
      "        [ 186.3911, 1969.0135,  969.6592],\n",
      "        [ 171.2718, 1976.2745,  873.1385],\n",
      "        [ 333.4315, 2004.8778, 1518.1039],\n",
      "        [ 185.3242, 2003.2301,  921.6976],\n",
      "        [ 176.3571, 2030.4484,  841.9771],\n",
      "        [ 332.3854, 2028.2476, 1596.2693],\n",
      "        [ 241.4663, 2037.2347, 1185.2568],\n",
      "        [ 108.7884, 2052.6116,  530.1525],\n",
      "        [ 243.3490, 2080.6228, 1132.3832],\n",
      "        [ 242.3280, 2079.7515, 1156.0129],\n",
      "        [ 237.8676, 2093.8997, 1133.5215],\n",
      "        [ 340.9937, 2114.1245, 1514.3031],\n",
      "        [ 215.2206, 2124.7649,  985.2462],\n",
      "        [ 335.6768, 2101.2778, 1652.0691],\n",
      "        [ 232.6991, 2119.3972, 1103.1975],\n",
      "        [ 216.5998, 2119.7825, 1021.9513],\n",
      "        [ 227.5330, 2121.1233, 1083.4718],\n",
      "        [ 181.9627, 2141.1750,  850.2764],\n",
      "        [ 308.2527, 2256.2251, 1211.4177],\n",
      "        [ 187.3819, 2158.7471,  863.3795],\n",
      "        [ 122.1345, 2161.3206,  568.6051],\n",
      "        [ 213.6679, 2210.0720,  936.4738],\n",
      "        [ 159.6058, 2175.0342,  735.6487],\n",
      "        [ 374.1180, 2206.0505, 1567.8452],\n",
      "        [ 182.6249, 2209.3740,  826.1526],\n",
      "        [ 259.3748, 2239.1836, 1120.1219],\n",
      "        [ 334.5424, 2250.5444, 1313.4530],\n",
      "        [ 334.8476, 2266.1895, 1406.9623],\n",
      "        [ 204.3143, 2263.2068,  901.3283],\n",
      "        [ 273.0510, 2243.6968, 1267.1268],\n",
      "        [ 213.0769, 2272.2778,  938.7393],\n",
      "        [ 151.2285, 2297.3062,  653.8292],\n",
      "        [ 377.1692, 2308.7566, 1520.7057],\n",
      "        [ 365.4544, 2323.5356, 1450.6198],\n",
      "        [ 312.6986, 2347.9165, 1253.8641],\n",
      "        [ 189.6999, 2331.4001,  813.8589],\n",
      "        [ 177.3947, 2332.1987,  764.6042],\n",
      "        [ 288.7284, 2351.3794, 1218.5532],\n",
      "        [ 216.8379, 2257.1355, 1543.8545],\n",
      "        [ 215.7007, 2376.2180,  898.4922],\n",
      "        [ 136.3011, 2382.3958,  569.3448],\n",
      "        [ 222.2993, 2392.2651,  919.7498],\n",
      "        [ 199.1469, 2388.5005,  838.2083],\n",
      "        [ 389.8238, 2416.3386, 1583.9821],\n",
      "        [ 260.2444, 2396.9792, 1091.1976],\n",
      "        [ 284.6951, 2431.6821, 1116.2766],\n",
      "        [ 217.0237, 2438.4797,  881.4920],\n",
      "        [ 247.3913, 2429.6992, 1023.7908],\n",
      "        [ 434.2234, 2462.6870, 1660.8633],\n",
      "        [ 226.8245, 2456.0793,  913.6356],\n",
      "        [ 211.4372, 2480.7729,  845.8685],\n",
      "        [ 278.5810, 2427.4148, 1215.4343],\n",
      "        [ 339.5382, 2485.2578, 1311.7677],\n",
      "        [ 162.7088, 2480.4490,  657.3185],\n",
      "        [ 234.8727, 2492.5806,  937.0098],\n",
      "        [ 253.0948, 2490.8713, 1018.3553],\n",
      "        [ 322.1454, 2516.2351, 1262.1047],\n",
      "        [ 224.9852, 2526.5906,  890.3793],\n",
      "        [ 277.0653, 2506.1812, 1122.8632],\n",
      "        [ 228.5955, 2520.6812,  912.8254],\n",
      "        [ 279.1754, 2549.1562, 1079.0458],\n",
      "        [ 309.9256, 2529.0483, 1228.2611],\n",
      "        [ 292.6223, 2560.6846, 1131.7585],\n",
      "        [ 269.2445, 2569.3923, 1039.8065],\n",
      "        [ 248.0097, 2574.0281,  959.4949],\n",
      "        [ 193.9311, 2578.4912,  752.7590],\n",
      "        [ 303.9674, 2595.8564, 1162.1646],\n",
      "        [ 251.8965, 2595.4319,  973.4361],\n",
      "        [ 301.9724, 2610.7307, 1120.8757],\n",
      "        [ 190.5526, 2608.6079,  730.1233],\n",
      "        [ 436.3556, 2633.4580, 1591.7238],\n",
      "        [ 342.7254, 2613.8752, 1320.7875],\n",
      "        [ 198.1020, 2630.2964,  755.7439],\n",
      "        [ 290.6194, 2649.9238, 1082.8497],\n",
      "        [ 253.8868, 2642.1714,  957.3599],\n",
      "        [ 279.4541, 2651.8286, 1051.6453],\n",
      "        [ 248.2793, 2662.4968,  933.9515],\n",
      "        [ 230.4548, 2659.1902,  869.6572],\n",
      "        [ 201.7057, 2669.7634,  754.2376],\n",
      "        [ 269.3593, 2669.6128, 1009.5405],\n",
      "        [ 226.0158, 2670.2495,  849.9992],\n",
      "        [ 326.8426, 2688.8381, 1204.3379]])\n",
      "data:  tensor([[[0.0035, 0.0031, 0.0036,  ..., 0.6690, 0.0886, 0.9130]],\n",
      "\n",
      "        [[0.0045, 0.0045, 0.0045,  ..., 0.6690, 0.0886, 0.9130]],\n",
      "\n",
      "        [[0.0040, 0.0043, 0.0042,  ..., 0.6690, 0.0886, 0.9565]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0031, 0.0031, 0.0032,  ..., 0.7972, 0.0928, 0.4348]],\n",
      "\n",
      "        [[0.0023, 0.0021, 0.0022,  ..., 0.7972, 0.0928, 0.4348]],\n",
      "\n",
      "        [[0.0040, 0.0040, 0.0040,  ..., 0.7972, 0.0928, 0.4348]]])\n",
      "output:  tensor([[0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914],\n",
      "        [0.1159, 5.1882, 3.9914]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 201.8157, 2685.6990,  749.6568],\n",
      "        [ 304.6076, 2681.3926, 1133.1870],\n",
      "        [ 281.8252, 2553.5911, 1644.0845],\n",
      "        [ 256.5816, 2682.2136,  940.8080],\n",
      "        [ 318.8583, 2681.6885, 1181.0005],\n",
      "        [ 265.1246, 2660.4580,  997.7576],\n",
      "        [ 179.4180, 2663.9995,  671.7902],\n",
      "        [ 258.0689, 2648.5945,  979.2125],\n",
      "        [ 448.5875, 2675.9075, 1571.5648],\n",
      "        [ 243.2829, 2648.6450,  919.2750],\n",
      "        [ 187.3376, 2647.0027,  705.6107],\n",
      "        [ 412.9870, 2652.4207, 1515.8207],\n",
      "        [ 301.3616, 2641.6741, 1132.8220],\n",
      "        [ 228.5902, 2633.5083,  864.2194],\n",
      "        [ 328.7934, 2639.8589, 1247.2369],\n",
      "        [ 216.6666, 2648.5164,  816.5737],\n",
      "        [ 426.1762, 2683.2190, 1440.2539],\n",
      "        [ 257.3763, 2646.3938,  961.3671],\n",
      "        [ 252.5616, 2624.0295,  966.2496],\n",
      "        [ 494.9953, 2671.5452, 1670.9176],\n",
      "        [ 335.2211, 2622.1250, 1252.5028],\n",
      "        [ 466.9065, 2602.2742, 1795.9111],\n",
      "        [ 344.6776, 2603.8379, 1320.5430],\n",
      "        [ 457.1361, 2602.6763, 1754.4515],\n",
      "        [ 430.9744, 2577.5305, 1676.0216],\n",
      "        [ 515.3316, 2553.7866, 2029.4961],\n",
      "        [ 411.6287, 2546.7375, 1609.2427],\n",
      "        [ 558.2775, 2553.0825, 2177.1946],\n",
      "        [ 373.3505, 2531.4165, 1472.4126],\n",
      "        [ 544.3746, 2525.6021, 2083.0972],\n",
      "        [ 579.7457, 2514.1072, 2308.7427],\n",
      "        [ 267.6416, 2503.0088, 1073.6501],\n",
      "        [ 437.6696, 2491.5642, 1761.2239],\n",
      "        [ 392.9000, 2474.2395, 1599.0016],\n",
      "        [ 364.5522, 2503.6677, 1442.9067],\n",
      "        [ 509.5924, 2482.2922, 2039.6652],\n",
      "        [ 353.4271, 2453.1804, 1438.5920],\n",
      "        [ 450.2581, 2414.5400, 1873.7618],\n",
      "        [ 611.4354, 2419.9524, 2521.5037],\n",
      "        [ 496.8613, 2425.5603, 2022.4044],\n",
      "        [ 535.9958, 2375.6091, 2440.1985],\n",
      "        [ 553.3951, 2385.1606, 2319.5938],\n",
      "        [ 604.8509, 2376.0674, 2576.9180],\n",
      "        [ 395.3082, 2408.5916, 1628.9266],\n",
      "        [ 461.9611, 2380.6084, 1949.9550],\n",
      "        [ 630.5635, 2368.6890, 2685.9810],\n",
      "        [ 354.4394, 2382.5129, 1493.7865],\n",
      "        [ 414.9635, 2376.2124, 1747.2078],\n",
      "        [ 672.9640, 2336.5500, 2908.9197],\n",
      "        [ 499.6189, 2327.7698, 2151.8813],\n",
      "        [ 483.7122, 2334.8687, 2058.3950],\n",
      "        [ 678.3243, 2313.8728, 2919.3701],\n",
      "        [ 476.9549, 2270.3562, 2137.7107],\n",
      "        [ 471.0890, 2293.1675, 2044.5072],\n",
      "        [ 253.5367, 2266.5527, 1137.4656],\n",
      "        [ 522.1686, 2286.0403, 2250.6719],\n",
      "        [ 555.0634, 2295.2563, 2258.6167],\n",
      "        [ 535.0636, 2258.7832, 2347.0923],\n",
      "        [ 502.5867, 2270.9373, 2204.4119],\n",
      "        [ 398.7975, 2256.8115, 1758.0245],\n",
      "        [ 350.0563, 2245.2048, 1555.8781],\n",
      "        [ 605.5643, 2273.3223, 2599.3010],\n",
      "        [ 609.6006, 2269.6023, 2614.7456],\n",
      "        [ 602.6830, 2232.3582, 2698.6506],\n",
      "        [ 540.1313, 2179.6255, 2504.0420],\n",
      "        [ 455.2196, 2172.2803, 2126.9158],\n",
      "        [ 426.2986, 2174.6990, 1986.5087],\n",
      "        [ 384.2734, 2151.4302, 1794.8047],\n",
      "        [ 552.4835, 2168.6833, 2533.3591],\n",
      "        [ 426.3868, 2146.8052, 1959.3368],\n",
      "        [ 531.8879, 2130.8921, 2471.4978],\n",
      "        [ 289.3409, 2120.2573, 1372.9849],\n",
      "        [ 777.7342, 2173.7383, 3360.4756],\n",
      "        [ 396.9954, 2102.8530, 1899.9092],\n",
      "        [ 517.3313, 2041.9600, 2855.9448],\n",
      "        [ 469.3303, 2123.0500, 2186.5728],\n",
      "        [ 420.0892, 2056.7671, 2004.7445],\n",
      "        [ 591.2305, 2101.8477, 2739.0251],\n",
      "        [ 410.6266, 2037.9888, 2034.1063],\n",
      "        [ 458.3578, 2049.1375, 2301.7253],\n",
      "        [ 430.0927, 2046.0095, 2113.1208],\n",
      "        [ 362.7452, 2054.5999, 1712.4600],\n",
      "        [ 399.2542, 2045.2778, 1901.9622],\n",
      "        [ 423.9023, 2002.9453, 2140.1499],\n",
      "        [ 380.0827, 2010.9884, 1879.4412],\n",
      "        [ 522.9682, 2003.4926, 2540.3291],\n",
      "        [ 632.4082, 1979.9554, 3239.6836],\n",
      "        [ 393.9977, 2014.4962, 1932.5563],\n",
      "        [ 372.3617, 2005.1851, 1811.1962],\n",
      "        [ 543.4619, 2092.6707, 2054.9822],\n",
      "        [ 428.4115, 2007.9053, 2019.2354],\n",
      "        [ 387.7067, 1964.2045, 2005.8606],\n",
      "        [ 442.6273, 1932.7076, 2330.1875],\n",
      "        [ 394.3733, 1965.1093, 1980.5315],\n",
      "        [ 531.2717, 1954.3292, 2707.0825],\n",
      "        [ 488.5089, 1949.7156, 2509.1589],\n",
      "        [ 508.9947, 1955.5525, 2546.7380],\n",
      "        [ 368.3719, 2037.7319, 1613.4943],\n",
      "        [ 643.5610, 1869.2231, 3591.4937],\n",
      "        [ 710.6068, 1975.6945, 3546.3484],\n",
      "        [ 339.5969, 1921.5927, 1770.0507],\n",
      "        [ 302.3598, 1889.8894, 1690.7729],\n",
      "        [ 688.4146, 1873.2054, 3771.0635],\n",
      "        [ 461.9839, 1909.0140, 2419.1682],\n",
      "        [ 560.4381, 1956.9265, 2853.7095],\n",
      "        [ 557.3431, 1941.7729, 2803.8469],\n",
      "        [ 384.0711, 1904.0989, 2046.9377],\n",
      "        [ 418.9309, 2008.3575, 1933.4055],\n",
      "        [ 320.2372, 1895.6425, 1699.0781],\n",
      "        [ 421.1712, 1914.5940, 2168.0793],\n",
      "        [ 605.1021, 1858.8116, 3305.0071],\n",
      "        [ 602.0553, 1858.9291, 3720.9541],\n",
      "        [ 490.5532, 1928.0104, 2537.3352],\n",
      "        [ 512.5330, 1874.6411, 2902.9465],\n",
      "        [ 240.1877, 1909.4386, 1287.7964],\n",
      "        [ 404.3701, 1898.7367, 2165.9683],\n",
      "        [ 281.0027, 1949.5089, 1417.4636],\n",
      "        [ 419.6389, 1933.3335, 2168.1338],\n",
      "        [ 459.3050, 1943.5380, 2355.0386],\n",
      "        [ 511.8570, 1936.4446, 2649.0647],\n",
      "        [ 243.8816, 1950.5535, 1247.5835],\n",
      "        [ 416.2769, 1933.9534, 2155.6191],\n",
      "        [ 526.7109, 1939.1758, 2728.0994],\n",
      "        [ 575.5148, 1959.6019, 2877.9519],\n",
      "        [ 397.1622, 1930.5964, 2052.3037],\n",
      "        [ 480.2402, 1945.8672, 2463.7346],\n",
      "        [ 356.8387, 1932.2443, 1851.4177],\n",
      "        [ 423.7305, 1938.7290, 2203.4009],\n",
      "        [ 411.9839, 1936.0311, 2098.5129],\n",
      "        [ 488.2325, 1959.5460, 2430.0725],\n",
      "        [ 479.1279, 1890.2357, 2560.4417],\n",
      "        [ 422.3550, 1957.0549, 2118.1389],\n",
      "        [ 298.2540, 1940.6208, 1511.2979],\n",
      "        [ 502.1487, 1901.2950, 2680.5662],\n",
      "        [ 376.5547, 1912.6893, 1986.7255],\n",
      "        [ 601.5094, 1938.0363, 2958.2761],\n",
      "        [ 482.8557, 1911.0529, 2525.7632],\n",
      "        [ 387.0614, 1922.4584, 1982.7477],\n",
      "        [ 386.4481, 1897.5918, 2058.5862],\n",
      "        [ 483.0797, 1989.5010, 2286.9395],\n",
      "        [ 326.4514, 1899.2675, 1701.8711],\n",
      "        [ 422.8792, 1941.1584, 2118.2463],\n",
      "        [ 513.7560, 1899.6920, 2707.5984],\n",
      "        [ 592.5234, 1903.7806, 3112.3335],\n",
      "        [ 475.6830, 1935.3721, 2395.6921],\n",
      "        [ 494.2457, 1851.0647, 2750.2124],\n",
      "        [ 497.4122, 1878.6067, 2667.2568],\n",
      "        [ 286.2725, 1907.6401, 1492.9897],\n",
      "        [ 490.0905, 1924.3240, 2543.9800],\n",
      "        [ 474.7629, 1881.5502, 2566.2046]])\n",
      "data:  tensor([[[0.0024, 0.0026, 0.0023,  ..., 0.7972, 0.0928, 0.4348]],\n",
      "\n",
      "        [[0.0034, 0.0034, 0.0035,  ..., 0.7972, 0.0928, 0.4348]],\n",
      "\n",
      "        [[0.0034, 0.0032, 0.0034,  ..., 0.7972, 0.0928, 0.4348]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0034, 0.0034, 0.0033,  ..., 0.6904, 0.0928, 0.9565]],\n",
      "\n",
      "        [[0.0061, 0.0057, 0.0057,  ..., 0.6904, 0.0928, 0.9565]],\n",
      "\n",
      "        [[0.0057, 0.0058, 0.0058,  ..., 0.6904, 0.0928, 0.9565]]])\n",
      "Epoch 0, Iteration 20 Train Loss: 780.42\n",
      "output:  tensor([[0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445],\n",
      "        [0.1690, 5.2413, 4.0445]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 547.1839, 1935.2771, 2781.9099],\n",
      "        [ 484.2061, 1933.1044, 2464.9246],\n",
      "        [ 294.0447, 1915.0297, 1532.9064],\n",
      "        [ 502.5236, 1912.7955, 2609.8860],\n",
      "        [ 458.1053, 1903.4956, 2387.5024],\n",
      "        [ 484.8524, 1889.9286, 2564.0083],\n",
      "        [ 446.2247, 1904.6240, 2342.5713],\n",
      "        [ 481.8535, 1879.6010, 2588.8723],\n",
      "        [ 397.9787, 1884.6448, 2142.5886],\n",
      "        [ 459.2399, 1901.7362, 2400.9778],\n",
      "        [ 387.2028, 1901.1609, 2040.1361],\n",
      "        [ 392.4973, 1896.0613, 2037.2426],\n",
      "        [ 498.6377, 1872.3956, 2644.6091],\n",
      "        [ 231.7876, 1852.1873, 1279.6124],\n",
      "        [ 365.7666, 1871.4629, 1967.3900],\n",
      "        [ 447.8287, 1855.6281, 2408.9990],\n",
      "        [ 418.5866, 1843.9136, 2289.9907],\n",
      "        [ 367.7351, 1857.9962, 1941.6356],\n",
      "        [ 381.9248, 1809.5197, 2131.2683],\n",
      "        [ 419.2141, 1867.0448, 2206.0291],\n",
      "        [ 293.5057, 1832.1689, 1586.3943],\n",
      "        [ 481.5736, 1797.5222, 2716.1545],\n",
      "        [ 273.7472, 1845.0658, 1476.9830],\n",
      "        [ 368.1918, 1857.0411, 1982.4441],\n",
      "        [ 329.7679, 1843.6807, 1790.3944],\n",
      "        [ 508.6471, 1835.1405, 2772.9531],\n",
      "        [ 418.9617, 1851.3160, 2228.5220],\n",
      "        [ 258.4991, 1826.4047, 1402.9185],\n",
      "        [ 426.9228, 1804.7999, 2381.9070],\n",
      "        [ 259.7443, 1803.6829, 1460.6272],\n",
      "        [ 324.0465, 1809.0171, 1793.3379],\n",
      "        [ 447.4223, 1789.9874, 2536.7737],\n",
      "        [ 360.9518, 1791.9255, 2043.8523],\n",
      "        [ 265.6481, 1812.2455, 1482.2465],\n",
      "        [ 428.1402, 1826.1980, 2354.6594],\n",
      "        [ 306.3226, 1843.8483, 1658.2089],\n",
      "        [ 362.5332, 1852.7571, 1938.6426],\n",
      "        [ 316.0525, 1817.1942, 1752.2083],\n",
      "        [ 261.0036, 1835.2633, 1430.1971],\n",
      "        [ 276.9050, 1825.9578, 1533.3005],\n",
      "        [ 279.9623, 1838.5812, 1541.7697],\n",
      "        [ 292.0512, 1833.6938, 1603.5745],\n",
      "        [ 226.8719, 1836.3804, 1253.6770],\n",
      "        [ 320.8436, 1864.5592, 1704.7916],\n",
      "        [ 289.0908, 1852.8633, 1553.9917],\n",
      "        [ 322.2775, 1840.8320, 1762.8506],\n",
      "        [ 378.4366, 1877.0652, 1997.1111],\n",
      "        [ 242.7733, 1864.9279, 1300.0167],\n",
      "        [ 261.7299, 1850.6289, 1437.7937],\n",
      "        [ 296.5660, 1859.1392, 1597.1338],\n",
      "        [ 244.5672, 1870.9099, 1311.6382],\n",
      "        [ 297.5415, 1863.1182, 1612.4125],\n",
      "        [ 239.4671, 1870.9880, 1295.8384],\n",
      "        [ 217.0996, 1877.0204, 1163.1860],\n",
      "        [ 173.2935, 1885.5718,  922.5452],\n",
      "        [ 319.8740, 1894.0338, 1694.9882],\n",
      "        [ 193.1850, 1911.4215, 1009.5262],\n",
      "        [ 230.9227, 1903.6910, 1218.1063],\n",
      "        [ 209.6293, 1922.1289, 1093.1014],\n",
      "        [ 136.8288, 1932.2554,  709.5948],\n",
      "        [ 148.2387, 1933.2550,  770.6332],\n",
      "        [ 200.5049, 1945.4818, 1032.4265],\n",
      "        [ 194.4713, 1962.6238,  994.5317],\n",
      "        [ 169.8741, 1980.3633,  859.5668],\n",
      "        [ 165.4697, 1990.5345,  832.8594],\n",
      "        [ 219.9818, 1985.5074, 1111.4539],\n",
      "        [ 220.3699, 1986.2727, 1113.8781],\n",
      "        [ 161.3947, 1991.9922,  813.6710],\n",
      "        [ 213.8636, 2021.8131, 1063.0914],\n",
      "        [ 197.9678, 2030.4482,  981.4630],\n",
      "        [ 179.5128, 2059.8726,  872.0390],\n",
      "        [ 226.3112, 2050.9192, 1110.0144],\n",
      "        [ 152.4417, 2047.9475,  747.0209],\n",
      "        [ 197.5452, 2055.7339,  964.5391],\n",
      "        [ 193.7490, 2057.6106,  942.2611],\n",
      "        [ 202.6716, 2060.2246,  987.7914],\n",
      "        [ 216.2126, 2085.3647, 1038.9150],\n",
      "        [ 181.8138, 2094.7317,  868.7952],\n",
      "        [ 144.2326, 2103.5903,  685.0854],\n",
      "        [ 172.8447, 2102.9199,  825.6475],\n",
      "        [ 150.2515, 2119.5032,  707.2263],\n",
      "        [ 175.1916, 2124.8989,  825.9495],\n",
      "        [ 200.1640, 2127.2649,  945.0131],\n",
      "        [ 185.3540, 2144.4258,  868.6406],\n",
      "        [ 171.0302, 2152.7649,  796.3840],\n",
      "        [ 174.7164, 2151.1899,  814.2388],\n",
      "        [ 220.0251, 2152.4355, 1025.5100],\n",
      "        [ 252.6736, 2167.6169, 1168.5347],\n",
      "        [ 220.5028, 2187.6687, 1009.6530],\n",
      "        [ 215.9358, 2194.7957,  986.6986],\n",
      "        [ 181.2161, 2194.6616,  828.4335],\n",
      "        [ 191.1371, 2193.7625,  873.1639],\n",
      "        [ 178.1768, 2209.3237,  806.3068],\n",
      "        [ 221.9062, 2226.6721,  999.7651],\n",
      "        [ 190.2193, 2245.8528,  850.1116],\n",
      "        [ 184.1812, 2265.7092,  815.7739],\n",
      "        [ 246.3486, 2279.6169, 1083.5771],\n",
      "        [ 215.1555, 2274.0540,  951.8630],\n",
      "        [ 202.4476, 2285.8506,  886.1833],\n",
      "        [ 218.9459, 2297.2727,  957.4539],\n",
      "        [ 277.3616, 2292.2178, 1214.9150],\n",
      "        [ 223.2379, 2290.1345,  976.9668],\n",
      "        [ 287.9050, 2299.0825, 1263.2781],\n",
      "        [ 218.7437, 2318.9836,  944.6622],\n",
      "        [ 197.3159, 2319.3242,  853.6765],\n",
      "        [ 154.8632, 2331.6960,  667.5792],\n",
      "        [ 230.8525, 2352.4460,  988.0305],\n",
      "        [ 167.3690, 2372.4143,  707.8127],\n",
      "        [ 207.7027, 2362.3438,  880.6810],\n",
      "        [ 202.2975, 2360.8357,  858.7217],\n",
      "        [ 274.5107, 2352.5859, 1172.6086],\n",
      "        [ 256.4439, 2359.7856, 1090.7817],\n",
      "        [ 175.3819, 2389.6958,  733.3226],\n",
      "        [ 197.6121, 2381.8706,  834.2743],\n",
      "        [ 204.2724, 2419.6101,  845.2130],\n",
      "        [ 201.8540, 2445.3555,  824.7951],\n",
      "        [ 201.0556, 2459.0173,  820.7419],\n",
      "        [ 267.6428, 2458.2297, 1094.0448],\n",
      "        [ 175.6306, 2479.4268,  705.6514],\n",
      "        [ 160.3598, 2483.2026,  645.9807],\n",
      "        [ 209.5453, 2484.7888,  847.2086],\n",
      "        [ 224.7675, 2478.0471,  911.5858],\n",
      "        [ 215.7583, 2488.7659,  868.8039],\n",
      "        [ 257.4211, 2484.5935, 1039.4153],\n",
      "        [ 256.3489, 2500.5063, 1025.8248],\n",
      "        [ 233.0249, 2501.7854,  932.9421],\n",
      "        [ 293.2805, 2510.4207, 1170.0306],\n",
      "        [ 216.2275, 2532.3435,  857.2471],\n",
      "        [ 306.7169, 2541.6938, 1212.7449],\n",
      "        [ 234.8119, 2556.0486,  921.1112],\n",
      "        [ 240.7328, 2574.9219,  935.3687],\n",
      "        [ 213.7737, 2569.4426,  835.0837],\n",
      "        [ 312.9625, 2580.1836, 1218.8583],\n",
      "        [ 257.9240, 2612.2776,  987.7654],\n",
      "        [ 218.0583, 2616.1094,  835.1760],\n",
      "        [ 196.0193, 2626.8726,  745.8872],\n",
      "        [ 238.7254, 2628.6431,  912.7783],\n",
      "        [ 273.0053, 2614.1992, 1050.5635],\n",
      "        [ 178.9800, 2596.8115,  691.2354],\n",
      "        [ 224.7249, 2588.3718,  871.8503],\n",
      "        [ 231.6949, 2585.7969,  897.5617],\n",
      "        [ 302.6402, 2585.2942, 1175.8175],\n",
      "        [ 295.8077, 2578.4575, 1155.9277],\n",
      "        [ 263.4676, 2584.9871, 1023.9526],\n",
      "        [ 262.8303, 2583.9089, 1017.2512],\n",
      "        [ 244.8376, 2587.9336,  949.7177],\n",
      "        [ 245.0893, 2575.1733,  953.2102],\n",
      "        [ 164.1114, 2571.5708,  637.2456],\n",
      "        [ 272.5274, 2563.0583, 1070.6826],\n",
      "        [ 328.8223, 2572.8999, 1283.5326]])\n",
      "data:  tensor([[[0.0059, 0.0059, 0.0057,  ..., 0.6904, 0.0928, 0.9565]],\n",
      "\n",
      "        [[0.0061, 0.0065, 0.0062,  ..., 0.6904, 0.0928, 0.9565]],\n",
      "\n",
      "        [[0.0033, 0.0033, 0.0039,  ..., 0.6904, 0.0928, 1.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0019, 0.0022, 0.0022,  ..., 0.8114, 0.0970, 0.4783]],\n",
      "\n",
      "        [[0.0033, 0.0033, 0.0033,  ..., 0.8114, 0.0970, 0.4783]],\n",
      "\n",
      "        [[0.0039, 0.0036, 0.0036,  ..., 0.8114, 0.0970, 0.4783]]])\n",
      "output:  tensor([[0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977],\n",
      "        [0.2222, 5.2944, 4.0977]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 237.1183, 2574.2629,  923.4302],\n",
      "        [ 380.4761, 2544.4531, 1506.2131],\n",
      "        [ 252.6855, 2566.2197,  990.6785],\n",
      "        [ 305.2073, 2560.1260, 1199.5327],\n",
      "        [ 243.0549, 2578.1448,  942.8810],\n",
      "        [ 287.9076, 2566.8508, 1123.7268],\n",
      "        [ 238.8825, 2562.2542,  932.4235],\n",
      "        [ 276.5708, 2561.8352, 1077.1018],\n",
      "        [ 333.5039, 2551.9934, 1314.9902],\n",
      "        [ 362.3065, 2537.1418, 1434.4719],\n",
      "        [ 353.4472, 2512.2917, 1428.1942],\n",
      "        [ 496.4484, 2521.5078, 1971.5692],\n",
      "        [ 377.1131, 2517.6038, 1500.9149],\n",
      "        [ 420.1599, 2499.9927, 1686.6465],\n",
      "        [ 483.4507, 2510.9849, 1926.9982],\n",
      "        [ 419.9792, 2476.5728, 1696.7288],\n",
      "        [ 473.5573, 2472.6794, 1905.6665],\n",
      "        [ 364.0444, 2452.5327, 1489.5276],\n",
      "        [ 425.3082, 2456.9062, 1719.7429],\n",
      "        [ 314.0557, 2431.2910, 1282.3346],\n",
      "        [ 579.3438, 2422.9072, 2391.7090],\n",
      "        [ 388.2316, 2403.0115, 1626.5096],\n",
      "        [ 443.2898, 2408.4744, 1839.9031],\n",
      "        [ 438.7781, 2385.7302, 1836.3843],\n",
      "        [ 452.9018, 2370.5879, 1912.0142],\n",
      "        [ 434.5414, 2363.8237, 1854.8394],\n",
      "        [ 507.8093, 2377.0784, 2142.7703],\n",
      "        [ 420.8535, 2372.3596, 1783.2589],\n",
      "        [ 608.5796, 2375.5479, 2556.5679],\n",
      "        [ 389.1333, 2369.9678, 1647.0353],\n",
      "        [ 489.2560, 2368.3088, 2063.7930],\n",
      "        [ 626.3449, 2320.1121, 2728.8354],\n",
      "        [ 441.1548, 2336.4998, 1892.1156],\n",
      "        [ 458.4858, 2294.8318, 2014.7645],\n",
      "        [ 431.6597, 2279.3601, 1902.2648],\n",
      "        [ 517.0384, 2260.1460, 2295.1411],\n",
      "        [ 476.7159, 2250.0083, 2126.5283],\n",
      "        [ 510.9918, 2247.9753, 2275.4504],\n",
      "        [ 489.8691, 2227.7615, 2212.1504],\n",
      "        [ 517.4481, 2194.0081, 2383.5178],\n",
      "        [ 409.8878, 2193.9580, 1869.9883],\n",
      "        [ 544.0923, 2171.7388, 2504.7815],\n",
      "        [ 377.1571, 2158.7134, 1767.9985],\n",
      "        [ 513.3817, 2166.7791, 2359.6743],\n",
      "        [ 527.1514, 2167.5552, 2400.1763],\n",
      "        [ 381.5942, 2136.6675, 1785.3494],\n",
      "        [ 512.0696, 2155.0662, 2378.3530],\n",
      "        [ 368.2081, 2162.2659, 1687.2631],\n",
      "        [ 420.0207, 2128.8423, 1986.6106],\n",
      "        [ 555.9659, 2124.7593, 2655.2314],\n",
      "        [ 594.9888, 2133.1543, 2794.5208],\n",
      "        [ 508.6236, 2126.8481, 2414.9299],\n",
      "        [ 399.1024, 2120.1848, 1897.1591],\n",
      "        [ 657.2313, 2123.5918, 3082.2444],\n",
      "        [ 546.4204, 2112.1248, 2586.2913],\n",
      "        [ 319.5306, 2105.1318, 1499.7119],\n",
      "        [ 474.7263, 2095.7205, 2273.6484],\n",
      "        [ 531.5375, 2086.4932, 2555.4946],\n",
      "        [ 482.6325, 2080.1257, 2313.9089],\n",
      "        [ 564.8684, 2058.9141, 2738.9058],\n",
      "        [ 419.6139, 2027.7839, 2091.4858],\n",
      "        [ 390.2916, 1993.8689, 1988.3496],\n",
      "        [ 418.0630, 1981.7429, 2111.9155],\n",
      "        [ 263.4824, 1952.4692, 1369.7592],\n",
      "        [ 306.9908, 1968.3097, 1529.3629],\n",
      "        [ 322.4617, 1972.0743, 1599.5021],\n",
      "        [ 361.4924, 1921.5759, 1901.2905],\n",
      "        [ 605.1919, 1899.2339, 3219.0491],\n",
      "        [ 356.6687, 1910.4272, 1881.3827],\n",
      "        [ 382.0204, 1905.7355, 2027.9650],\n",
      "        [ 386.7528, 1898.0330, 2081.7922],\n",
      "        [ 749.0369, 1900.8203, 3961.2444],\n",
      "        [ 385.9540, 1902.5908, 2024.3011],\n",
      "        [ 571.8879, 1896.6143, 3005.0532],\n",
      "        [ 494.8029, 1900.6246, 2584.9065],\n",
      "        [ 424.4691, 1862.2524, 2309.6836],\n",
      "        [ 404.0782, 1872.7251, 2148.9529],\n",
      "        [ 523.6206, 1842.8708, 2879.9443],\n",
      "        [ 370.3627, 1854.6953, 2015.8792],\n",
      "        [ 485.4867, 1831.3423, 2681.9260],\n",
      "        [ 333.8307, 1830.8116, 1836.5358],\n",
      "        [ 426.2400, 1841.2844, 2307.1455],\n",
      "        [ 311.6588, 1853.8296, 1640.4208],\n",
      "        [ 478.2560, 1841.9827, 2559.4460],\n",
      "        [ 444.8671, 1820.9028, 2461.8838],\n",
      "        [ 461.0489, 1815.8594, 2550.3660],\n",
      "        [ 391.7780, 1805.6881, 2200.1377],\n",
      "        [ 373.7005, 1797.4998, 2105.1331],\n",
      "        [ 602.1917, 1787.8202, 3396.2288],\n",
      "        [ 473.0963, 1788.0267, 2659.6272],\n",
      "        [ 482.5882, 1764.6069, 2788.1628],\n",
      "        [ 361.3170, 1771.5887, 2038.8597],\n",
      "        [ 324.4410, 1797.2988, 1763.7125],\n",
      "        [ 260.3247, 1770.9576, 1458.2279],\n",
      "        [ 325.1549, 1734.9813, 1916.7262],\n",
      "        [ 381.9776, 1754.3798, 2202.1064],\n",
      "        [ 480.6269, 1768.8909, 2676.2517],\n",
      "        [ 269.5407, 1747.8560, 1519.4678],\n",
      "        [ 417.1404, 1733.6968, 2409.2673],\n",
      "        [ 485.7409, 1779.6932, 2672.9226],\n",
      "        [ 354.3832, 1736.7296, 1993.6417],\n",
      "        [ 469.2645, 1704.8533, 2756.0215],\n",
      "        [ 331.6371, 1693.6656, 1986.1885],\n",
      "        [ 390.9878, 1676.6021, 2361.0190],\n",
      "        [ 309.9064, 1690.6718, 1868.2961],\n",
      "        [ 379.4031, 1715.5327, 2207.0813],\n",
      "        [ 315.1606, 1721.5148, 1835.1719],\n",
      "        [ 429.4569, 1756.9825, 2397.8325],\n",
      "        [ 489.0632, 1749.2413, 2777.6511],\n",
      "        [ 331.2197, 1701.0607, 1986.6226],\n",
      "        [ 300.7693, 1736.8524, 1723.6720],\n",
      "        [ 470.7949, 1758.0663, 2636.9365],\n",
      "        [ 509.8995, 1724.8884, 2966.5503],\n",
      "        [ 433.6664, 1719.5096, 2550.6223],\n",
      "        [ 239.5698, 1723.5479, 1410.9647],\n",
      "        [ 321.6898, 1736.3052, 1863.3809],\n",
      "        [ 262.0845, 1721.8833, 1539.1670],\n",
      "        [ 255.2699, 1709.7909, 1537.9083],\n",
      "        [ 334.6132, 1724.8885, 1975.5874],\n",
      "        [ 402.3472, 1740.4330, 2298.6335],\n",
      "        [ 465.1051, 1687.3259, 2831.5054],\n",
      "        [ 391.0349, 1723.7434, 2289.2517],\n",
      "        [ 319.7752, 1756.4186, 1785.3962],\n",
      "        [ 429.4635, 1746.0352, 2452.9241],\n",
      "        [ 270.7062, 1750.5426, 1517.6132],\n",
      "        [ 529.6094, 1757.2340, 2982.1443],\n",
      "        [ 408.2649, 1707.3639, 2403.9778],\n",
      "        [ 211.3243, 1706.4341, 1251.1964],\n",
      "        [ 285.0171, 1706.1715, 1675.0776],\n",
      "        [ 382.8974, 1714.1084, 2224.3103],\n",
      "        [ 321.4344, 1697.5027, 1931.5176],\n",
      "        [ 384.3007, 1679.7687, 2337.2776],\n",
      "        [ 366.6102, 1653.6957, 2271.9700],\n",
      "        [ 421.9977, 1725.6368, 2423.1704],\n",
      "        [ 374.0678, 1707.1769, 2177.6550],\n",
      "        [ 414.5852, 1681.5562, 2449.8108],\n",
      "        [ 311.5010, 1692.3921, 1814.4250],\n",
      "        [ 450.0816, 1682.6788, 2688.5684],\n",
      "        [ 356.6672, 1689.1693, 2106.0706],\n",
      "        [ 442.3851, 1653.8744, 2721.5444],\n",
      "        [ 359.9731, 1656.8517, 2236.4160],\n",
      "        [ 363.1210, 1669.4692, 2174.8877],\n",
      "        [ 378.2762, 1643.7704, 2332.1633],\n",
      "        [ 420.2704, 1658.1865, 2546.0745],\n",
      "        [ 428.0933, 1648.7415, 2570.0029],\n",
      "        [ 313.5121, 1648.2610, 1855.7029],\n",
      "        [ 429.8590, 1661.9790, 2522.5630],\n",
      "        [ 358.8918, 1649.5178, 2129.6096],\n",
      "        [ 303.6410, 1637.8497, 1877.3109],\n",
      "        [ 315.9859, 1618.7418, 1989.4115]])\n",
      "data:  tensor([[[0.0027, 0.0030, 0.0028,  ..., 0.8114, 0.0970, 0.4783]],\n",
      "\n",
      "        [[0.0043, 0.0040, 0.0039,  ..., 0.8114, 0.0970, 0.4783]],\n",
      "\n",
      "        [[0.0030, 0.0030, 0.0034,  ..., 0.8114, 0.0970, 0.4783]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0042, 0.0044, 0.0049,  ..., 0.6868, 0.0970, 1.0000]],\n",
      "\n",
      "        [[0.0037, 0.0035, 0.0035,  ..., 0.6868, 0.0970, 1.0000]],\n",
      "\n",
      "        [[0.0039, 0.0041, 0.0038,  ..., 0.6868, 0.0970, 1.0000]]])\n",
      "output:  tensor([[0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509],\n",
      "        [0.2755, 5.3477, 4.1509]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 489.6948, 1691.0123, 2548.9255],\n",
      "        [ 414.2685, 1596.2937, 2649.1548],\n",
      "        [ 297.7146, 1612.2961, 1850.0476],\n",
      "        [ 389.7146, 1630.1808, 2407.2361],\n",
      "        [ 427.0381, 1611.1232, 2678.6455],\n",
      "        [ 354.4681, 1633.4707, 2146.3669],\n",
      "        [ 229.3444, 1631.8564, 1371.2571],\n",
      "        [ 398.1903, 1622.7466, 2391.2029],\n",
      "        [ 410.6217, 1614.0947, 2535.5557],\n",
      "        [ 337.8153, 1600.2930, 2108.0122],\n",
      "        [ 370.4585, 1621.6339, 2185.6982],\n",
      "        [ 284.1141, 1588.4962, 1827.8739],\n",
      "        [ 273.1960, 1609.6263, 1699.6284],\n",
      "        [ 203.5858, 1612.1174, 1231.7025],\n",
      "        [ 444.2894, 1593.2776, 2763.1577],\n",
      "        [ 456.1303, 1556.7094, 2980.5088],\n",
      "        [ 383.7872, 1572.9351, 2514.6589],\n",
      "        [ 299.2348, 1577.9677, 1928.5261],\n",
      "        [ 342.9252, 1624.3104, 2058.3630],\n",
      "        [ 335.6495, 1603.8900, 2107.1699],\n",
      "        [ 347.7491, 1581.8271, 2168.2036],\n",
      "        [ 309.2625, 1576.4874, 1985.7272],\n",
      "        [ 314.6193, 1589.2783, 1972.3954],\n",
      "        [ 325.8671, 1594.7019, 2043.1887],\n",
      "        [ 295.8167, 1590.1274, 1867.1156],\n",
      "        [ 189.6266, 1604.8898, 1182.3658],\n",
      "        [ 280.6965, 1618.5350, 1657.5573],\n",
      "        [ 174.5184, 1742.1588, 1017.7939],\n",
      "        [ 231.4699, 1901.0828, 1206.5797],\n",
      "        [ 368.7199, 2069.4126, 1854.8275],\n",
      "        [ 385.4846, 2639.4287, 1493.9935],\n",
      "        [1125.6781, 3221.7720, 3495.5129],\n",
      "        [ 932.6251, 3696.7009, 2522.4900],\n",
      "        [ 492.7321, 3885.8311, 1275.0103],\n",
      "        [ 705.0527, 3865.6897, 1837.7992],\n",
      "        [ 624.4236, 3871.0796, 1620.7802],\n",
      "        [ 805.2297, 3893.3044, 2040.2849],\n",
      "        [ 830.6201, 3864.9636, 2158.5029],\n",
      "        [ 802.7292, 3824.8262, 2142.8140],\n",
      "        [ 630.5972, 3837.7065, 1652.1057],\n",
      "        [ 545.6049, 3753.9968, 1479.3228],\n",
      "        [ 569.5197, 3779.5562, 1511.3850],\n",
      "        [ 603.8277, 3753.2930, 1615.8396],\n",
      "        [ 809.8318, 3766.8225, 2073.2786],\n",
      "        [ 757.7065, 3718.4563, 1992.3815],\n",
      "        [ 617.6575, 3697.3318, 1668.5524],\n",
      "        [ 655.4916, 3653.3459, 1809.4659],\n",
      "        [ 548.6193, 3646.9341, 1508.5690],\n",
      "        [ 543.8080, 3637.5112, 1496.7014],\n",
      "        [ 524.4741, 3606.0537, 1457.2629],\n",
      "        [ 556.1630, 3586.3984, 1547.7826],\n",
      "        [ 401.5639, 3541.8599, 1139.2996],\n",
      "        [ 386.4590, 3526.3601, 1088.1665],\n",
      "        [ 359.1410, 3505.2524, 1007.8917],\n",
      "        [ 373.4545, 3457.1333, 1083.7483],\n",
      "        [ 362.4755, 3430.3958, 1061.6742],\n",
      "        [ 356.1757, 3305.0071, 1065.1588],\n",
      "        [ 374.4554, 3383.1423, 1104.9293],\n",
      "        [ 457.3018, 3334.8672, 1376.7506],\n",
      "        [ 439.8019, 3326.1313, 1320.1183],\n",
      "        [ 317.8307, 3295.0146,  967.1435],\n",
      "        [ 426.7152, 3282.7322, 1292.1826],\n",
      "        [ 498.0964, 3223.1907, 1592.0176],\n",
      "        [ 389.8310, 3233.9988, 1200.4799],\n",
      "        [ 424.9360, 3221.3923, 1295.7317],\n",
      "        [ 350.9658, 3191.9734, 1098.4871],\n",
      "        [ 508.7437, 3162.2754, 1610.8750],\n",
      "        [ 339.2607, 3148.9207, 1079.9205],\n",
      "        [ 379.5906, 3150.2834, 1196.8201],\n",
      "        [ 292.0736, 3124.5623,  937.0878],\n",
      "        [ 358.7198, 3110.4141, 1155.3400],\n",
      "        [ 349.2314, 3102.9631, 1121.5298],\n",
      "        [ 356.5681, 3087.8989, 1151.0587],\n",
      "        [ 244.7191, 3074.1921,  796.1591],\n",
      "        [ 379.2252, 3081.4368, 1195.1487],\n",
      "        [ 456.3081, 3067.5901, 1422.9891],\n",
      "        [ 357.4099, 3028.2798, 1190.0457],\n",
      "        [ 307.3991, 3029.8713, 1016.8394],\n",
      "        [ 379.2297, 3012.7517, 1266.0106],\n",
      "        [ 319.0619, 3014.4387, 1058.7347],\n",
      "        [ 331.4149, 3016.9688, 1091.1794],\n",
      "        [ 257.7332, 3002.1785,  857.9377],\n",
      "        [ 487.3024, 3014.7515, 1512.3951],\n",
      "        [ 278.0613, 2977.7810,  938.3727],\n",
      "        [ 402.9819, 2977.5129, 1349.6259],\n",
      "        [ 312.4654, 2974.2788, 1054.9115],\n",
      "        [ 352.8140, 2957.0757, 1198.8674],\n",
      "        [ 244.5913, 2983.7185,  816.9012],\n",
      "        [ 327.4973, 2975.4353, 1095.6271],\n",
      "        [ 456.1967, 2965.3364, 1530.7322],\n",
      "        [ 355.1927, 2957.7012, 1203.5079],\n",
      "        [ 308.4466, 2956.2715, 1045.0818],\n",
      "        [ 518.1939, 2953.4897, 1704.4999],\n",
      "        [ 362.1563, 2952.2441, 1227.0685],\n",
      "        [ 264.5630, 2944.8154,  901.9034],\n",
      "        [ 604.5149, 2944.7373, 1929.4717],\n",
      "        [ 432.1179, 2932.3376, 1477.9437],\n",
      "        [ 338.6910, 2930.9580, 1159.6824],\n",
      "        [ 318.9336, 2929.5112, 1116.9841],\n",
      "        [ 385.0726, 2873.9858, 1358.3715],\n",
      "        [ 349.3093, 2932.8179, 1187.6522],\n",
      "        [ 454.5792, 2928.0200, 1539.2546],\n",
      "        [ 360.8493, 2924.2498, 1232.5190],\n",
      "        [ 233.2176, 2935.6106,  789.2960],\n",
      "        [ 401.4206, 2895.5684, 1394.1692],\n",
      "        [ 295.4554, 2913.7825, 1017.0680],\n",
      "        [ 240.8396, 2909.9844,  829.6888],\n",
      "        [ 320.7197, 2912.0286, 1106.4725],\n",
      "        [ 306.9864, 2906.7446, 1057.9102],\n",
      "        [ 295.5027, 2894.2725, 1024.8074],\n",
      "        [ 432.2812, 2914.7769, 1420.6836],\n",
      "        [ 360.8768, 2889.5527, 1300.0773],\n",
      "        [ 487.4808, 2888.3406, 1681.6978],\n",
      "        [ 423.3008, 2876.2017, 1467.3077],\n",
      "        [ 290.4984, 2877.7866, 1011.3382],\n",
      "        [ 348.0559, 2865.9287, 1220.5408],\n",
      "        [ 307.1218, 2876.7590, 1068.1561],\n",
      "        [ 401.2052, 2866.0266, 1403.2556],\n",
      "        [ 265.0554, 2875.9631,  922.9470],\n",
      "        [ 325.3256, 2868.3462, 1136.4978],\n",
      "        [ 353.5633, 2862.9768, 1238.4015],\n",
      "        [ 390.3068, 2853.9004, 1373.8611],\n",
      "        [ 367.3419, 2848.2031, 1293.4540],\n",
      "        [ 245.1410, 2855.4587,  858.4755],\n",
      "        [ 287.5174, 2837.3394, 1019.4005],\n",
      "        [ 339.6863, 2828.8159, 1213.3838],\n",
      "        [ 317.2666, 2829.8271, 1123.6090],\n",
      "        [ 256.4662, 2825.2983,  909.4003],\n",
      "        [ 409.2146, 2819.6279, 1459.1135],\n",
      "        [ 269.3926, 2815.8242,  967.6812],\n",
      "        [ 298.2333, 2821.6387, 1057.5166],\n",
      "        [ 313.2950, 2805.6250, 1123.9325],\n",
      "        [ 271.5161, 2801.3911,  972.9788],\n",
      "        [ 374.4882, 2794.1189, 1344.4199],\n",
      "        [ 303.9394, 2798.3416, 1085.9597],\n",
      "        [ 324.3550, 2788.6509, 1164.9242],\n",
      "        [ 333.3660, 2762.1643, 1214.3525],\n",
      "        [ 325.3951, 2762.2202, 1182.9203],\n",
      "        [ 434.3460, 2755.9421, 1578.7218],\n",
      "        [ 391.6492, 2747.2903, 1427.9419],\n",
      "        [ 304.7705, 2744.1233, 1112.2968],\n",
      "        [ 381.1370, 2722.1833, 1409.9227],\n",
      "        [ 429.9924, 2724.8867, 1583.9861],\n",
      "        [ 618.8439, 2704.9409, 2292.5320],\n",
      "        [ 387.2968, 2707.1584, 1433.2441],\n",
      "        [ 346.5100, 2694.7476, 1296.4009],\n",
      "        [ 371.5649, 2693.9041, 1385.6786],\n",
      "        [ 352.0489, 2692.6194, 1308.3503],\n",
      "        [ 443.9201, 2677.7173, 1661.5165],\n",
      "        [ 342.0053, 2660.7766, 1301.5861]])\n",
      "data:  tensor([[[0.0052, 0.0052, 0.0055,  ..., 0.6846, 0.0970, 1.0000]],\n",
      "\n",
      "        [[0.0053, 0.0053, 0.0054,  ..., 0.6833, 0.0970, 1.0000]],\n",
      "\n",
      "        [[0.0030, 0.0027, 0.0027,  ..., 0.6833, 0.1013, 0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0036, 0.0036, 0.0041,  ..., 0.7900, 0.1013, 0.5217]],\n",
      "\n",
      "        [[0.0054, 0.0054, 0.0052,  ..., 0.7900, 0.1013, 0.5217]],\n",
      "\n",
      "        [[0.0042, 0.0042, 0.0045,  ..., 0.7900, 0.1013, 0.5217]]])\n",
      "output:  tensor([[0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042],\n",
      "        [0.3287, 5.4010, 4.2042]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 510.1944, 2665.1611, 1922.0375],\n",
      "        [ 257.2164, 2649.1477,  978.0996],\n",
      "        [ 525.9962, 2653.1467, 1981.1624],\n",
      "        [ 334.7674, 2658.8662, 1242.5416],\n",
      "        [ 664.2611, 2599.8333, 2572.0439],\n",
      "        [ 523.2451, 2607.9771, 2019.4115],\n",
      "        [ 555.5150, 2606.5581, 2134.9871],\n",
      "        [ 457.6558, 2581.2615, 1796.7300],\n",
      "        [ 340.7078, 2594.4111, 1307.7651],\n",
      "        [ 477.8989, 2575.1116, 1868.8569],\n",
      "        [ 448.2497, 2578.9043, 1728.6270],\n",
      "        [ 340.8904, 2563.0583, 1333.9919],\n",
      "        [ 485.0514, 2566.4041, 1887.8010],\n",
      "        [ 606.9694, 2541.1409, 2396.9993],\n",
      "        [ 646.5224, 2551.0718, 2520.9229],\n",
      "        [ 574.5156, 2482.0632, 2349.0503],\n",
      "        [ 605.8058, 2536.1362, 2374.6257],\n",
      "        [ 445.9943, 2491.4858, 1816.2238],\n",
      "        [ 533.1213, 2475.9583, 2168.1797],\n",
      "        [ 508.2394, 2476.9636, 2057.6357],\n",
      "        [ 446.9213, 2465.7703, 1821.0739],\n",
      "        [ 476.6646, 2446.2659, 1954.4940],\n",
      "        [ 535.3520, 2430.1572, 2193.5879],\n",
      "        [ 612.7944, 2415.3108, 2532.0496],\n",
      "        [ 558.4924, 2386.6294, 2345.9116],\n",
      "        [ 446.4543, 2378.0054, 1882.7822],\n",
      "        [ 667.6059, 2411.6021, 2732.1653],\n",
      "        [ 420.1712, 2371.4927, 1749.4065],\n",
      "        [ 450.3155, 2341.1245, 1936.1833],\n",
      "        [ 507.8845, 2329.6462, 2196.9314],\n",
      "        [ 402.7798, 2302.5398, 1762.9628],\n",
      "        [ 411.4475, 2308.1196, 1791.6721],\n",
      "        [ 523.6011, 2332.1934, 2226.0366],\n",
      "        [ 347.0676, 2322.0222, 1492.5535],\n",
      "        [ 586.5253, 2298.3899, 2590.0601],\n",
      "        [ 521.8685, 2324.9937, 2238.0857],\n",
      "        [ 508.7063, 2319.7266, 2197.8052],\n",
      "        [ 362.8521, 2302.6069, 1598.8760],\n",
      "        [ 443.0116, 2296.8484, 1938.6243],\n",
      "        [ 593.3785, 2260.9558, 2644.4236],\n",
      "        [ 629.7595, 2279.3545, 2767.1616],\n",
      "        [ 311.1400, 2282.4871, 1355.0604],\n",
      "        [ 694.3740, 2269.0103, 3056.7188],\n",
      "        [ 582.1519, 2240.2227, 2615.3311],\n",
      "        [ 376.4892, 2224.3374, 1724.4884],\n",
      "        [ 442.6656, 2214.9202, 2012.1055],\n",
      "        [ 394.0802, 2222.8462, 1777.9351],\n",
      "        [ 413.7437, 2234.0896, 1835.9613],\n",
      "        [ 531.1515, 2209.8152, 2401.0312],\n",
      "        [ 258.2517, 2182.6697, 1199.9557],\n",
      "        [ 605.8898, 2201.5989, 2737.8049],\n",
      "        [ 374.3432, 2171.6328, 1736.5927],\n",
      "        [ 667.7853, 2186.8533, 3036.3691],\n",
      "        [ 592.5753, 2179.0112, 2688.0562],\n",
      "        [ 609.2654, 2165.9077, 2784.9370],\n",
      "        [ 472.5426, 2135.3438, 2237.2300],\n",
      "        [ 427.4707, 2146.6880, 1994.9292],\n",
      "        [ 611.4460, 2156.3062, 2821.8103],\n",
      "        [ 537.4847, 2121.6929, 2608.6147],\n",
      "        [ 582.9172, 2166.4382, 2656.4033],\n",
      "        [ 674.4763, 2111.3821, 3213.7529],\n",
      "        [ 479.8979, 2117.8501, 2272.0798],\n",
      "        [ 386.2224, 2151.9382, 1763.8304],\n",
      "        [ 640.7358, 2064.7153, 3155.9233],\n",
      "        [ 266.6481, 2114.0686, 1246.1256],\n",
      "        [ 762.4237, 2172.2693, 3348.9983],\n",
      "        [ 604.6605, 2054.0525, 3076.6782],\n",
      "        [ 296.4265, 2088.5430, 1423.4417],\n",
      "        [ 476.1758, 2035.0227, 2388.9023],\n",
      "        [ 674.6735, 2117.1350, 3013.5056],\n",
      "        [ 469.8401, 2046.5625, 2362.6497],\n",
      "        [ 402.3498, 2038.4523, 2010.0828],\n",
      "        [ 402.4650, 2051.0864, 1984.5593],\n",
      "        [ 517.2260, 2059.0684, 2504.0317],\n",
      "        [ 446.2315, 2048.8086, 2187.5500],\n",
      "        [ 403.1845, 2064.7600, 1928.7993],\n",
      "        [ 491.8457, 2056.9626, 2315.6028],\n",
      "        [ 555.5490, 2088.5261, 2498.2625],\n",
      "        [ 532.1936, 2033.5314, 2631.9719],\n",
      "        [ 294.8326, 2037.4749, 1440.5527],\n",
      "        [ 495.1932, 2033.7380, 2406.1262],\n",
      "        [ 529.6965, 2033.8219, 2627.2175],\n",
      "        [ 407.4392, 1995.2821, 2079.0510],\n",
      "        [ 404.2959, 2012.7925, 2017.6262],\n",
      "        [ 564.6598, 2025.9631, 2770.3674],\n",
      "        [ 549.3759, 1980.1843, 2808.7830],\n",
      "        [ 385.0366, 2014.5073, 1891.3024],\n",
      "        [ 438.9524, 1999.9180, 2180.3181],\n",
      "        [ 611.8768, 1987.3451, 3077.2463],\n",
      "        [ 619.7690, 1947.5261, 3233.5374],\n",
      "        [ 449.4465, 1972.2587, 2308.9221],\n",
      "        [ 407.7059, 1947.2916, 2132.6506],\n",
      "        [ 444.7125, 1958.7809, 2307.1416],\n",
      "        [ 626.9140, 1975.6991, 3115.0532],\n",
      "        [ 488.8741, 1965.7906, 2485.6541],\n",
      "        [ 373.0432, 1954.5135, 1914.2256],\n",
      "        [ 337.4365, 1981.0389, 1647.2305],\n",
      "        [ 529.3894, 1938.0977, 2638.8550],\n",
      "        [ 362.3832, 1957.5854, 1834.5321],\n",
      "        [ 479.0229, 1973.4484, 2378.7356],\n",
      "        [ 534.7802, 1941.7842, 2772.5374],\n",
      "        [ 360.3092, 1934.2271, 1880.3884],\n",
      "        [ 563.0387, 1955.1949, 2837.7854],\n",
      "        [ 441.0762, 1934.0929, 2176.8059],\n",
      "        [ 295.2288, 1923.5197, 1541.9417],\n",
      "        [ 369.2926, 1930.8143, 1873.5404],\n",
      "        [ 332.5516, 1908.7349, 1740.2650],\n",
      "        [ 431.1910, 1920.9056, 2218.7998],\n",
      "        [ 540.9710, 1936.4662, 2725.4666],\n",
      "        [ 424.0984, 1900.6973, 2228.1677],\n",
      "        [ 643.2698, 1948.8778, 3136.7705],\n",
      "        [ 466.1187, 1927.2507, 2406.9819],\n",
      "        [ 509.9565, 1969.0414, 2494.9038],\n",
      "        [ 544.7207, 1914.3707, 2869.7976],\n",
      "        [ 240.3109, 1898.7982, 1296.0167],\n",
      "        [ 283.2205, 1922.5365, 1461.6412],\n",
      "        [ 393.0970, 1899.3904, 2033.7087],\n",
      "        [ 281.0611, 1889.4034, 1489.5061],\n",
      "        [ 420.8766, 1903.7080, 2164.7048],\n",
      "        [ 468.1704, 1852.5840, 2558.2932],\n",
      "        [ 425.6105, 1874.3839, 2249.5471],\n",
      "        [ 552.4622, 1880.7236, 2843.0305],\n",
      "        [ 478.9017, 1878.3610, 2502.1628],\n",
      "        [ 342.6406, 1814.7925, 2069.7185],\n",
      "        [ 423.7889, 1860.1913, 2228.7747],\n",
      "        [ 519.0651, 1848.6238, 2792.7202],\n",
      "        [ 241.4270, 1843.6527, 1312.4905],\n",
      "        [ 434.7585, 1836.8887, 2322.1951],\n",
      "        [ 480.8103, 1808.1681, 2694.3130],\n",
      "        [ 472.8090, 1835.1964, 2485.7656],\n",
      "        [ 461.8346, 1834.0234, 2468.6206],\n",
      "        [ 363.2263, 1833.2861, 1967.8108],\n",
      "        [ 383.0299, 1808.5980, 2129.7236],\n",
      "        [ 449.7219, 1822.3777, 2436.4673],\n",
      "        [ 362.4077, 1757.5746, 2124.4275],\n",
      "        [ 348.8064, 1832.4651, 1869.8600],\n",
      "        [ 339.8110, 1805.9060, 1919.2635],\n",
      "        [ 561.5036, 1874.4512, 2903.9573],\n",
      "        [ 320.8692, 1822.4781, 1745.0687],\n",
      "        [ 387.1779, 1814.0273, 2151.4304],\n",
      "        [ 324.2653, 1866.1344, 1703.3250],\n",
      "        [ 370.1319, 1859.7301, 1965.5817],\n",
      "        [ 308.2426, 1865.1959, 1605.4004],\n",
      "        [ 392.2373, 1818.0990, 2194.9460],\n",
      "        [ 295.6720, 1859.0463, 1617.0020],\n",
      "        [ 404.6471, 1864.0007, 2197.5098],\n",
      "        [ 285.9724, 1898.8151, 1473.8737],\n",
      "        [ 303.5552, 1911.4495, 1565.6345],\n",
      "        [ 380.0145, 1899.0385, 2011.7628],\n",
      "        [ 623.4505, 2029.5490, 2674.1311]])\n",
      "data:  tensor([[[0.0056, 0.0055, 0.0051,  ..., 0.7896, 0.1013, 0.5217]],\n",
      "\n",
      "        [[0.0035, 0.0031, 0.0034,  ..., 0.7865, 0.1013, 0.5217]],\n",
      "\n",
      "        [[0.0056, 0.0055, 0.0056,  ..., 0.7865, 0.1013, 0.5217]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0035, 0.0035, 0.0036,  ..., 0.6584, 0.1055, 0.0000]],\n",
      "\n",
      "        [[0.0043, 0.0048, 0.0048,  ..., 0.6584, 0.1055, 0.0000]],\n",
      "\n",
      "        [[0.0072, 0.0050, 0.0045,  ..., 0.6584, 0.1055, 0.0000]]])\n",
      "output:  tensor([[0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575],\n",
      "        [0.3821, 5.4543, 4.2575]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 324.2432, 1878.7297, 1781.5231],\n",
      "        [ 268.0775, 1929.5575, 1373.0007],\n",
      "        [ 369.1347, 1936.8466, 1871.3711],\n",
      "        [ 254.5130, 1913.6222, 1352.8888],\n",
      "        [ 322.5624, 1900.9934, 1716.1627],\n",
      "        [ 343.1338, 1940.7510, 1732.2804],\n",
      "        [ 285.3202, 1932.7469, 1488.8505],\n",
      "        [ 176.7020, 1949.4028,  895.7708],\n",
      "        [ 301.1129, 1959.4510, 1528.7302],\n",
      "        [ 206.5593, 1967.8851, 1045.4514],\n",
      "        [ 294.8547, 1969.7004, 1503.2098],\n",
      "        [ 279.0136, 1989.2218, 1391.8351],\n",
      "        [ 229.6910, 1964.3999, 1333.7616],\n",
      "        [ 155.4110, 2004.8610,  769.1603],\n",
      "        [ 291.3253, 1985.1499, 1483.5974],\n",
      "        [ 335.3444, 2122.4246, 1473.7510],\n",
      "        [ 263.3284, 2026.1195, 1295.9802],\n",
      "        [ 325.2322, 2092.3245, 1523.7026],\n",
      "        [ 199.0334, 2025.3431,  993.3489],\n",
      "        [ 240.4017, 2054.7229, 1143.4071],\n",
      "        [ 307.4432, 2065.1453, 1455.5358],\n",
      "        [ 185.5520, 2057.8059,  900.9691],\n",
      "        [ 368.4618, 2042.1387, 1841.8206],\n",
      "        [ 253.7477, 2072.0881, 1224.8889],\n",
      "        [ 246.4972, 2083.6501, 1173.8657],\n",
      "        [ 239.2559, 2080.8862, 1153.1852],\n",
      "        [ 180.5279, 2088.4536,  854.4572],\n",
      "        [ 250.6068, 2075.4619, 1199.7135],\n",
      "        [ 239.6781, 2092.9556, 1143.1816],\n",
      "        [ 216.1890, 2087.3030, 1041.1321],\n",
      "        [ 283.6816, 2107.6958, 1340.7178],\n",
      "        [ 223.5957, 2125.2454, 1047.2344],\n",
      "        [ 168.7567, 2120.5925,  789.4411],\n",
      "        [ 161.5267, 2131.7915,  756.3344],\n",
      "        [ 211.4755, 2141.1638,  981.3077],\n",
      "        [ 248.7778, 2137.4382, 1166.8043],\n",
      "        [ 183.9340, 2171.7556,  817.4883],\n",
      "        [ 187.2074, 2159.8418,  877.2850],\n",
      "        [ 179.9338, 2159.7468,  836.0207],\n",
      "        [ 190.6728, 2181.1226,  873.9152],\n",
      "        [ 240.4583, 2181.5581, 1182.5978],\n",
      "        [ 296.9530, 2215.8533, 1376.0186],\n",
      "        [ 222.6122, 2247.5898,  990.2372],\n",
      "        [ 213.7759, 2240.7085,  954.8847],\n",
      "        [ 261.9104, 2235.7654, 1205.4279],\n",
      "        [ 238.2744, 2258.9729, 1055.3904],\n",
      "        [ 163.8097, 2285.1689,  700.3322],\n",
      "        [ 298.4374, 2268.9265, 1325.8038],\n",
      "        [ 216.7020, 2290.3860,  948.7745],\n",
      "        [ 243.6473, 2344.7327, 1019.8005],\n",
      "        [ 227.9595, 2338.9404,  976.7071],\n",
      "        [ 155.9764, 2360.6682,  660.4923],\n",
      "        [ 280.5678, 2361.4331, 1228.6426],\n",
      "        [ 209.9258, 2398.9846,  874.9532],\n",
      "        [ 401.2503, 2490.2739, 1392.5424],\n",
      "        [ 246.4420, 2407.3237, 1055.4747],\n",
      "        [ 241.3818, 2436.7871,  967.2438],\n",
      "        [ 202.6613, 2441.8420,  825.7026],\n",
      "        [ 308.0138, 2466.1235, 1218.1962],\n",
      "        [ 192.2958, 2476.7290,  775.1055],\n",
      "        [ 374.3966, 2472.5679, 1484.0891],\n",
      "        [ 239.9937, 2486.4868,  952.9298],\n",
      "        [ 209.8519, 2492.4131,  836.9041],\n",
      "        [ 198.8571, 2503.7795,  793.0388],\n",
      "        [ 223.7433, 2503.6455,  895.4482],\n",
      "        [ 202.4525, 2514.9561,  806.9145],\n",
      "        [ 196.1750, 2534.5891,  774.9112],\n",
      "        [ 307.6460, 2560.2322, 1180.2849],\n",
      "        [ 221.7936, 2554.8533,  863.7491],\n",
      "        [ 194.9492, 2560.7405,  758.2122],\n",
      "        [ 458.7622, 2573.8328, 1658.6581],\n",
      "        [ 233.7553, 2571.4868,  905.5488],\n",
      "        [ 242.5547, 2583.5906,  939.0573],\n",
      "        [ 221.2131, 2600.7715,  852.1351],\n",
      "        [ 228.9096, 2616.1875,  876.8771],\n",
      "        [ 241.5402, 2629.3804,  920.2420],\n",
      "        [ 327.8776, 2660.3967, 1163.3716],\n",
      "        [ 276.5383, 2659.9722, 1021.2695],\n",
      "        [ 239.9425, 2672.4893,  886.3626],\n",
      "        [ 301.2253, 2674.6062, 1125.8643],\n",
      "        [ 254.8987, 2675.8572,  955.4495],\n",
      "        [ 252.0061, 2670.5735,  947.8568],\n",
      "        [ 191.9041, 2676.6001,  718.7466],\n",
      "        [ 276.6960, 2670.6182, 1038.2133],\n",
      "        [ 369.4366, 2693.0605, 1350.9753],\n",
      "        [ 243.6060, 2696.5125,  887.7910],\n",
      "        [ 245.4103, 2702.9749,  910.7988],\n",
      "        [ 194.9828, 2695.3228,  724.9722],\n",
      "        [ 471.3192, 2729.2212, 1603.3801],\n",
      "        [ 389.1057, 2740.0308, 1382.1123],\n",
      "        [ 404.8801, 2763.4768, 1384.2924],\n",
      "        [ 432.8685, 2736.7615, 1568.3887],\n",
      "        [ 256.9022, 2649.6223, 1489.4901],\n",
      "        [ 360.4239, 2744.7043, 1373.6964],\n",
      "        [ 292.7370, 2804.4800,  979.7223],\n",
      "        [ 185.0547, 2689.0056,  736.0663],\n",
      "        [ 523.3226, 2817.9355, 1708.4274],\n",
      "        [ 360.9478, 2803.5583, 1256.3561],\n",
      "        [ 392.8329, 2810.8533, 1352.5327],\n",
      "        [ 336.4456, 2802.4189, 1180.0865],\n",
      "        [ 281.7066, 2788.2263, 1012.0531],\n",
      "        [ 266.7914, 2787.4163,  956.2836],\n",
      "        [ 377.0895, 2808.0659, 1293.8507],\n",
      "        [ 258.3790, 2808.6523,  918.2076],\n",
      "        [ 247.7312, 2813.5452,  880.2405],\n",
      "        [ 304.0816, 2838.4453, 1056.7869],\n",
      "        [ 268.6377, 2843.2766,  945.8731],\n",
      "        [ 253.2484, 2853.9116,  886.8859],\n",
      "        [ 376.3023, 2865.2559, 1305.2045],\n",
      "        [ 283.3006, 2877.7896,  984.3516],\n",
      "        [ 225.5065, 2893.5908,  777.5735],\n",
      "        [ 285.6898, 2889.9102,  986.6922],\n",
      "        [ 253.2422, 2871.7852,  881.6385],\n",
      "        [ 510.6099, 2878.9626, 1678.7988],\n",
      "        [ 598.9474, 2876.4324, 1955.7290],\n",
      "        [ 441.4876, 2871.2490, 1523.6700],\n",
      "        [ 381.3975, 2868.0205, 1330.5679],\n",
      "        [ 267.4599, 2845.6338,  945.2414],\n",
      "        [ 332.6441, 2820.7561, 1236.1389],\n",
      "        [ 471.0273, 2879.2473, 1577.0784],\n",
      "        [ 437.8733, 2859.7048, 1504.1472],\n",
      "        [ 383.6237, 2839.5793, 1333.5624],\n",
      "        [ 332.6133, 2844.9077, 1148.9915],\n",
      "        [ 369.4105, 2844.7961, 1287.2753],\n",
      "        [ 265.2680, 2836.9316,  924.5336],\n",
      "        [ 396.8435, 2817.3433, 1379.6472],\n",
      "        [ 460.1213, 2818.9856, 1610.8256],\n",
      "        [ 340.0924, 2810.4734, 1187.1123],\n",
      "        [ 418.7646, 2794.7668, 1500.3207],\n",
      "        [ 440.9004, 2832.5471, 1547.3022],\n",
      "        [ 384.7692, 2786.0535, 1386.4752],\n",
      "        [ 432.4041, 2796.5486, 1542.8157],\n",
      "        [ 307.0783, 2769.4255, 1114.8618],\n",
      "        [ 416.0557, 2661.5920, 1536.6025],\n",
      "        [ 340.9508, 2743.5032, 1242.9662],\n",
      "        [ 603.7509, 2770.3079, 2106.6506],\n",
      "        [ 535.5701, 2691.4075, 2026.7365],\n",
      "        [ 476.4306, 2725.8586, 1753.4933],\n",
      "        [ 406.1048, 2731.3044, 1478.0942],\n",
      "        [ 518.7579, 2714.2463, 1907.3634],\n",
      "        [ 458.9562, 2690.6143, 1618.1505],\n",
      "        [ 410.0108, 2687.0002, 1522.6909],\n",
      "        [ 458.0063, 2707.7283, 1670.3798],\n",
      "        [ 352.0125, 2643.1877, 1343.5022],\n",
      "        [ 456.5794, 2645.5728, 1725.7885],\n",
      "        [ 546.3676, 2616.6233, 2055.8513],\n",
      "        [ 417.5731, 2608.9041, 1601.7008],\n",
      "        [ 381.3758, 2600.6990, 1455.9884],\n",
      "        [ 331.3898, 2570.9841, 1294.7726],\n",
      "        [ 642.4827, 2592.8904, 2392.4993]])\n",
      "data:  tensor([[[0.0039, 0.0061, 0.0066,  ..., 0.6584, 0.1055, 0.0000]],\n",
      "\n",
      "        [[0.0028, 0.0026, 0.0022,  ..., 0.6584, 0.1055, 0.0000]],\n",
      "\n",
      "        [[0.0046, 0.0048, 0.0051,  ..., 0.6584, 0.1055, 0.0435]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0044, 0.0047, 0.0053,  ..., 0.7972, 0.1055, 0.5652]],\n",
      "\n",
      "        [[0.0032, 0.0032, 0.0032,  ..., 0.7972, 0.1055, 0.5652]],\n",
      "\n",
      "        [[0.0064, 0.0063, 0.0065,  ..., 0.7960, 0.1055, 0.5652]]])\n",
      "output:  tensor([[0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108],\n",
      "        [0.4354, 5.5076, 4.3108]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 600.9175, 2554.3618, 2351.8640],\n",
      "        [ 422.3386, 2546.4817, 1660.3019],\n",
      "        [ 535.4182, 2528.5623, 2119.7634],\n",
      "        [ 389.4631, 2496.4177, 1571.3528],\n",
      "        [ 504.7560, 2526.3840, 1894.2260],\n",
      "        [ 640.8691, 2495.4292, 2572.5708],\n",
      "        [ 366.2417, 2489.9778, 1459.0776],\n",
      "        [ 480.6283, 2471.0820, 1942.4171],\n",
      "        [ 380.5495, 2460.3132, 1525.1343],\n",
      "        [ 414.6990, 2452.0466, 1690.1947],\n",
      "        [ 531.4864, 2449.2205, 2158.6050],\n",
      "        [ 358.2096, 2437.6809, 1473.7064],\n",
      "        [ 479.3932, 2435.2178, 1965.7028],\n",
      "        [ 387.9180, 2376.6538, 1660.8004],\n",
      "        [ 698.6799, 2394.3655, 2914.7136],\n",
      "        [ 339.2942, 2358.7859, 1461.9320],\n",
      "        [ 474.1344, 2324.1614, 2058.5408],\n",
      "        [ 532.3972, 2313.5044, 2329.1663],\n",
      "        [ 336.4851, 2331.0986, 1452.6863],\n",
      "        [ 320.5959, 2322.2900, 1390.3644],\n",
      "        [ 513.0327, 2328.7080, 2204.8530],\n",
      "        [ 507.7423, 2344.7493, 2163.7656],\n",
      "        [ 593.7789, 2371.6155, 2494.8633],\n",
      "        [ 471.4832, 2348.7319, 2020.7267],\n",
      "        [ 553.9055, 2359.0818, 2337.5754],\n",
      "        [ 282.8578, 2372.5652, 1169.4141],\n",
      "        [ 440.2163, 2314.2192, 1922.3809],\n",
      "        [ 627.0098, 2299.0825, 2731.5535],\n",
      "        [ 403.9890, 2312.7668, 1747.8778],\n",
      "        [ 564.6563, 2334.7849, 2398.0022],\n",
      "        [ 339.8828, 2324.4463, 1450.5242],\n",
      "        [ 415.2747, 2285.9287, 1836.5509],\n",
      "        [ 249.9380, 2291.0952, 1074.8076],\n",
      "        [ 439.7239, 2251.0002, 1975.5625],\n",
      "        [ 412.8093, 2249.3662, 1843.1604],\n",
      "        [ 351.7200, 2236.3018, 1578.4016],\n",
      "        [ 488.7116, 2247.1597, 2156.6436],\n",
      "        [ 752.0820, 2230.4368, 3366.3875],\n",
      "        [ 532.1676, 2227.8118, 2389.2432],\n",
      "        [ 389.2573, 2197.1528, 1785.3605],\n",
      "        [ 408.1696, 2202.2859, 1853.1886],\n",
      "        [ 334.3509, 2172.4929, 1569.1119],\n",
      "        [ 616.2794, 2130.6689, 2945.7952],\n",
      "        [ 472.7003, 2210.0217, 2097.9744],\n",
      "        [ 398.4253, 2194.0864, 1784.4961],\n",
      "        [ 468.2286, 2164.3213, 2143.3997],\n",
      "        [ 544.6885, 2128.0715, 2602.3784],\n",
      "        [ 340.5602, 2151.2122, 1610.7538],\n",
      "        [ 662.0822, 2174.8052, 3056.8691],\n",
      "        [ 457.3128, 2174.8557, 2126.2520],\n",
      "        [ 639.3347, 2197.7617, 2895.5933],\n",
      "        [ 575.7006, 2162.9307, 2677.2466],\n",
      "        [ 560.6183, 2114.5659, 2705.2278],\n",
      "        [ 461.6298, 2149.0002, 2141.0542],\n",
      "        [ 258.1111, 2135.8464, 1210.5892],\n",
      "        [ 437.0719, 2146.5093, 2036.7645],\n",
      "        [ 368.7139, 2147.0286, 1729.9509],\n",
      "        [ 539.3044, 2142.2363, 2521.5068],\n",
      "        [ 602.8757, 2141.2253, 2815.0820],\n",
      "        [ 359.3519, 2118.3584, 1708.2839],\n",
      "        [ 564.3425, 2119.1794, 2653.7195],\n",
      "        [ 456.4819, 2100.2112, 2184.6838],\n",
      "        [ 648.5902, 2106.3943, 3080.9746],\n",
      "        [ 450.5786, 2072.5740, 2185.4397],\n",
      "        [ 483.6738, 2037.9999, 2417.6753],\n",
      "        [ 358.6004, 2058.7051, 1758.3353],\n",
      "        [ 310.3836, 2057.8315, 1506.0283],\n",
      "        [ 554.3633, 2091.6709, 2620.6956],\n",
      "        [ 355.3421, 2064.8047, 1703.6292],\n",
      "        [ 323.7805, 2010.1117, 1644.2079],\n",
      "        [ 435.3723, 2058.3870, 2095.4624],\n",
      "        [ 313.9535, 2025.1477, 1554.8179],\n",
      "        [ 546.9237, 2026.0470, 2716.7598],\n",
      "        [ 450.5447, 2017.9814, 2274.5359],\n",
      "        [ 442.4830, 2034.6431, 2187.2148],\n",
      "        [ 458.6079, 2030.6215, 2262.3147],\n",
      "        [ 408.8802, 2024.1924, 2019.7604],\n",
      "        [ 410.3094, 2017.5123, 2042.6207],\n",
      "        [ 519.5342, 2011.2676, 2586.5532],\n",
      "        [ 361.8357, 2005.4197, 1828.5869],\n",
      "        [ 476.5500, 2001.6774, 2377.9021],\n",
      "        [ 510.7199, 1988.0377, 2580.7705],\n",
      "        [ 382.7825, 1977.1738, 1952.7247],\n",
      "        [ 281.9091, 1963.9363, 1466.2539],\n",
      "        [ 574.6591, 1978.4752, 2896.7461],\n",
      "        [ 291.5511, 1978.5145, 1479.6525],\n",
      "        [ 506.7884, 1943.1583, 2640.0708],\n",
      "        [ 380.0270, 1943.6163, 1995.3323],\n",
      "        [ 455.8942, 1971.7504, 2275.0552],\n",
      "        [ 459.0774, 1917.0851, 2451.8345],\n",
      "        [ 449.0378, 1901.2056, 2398.6316],\n",
      "        [ 357.4528, 1962.6293, 1782.6082],\n",
      "        [ 472.2238, 1941.1195, 2419.2717],\n",
      "        [ 402.5014, 1925.5193, 2079.3887],\n",
      "        [ 294.3183, 1926.5135, 1501.3522],\n",
      "        [ 289.4825, 1897.2567, 1534.7106],\n",
      "        [ 480.3179, 1901.9429, 2531.9573],\n",
      "        [ 303.3649, 1900.9655, 1613.8640],\n",
      "        [ 395.6145, 1918.4871, 2049.8726],\n",
      "        [ 479.3186, 1914.0902, 2487.6921],\n",
      "        [ 343.6207, 1891.9840, 1825.6830],\n",
      "        [ 510.2089, 1899.4406, 2684.9868],\n",
      "        [ 391.5891, 1870.5859, 2121.5789],\n",
      "        [ 419.7975, 1908.6064, 2151.3748],\n",
      "        [ 387.0859, 1857.8344, 2106.4011],\n",
      "        [ 438.9253, 1884.3654, 2282.4634],\n",
      "        [ 262.6170, 1870.0273, 1381.1943],\n",
      "        [ 431.2855, 1852.6901, 2310.0659],\n",
      "        [ 492.2144, 1851.3328, 2665.2883],\n",
      "        [ 301.8956, 1853.4775, 1603.7091],\n",
      "        [ 299.7073, 1829.7113, 1639.9148],\n",
      "        [ 388.7506, 1819.2554, 2160.4956],\n",
      "        [ 348.8490, 1836.1626, 1889.1586],\n",
      "        [ 409.2854, 1843.3512, 2178.1458],\n",
      "        [ 386.9783, 1802.4764, 2173.3772],\n",
      "        [ 498.5844, 1811.1061, 2756.6135],\n",
      "        [ 410.3478, 1801.9291, 2284.2246],\n",
      "        [ 491.7275, 1796.2599, 2755.9458],\n",
      "        [ 606.3743, 1806.2578, 3338.5781],\n",
      "        [ 267.6616, 1792.2161, 1489.6624],\n",
      "        [ 326.2516, 1820.1936, 1775.5338],\n",
      "        [ 454.5475, 1789.8141, 2561.9841],\n",
      "        [ 431.8189, 1825.4497, 2351.8203],\n",
      "        [ 444.3274, 1825.1479, 2397.5022],\n",
      "        [ 494.4124, 1798.2705, 2761.4675],\n",
      "        [ 364.5364, 1788.4011, 2068.8691],\n",
      "        [ 361.8308, 1782.7990, 2062.0652],\n",
      "        [ 398.1408, 1802.5212, 2183.0889],\n",
      "        [ 438.8658, 1799.3822, 2446.3989],\n",
      "        [ 481.9236, 1788.8982, 2684.0022],\n",
      "        [ 411.7603, 1788.3898, 2323.9187],\n",
      "        [ 285.4738, 1835.1348, 1535.8164],\n",
      "        [ 262.0400, 1837.6036, 1422.4545],\n",
      "        [ 379.5349, 1855.8916, 2052.1213],\n",
      "        [ 253.5288, 1891.3696, 1346.6726],\n",
      "        [ 238.4399, 1921.1737, 1240.5437],\n",
      "        [ 334.7247, 1942.7728, 1720.3898],\n",
      "        [ 318.0743, 1929.6471, 1664.0100],\n",
      "        [ 355.6891, 1952.6423, 1815.9291],\n",
      "        [ 315.5533, 1935.6681, 1645.1655],\n",
      "        [ 280.5932, 1924.0951, 1479.3379],\n",
      "        [ 261.9533, 1950.8606, 1331.5659],\n",
      "        [ 354.6200, 1946.8503, 1821.7213],\n",
      "        [ 363.6362, 1942.1082, 1875.7081],\n",
      "        [ 336.6441, 1951.0282, 1734.2736],\n",
      "        [ 329.9508, 1965.6732, 1679.8264],\n",
      "        [ 239.9212, 1973.9678, 1219.7244],\n",
      "        [ 181.6122, 1977.0566,  927.0494],\n",
      "        [ 306.0460, 1967.8795, 1565.8169],\n",
      "        [ 226.3539, 1970.2758, 1155.6633]])\n",
      "data:  tensor([[[0.0084, 0.0083, 0.0077,  ..., 0.7936, 0.1055, 0.5652]],\n",
      "\n",
      "        [[0.0048, 0.0043, 0.0049,  ..., 0.7936, 0.1055, 0.5652]],\n",
      "\n",
      "        [[0.0067, 0.0072, 0.0068,  ..., 0.7936, 0.1055, 0.5652]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0021, 0.0017, 0.0018,  ..., 0.6690, 0.1097, 0.0435]],\n",
      "\n",
      "        [[0.0036, 0.0040, 0.0041,  ..., 0.6697, 0.1097, 0.0435]],\n",
      "\n",
      "        [[0.0025, 0.0025, 0.0023,  ..., 0.6726, 0.1097, 0.0435]]])\n",
      "output:  tensor([[0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641],\n",
      "        [0.4887, 5.5609, 4.3641]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 273.1860, 1959.6635, 1402.8350],\n",
      "        [ 276.1494, 1937.1537, 1440.9996],\n",
      "        [ 280.6752, 1960.1102, 1435.7131],\n",
      "        [ 248.5408, 1965.1147, 1257.2109],\n",
      "        [ 286.3731, 1963.5844, 1452.8129],\n",
      "        [ 267.9643, 1981.3685, 1346.4282],\n",
      "        [ 321.9928, 1954.0555, 1663.7017],\n",
      "        [ 211.4792, 1986.6301, 1052.4348],\n",
      "        [ 221.0588, 1968.3320, 1128.4390],\n",
      "        [ 196.9364, 1977.1348,  996.4502],\n",
      "        [ 189.6035, 1980.2961,  954.5040],\n",
      "        [ 178.5351, 1996.8124,  893.8583],\n",
      "        [ 264.1533, 2021.7181, 1307.7178],\n",
      "        [ 285.5439, 2020.8413, 1418.6156],\n",
      "        [ 269.5493, 2041.0831, 1316.4626],\n",
      "        [ 206.5448, 2043.6301, 1016.7167],\n",
      "        [ 174.9831, 2045.3280,  860.2401],\n",
      "        [ 173.5892, 2041.7050,  856.1143],\n",
      "        [ 196.5453, 2056.4153,  956.4494],\n",
      "        [ 231.2923, 2040.9658, 1156.3149],\n",
      "        [ 184.8360, 2072.2390,  894.7226],\n",
      "        [ 167.5792, 2077.8132,  809.2765],\n",
      "        [ 204.1257, 2099.4458,  975.1226],\n",
      "        [ 262.4398, 2097.0942, 1259.5944],\n",
      "        [ 209.7956, 2134.7742,  984.4289],\n",
      "        [ 191.1863, 2140.3430,  894.6326],\n",
      "        [ 191.7408, 2158.7974,  890.1634],\n",
      "        [ 216.5630, 2150.9160, 1015.1704],\n",
      "        [ 151.4864, 2179.0278,  695.0219],\n",
      "        [ 123.0753, 2167.5386,  568.8151],\n",
      "        [ 202.7112, 2152.7258,  947.8074],\n",
      "        [ 210.1450, 2172.9734,  978.6374],\n",
      "        [ 228.2023, 2171.2251, 1055.2231],\n",
      "        [ 149.4760, 2181.1729,  688.6192],\n",
      "        [ 159.8288, 2205.6428,  725.2742],\n",
      "        [ 140.7467, 2236.2512,  629.8361],\n",
      "        [ 295.2323, 2239.1389, 1331.8561],\n",
      "        [ 225.6095, 2279.3770,  991.0513],\n",
      "        [ 179.2247, 2280.3208,  789.8689],\n",
      "        [ 192.1280, 2291.6147,  841.8687],\n",
      "        [ 188.0129, 2311.4377,  823.6694],\n",
      "        [ 176.9652, 2331.0146,  759.9303],\n",
      "        [ 188.5423, 2335.6394,  811.3542],\n",
      "        [ 216.3790, 2332.9250,  934.1944],\n",
      "        [ 248.6581, 2341.9790, 1064.7740],\n",
      "        [ 202.6814, 2470.8867,  823.4640],\n",
      "        [ 207.0984, 2275.4558,  905.8532],\n",
      "        [ 316.5002, 2194.7847, 1447.4667],\n",
      "        [ 261.7877, 2080.5334, 1266.6655],\n",
      "        [ 335.5537, 2077.3228, 1622.1326],\n",
      "        [ 360.8583, 2272.8643, 1590.8594],\n",
      "        [ 346.3877, 2409.7087, 1435.8564],\n",
      "        [ 375.8333, 2535.4045, 1490.4917],\n",
      "        [ 390.9928, 2645.2488, 1481.1605],\n",
      "        [ 319.9078, 2790.3374, 1155.9747],\n",
      "        [ 392.9266, 2990.6165, 1316.0251],\n",
      "        [ 431.6783, 3188.6614, 1357.9500],\n",
      "        [ 654.5239, 3391.2803, 1933.9561],\n",
      "        [ 738.3508, 3476.1521, 2127.6580],\n",
      "        [ 695.6948, 3569.0107, 1953.8568],\n",
      "        [ 741.0438, 3705.0063, 1999.3966],\n",
      "        [ 738.4787, 4163.3906, 1765.4651],\n",
      "        [1122.1448, 4414.1348, 2541.4529],\n",
      "        [ 863.7588, 4496.1016, 1922.9146],\n",
      "        [ 742.5433, 4508.7974, 1650.3021],\n",
      "        [ 755.5471, 4497.6821, 1684.4216],\n",
      "        [ 795.2856, 4717.8066, 1691.7083],\n",
      "        [ 765.8605, 4877.3784, 1573.3962],\n",
      "        [ 913.1502, 4866.7661, 1879.4618],\n",
      "        [ 823.4947, 4838.0400, 1704.1678],\n",
      "        [ 596.1689, 4810.0791, 1241.4231],\n",
      "        [ 878.5097, 4748.4209, 1853.2372],\n",
      "        [ 684.5840, 4705.0718, 1457.2511],\n",
      "        [ 613.9435, 4665.6606, 1319.3792],\n",
      "        [ 736.9422, 4610.1577, 1602.5182],\n",
      "        [ 822.0544, 4558.9497, 1805.8206],\n",
      "        [ 780.1700, 4513.8525, 1735.2239],\n",
      "        [ 650.8824, 4473.1792, 1458.6284],\n",
      "        [ 512.0436, 4440.6880, 1157.0726],\n",
      "        [ 585.1330, 4395.7861, 1334.5742],\n",
      "        [ 467.9949, 4364.9600, 1074.1417],\n",
      "        [ 418.1628, 4327.4458,  967.1135],\n",
      "        [ 531.0431, 4283.2275, 1242.5615],\n",
      "        [ 435.4933, 4252.9541, 1025.6909],\n",
      "        [ 603.2379, 4205.5107, 1437.8392],\n",
      "        [ 578.2410, 4165.4854, 1391.6637],\n",
      "        [ 469.4375, 4144.8413, 1135.3904],\n",
      "        [ 519.6016, 4108.2397, 1266.9507],\n",
      "        [ 485.3174, 4073.8330, 1193.1411],\n",
      "        [ 554.9605, 4039.2869, 1375.7144],\n",
      "        [ 565.8094, 4004.1038, 1416.8431],\n",
      "        [ 469.7079, 3988.6377, 1179.0972],\n",
      "        [ 621.3740, 3949.1873, 1574.8661],\n",
      "        [ 585.8046, 3920.4724, 1497.1013],\n",
      "        [ 551.4183, 3889.9978, 1421.1703],\n",
      "        [ 433.6699, 3874.8555, 1119.6608],\n",
      "        [ 418.6336, 3837.7734, 1091.8557],\n",
      "        [ 517.3715, 3798.9153, 1366.7552],\n",
      "        [ 494.0089, 3781.3823, 1309.8521],\n",
      "        [ 508.0932, 3749.6570, 1358.6104],\n",
      "        [ 546.1193, 3713.4629, 1475.1763],\n",
      "        [ 460.4699, 3694.7959, 1250.1216],\n",
      "        [ 489.3385, 3666.6116, 1338.5573],\n",
      "        [ 383.1282, 3650.8271, 1052.0372],\n",
      "        [ 468.1823, 3623.8491, 1296.1945],\n",
      "        [ 422.2307, 3594.5254, 1177.9923],\n",
      "        [ 638.7888, 3554.3545, 1808.2096],\n",
      "        [ 493.0454, 3535.0007, 1396.6816],\n",
      "        [ 488.0203, 3512.8040, 1390.6807],\n",
      "        [ 490.5405, 3490.1938, 1407.5216],\n",
      "        [ 482.8425, 3460.8813, 1400.0914],\n",
      "        [ 403.0121, 3438.5552, 1179.2465],\n",
      "        [ 566.5717, 3402.2561, 1672.1732],\n",
      "        [ 431.9424, 3388.3313, 1278.7966],\n",
      "        [ 500.1573, 3356.9800, 1495.3168],\n",
      "        [ 381.5063, 3350.2998, 1145.4409],\n",
      "        [ 596.5450, 3302.5996, 1820.2094],\n",
      "        [ 396.1116, 3285.0388, 1212.4055],\n",
      "        [ 557.7853, 3262.3506, 1720.3801],\n",
      "        [ 469.6123, 3245.4771, 1454.8929],\n",
      "        [ 525.1720, 3216.5720, 1643.1891],\n",
      "        [ 536.0867, 3198.7932, 1684.1482],\n",
      "        [ 575.9736, 3168.3523, 1827.9297],\n",
      "        [ 625.5720, 3184.6787, 1897.5034],\n",
      "        [ 643.3437, 3162.3257, 2005.8080],\n",
      "        [ 525.8349, 3113.4246, 1696.6897],\n",
      "        [ 602.7289, 3092.8813, 1949.7963],\n",
      "        [ 464.5952, 3062.6191, 1524.7789],\n",
      "        [ 742.5361, 3045.5723, 2426.8447],\n",
      "        [ 461.2886, 3046.4043, 1488.3103],\n",
      "        [ 650.7028, 3005.6750, 2163.8030],\n",
      "        [ 423.4087, 2993.0908, 1415.4865],\n",
      "        [ 467.5761, 2960.4604, 1588.6653],\n",
      "        [ 468.0911, 2949.7085, 1584.4369],\n",
      "        [ 671.8793, 2966.9619, 2181.5576],\n",
      "        [ 637.1942, 2878.7336, 2227.8091],\n",
      "        [ 425.4081, 2890.5916, 1466.5358],\n",
      "        [ 567.8873, 2848.9685, 2017.8262],\n",
      "        [ 426.6499, 2827.0007, 1585.1865],\n",
      "        [ 428.5619, 2819.4045, 1527.2827],\n",
      "        [ 626.5765, 2807.9597, 2232.0273],\n",
      "        [ 689.7267, 2788.0364, 2477.3752],\n",
      "        [ 641.5428, 2752.2278, 2340.8833],\n",
      "        [ 504.0186, 2744.4895, 1834.5702],\n",
      "        [ 561.1336, 2703.6675, 2090.7522],\n",
      "        [ 617.3604, 2706.2815, 2255.4971],\n",
      "        [ 734.9415, 2700.8079, 2700.6167],\n",
      "        [ 647.2936, 2662.8711, 2425.3103],\n",
      "        [ 817.0992, 2668.8755, 3007.9150],\n",
      "        [ 633.0292, 2599.7998, 2446.4658]])\n",
      "data:  tensor([[[0.0029, 0.0026, 0.0029,  ..., 0.6726, 0.1097, 0.0435]],\n",
      "\n",
      "        [[0.0032, 0.0035, 0.0035,  ..., 0.6726, 0.1097, 0.0435]],\n",
      "\n",
      "        [[0.0036, 0.0035, 0.0033,  ..., 0.6726, 0.1097, 0.0870]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0077, 0.0076, 0.0079,  ..., 0.7651, 0.1097, 0.6087]],\n",
      "\n",
      "        [[0.0093, 0.0081, 0.0081,  ..., 0.7651, 0.1097, 0.6087]],\n",
      "\n",
      "        [[0.0074, 0.0083, 0.0077,  ..., 0.7651, 0.1097, 0.6087]]])\n",
      "output:  tensor([[0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175],\n",
      "        [0.5420, 5.6142, 4.4175]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 486.6400, 2576.7539, 1913.2368],\n",
      "        [ 328.9687, 2574.4976, 1290.6952],\n",
      "        [ 465.5368, 2575.0393, 1807.4200],\n",
      "        [ 586.3817, 2519.8713, 2355.8623],\n",
      "        [ 589.4519, 2551.3845, 2319.3494],\n",
      "        [ 598.5516, 2499.0820, 2501.0195],\n",
      "        [ 438.0869, 2513.5317, 1743.0588],\n",
      "        [ 555.5349, 2437.3232, 2494.1294],\n",
      "        [ 688.6590, 2459.9111, 2812.6213],\n",
      "        [ 599.2325, 2459.4028, 2422.5173],\n",
      "        [ 586.7199, 2424.3594, 2437.1204],\n",
      "        [ 667.7805, 2390.6008, 2900.7898],\n",
      "        [ 752.7947, 2407.8376, 3150.3748],\n",
      "        [ 671.6346, 2417.0537, 2785.8308],\n",
      "        [ 706.7463, 2410.6694, 2847.9067],\n",
      "        [ 568.0157, 2349.8992, 2522.4851],\n",
      "        [ 348.6794, 2377.9775, 1450.1783],\n",
      "        [ 441.6600, 2348.7039, 1872.6147],\n",
      "        [ 531.0681, 2361.5059, 2206.9292],\n",
      "        [ 521.0659, 2331.2214, 2242.4036],\n",
      "        [ 384.5290, 2308.1143, 1678.1456],\n",
      "        [ 408.2781, 2309.0974, 1765.9965],\n",
      "        [ 456.6217, 2279.0530, 2015.1772],\n",
      "        [ 612.5607, 2290.5979, 2614.5376],\n",
      "        [ 583.7018, 2274.4226, 2565.9482],\n",
      "        [ 477.4724, 2270.9302, 2089.2012],\n",
      "        [ 445.4317, 2269.7810, 1911.5950],\n",
      "        [ 537.8582, 2250.7681, 2368.5935],\n",
      "        [ 697.8431, 2245.2383, 3065.0435],\n",
      "        [ 738.2996, 2244.2219, 3249.5237],\n",
      "        [ 491.9866, 2190.7966, 2270.7590],\n",
      "        [ 553.8903, 2226.3425, 2416.2222],\n",
      "        [ 473.4236, 2207.8660, 2065.7861],\n",
      "        [ 347.5696, 2181.6140, 1590.5695],\n",
      "        [ 570.5284, 2172.6045, 2621.9456],\n",
      "        [ 400.1171, 2147.1404, 1868.8896],\n",
      "        [ 465.1309, 2135.1985, 2194.7510],\n",
      "        [ 471.4337, 2114.5100, 2273.4006],\n",
      "        [ 500.0679, 2143.8840, 2290.9587],\n",
      "        [ 497.4109, 2076.7969, 2447.3054],\n",
      "        [ 574.7839, 2118.2244, 2673.9358],\n",
      "        [ 483.9869, 2089.6042, 2338.3931],\n",
      "        [ 544.2434, 2084.2981, 2662.1655],\n",
      "        [ 570.8435, 2086.9958, 2740.6345],\n",
      "        [ 529.3326, 2068.5415, 2592.3425],\n",
      "        [ 389.0727, 2091.0676, 1821.9733],\n",
      "        [ 564.7236, 2068.8484, 2729.1023],\n",
      "        [ 504.5427, 2086.6326, 2391.1265],\n",
      "        [ 607.8070, 2090.1904, 2773.3557],\n",
      "        [ 341.0006, 2040.1837, 1700.0659],\n",
      "        [ 548.1990, 2045.0823, 2573.4675],\n",
      "        [ 510.5052, 2020.7966, 2512.1335],\n",
      "        [ 395.1437, 2026.8177, 2015.6577],\n",
      "        [ 487.5107, 2022.7012, 2412.5457],\n",
      "        [ 496.1594, 2141.8008, 1984.9783],\n",
      "        [ 567.9966, 2129.6018, 2466.2163],\n",
      "        [ 538.3785, 2028.2699, 2647.7942],\n",
      "        [ 424.5725, 2047.7689, 2046.0702],\n",
      "        [ 478.8306, 1984.6881, 2482.4502],\n",
      "        [ 454.8916, 1991.4951, 2381.6616],\n",
      "        [ 397.1764, 2036.0952, 1903.8597],\n",
      "        [ 424.9673, 2034.8887, 2067.2034],\n",
      "        [ 349.5529, 1981.7261, 1827.8069],\n",
      "        [ 325.9797, 1996.8796, 1654.9108],\n",
      "        [ 515.1296, 2015.8367, 2531.5437],\n",
      "        [ 361.8536, 2021.2267, 1762.5255],\n",
      "        [ 400.0195, 1982.5470, 2044.6458],\n",
      "        [ 479.8423, 2023.5055, 2354.4260],\n",
      "        [ 370.3964, 1986.6470, 1889.0605],\n",
      "        [ 524.9583, 1996.2314, 2647.8052],\n",
      "        [ 565.0504, 2020.9362, 2773.7747],\n",
      "        [ 436.6047, 2004.9840, 2177.6790],\n",
      "        [ 302.9064, 2004.2411, 1518.8041],\n",
      "        [ 469.6327, 2000.3817, 2333.0339],\n",
      "        [ 448.6829, 1986.9763, 2276.8677],\n",
      "        [ 458.5480, 2017.3279, 2241.1758],\n",
      "        [ 566.7273, 1944.9288, 2969.1401],\n",
      "        [ 297.6865, 2006.0732, 1469.0015],\n",
      "        [ 305.1473, 1984.4293, 1552.1936],\n",
      "        [ 337.4442, 2016.9203, 1606.2281],\n",
      "        [ 518.9502, 1998.8120, 2593.9468],\n",
      "        [ 335.3044, 1981.2401, 1707.0316],\n",
      "        [ 314.1161, 1970.1754, 1604.1418],\n",
      "        [ 443.6298, 1995.7177, 2215.3274],\n",
      "        [ 483.6004, 2086.4932, 1942.9366],\n",
      "        [ 262.0418, 2004.2356, 1256.6230],\n",
      "        [ 364.1298, 1994.3101, 1812.4749],\n",
      "        [ 366.7966, 1990.3668, 1862.6719],\n",
      "        [ 595.8668, 1963.3610, 3005.8787],\n",
      "        [ 440.3136, 1994.2319, 2179.1599],\n",
      "        [ 416.4597, 1945.2416, 2161.3469],\n",
      "        [ 330.3607, 1961.4756, 1672.8729],\n",
      "        [ 555.6191, 1939.1144, 2882.8862],\n",
      "        [ 603.5027, 1930.6132, 3142.2625],\n",
      "        [ 714.9604, 1983.6250, 3579.0654],\n",
      "        [ 444.1194, 1968.3320, 2225.1035],\n",
      "        [ 427.1685, 1961.3501, 2155.1340],\n",
      "        [ 465.5343, 2040.3850, 2024.2272],\n",
      "        [ 526.7318, 2053.8738, 2099.8721],\n",
      "        [ 418.5933, 1941.8232, 2141.7959],\n",
      "        [ 480.4651, 1960.9089, 2350.9648],\n",
      "        [ 342.0865, 1940.6111, 1748.3190],\n",
      "        [ 336.9774, 1911.5890, 1768.1299],\n",
      "        [ 331.8137, 1928.1614, 1715.9836],\n",
      "        [ 398.0557, 1930.4346, 2043.8611],\n",
      "        [ 376.8466, 1926.4967, 1955.1394],\n",
      "        [ 430.8738, 1946.7554, 2160.9297],\n",
      "        [ 431.8844, 1928.0496, 2233.8772],\n",
      "        [ 334.4164, 1901.4459, 1738.3348],\n",
      "        [ 432.3258, 1882.0641, 2344.1174],\n",
      "        [ 499.3984, 1912.4716, 2611.3359],\n",
      "        [ 236.0266, 1893.7881, 1247.6273],\n",
      "        [ 372.8941, 1881.9969, 2016.1284],\n",
      "        [ 615.6411, 1934.0428, 3031.0715],\n",
      "        [ 331.3969, 1902.3339, 1727.4065],\n",
      "        [ 437.8637, 1940.8961, 2184.4670],\n",
      "        [ 436.3569, 1862.4814, 2338.3660],\n",
      "        [ 440.0166, 1903.6298, 2272.4758],\n",
      "        [ 403.6223, 1917.2639, 2037.1996],\n",
      "        [ 416.9546, 1890.7106, 2205.8562],\n",
      "        [ 344.8776, 1901.1051, 1798.1171],\n",
      "        [ 506.7951, 1887.0072, 2685.3621],\n",
      "        [ 300.4043, 1878.6123, 1612.8339],\n",
      "        [ 338.8335, 1795.9637, 1949.3661],\n",
      "        [ 426.5763, 1894.4703, 2236.5005],\n",
      "        [ 416.0925, 1858.1582, 2278.4961],\n",
      "        [ 415.6510, 1895.0615, 2168.9009],\n",
      "        [ 396.0512, 1883.7286, 2102.4937],\n",
      "        [ 397.5266, 1852.8074, 2182.7998],\n",
      "        [ 253.9391, 1877.6516, 1343.1277],\n",
      "        [ 316.1414, 1897.0444, 1607.2781],\n",
      "        [ 345.9893, 1902.4957, 1818.4091],\n",
      "        [ 478.6550, 1919.4143, 2459.4551],\n",
      "        [ 326.6694, 1903.6409, 1702.7664],\n",
      "        [ 422.7701, 1922.5590, 2182.2620],\n",
      "        [ 332.8874, 1880.4387, 1793.0503],\n",
      "        [ 361.3936, 1916.5267, 1873.9850],\n",
      "        [ 179.8751, 1912.8682,  904.7530],\n",
      "        [ 353.3118, 1897.4856, 1838.2764],\n",
      "        [ 308.1666, 1931.7638, 1528.5980],\n",
      "        [ 210.5123, 1927.9879, 1065.1085],\n",
      "        [ 228.4649, 1911.4606, 1187.9933],\n",
      "        [ 222.8277, 1902.0043, 1179.4861],\n",
      "        [ 217.7877, 1890.0345, 1213.5391],\n",
      "        [ 207.9421, 1914.2310, 1081.1697],\n",
      "        [ 307.5490, 1882.3992, 1683.2766],\n",
      "        [ 222.1176, 1942.8342, 1038.2578],\n",
      "        [ 238.0580, 1910.9690, 1261.4993],\n",
      "        [ 186.2005, 1942.5271,  937.7028],\n",
      "        [ 218.8424, 1931.7639, 1143.2748]])\n",
      "data:  tensor([[[0.0058, 0.0061, 0.0067,  ..., 0.7643, 0.1097, 0.6087]],\n",
      "\n",
      "        [[0.0039, 0.0037, 0.0037,  ..., 0.7616, 0.1097, 0.6087]],\n",
      "\n",
      "        [[0.0052, 0.0050, 0.0048,  ..., 0.7616, 0.1097, 0.6087]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0040, 0.0040, 0.0036,  ..., 0.6833, 0.1139, 0.0870]],\n",
      "\n",
      "        [[0.0022, 0.0025, 0.0026,  ..., 0.6833, 0.1139, 0.0870]],\n",
      "\n",
      "        [[0.0025, 0.0023, 0.0024,  ..., 0.6855, 0.1139, 0.0870]]])\n",
      "output:  tensor([[0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708],\n",
      "        [0.5953, 5.6675, 4.4708]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 246.8645, 1927.5411, 1329.2126],\n",
      "        [ 143.4022, 1952.7540,  734.6061],\n",
      "        [ 321.7142, 1973.5098, 1516.8827],\n",
      "        [ 193.5423, 1942.8230, 1004.5193],\n",
      "        [ 159.7964, 1963.5397,  809.5029],\n",
      "        [ 202.1554, 1960.5011, 1026.9104],\n",
      "        [ 160.2731, 1968.7902,  814.8865],\n",
      "        [ 147.7375, 1981.7975,  743.5612],\n",
      "        [ 168.8522, 1981.0837,  852.6354],\n",
      "        [ 282.5685, 1989.3669, 1423.2034],\n",
      "        [ 168.0623, 2002.1689,  837.2962],\n",
      "        [ 193.1953, 2006.3693,  966.7172],\n",
      "        [ 196.7036, 2021.2938,  974.3530],\n",
      "        [ 180.8863, 2032.5707,  887.9647],\n",
      "        [ 188.9255, 2036.1678,  924.2501],\n",
      "        [ 165.6426, 2046.4899,  804.3868],\n",
      "        [ 165.9162, 2038.7708,  815.6513],\n",
      "        [ 159.1920, 2033.8330,  784.0884],\n",
      "        [ 216.9844, 2028.8173, 1068.7013],\n",
      "        [ 197.8909, 2049.2378,  963.8143],\n",
      "        [ 234.6266, 2058.5825, 1130.1516],\n",
      "        [ 165.7572, 2060.7551,  803.2510],\n",
      "        [ 207.4667, 2083.7507,  980.0792],\n",
      "        [ 166.0590, 2084.0300,  793.1724],\n",
      "        [ 168.9363, 2071.0381,  819.1630],\n",
      "        [ 226.7148, 2073.1719, 1101.6727],\n",
      "        [ 242.3382, 2103.0374, 1141.8234],\n",
      "        [ 163.1391, 2116.1353,  771.3064],\n",
      "        [ 155.9408, 2125.3291,  734.9702],\n",
      "        [ 225.1261, 2117.4255, 1081.9510],\n",
      "        [ 192.8322, 2159.1045,  889.3931],\n",
      "        [ 151.1307, 2166.8796,  698.5311],\n",
      "        [ 252.2525, 2198.7112, 1060.6562],\n",
      "        [ 189.7529, 2182.2285,  871.5953],\n",
      "        [ 140.1227, 2186.6689,  642.3806],\n",
      "        [ 220.3737, 2223.5610,  947.2394],\n",
      "        [ 171.6393, 2218.9197,  768.3286],\n",
      "        [ 187.5780, 2228.3145,  843.2261],\n",
      "        [ 203.2470, 2253.1138,  898.0157],\n",
      "        [ 228.3706, 2269.9763,  995.4858],\n",
      "        [ 211.1547, 2288.2244,  904.4415],\n",
      "        [ 147.6927, 2305.9695,  638.2636],\n",
      "        [ 206.0556, 2325.4626,  884.6530],\n",
      "        [ 317.4553, 2259.2412, 1814.3221],\n",
      "        [ 278.4692, 2346.0061, 1197.3506],\n",
      "        [ 132.1283, 2359.6067,  562.8578],\n",
      "        [ 219.0935, 2381.0718,  922.0275],\n",
      "        [ 292.5139, 2418.0645, 1127.3014],\n",
      "        [ 216.2427, 2426.5376,  862.1630],\n",
      "        [ 282.5920, 2436.8318, 1133.3567],\n",
      "        [ 182.9641, 2460.7544,  742.5533],\n",
      "        [ 218.6199, 2461.5532,  891.2427],\n",
      "        [ 223.5157, 2464.7705,  908.1643],\n",
      "        [ 205.3305, 2454.0742,  837.0537],\n",
      "        [ 239.2453, 2425.0520, 1034.9048],\n",
      "        [ 219.3097, 2374.3247,  948.1771],\n",
      "        [ 258.3886, 2469.0098, 1045.5333],\n",
      "        [ 248.3688, 2490.5417,  976.4218],\n",
      "        [ 212.3326, 2481.6274,  857.8492],\n",
      "        [ 285.9912, 2480.1360, 1152.7086],\n",
      "        [ 217.9816, 2508.2087,  824.6032],\n",
      "        [ 314.3860, 2510.5603, 1236.7637],\n",
      "        [ 250.8523, 2505.7625,  998.0952],\n",
      "        [ 296.2018, 2496.2168, 1188.0427],\n",
      "        [ 193.8025, 2513.9229,  768.5380],\n",
      "        [ 267.4320, 2503.8467, 1072.4258],\n",
      "        [ 139.6455, 2522.4741,  553.9776],\n",
      "        [ 180.9062, 2543.0398,  712.0969],\n",
      "        [ 192.5930, 2560.9077,  751.3920],\n",
      "        [ 217.5590, 2565.9238,  847.2454],\n",
      "        [ 246.2089, 2589.5193,  946.4493],\n",
      "        [ 254.6037, 2578.8821,  989.2263],\n",
      "        [ 263.6695, 2601.5703, 1011.5402],\n",
      "        [ 295.3561, 2601.7380, 1172.3610],\n",
      "        [ 383.7002, 2659.2236, 1329.3807],\n",
      "        [ 360.6535, 2657.8162, 1294.4435],\n",
      "        [ 234.2050, 2661.5864,  878.3779],\n",
      "        [ 409.8548, 2667.2275, 1514.3414],\n",
      "        [ 309.4660, 2671.0483, 1148.9093],\n",
      "        [ 282.3173, 2672.2771, 1044.7456],\n",
      "        [ 351.9153, 2671.9197, 1306.2609],\n",
      "        [ 282.3378, 2692.4685, 1027.6984],\n",
      "        [ 304.5757, 2691.6809, 1117.3164],\n",
      "        [ 231.8467, 2684.7383,  862.1011],\n",
      "        [ 334.2936, 2674.7683, 1255.0226],\n",
      "        [ 387.1434, 2662.1506, 1471.6184],\n",
      "        [ 350.1257, 2697.4844, 1246.2859],\n",
      "        [ 290.4513, 2703.7734, 1071.5566],\n",
      "        [ 329.8364, 2699.5286, 1210.2625],\n",
      "        [ 240.8812, 2703.6284,  893.1045],\n",
      "        [ 406.8102, 2711.3308, 1486.0990],\n",
      "        [ 404.2267, 2747.3853, 1435.9783],\n",
      "        [ 262.8429, 2717.1902,  965.7971],\n",
      "        [ 147.4884, 2701.9807,  547.5215],\n",
      "        [ 384.0660, 2701.0869, 1421.3503],\n",
      "        [ 251.0055, 2709.9290,  922.8566],\n",
      "        [ 352.5045, 2693.5466, 1307.6040],\n",
      "        [ 288.6866, 2692.6194, 1069.2001],\n",
      "        [ 403.0381, 2647.9075, 1569.8464],\n",
      "        [ 532.0594, 2703.1592, 1833.3118],\n",
      "        [ 532.3021, 2691.2703, 1942.1310],\n",
      "        [ 560.4446, 2705.0583, 1950.8790],\n",
      "        [ 347.4359, 2662.9941, 1304.2389],\n",
      "        [ 411.4753, 2661.2068, 1518.6544],\n",
      "        [ 426.2177, 2662.6978, 1508.9561],\n",
      "        [ 342.6805, 2577.3237, 1383.3514],\n",
      "        [ 477.6377, 2614.0706, 1823.3394],\n",
      "        [ 567.5286, 2608.9822, 2168.2881],\n",
      "        [ 377.4153, 2588.0476, 1457.2216],\n",
      "        [ 561.8401, 2599.1294, 2014.9254],\n",
      "        [ 486.4127, 2524.8535, 2078.4353],\n",
      "        [ 559.4382, 2581.1890, 2131.0317],\n",
      "        [ 442.4687, 2550.8708, 1712.7722],\n",
      "        [ 368.4822, 2538.1973, 1459.8337],\n",
      "        [ 414.3739, 2521.1335, 1649.4865],\n",
      "        [ 376.9243, 2509.1638, 1499.3685],\n",
      "        [ 485.9767, 2491.4746, 1958.9473],\n",
      "        [ 549.5403, 2498.1550, 2194.5549],\n",
      "        [ 565.1999, 2488.1011, 2274.3591],\n",
      "        [ 408.0099, 2495.7253, 1631.1611],\n",
      "        [ 573.8793, 2482.9849, 2307.0239],\n",
      "        [ 513.9229, 2487.8274, 2052.1199],\n",
      "        [ 453.8791, 2482.8953, 1808.5737],\n",
      "        [ 500.0797, 2483.2249, 1928.9730],\n",
      "        [ 343.4566, 2452.6108, 1400.0070],\n",
      "        [ 495.6128, 2445.2715, 2010.4023],\n",
      "        [ 493.9722, 2379.9490, 2120.7249],\n",
      "        [ 537.9165, 2423.8679, 2054.4539],\n",
      "        [ 664.0616, 2419.7068, 2623.4194],\n",
      "        [ 346.2643, 2353.0886, 1501.9202],\n",
      "        [ 463.7328, 2397.4207, 1912.7374],\n",
      "        [ 300.2519, 2366.8357, 1283.7827],\n",
      "        [ 615.3142, 2400.3809, 2501.8284],\n",
      "        [ 360.9236, 2339.8008, 1567.9966],\n",
      "        [ 579.2382, 2288.7886, 2585.4282],\n",
      "        [ 546.3072, 2300.7803, 2393.0137],\n",
      "        [ 366.0716, 2300.6018, 1568.5176],\n",
      "        [ 420.2587, 2274.1321, 1858.3718],\n",
      "        [ 580.2304, 2316.4702, 2456.5361],\n",
      "        [ 330.8633, 2271.8923, 1438.4559],\n",
      "        [ 353.4887, 2266.7593, 1527.1433],\n",
      "        [ 436.8329, 2288.5986, 1852.2828],\n",
      "        [ 497.0123, 2264.6479, 2154.9373],\n",
      "        [ 366.2678, 2229.4927, 1552.4701],\n",
      "        [ 430.8408, 2215.7861, 1935.4679],\n",
      "        [ 681.7633, 2276.7239, 2741.4949],\n",
      "        [ 582.1098, 2180.1450, 2700.6377],\n",
      "        [ 515.2574, 2160.9868, 2389.1174],\n",
      "        [ 434.6257, 2163.5168, 2027.6367],\n",
      "        [ 358.4866, 2166.9019, 1660.8258]])\n",
      "data:  tensor([[[0.0029, 0.0031, 0.0029,  ..., 0.6868, 0.1139, 0.0870]],\n",
      "\n",
      "        [[0.0017, 0.0017, 0.0018,  ..., 0.6868, 0.1139, 0.0870]],\n",
      "\n",
      "        [[0.0037, 0.0038, 0.0036,  ..., 0.6901, 0.1139, 0.1304]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0065, 0.0066, 0.0064,  ..., 0.7794, 0.1139, 0.6522]],\n",
      "\n",
      "        [[0.0049, 0.0046, 0.0047,  ..., 0.7794, 0.1139, 0.6522]],\n",
      "\n",
      "        [[0.0042, 0.0041, 0.0041,  ..., 0.7784, 0.1139, 0.6522]]])\n",
      "output:  tensor([[0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240],\n",
      "        [0.6486, 5.7208, 4.5240]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 395.5990, 2148.4529, 1839.3479],\n",
      "        [ 458.3597, 2154.7812, 2100.5930],\n",
      "        [ 523.9236, 2216.8640, 2398.9534],\n",
      "        [ 490.7104, 2116.6716, 2323.5779],\n",
      "        [ 546.3172, 2091.8330, 2616.7993],\n",
      "        [ 444.5255, 2103.1267, 2107.8591],\n",
      "        [ 333.1747, 2051.6450, 1650.6949],\n",
      "        [ 462.0792, 2088.8838, 2209.8433],\n",
      "        [ 364.7494, 2095.7092, 1712.3350],\n",
      "        [ 474.2868, 2103.2161, 2221.3667],\n",
      "        [ 487.2650, 2115.0015, 2127.9910],\n",
      "        [ 470.7413, 2076.8916, 2271.4377],\n",
      "        [ 673.3809, 2106.5339, 3151.8118],\n",
      "        [ 392.4520, 2078.9846, 1831.3236],\n",
      "        [ 424.2586, 2094.1006, 1993.7046],\n",
      "        [ 468.7759, 2054.1531, 2329.2363],\n",
      "        [ 558.6724, 2066.1787, 2644.0625],\n",
      "        [ 587.9698, 2037.8768, 2904.8848],\n",
      "        [ 424.5190, 2080.0754, 1994.8854],\n",
      "        [ 535.6187, 2104.6069, 2485.0073],\n",
      "        [ 463.9348, 2055.6333, 2320.7634],\n",
      "        [ 369.0999, 2064.8997, 1813.6196],\n",
      "        [ 448.5913, 2101.7861, 2143.4412],\n",
      "        [ 327.6039, 2118.4478, 1521.4028],\n",
      "        [ 537.4596, 2078.7461, 2590.9053],\n",
      "        [ 291.9819, 2085.7446, 1414.8802],\n",
      "        [ 471.9320, 2082.4268, 2279.2664],\n",
      "        [ 356.4106, 2136.1816, 1553.1440],\n",
      "        [ 518.7472, 2098.1780, 2457.7383],\n",
      "        [ 751.1848, 2099.0940, 3580.4961],\n",
      "        [ 337.3647, 2065.8716, 1697.9667],\n",
      "        [ 423.7905, 2035.6036, 2123.5977],\n",
      "        [ 368.5717, 2080.1870, 1751.6243],\n",
      "        [ 403.4718, 2100.5071, 1879.0898],\n",
      "        [ 622.6961, 2100.3730, 2940.5415],\n",
      "        [ 276.0622, 2046.9255, 1392.1935],\n",
      "        [ 305.3074, 2100.9094, 1409.4073],\n",
      "        [ 528.2042, 2097.4741, 2487.6775],\n",
      "        [ 382.1227, 2113.7615, 1767.7101],\n",
      "        [ 362.7943, 2058.3870, 1801.0614],\n",
      "        [ 444.6646, 2094.7148, 2107.0911],\n",
      "        [ 551.7687, 2062.5818, 2688.1375],\n",
      "        [ 629.4403, 2057.1301, 3068.1191],\n",
      "        [ 277.7666, 2066.2290, 1324.3755],\n",
      "        [ 527.7358, 2035.0172, 2602.1443],\n",
      "        [ 363.0180, 2054.3542, 1762.9015],\n",
      "        [ 418.1981, 2027.2225, 2078.4180],\n",
      "        [ 621.7333, 2086.3367, 2567.6775],\n",
      "        [ 397.5672, 2024.0975, 1946.9197],\n",
      "        [ 321.6380, 2021.9918, 1567.5864],\n",
      "        [ 561.8016, 2090.7827, 2636.1377],\n",
      "        [ 392.7156, 1993.7740, 1977.0023],\n",
      "        [ 652.6961, 1856.3877, 3716.9268],\n",
      "        [ 602.7630, 2005.7269, 3008.5300],\n",
      "        [ 394.6163, 2022.2710, 1910.4758],\n",
      "        [ 497.1070, 1998.6222, 2489.8745],\n",
      "        [ 356.2789, 2023.4329, 1746.7736],\n",
      "        [ 550.2203, 1995.4720, 2785.7012],\n",
      "        [ 354.8976, 1986.9149, 1772.6260],\n",
      "        [ 310.9086, 1963.8301, 1624.8376],\n",
      "        [ 501.0002, 1983.7201, 2489.4597],\n",
      "        [ 300.5842, 1985.0941, 1491.7902],\n",
      "        [ 402.3683, 1978.1458, 2030.9484],\n",
      "        [ 396.4486, 1998.5328, 1959.8643],\n",
      "        [ 366.9509, 1996.6115, 1845.5393],\n",
      "        [ 482.0648, 1996.9353, 2406.4775],\n",
      "        [ 381.5314, 1998.7841, 1887.1160],\n",
      "        [ 368.4591, 1977.3022, 1845.5688],\n",
      "        [ 501.6514, 1936.6791, 2580.7092],\n",
      "        [ 768.0742, 2036.4414, 3610.2290],\n",
      "        [ 541.5187, 1899.7644, 2861.1963],\n",
      "        [ 517.4727, 1871.2506, 2785.6702],\n",
      "        [ 314.6043, 1859.4316, 1718.5128],\n",
      "        [ 557.6865, 1870.3905, 2969.7358],\n",
      "        [ 559.7902, 1827.2817, 3052.5991],\n",
      "        [ 491.9644, 1809.8439, 2744.6357],\n",
      "        [ 321.5743, 1810.2068, 1778.3173],\n",
      "        [ 737.9185, 1984.3792, 3456.0037],\n",
      "        [ 376.5796, 1775.7611, 2213.6208],\n",
      "        [ 321.1836, 1793.8778, 1782.5520],\n",
      "        [ 338.3071, 1786.7701, 1915.0931],\n",
      "        [ 336.9734, 1779.7548, 1933.2039],\n",
      "        [ 526.8743, 1816.9597, 2835.0107],\n",
      "        [ 478.3451, 1824.7849, 2621.8787],\n",
      "        [ 443.8886, 1846.0433, 2344.5613],\n",
      "        [ 596.4731, 1816.6692, 3189.4785],\n",
      "        [ 397.9453, 1825.6841, 2213.9897],\n",
      "        [ 449.2749, 1841.6364, 2435.5144],\n",
      "        [ 545.0272, 1840.3573, 2996.5469],\n",
      "        [ 511.8210, 1861.0348, 2658.3813],\n",
      "        [ 528.3148, 1783.5807, 2873.3784],\n",
      "        [ 466.9728, 1867.8099, 2347.3298],\n",
      "        [ 419.2309, 1807.5482, 2348.7183],\n",
      "        [ 350.8196, 1882.0363, 1735.1611],\n",
      "        [ 447.2292, 1846.8475, 2436.8813],\n",
      "        [ 421.8640, 1890.2748, 2228.2568],\n",
      "        [ 349.7575, 1918.5709, 1627.4609],\n",
      "        [ 453.8967, 1875.8198, 2436.4775],\n",
      "        [ 506.5883, 1889.5598, 2683.7463],\n",
      "        [ 303.9859, 1838.7151, 1699.7081],\n",
      "        [ 372.9748, 1861.1354, 2013.8374],\n",
      "        [ 301.4489, 1831.6160, 1680.4279],\n",
      "        [ 412.0598, 1860.5042, 2161.2114],\n",
      "        [ 384.9350, 1811.2010, 2191.8616],\n",
      "        [ 410.6849, 1843.3456, 2213.5969],\n",
      "        [ 453.9555, 1827.0861, 2423.6399],\n",
      "        [ 569.6440, 1936.3103, 3055.5833],\n",
      "        [ 454.2392, 1835.5760, 2506.8669],\n",
      "        [ 359.8154, 1859.8673, 1926.0120],\n",
      "        [ 480.6549, 1859.4261, 2590.5850],\n",
      "        [ 559.7865, 1951.1957, 2489.1328],\n",
      "        [ 365.1740, 1856.3149, 1974.0476],\n",
      "        [ 519.8376, 1863.8219, 2752.2693],\n",
      "        [ 292.0218, 1833.9895, 1593.0652],\n",
      "        [ 286.9075, 1843.0215, 1518.0178],\n",
      "        [ 265.1107, 1823.2153, 1488.3740],\n",
      "        [ 219.7413, 1826.1311, 1195.1990],\n",
      "        [ 292.9920, 1836.3358, 1552.7489],\n",
      "        [ 357.1841, 1812.2344, 1942.1167],\n",
      "        [ 362.3393, 1801.7616, 2042.5905],\n",
      "        [ 428.2923, 1842.6306, 2285.3877],\n",
      "        [ 248.7645, 1846.4735, 1329.6469],\n",
      "        [ 317.8928, 1841.0331, 1738.8398],\n",
      "        [ 285.8772, 1696.5587, 1578.3347],\n",
      "        [ 281.1999, 1661.2476, 1692.9001],\n",
      "        [ 218.1535, 1627.6841, 1365.1119],\n",
      "        [ 401.7733, 1795.1650, 1741.9954],\n",
      "        [ 217.1150, 1604.7670, 1233.2401],\n",
      "        [ 262.8199, 1586.4689, 1663.0244],\n",
      "        [ 225.4131, 1665.8331, 1361.8615],\n",
      "        [ 255.4653, 1888.3704, 1349.2745],\n",
      "        [ 381.3540, 2073.3169, 1842.9996],\n",
      "        [ 514.5855, 2267.5300, 2301.1438],\n",
      "        [ 409.3087, 2523.9041, 1620.0491],\n",
      "        [ 438.6181, 2668.8530, 1654.0184],\n",
      "        [ 700.2913, 3029.4246, 2343.6038],\n",
      "        [ 503.4139, 3370.8989, 1504.2089],\n",
      "        [ 856.6772, 3617.4202, 2344.2856],\n",
      "        [ 876.7410, 3673.9287, 2398.1370],\n",
      "        [ 679.1412, 3684.6250, 1852.4877],\n",
      "        [ 702.1785, 3654.3069, 1928.6360],\n",
      "        [ 575.4863, 3648.8611, 1585.1802],\n",
      "        [ 501.8374, 3635.6233, 1382.8091],\n",
      "        [ 643.9648, 3618.0234, 1788.3055],\n",
      "        [ 685.6227, 3613.9460, 1899.8422],\n",
      "        [ 512.8774, 3587.7388, 1435.8462],\n",
      "        [ 464.9538, 3572.5247, 1300.3082],\n",
      "        [ 586.1968, 3544.0771, 1655.8555],\n",
      "        [ 455.6556, 3513.0945, 1299.7650],\n",
      "        [ 474.3606, 3483.4521, 1367.9319]])\n",
      "data:  tensor([[[0.0045, 0.0049, 0.0048,  ..., 0.7758, 0.1139, 0.6522]],\n",
      "\n",
      "        [[0.0053, 0.0053, 0.0049,  ..., 0.7758, 0.1139, 0.6522]],\n",
      "\n",
      "        [[0.0063, 0.0057, 0.0062,  ..., 0.7734, 0.1139, 0.6522]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0068, 0.0062, 0.0064,  ..., 0.6244, 0.1181, 0.1304]],\n",
      "\n",
      "        [[0.0055, 0.0059, 0.0060,  ..., 0.6284, 0.1181, 0.1304]],\n",
      "\n",
      "        [[0.0056, 0.0053, 0.0052,  ..., 0.6320, 0.1181, 0.1304]]])\n",
      "Epoch 0, Iteration 30 Train Loss: 780.26\n",
      "output:  tensor([[0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773],\n",
      "        [0.7019, 5.7741, 4.5773]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[4.9181e+02, 3.4694e+03, 1.4206e+03],\n",
      "        [3.8264e+02, 3.4477e+03, 1.1113e+03],\n",
      "        [3.9616e+02, 3.4129e+03, 1.1659e+03],\n",
      "        [4.6136e+02, 3.3861e+03, 1.3711e+03],\n",
      "        [3.3666e+02, 3.3633e+03, 1.0008e+03],\n",
      "        [4.2788e+02, 3.3380e+03, 1.2865e+03],\n",
      "        [2.8304e+02, 3.3198e+03, 8.5576e+02],\n",
      "        [3.5816e+02, 3.2930e+03, 1.0934e+03],\n",
      "        [3.7194e+02, 3.2573e+03, 1.1452e+03],\n",
      "        [3.2399e+02, 3.2392e+03, 1.0059e+03],\n",
      "        [3.7591e+02, 3.2259e+03, 1.1573e+03],\n",
      "        [3.1970e+02, 3.1990e+03, 1.0021e+03],\n",
      "        [4.2846e+02, 3.1942e+03, 1.3045e+03],\n",
      "        [2.3964e+02, 3.1563e+03, 7.5828e+02],\n",
      "        [3.0245e+02, 3.1203e+03, 9.7239e+02],\n",
      "        [4.4380e+02, 3.1425e+03, 1.3152e+03],\n",
      "        [3.9414e+02, 3.1177e+03, 1.2109e+03],\n",
      "        [3.0477e+02, 3.0719e+03, 9.9345e+02],\n",
      "        [3.5554e+02, 3.0452e+03, 1.1736e+03],\n",
      "        [3.0475e+02, 3.0212e+03, 1.0123e+03],\n",
      "        [3.8684e+02, 3.0199e+03, 1.2667e+03],\n",
      "        [3.6203e+02, 3.0183e+03, 1.1670e+03],\n",
      "        [2.9673e+02, 2.9789e+03, 1.0003e+03],\n",
      "        [2.5118e+02, 2.9688e+03, 8.4742e+02],\n",
      "        [2.3429e+02, 2.9592e+03, 7.8978e+02],\n",
      "        [3.7459e+02, 2.9220e+03, 1.3412e+03],\n",
      "        [2.6198e+02, 2.9395e+03, 8.8986e+02],\n",
      "        [2.5326e+02, 2.9168e+03, 8.7021e+02],\n",
      "        [2.6064e+02, 2.9222e+03, 8.7357e+02],\n",
      "        [1.1866e+02, 1.4399e+03, 8.4728e+02],\n",
      "        [7.6432e+01, 7.4487e+02, 5.1770e+02],\n",
      "        [5.8161e+01, 6.2254e+02, 6.4237e+02],\n",
      "        [4.3471e+01, 8.7099e+02, 4.7561e+02],\n",
      "        [3.9590e+01, 7.4003e+02, 6.5620e+02],\n",
      "        [5.3851e+01, 4.9281e+02, 7.2517e+02],\n",
      "        [1.2820e+01, 2.4534e+02, 6.3520e+02],\n",
      "        [8.2019e+01, 4.8832e+02, 8.3998e+02],\n",
      "        [4.5075e+01, 4.9228e+02, 9.7310e+02],\n",
      "        [1.6348e+02, 5.1197e+02, 1.2663e+03],\n",
      "        [1.2826e+01, 1.2214e+02, 1.0501e+03],\n",
      "        [6.8167e+00, 1.2214e+02, 5.5809e+02],\n",
      "        [3.3004e+01, 2.5589e+02, 6.0807e+02],\n",
      "        [6.0981e+01, 2.6767e+02, 6.6576e+02],\n",
      "        [2.6234e+01, 3.2690e+02, 4.4684e+02],\n",
      "        [7.5693e+02, 1.8957e+03, 9.2001e+02],\n",
      "        [2.6570e+02, 1.2890e+03, 5.5468e+02],\n",
      "        [3.5661e+02, 1.8211e+03, 6.6847e+02],\n",
      "        [2.3446e+03, 8.9347e+03, 2.1493e+03],\n",
      "        [3.6313e+03, 1.1930e+04, 3.0447e+03],\n",
      "        [3.3878e+03, 1.1593e+04, 2.9290e+03],\n",
      "        [3.1340e+03, 1.1288e+04, 2.7788e+03],\n",
      "        [2.8943e+03, 1.0965e+04, 2.6480e+03],\n",
      "        [2.5530e+03, 1.0677e+04, 2.3920e+03],\n",
      "        [2.2773e+03, 1.0400e+04, 2.1916e+03],\n",
      "        [2.2141e+03, 1.0092e+04, 2.1932e+03],\n",
      "        [1.9425e+03, 9.7720e+03, 1.9890e+03],\n",
      "        [1.9007e+03, 9.2398e+03, 2.0765e+03],\n",
      "        [1.8673e+03, 9.1601e+03, 2.0425e+03],\n",
      "        [1.5123e+03, 8.8776e+03, 1.7040e+03],\n",
      "        [1.6705e+03, 8.5945e+03, 1.9488e+03],\n",
      "        [1.6519e+03, 8.3343e+03, 1.9875e+03],\n",
      "        [1.4608e+03, 8.0921e+03, 1.8071e+03],\n",
      "        [1.7983e+03, 7.8750e+03, 2.2832e+03],\n",
      "        [1.4216e+03, 7.6456e+03, 1.8611e+03],\n",
      "        [1.6711e+03, 7.4303e+03, 2.2541e+03],\n",
      "        [1.2504e+03, 7.2378e+03, 1.7282e+03],\n",
      "        [1.4206e+03, 7.0422e+03, 2.0259e+03],\n",
      "        [1.4458e+03, 6.8567e+03, 2.1121e+03],\n",
      "        [1.2325e+03, 6.6872e+03, 1.8493e+03],\n",
      "        [1.2882e+03, 6.5584e+03, 1.9507e+03],\n",
      "        [1.2965e+03, 6.4161e+03, 2.0175e+03],\n",
      "        [1.5805e+03, 6.2467e+03, 2.5329e+03],\n",
      "        [1.4178e+03, 6.0998e+03, 2.3347e+03],\n",
      "        [1.4382e+03, 5.9539e+03, 2.4274e+03],\n",
      "        [1.2169e+03, 5.8335e+03, 2.0888e+03],\n",
      "        [1.1380e+03, 5.7306e+03, 1.9841e+03],\n",
      "        [1.5390e+03, 5.6274e+03, 2.7425e+03],\n",
      "        [1.2527e+03, 5.5339e+03, 2.2677e+03],\n",
      "        [1.2294e+03, 5.4346e+03, 2.2633e+03],\n",
      "        [9.4548e+02, 5.3188e+03, 1.7839e+03],\n",
      "        [1.4894e+03, 5.2544e+03, 2.8273e+03],\n",
      "        [1.1148e+03, 5.1531e+03, 2.1644e+03],\n",
      "        [1.2431e+03, 5.0524e+03, 2.4662e+03],\n",
      "        [1.0187e+03, 5.0027e+03, 2.0342e+03],\n",
      "        [1.0851e+03, 4.9122e+03, 2.2274e+03],\n",
      "        [1.0462e+03, 4.8037e+03, 2.1961e+03],\n",
      "        [9.7997e+02, 4.7396e+03, 2.0786e+03],\n",
      "        [1.1322e+03, 4.6982e+03, 2.4190e+03],\n",
      "        [8.3936e+02, 4.6072e+03, 1.8378e+03],\n",
      "        [8.5542e+02, 4.5738e+03, 1.8666e+03],\n",
      "        [1.0250e+03, 4.4662e+03, 2.3106e+03],\n",
      "        [1.2546e+03, 4.4101e+03, 2.8664e+03],\n",
      "        [9.7896e+02, 4.4279e+03, 2.1695e+03],\n",
      "        [7.8032e+02, 4.2819e+03, 1.8401e+03],\n",
      "        [4.2543e+02, 4.2689e+03, 9.9205e+02],\n",
      "        [9.9137e+02, 4.2277e+03, 2.3380e+03],\n",
      "        [1.0378e+03, 4.1428e+03, 2.5112e+03],\n",
      "        [1.0441e+03, 4.0885e+03, 2.5779e+03],\n",
      "        [5.4458e+02, 4.0611e+03, 1.3396e+03],\n",
      "        [7.7086e+02, 4.0196e+03, 1.9259e+03],\n",
      "        [7.9354e+02, 3.9472e+03, 2.0095e+03],\n",
      "        [9.9849e+02, 3.9290e+03, 2.5358e+03],\n",
      "        [6.3190e+02, 3.8912e+03, 1.5916e+03],\n",
      "        [7.2943e+02, 3.8256e+03, 1.9045e+03],\n",
      "        [1.0653e+03, 3.7757e+03, 2.8199e+03],\n",
      "        [1.0372e+03, 3.7142e+03, 2.8027e+03],\n",
      "        [6.5212e+02, 3.7048e+03, 1.7595e+03],\n",
      "        [1.1281e+03, 3.6329e+03, 3.1343e+03],\n",
      "        [6.5837e+02, 3.6426e+03, 1.7858e+03],\n",
      "        [6.5644e+02, 3.6273e+03, 1.7814e+03],\n",
      "        [5.5288e+02, 3.5416e+03, 1.5847e+03],\n",
      "        [4.4654e+02, 3.5231e+03, 1.2833e+03],\n",
      "        [7.7757e+02, 3.5059e+03, 2.2113e+03],\n",
      "        [8.7544e+02, 3.4743e+03, 2.4861e+03],\n",
      "        [9.9463e+02, 3.4483e+03, 2.8425e+03],\n",
      "        [1.1190e+03, 3.3919e+03, 3.2678e+03],\n",
      "        [7.0909e+02, 3.3644e+03, 2.1022e+03],\n",
      "        [7.2024e+02, 3.3153e+03, 2.1955e+03],\n",
      "        [7.5378e+02, 3.3560e+03, 2.2065e+03],\n",
      "        [1.0149e+03, 3.2563e+03, 3.1506e+03],\n",
      "        [4.1852e+02, 3.2527e+03, 1.2800e+03],\n",
      "        [6.6241e+02, 3.2346e+03, 2.0445e+03],\n",
      "        [1.0905e+03, 3.2049e+03, 3.3936e+03],\n",
      "        [7.9822e+02, 3.1873e+03, 2.4995e+03],\n",
      "        [7.2250e+02, 3.1648e+03, 2.2991e+03],\n",
      "        [6.9905e+02, 3.1269e+03, 2.2554e+03],\n",
      "        [6.5572e+02, 3.0935e+03, 2.1578e+03],\n",
      "        [8.8931e+02, 3.0919e+03, 2.8748e+03],\n",
      "        [4.6207e+02, 3.0732e+03, 1.5071e+03],\n",
      "        [4.8059e+02, 3.0593e+03, 1.5513e+03],\n",
      "        [7.7766e+02, 2.9932e+03, 2.6106e+03],\n",
      "        [7.5445e+02, 2.9256e+03, 2.6449e+03],\n",
      "        [7.4335e+02, 2.9974e+03, 2.4701e+03],\n",
      "        [6.7429e+02, 2.9046e+03, 2.3447e+03],\n",
      "        [6.7539e+02, 2.9095e+03, 2.3663e+03],\n",
      "        [5.9101e+02, 2.9471e+03, 1.9701e+03],\n",
      "        [7.4762e+02, 2.9197e+03, 2.5157e+03],\n",
      "        [6.3394e+02, 2.8830e+03, 2.1811e+03],\n",
      "        [4.5268e+02, 2.8472e+03, 1.5889e+03],\n",
      "        [5.3709e+02, 2.8126e+03, 1.9214e+03],\n",
      "        [4.7768e+02, 2.8217e+03, 1.6586e+03],\n",
      "        [7.3298e+02, 2.7517e+03, 2.6922e+03],\n",
      "        [6.7419e+02, 2.7360e+03, 2.4502e+03],\n",
      "        [6.6037e+02, 2.7502e+03, 2.3855e+03],\n",
      "        [5.4457e+02, 2.7266e+03, 1.9909e+03],\n",
      "        [5.8221e+02, 2.6825e+03, 2.1919e+03],\n",
      "        [5.8227e+02, 2.6376e+03, 2.2352e+03],\n",
      "        [6.4512e+02, 2.6590e+03, 2.4305e+03],\n",
      "        [6.5329e+02, 2.6515e+03, 2.4289e+03],\n",
      "        [6.0728e+02, 2.5633e+03, 2.4234e+03]])\n",
      "data:  tensor([[[0.0058, 0.0062, 0.0064,  ..., 0.6362, 0.1181, 0.1304]],\n",
      "\n",
      "        [[0.0043, 0.0042, 0.0042,  ..., 0.6396, 0.1181, 0.1304]],\n",
      "\n",
      "        [[0.0047, 0.0043, 0.0042,  ..., 0.6436, 0.1181, 0.1739]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0082, 0.0082, 0.0082,  ..., 0.5480, 0.1350, 0.7391]],\n",
      "\n",
      "        [[0.0071, 0.0069, 0.0065,  ..., 0.5480, 0.1350, 0.7391]],\n",
      "\n",
      "        [[0.0077, 0.0077, 0.0073,  ..., 0.5455, 0.1350, 0.7391]]])\n",
      "output:  tensor([[0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305],\n",
      "        [0.7551, 5.8273, 4.6305]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 528.6313, 2596.1411, 2058.3464],\n",
      "        [ 541.9802, 2579.6416, 2118.4521],\n",
      "        [ 557.8336, 2576.5171, 2177.4844],\n",
      "        [ 448.7554, 2582.0603, 1741.5284],\n",
      "        [ 526.2582, 2552.4514, 2075.6318],\n",
      "        [ 507.7963, 2526.9202, 2046.3068],\n",
      "        [ 537.9651, 2541.5264, 2126.0647],\n",
      "        [ 519.0670, 2549.2957, 2011.8777],\n",
      "        [ 611.7596, 2531.1038, 2374.3652],\n",
      "        [ 432.8085, 2513.7944, 1693.8496],\n",
      "        [ 692.2892, 2468.7529, 2835.5144],\n",
      "        [ 457.9107, 2486.9617, 1844.2822],\n",
      "        [ 703.4326, 2467.6248, 2869.6694],\n",
      "        [ 557.6246, 2468.9373, 2303.3257],\n",
      "        [ 471.1835, 2452.0522, 1959.4507],\n",
      "        [ 486.7293, 2489.2798, 1951.3553],\n",
      "        [ 614.8922, 2479.7339, 2480.6409],\n",
      "        [ 739.2565, 2514.6211, 2934.2449],\n",
      "        [ 596.5715, 2470.3560, 2473.8486],\n",
      "        [ 625.9958, 2516.9668, 2474.7300],\n",
      "        [ 465.0750, 2498.8755, 1891.7859],\n",
      "        [ 587.2109, 2507.2983, 2377.6321],\n",
      "        [ 575.6003, 2529.8694, 2270.8340],\n",
      "        [ 528.6906, 2520.5024, 2099.0745],\n",
      "        [ 510.8576, 2533.1873, 2006.1992],\n",
      "        [ 484.3843, 2502.8357, 1947.3754],\n",
      "        [ 676.0825, 2498.4119, 2711.0728],\n",
      "        [ 727.6564, 2460.2520, 2963.2522],\n",
      "        [ 632.3569, 2452.3818, 2598.4031],\n",
      "        [ 521.9008, 2422.8513, 2165.4902],\n",
      "        [ 536.3841, 2411.4124, 2242.9587],\n",
      "        [ 691.9307, 2354.3062, 2970.1938],\n",
      "        [ 367.0793, 2400.1128, 1490.2216],\n",
      "        [ 466.3668, 2368.6943, 1973.3369],\n",
      "        [ 709.0573, 2379.2397, 2959.3701],\n",
      "        [ 596.0491, 2339.7952, 2559.3696],\n",
      "        [ 489.5047, 2322.8093, 2101.3352],\n",
      "        [ 425.2235, 2305.2600, 1852.7950],\n",
      "        [ 669.8846, 2297.2615, 2926.9500],\n",
      "        [ 621.1738, 2311.5771, 2667.2512],\n",
      "        [ 536.9719, 2257.8223, 2394.9695],\n",
      "        [ 499.6308, 2213.0713, 2305.5811],\n",
      "        [ 605.7538, 2234.8884, 2705.1538],\n",
      "        [ 556.7622, 2169.7615, 2639.1355],\n",
      "        [ 443.5390, 2161.5957, 2107.4431],\n",
      "        [ 530.5193, 2176.7881, 2455.4121],\n",
      "        [ 481.3094, 2186.5012, 2156.6062],\n",
      "        [ 666.5626, 2175.5815, 3048.4773],\n",
      "        [ 471.9657, 2146.9783, 2233.6882],\n",
      "        [ 587.6332, 2157.1553, 2702.0366],\n",
      "        [ 744.5766, 2113.7671, 3570.0569],\n",
      "        [ 686.1304, 2096.4575, 3360.7065],\n",
      "        [ 533.1407, 2161.9307, 2428.1230],\n",
      "        [ 681.5977, 2154.0386, 3159.6165],\n",
      "        [ 711.7597, 2203.0063, 3203.6650],\n",
      "        [ 390.2388, 2143.5601, 1899.5945],\n",
      "        [ 467.9973, 2201.6604, 2133.5522],\n",
      "        [ 723.9701, 2216.4675, 3236.3599],\n",
      "        [ 452.8812, 2262.4417, 1953.5715],\n",
      "        [ 550.1366, 2253.5271, 2459.7825],\n",
      "        [ 413.5468, 2285.9956, 1785.0665],\n",
      "        [ 535.1830, 2263.0171, 2362.3511],\n",
      "        [ 509.5000, 2270.1721, 2257.5173],\n",
      "        [ 588.2812, 2317.6040, 2509.6775],\n",
      "        [ 723.0831, 2270.0825, 3128.2805],\n",
      "        [ 459.2288, 2312.3760, 1963.5901],\n",
      "        [ 412.5580, 2308.2483, 1797.6486],\n",
      "        [ 335.1427, 2343.6548, 1424.7346],\n",
      "        [ 518.2584, 2343.1689, 2226.4387],\n",
      "        [ 536.7751, 2372.4355, 2270.6663],\n",
      "        [ 443.7386, 2395.4824, 1854.1056],\n",
      "        [ 339.8350, 2417.8970, 1392.7511],\n",
      "        [ 347.5935, 2441.4231, 1433.2515],\n",
      "        [ 424.5899, 2470.6521, 1718.5814],\n",
      "        [ 531.8747, 2490.9163, 2155.0256],\n",
      "        [ 375.5703, 2545.8328, 1474.7222],\n",
      "        [ 358.8189, 2574.5979, 1383.8081],\n",
      "        [ 287.0067, 2596.1636, 1098.3314],\n",
      "        [ 533.4595, 2594.4824, 2070.7200],\n",
      "        [ 409.5079, 2656.2075, 1545.3036],\n",
      "        [ 471.2814, 2662.2400, 1796.4696],\n",
      "        [ 647.9659, 2755.7412, 2333.7432],\n",
      "        [ 477.7374, 2790.0918, 1702.7656],\n",
      "        [ 496.4542, 2870.1431, 1720.6956],\n",
      "        [ 546.0435, 2887.2571, 1928.5977],\n",
      "        [ 419.9588, 2729.3552, 1520.5670],\n",
      "        [ 367.6109, 2468.9429, 1494.0461],\n",
      "        [ 189.5370, 2307.2373,  823.5412],\n",
      "        [ 298.8306, 2324.9824, 1288.7777],\n",
      "        [ 305.7345, 2383.3955, 1273.7539],\n",
      "        [ 439.2080, 2474.1877, 1764.1515],\n",
      "        [ 345.3239, 2581.4458, 1373.3259],\n",
      "        [ 393.0824, 2690.2512, 1463.1111],\n",
      "        [ 448.3213, 2739.2246, 1640.6964],\n",
      "        [ 450.9239, 2780.4065, 1598.9166],\n",
      "        [ 328.1511, 2777.3625, 1189.8949],\n",
      "        [ 324.8107, 2836.4905, 1146.1577],\n",
      "        [ 484.6305, 2894.8423, 1642.5516],\n",
      "        [ 344.4867, 2913.6038, 1188.2179],\n",
      "        [ 204.9210, 2938.3306,  698.2977],\n",
      "        [ 492.2154, 3002.8152, 1645.3025],\n",
      "        [ 429.7827, 2954.4561, 1744.7460],\n",
      "        [ 445.0894, 3043.9802, 1471.2200],\n",
      "        [ 384.9769, 3093.5178, 1249.8973],\n",
      "        [ 411.5650, 3273.8958, 1249.6212],\n",
      "        [ 369.3539, 3284.0449, 1422.1765],\n",
      "        [ 496.7359, 3577.6685, 1397.9746],\n",
      "        [ 627.1202, 3714.8704, 1701.2423],\n",
      "        [ 483.0083, 3855.8259, 1256.9585],\n",
      "        [ 480.6782, 3813.7671, 1578.2096],\n",
      "        [ 535.6057, 4081.2004, 1314.6877],\n",
      "        [ 684.4261, 4190.5308, 1638.5096],\n",
      "        [ 616.4777, 4105.8267, 1693.5771],\n",
      "        [ 468.1826, 4405.2759, 1060.7334],\n",
      "        [ 500.1513, 4509.8418, 1112.3732],\n",
      "        [ 678.7925, 4603.3096, 1477.5660],\n",
      "        [ 611.0895, 4704.9209, 1304.3002],\n",
      "        [ 580.8116, 4807.4648, 1210.5431],\n",
      "        [ 670.2151, 4902.5688, 1371.1647],\n",
      "        [ 789.7556, 5006.5713, 1569.5763],\n",
      "        [ 577.5550, 5091.7832, 1137.6528],\n",
      "        [ 698.5821, 5194.8911, 1346.8041],\n",
      "        [ 743.2097, 5282.4995, 1410.8619],\n",
      "        [ 625.0339, 5392.4727, 1159.3821],\n",
      "        [ 954.1485, 5487.2358, 1743.0986],\n",
      "        [ 710.5675, 5584.9263, 1277.0201],\n",
      "        [ 776.0307, 5688.9951, 1368.2313],\n",
      "        [ 878.9750, 5783.9932, 1525.4918],\n",
      "        [1002.9108, 5794.3818, 1925.4606],\n",
      "        [ 875.1979, 5991.9971, 1463.5620],\n",
      "        [1017.6286, 6096.1831, 1670.0972],\n",
      "        [1060.6893, 6180.2168, 1721.2651],\n",
      "        [1013.1115, 6277.4209, 1616.8474],\n",
      "        [ 876.9472, 6362.1416, 1381.3304],\n",
      "        [1257.2219, 6446.7842, 1954.1270],\n",
      "        [ 898.3683, 6547.7920, 1374.5908],\n",
      "        [ 978.1873, 6637.6807, 1475.2258],\n",
      "        [1163.5553, 6714.4966, 1735.3330],\n",
      "        [1298.3368, 6799.9321, 1912.7295],\n",
      "        [1047.5455, 6893.2603, 1522.8917],\n",
      "        [1032.5765, 6977.5620, 1480.9821],\n",
      "        [1105.7341, 7051.3799, 1570.8365],\n",
      "        [1088.1653, 7135.8774, 1527.8730],\n",
      "        [1059.0371, 7210.6055, 1471.9976],\n",
      "        [1143.2570, 7265.1143, 1575.7662],\n",
      "        [1429.6333, 7298.1025, 1960.5287],\n",
      "        [1199.7545, 7343.3003, 1635.5970],\n",
      "        [1294.7527, 7388.9727, 1755.2961],\n",
      "        [1043.6357, 7446.3301, 1404.2650],\n",
      "        [1791.1823, 7439.5664, 2412.2595]])\n",
      "data:  tensor([[[0.0062, 0.0054, 0.0062,  ..., 0.5445, 0.1350, 0.7391]],\n",
      "\n",
      "        [[0.0062, 0.0071, 0.0070,  ..., 0.5424, 0.1350, 0.7391]],\n",
      "\n",
      "        [[0.0058, 0.0058, 0.0058,  ..., 0.5409, 0.1350, 0.7391]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0158, 0.0156,  ..., 0.8766, 0.1392, 0.2174]],\n",
      "\n",
      "        [[0.0123, 0.0126, 0.0131,  ..., 0.8806, 0.1392, 0.2174]],\n",
      "\n",
      "        [[0.0201, 0.0190, 0.0179,  ..., 0.8847, 0.1392, 0.2174]]])\n",
      "output:  tensor([[0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837],\n",
      "        [0.8083, 5.8805, 4.6837]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[1260.7932, 7451.5356, 1695.1729],\n",
      "        [1495.9409, 7450.5474, 2011.3168],\n",
      "        [1458.2821, 7464.3267, 1956.6848],\n",
      "        [1378.7543, 7475.1123, 1847.5485],\n",
      "        [1151.7037, 7485.8755, 1540.3030],\n",
      "        [1519.4740, 7476.4639, 2035.0137],\n",
      "        [1454.1960, 7481.0996, 1947.1309],\n",
      "        [1278.3214, 7483.7139, 1711.3566],\n",
      "        [1389.2631, 7458.7188, 1867.1968],\n",
      "        [ 725.2681, 7465.1250,  973.5820],\n",
      "        [1368.8810, 7431.5566, 1844.9777],\n",
      "        [1273.4055, 7414.2192, 1719.9697],\n",
      "        [1680.4174, 7358.6660, 2286.6265],\n",
      "        [ 875.8492, 7338.3403, 1194.5242],\n",
      "        [ 978.4992, 7297.4824, 1342.5397],\n",
      "        [1531.6349, 7254.4185, 2113.8809],\n",
      "        [1207.8184, 7230.7021, 1676.1682],\n",
      "        [1030.9955, 7186.5767, 1437.4912],\n",
      "        [1311.1591, 7136.8872, 1843.0043],\n",
      "        [1113.3079, 7109.5752, 1568.0715],\n",
      "        [ 973.1069, 7090.8691, 1373.6979],\n",
      "        [1090.5013, 7049.9331, 1549.7758],\n",
      "        [1446.8831, 6994.8213, 2071.6621],\n",
      "        [ 960.7077, 6955.7397, 1382.5493],\n",
      "        [ 960.5961, 6900.9126, 1394.3109],\n",
      "        [1046.2185, 6854.3130, 1529.9324],\n",
      "        [ 874.9539, 6786.5161, 1291.1971],\n",
      "        [ 985.3431, 6723.7798, 1468.0200],\n",
      "        [1195.3695, 6652.5981, 1803.0219],\n",
      "        [ 970.5815, 6582.3833, 1478.4512],\n",
      "        [ 839.9654, 6506.6382, 1291.4696],\n",
      "        [ 997.9729, 6428.6089, 1556.7113],\n",
      "        [1129.2051, 6340.6602, 1787.0708],\n",
      "        [ 989.4506, 6252.8838, 1585.7570],\n",
      "        [1026.7323, 6171.4082, 1668.8607],\n",
      "        [ 789.8187, 6071.2603, 1303.0598],\n",
      "        [1061.0841, 5968.5654, 1782.1794],\n",
      "        [ 934.1166, 5867.7920, 1596.0621],\n",
      "        [ 716.3469, 5762.1094, 1242.8370],\n",
      "        [ 769.1071, 5628.1187, 1367.5646],\n",
      "        [ 755.1788, 5498.4014, 1378.1902],\n",
      "        [ 620.6782, 5376.2915, 1155.9285],\n",
      "        [ 542.9731, 5226.1030, 1041.3959],\n",
      "        [ 452.0333, 5058.6611,  892.9078],\n",
      "        [ 638.6200, 4867.5649, 1324.2457],\n",
      "        [ 601.0442, 4700.4858, 1282.0660],\n",
      "        [ 500.3184, 4547.8623, 1100.8840],\n",
      "        [ 403.4540, 4394.0605,  921.0315],\n",
      "        [ 361.6297, 4273.1123,  841.2921],\n",
      "        [ 398.0988, 4148.1274,  959.9661],\n",
      "        [ 391.4864, 4022.9102,  975.6595],\n",
      "        [ 481.3993, 3921.9412, 1235.0077],\n",
      "        [ 632.1179, 3839.6558, 1641.3759],\n",
      "        [ 586.5535, 3773.8923, 1539.9773],\n",
      "        [ 619.1343, 3642.6443, 1708.1637],\n",
      "        [ 679.7175, 3595.7429, 1891.7086],\n",
      "        [ 458.7748, 3527.8513, 1303.1714],\n",
      "        [ 508.7249, 3457.4238, 1470.5764],\n",
      "        [ 605.8319, 3405.4397, 1770.8792],\n",
      "        [ 406.9110, 3335.2134, 1227.4948],\n",
      "        [ 466.1131, 3276.9734, 1428.0892],\n",
      "        [ 631.7266, 3226.5029, 1976.9403],\n",
      "        [ 319.2619, 3195.2966, 1001.1638],\n",
      "        [ 467.3115, 3168.4250, 1464.1443],\n",
      "        [ 502.4077, 3103.7227, 1621.3221],\n",
      "        [ 491.7336, 3083.1289, 1589.9895],\n",
      "        [ 732.8602, 3034.6077, 2429.3525],\n",
      "        [ 408.8918, 3019.0913, 1354.9674],\n",
      "        [ 688.5654, 2992.6162, 2283.2822],\n",
      "        [ 603.6550, 2936.1299, 2069.6182],\n",
      "        [ 653.9345, 2897.3892, 2272.5881],\n",
      "        [ 496.2924, 2862.1560, 1750.0654],\n",
      "        [ 750.9023, 2840.4617, 2655.0181],\n",
      "        [ 541.6571, 2817.1926, 1909.9547],\n",
      "        [ 439.6388, 2776.3069, 1598.2809],\n",
      "        [ 478.5854, 2758.8020, 1733.2458],\n",
      "        [ 544.4525, 2721.2393, 2027.9912],\n",
      "        [ 651.0200, 2712.9280, 2405.6819],\n",
      "        [ 587.1152, 2665.0327, 2240.6182],\n",
      "        [ 572.6185, 2696.9761, 2093.7595],\n",
      "        [ 438.5022, 2637.2112, 1687.6279],\n",
      "        [ 554.0037, 2621.8076, 2139.8767],\n",
      "        [ 536.9696, 2602.0730, 2102.9302],\n",
      "        [ 698.5706, 2624.3032, 2643.2588],\n",
      "        [ 558.4984, 2594.4546, 2152.1460],\n",
      "        [ 543.8293, 2567.2139, 2127.5027],\n",
      "        [ 688.5721, 2538.2141, 2755.4749],\n",
      "        [ 429.5088, 2538.1470, 1710.1991],\n",
      "        [ 645.4352, 2511.5601, 2604.1265],\n",
      "        [ 556.1985, 2519.1228, 2223.1399],\n",
      "        [ 493.2771, 2522.0833, 1931.9485],\n",
      "        [ 610.5743, 2513.1240, 2379.1545],\n",
      "        [ 584.7922, 2464.0723, 2381.6147],\n",
      "        [ 672.5811, 2473.2102, 2692.1255],\n",
      "        [ 608.4406, 2444.2549, 2482.1855],\n",
      "        [ 416.8636, 2402.6096, 1774.5947],\n",
      "        [ 636.2047, 2409.5300, 2650.0120],\n",
      "        [ 226.2978, 2396.8511,  949.5536],\n",
      "        [ 515.2732, 2392.2988, 2144.8845],\n",
      "        [ 664.3929, 2383.9707, 2786.8770],\n",
      "        [ 450.0928, 2382.2617, 1890.4045],\n",
      "        [ 820.3932, 2374.1404, 3470.9185],\n",
      "        [ 354.4662, 2361.3384, 1495.7644],\n",
      "        [ 612.3865, 2351.6362, 2587.7786],\n",
      "        [ 396.8161, 2311.1079, 1760.0361],\n",
      "        [ 471.5421, 2309.1196, 2072.1338],\n",
      "        [ 600.2613, 2366.6948, 2480.3420],\n",
      "        [ 480.3375, 2292.3018, 2115.0217],\n",
      "        [ 662.2184, 2293.4971, 2910.1448],\n",
      "        [ 495.5240, 2285.1970, 2179.1328],\n",
      "        [ 725.9967, 2245.3445, 3276.2134],\n",
      "        [ 398.1147, 2285.1858, 1721.7249],\n",
      "        [ 694.5625, 2345.2969, 2902.1675],\n",
      "        [ 577.6339, 2306.4832, 2449.7925],\n",
      "        [ 598.5140, 2248.1680, 2670.8137],\n",
      "        [ 480.8766, 2241.7866, 2166.9292],\n",
      "        [ 577.1987, 2234.8159, 2580.5410],\n",
      "        [ 434.2915, 2213.8813, 1979.3531],\n",
      "        [ 290.5014, 2227.8562, 1300.1786],\n",
      "        [ 496.9868, 2221.5840, 2230.8882],\n",
      "        [ 437.9622, 2164.6062, 2096.7029],\n",
      "        [ 551.8976, 2198.2253, 2516.5637],\n",
      "        [ 414.6707, 2182.7480, 1922.7313],\n",
      "        [ 553.0538, 2180.3572, 2543.2576],\n",
      "        [ 844.5750, 2255.9346, 3679.2036],\n",
      "        [ 660.7175, 2185.8423, 3037.0327],\n",
      "        [ 419.6783, 2139.5776, 2027.1954],\n",
      "        [ 383.3909, 2152.6143, 1805.8986],\n",
      "        [ 530.3452, 2166.9521, 2443.3120],\n",
      "        [ 712.2242, 2162.0928, 3314.4092],\n",
      "        [ 526.8174, 2155.9600, 2448.9880],\n",
      "        [ 599.5057, 2192.2935, 2713.5916],\n",
      "        [ 519.8179, 2174.6042, 2377.6602],\n",
      "        [ 619.1758, 2173.4927, 2825.8271],\n",
      "        [ 442.2977, 2149.7544, 2064.0144],\n",
      "        [ 539.7619, 2194.0137, 2424.3750],\n",
      "        [ 576.4748, 2189.1599, 2595.0168],\n",
      "        [ 351.6722, 2157.2947, 1627.7229],\n",
      "        [ 537.7970, 2123.4021, 2575.3206],\n",
      "        [ 400.1608, 2168.8398, 1816.0261],\n",
      "        [ 544.2728, 2145.5261, 2529.8237],\n",
      "        [ 489.2104, 2135.9639, 2315.9531],\n",
      "        [ 295.0463, 2143.4873, 1382.2490],\n",
      "        [ 703.8439, 2137.0361, 3313.6101],\n",
      "        [ 548.4918, 2153.3123, 2526.7249],\n",
      "        [ 522.2043, 2127.1443, 2481.8398],\n",
      "        [ 506.9283, 2131.3167, 2388.2778],\n",
      "        [ 493.3132, 2160.0093, 2245.0061],\n",
      "        [ 497.6667, 2075.9302, 2468.3269],\n",
      "        [ 498.0511, 2111.5161, 2385.5181]])\n",
      "data:  tensor([[[0.0154, 0.0162, 0.0173,  ..., 0.8861, 0.1392, 0.2174]],\n",
      "\n",
      "        [[0.0179, 0.0178, 0.0173,  ..., 0.8861, 0.1392, 0.2609]],\n",
      "\n",
      "        [[0.0156, 0.0158, 0.0158,  ..., 0.8890, 0.1392, 0.2609]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0051, 0.0054, 0.0061,  ..., 0.5552, 0.1392, 0.7826]],\n",
      "\n",
      "        [[0.0064, 0.0068, 0.0068,  ..., 0.5552, 0.1392, 0.7826]],\n",
      "\n",
      "        [[0.0055, 0.0049, 0.0048,  ..., 0.5552, 0.1392, 0.7826]]])\n",
      "output:  tensor([[0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369],\n",
      "        [0.8615, 5.9337, 4.7369]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 513.6004, 2114.3816, 2452.4048],\n",
      "        [ 522.7088, 2147.8889, 2391.9829],\n",
      "        [ 595.4016, 2121.2517, 2811.5227],\n",
      "        [ 541.8352, 2102.5513, 2582.6809],\n",
      "        [ 515.2509, 2085.9177, 2501.1392],\n",
      "        [ 486.2205, 2101.1550, 2341.8450],\n",
      "        [ 380.1415, 2103.9534, 1834.6038],\n",
      "        [ 427.3831, 2104.5398, 2057.6389],\n",
      "        [ 512.7421, 2105.4280, 2452.9194],\n",
      "        [ 286.8080, 2086.9399, 1420.9490],\n",
      "        [ 544.0889, 2119.9111, 2551.7004],\n",
      "        [ 583.1027, 2079.0923, 2831.2451],\n",
      "        [ 475.9320, 2137.5054, 2170.7451],\n",
      "        [ 589.7985, 2083.1865, 2858.4956],\n",
      "        [ 633.6249, 2105.0425, 3005.6509],\n",
      "        [ 322.4173, 2084.3875, 1568.1272],\n",
      "        [ 440.1425, 2082.6726, 2104.0767],\n",
      "        [ 451.2429, 2089.6155, 2151.2537],\n",
      "        [ 480.6865, 2048.2158, 2389.8765],\n",
      "        [ 482.3196, 2054.4548, 2377.6138],\n",
      "        [ 501.9139, 2053.3320, 2472.3440],\n",
      "        [ 630.0112, 2067.3347, 3045.1306],\n",
      "        [ 431.9914, 2035.1458, 2191.9683],\n",
      "        [ 480.5021, 2060.2693, 2350.4749],\n",
      "        [ 381.1731, 2039.9492, 1880.6345],\n",
      "        [ 471.5610, 2065.3967, 2288.7949],\n",
      "        [ 334.9420, 2014.7865, 1696.5566],\n",
      "        [ 631.8646, 2037.4525, 3095.7874],\n",
      "        [ 386.4528, 2054.7507, 1849.5997],\n",
      "        [ 681.0077, 2004.5486, 3425.9968],\n",
      "        [ 639.4525, 2043.6747, 3132.8726],\n",
      "        [ 435.2978, 2005.4434, 2214.6011],\n",
      "        [ 437.9887, 2031.6548, 2131.6387],\n",
      "        [ 539.4963, 1995.6451, 2724.1416],\n",
      "        [ 355.5707, 2014.3285, 1758.3129],\n",
      "        [ 334.5404, 2000.5771, 1667.0378],\n",
      "        [ 490.8806, 1992.3607, 2461.9844],\n",
      "        [ 375.1606, 1979.7711, 1894.6219],\n",
      "        [ 375.0092, 2004.2747, 1832.5231],\n",
      "        [ 468.0673, 1961.1378, 2429.0210],\n",
      "        [ 425.7691, 1969.5721, 2173.0171],\n",
      "        [ 459.5609, 2013.6862, 2226.3816],\n",
      "        [ 432.6710, 1937.6118, 2283.8049],\n",
      "        [ 479.4873, 1981.7484, 2396.4075],\n",
      "        [ 320.2873, 1942.6555, 1703.8961],\n",
      "        [ 270.0698, 1940.5164, 1435.2988],\n",
      "        [ 444.8856, 1970.4043, 2264.6243],\n",
      "        [ 546.6309, 1928.6583, 2902.7466],\n",
      "        [ 412.2875, 1946.7386, 2148.7019],\n",
      "        [ 347.6297, 1985.5298, 1714.3218],\n",
      "        [ 449.9391, 2002.8114, 2204.2898],\n",
      "        [ 495.3298, 1980.4581, 2505.9470],\n",
      "        [ 432.0146, 1974.2974, 2198.9299],\n",
      "        [ 372.6382, 1975.3027, 1921.2799],\n",
      "        [ 395.7129, 1974.2192, 2039.4310],\n",
      "        [ 376.4694, 2045.3782, 1798.2190],\n",
      "        [ 393.7672, 2015.4513, 1945.9613],\n",
      "        [ 396.9008, 2035.4529, 1925.5220],\n",
      "        [ 416.6737, 2017.7693, 2075.7754],\n",
      "        [ 330.8489, 2019.8246, 1655.5959],\n",
      "        [ 374.0731, 2027.9125, 1859.3682],\n",
      "        [ 361.9547, 2021.4613, 1814.0275],\n",
      "        [ 365.4783, 2080.5388, 1722.4227],\n",
      "        [ 308.6334, 2070.7812, 1496.6353],\n",
      "        [ 450.2421, 2073.2219, 2193.5256],\n",
      "        [ 397.2368, 2109.6228, 1873.1475],\n",
      "        [ 287.2726, 2109.0808, 1376.4551],\n",
      "        [ 433.2921, 2147.0789, 2014.4110],\n",
      "        [ 315.7563, 2145.9619, 1479.7354],\n",
      "        [ 333.3493, 2184.4346, 1524.9049],\n",
      "        [ 335.7346, 2215.1213, 1503.0936],\n",
      "        [ 405.6023, 2219.2378, 1833.1071],\n",
      "        [ 300.8451, 2224.5664, 1354.9260],\n",
      "        [ 356.0915, 2217.1936, 1612.7302],\n",
      "        [ 293.3848, 2211.0049, 1336.2551],\n",
      "        [ 400.0582, 2172.8223, 1851.8320],\n",
      "        [ 223.5344, 2164.8240, 1020.0100],\n",
      "        [ 327.1323, 2107.1873, 1559.5394],\n",
      "        [ 243.8849, 2088.1519, 1174.3939],\n",
      "        [ 222.8646, 2070.2449, 1071.1367],\n",
      "        [ 187.4221, 2039.0166,  924.3279],\n",
      "        [ 153.7635, 2026.9294,  749.8139],\n",
      "        [ 138.8260, 1984.5745,  708.1568],\n",
      "        [ 223.6224, 1953.6366, 1153.6224],\n",
      "        [ 244.2260, 1940.0582, 1262.9873],\n",
      "        [ 120.4430, 1910.8184,  636.4736],\n",
      "        [ 185.4101, 1898.0778,  980.8687],\n",
      "        [ 222.4419, 1894.3578, 1174.8090],\n",
      "        [ 262.1320, 1873.4680, 1402.3801],\n",
      "        [ 173.5503, 1851.7069,  941.4900],\n",
      "        [ 150.6821, 1833.3085,  824.7362],\n",
      "        [ 171.3623, 1803.7443,  950.7405],\n",
      "        [ 228.0674, 1776.9956, 1290.2810],\n",
      "        [ 167.0885, 1728.6642,  970.9864],\n",
      "        [ 154.1055, 1701.6193,  915.1356],\n",
      "        [ 156.3532, 1678.2775,  934.9210],\n",
      "        [ 119.4974, 1647.9763,  725.6573],\n",
      "        [ 152.5404, 1621.6183,  947.8138],\n",
      "        [ 166.9249, 1618.5183, 1029.2159],\n",
      "        [  84.0292, 1587.6135,  534.9575],\n",
      "        [ 131.2525, 1574.8510,  841.5167],\n",
      "        [ 125.8921, 1577.0852,  793.9349],\n",
      "        [ 133.6750, 1552.9335,  867.7937],\n",
      "        [  96.3941, 1562.5630,  615.6303],\n",
      "        [  78.9229, 1570.9746,  501.0187],\n",
      "        [ 156.6594, 1569.6285,  999.6687],\n",
      "        [ 124.0449, 1553.7489,  803.4604],\n",
      "        [  85.3115, 1545.4487,  548.6107],\n",
      "        [ 142.4249, 1536.3949,  929.9914],\n",
      "        [ 170.2803, 1541.5670, 1106.8813],\n",
      "        [ 160.9387, 1530.6976, 1052.3568],\n",
      "        [ 104.5773, 1531.1781,  680.9786],\n",
      "        [ 123.2685, 1506.5627,  822.2371],\n",
      "        [ 131.7075, 1497.9498,  877.2147],\n",
      "        [  86.4021, 1471.3295,  588.0076],\n",
      "        [ 101.2428, 1451.4172,  706.7022],\n",
      "        [ 119.5398, 1421.2947,  844.8641],\n",
      "        [ 111.0378, 1395.9979,  801.2919],\n",
      "        [  97.0652, 1378.3590,  705.9883],\n",
      "        [  93.9063, 1338.9087,  699.6806],\n",
      "        [  85.8431, 1310.9812,  664.2659],\n",
      "        [  86.6392, 1299.2295,  668.5327],\n",
      "        [ 108.7657, 1274.3459,  851.3375],\n",
      "        [ 109.6386, 1250.5016,  872.5592],\n",
      "        [  89.1618, 1213.1514,  739.1708],\n",
      "        [  58.6890, 1190.6809,  492.3250],\n",
      "        [  70.5293, 1180.8506,  598.5792],\n",
      "        [  75.0871, 1161.5695,  654.2147],\n",
      "        [  64.3685, 1165.6915,  559.9244],\n",
      "        [  58.9365, 1166.1550,  508.4625],\n",
      "        [  64.0266, 1170.8746,  544.9174],\n",
      "        [  57.0662, 1173.0371,  492.1764],\n",
      "        [  33.1610, 1189.7537,  282.4281],\n",
      "        [  89.4349, 1195.6016,  742.6999],\n",
      "        [  89.7459, 1211.0066,  737.7384],\n",
      "        [  84.8409, 1216.7205,  704.9176],\n",
      "        [ 102.4211, 1243.4359,  819.3087],\n",
      "        [ 105.1446, 1227.1207,  858.9670],\n",
      "        [ 115.9606, 1233.0972,  942.1951],\n",
      "        [  65.2109, 1254.8805,  518.2230],\n",
      "        [ 120.9983, 1302.4354,  921.1192],\n",
      "        [ 145.1884, 1349.9904, 1081.5878],\n",
      "        [  80.3576, 1355.6874,  595.3425],\n",
      "        [  98.1743, 1365.1437,  714.7371],\n",
      "        [ 115.8541, 1365.7806,  846.5450],\n",
      "        [ 129.9360, 1408.4760,  929.0569],\n",
      "        [ 130.3419, 1359.1840,  960.4476],\n",
      "        [  99.4511, 1345.0807,  745.5398],\n",
      "        [  86.2794, 1345.1421,  643.7103],\n",
      "        [ 140.2930, 1408.9674,  996.7649]])\n",
      "data:  tensor([[[0.0061, 0.0066, 0.0064,  ..., 0.5552, 0.1392, 0.7826]],\n",
      "\n",
      "        [[0.0062, 0.0063, 0.0065,  ..., 0.5552, 0.1392, 0.7826]],\n",
      "\n",
      "        [[0.0072, 0.0069, 0.0069,  ..., 0.5552, 0.1392, 0.7826]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0011, 0.0011, 0.0011,  ..., 0.9929, 0.1435, 0.2609]],\n",
      "\n",
      "        [[0.0011, 0.0012, 0.0012,  ..., 0.9961, 0.1435, 0.2609]],\n",
      "\n",
      "        [[0.0017, 0.0017, 0.0015,  ..., 0.9976, 0.1435, 0.2609]]])\n",
      "output:  tensor([[0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900],\n",
      "        [0.9146, 5.9868, 4.7900]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 104.0243, 1419.1777,  738.0602],\n",
      "        [ 118.7446, 1436.4200,  829.5893],\n",
      "        [ 107.4355, 1423.8193,  756.8887],\n",
      "        [ 124.7886, 1401.1422,  891.6237],\n",
      "        [ 118.8392, 1397.4333,  856.4862],\n",
      "        [ 141.3847, 1398.8857, 1009.3023],\n",
      "        [ 100.3445, 1370.8186,  731.0753],\n",
      "        [ 130.6492, 1403.8289,  924.6683],\n",
      "        [ 150.3798, 1421.0043, 1066.3545],\n",
      "        [ 134.7892, 1416.3346,  945.3028],\n",
      "        [ 155.7686, 1415.5137, 1093.2203],\n",
      "        [ 121.6975, 1390.0997,  869.4102],\n",
      "        [ 102.9814, 1383.5612,  735.2612],\n",
      "        [ 119.4969, 1373.8514,  864.3562],\n",
      "        [ 112.1223, 1378.7947,  817.8890],\n",
      "        [ 161.9049, 1431.3318, 1125.6316],\n",
      "        [ 132.1763, 1431.4211,  925.2371],\n",
      "        [ 143.9713, 1461.4097,  977.2296],\n",
      "        [  90.6468, 1427.7179,  627.4994],\n",
      "        [ 129.9223, 1417.5692,  914.1439],\n",
      "        [ 105.2688, 1461.2979,  725.1188],\n",
      "        [ 117.3454, 1476.9429,  796.1926],\n",
      "        [  96.4415, 1433.2643,  679.7725],\n",
      "        [ 156.5347, 1434.7726, 1092.0046],\n",
      "        [ 109.0099, 1469.8494,  742.1885],\n",
      "        [ 171.9624, 1467.7660, 1176.1299],\n",
      "        [ 138.4423, 1476.3508,  933.2744],\n",
      "        [  94.1250, 1477.0824,  641.8955],\n",
      "        [ 173.4102, 1487.8514, 1173.5885],\n",
      "        [ 105.7914, 1538.7632,  695.7762],\n",
      "        [ 166.8916, 1590.3508, 1047.2701],\n",
      "        [ 156.2570, 1652.2268,  937.1436],\n",
      "        [ 164.0560, 1727.9270,  956.2805],\n",
      "        [ 156.7229, 1820.0150,  867.0188],\n",
      "        [ 163.1319, 1913.1642,  855.6881],\n",
      "        [ 164.4760, 1990.4282,  829.6347],\n",
      "        [ 221.6979, 2047.9979, 1085.1416],\n",
      "        [ 249.5863, 2092.7153, 1187.7997],\n",
      "        [ 272.1951, 2121.4358, 1286.8212],\n",
      "        [ 285.1571, 2136.7065, 1341.0715],\n",
      "        [ 249.1055, 2175.5425, 1137.7350],\n",
      "        [ 307.5995, 2160.6125, 1428.5560],\n",
      "        [ 467.7684, 2180.1448, 2139.9226],\n",
      "        [ 249.7641, 2169.1138, 1152.8345],\n",
      "        [ 317.6359, 2169.5884, 1468.3610],\n",
      "        [ 206.7870, 2180.9048,  935.1782],\n",
      "        [ 367.4165, 2167.9800, 1699.0326],\n",
      "        [ 312.5237, 2184.6023, 1423.7037],\n",
      "        [ 389.4367, 2153.2788, 1832.6425],\n",
      "        [ 306.4201, 2191.1316, 1395.2271],\n",
      "        [ 405.8237, 2188.4841, 1856.3396],\n",
      "        [ 261.1622, 2202.4983, 1198.8444],\n",
      "        [ 347.6863, 2213.1887, 1571.9545],\n",
      "        [ 318.5706, 2216.7188, 1452.1963],\n",
      "        [ 246.2991, 2225.5774, 1111.6584],\n",
      "        [ 362.9600, 2226.2644, 1637.4253],\n",
      "        [ 486.6647, 2235.2681, 2180.6321],\n",
      "        [ 343.6201, 2249.5896, 1513.6731],\n",
      "        [ 369.7557, 2248.5449, 1658.5354],\n",
      "        [ 358.5691, 2250.1423, 1615.6133],\n",
      "        [ 422.5720, 2291.5308, 1841.7019],\n",
      "        [ 643.9211, 2303.6011, 2801.3528],\n",
      "        [ 366.6278, 2317.1348, 1568.4283],\n",
      "        [ 441.6943, 2328.9480, 1898.8735],\n",
      "        [ 329.2661, 2345.0344, 1391.5792],\n",
      "        [ 532.6309, 2347.5420, 2272.3596],\n",
      "        [ 326.4118, 2369.6384, 1363.9078],\n",
      "        [ 477.2111, 2352.0720, 2054.0093],\n",
      "        [ 601.8976, 2379.8430, 2526.8491],\n",
      "        [ 533.8870, 2405.7039, 2203.6255],\n",
      "        [ 553.0481, 2438.7085, 2240.1401],\n",
      "        [ 461.8225, 2397.0073, 1936.7721],\n",
      "        [ 551.3570, 2405.2236, 2311.4634],\n",
      "        [ 506.4331, 2402.7380, 2142.2571],\n",
      "        [ 336.1934, 2454.6838, 1350.7607],\n",
      "        [ 522.5241, 2434.5417, 2150.7412],\n",
      "        [ 501.6550, 2434.3911, 2061.4731],\n",
      "        [ 472.3468, 2439.0437, 1941.7247],\n",
      "        [ 591.8051, 2445.2043, 2434.4287],\n",
      "        [ 484.3634, 2441.9370, 2001.7004],\n",
      "        [ 529.2379, 2463.1787, 2140.8391],\n",
      "        [ 520.9797, 2465.8262, 2120.5925],\n",
      "        [ 285.6967, 2456.1240, 1162.4348],\n",
      "        [ 397.6757, 2468.4568, 1616.6179],\n",
      "        [ 336.2945, 2457.7383, 1381.1234],\n",
      "        [ 587.1157, 2473.0706, 2366.3950],\n",
      "        [ 545.0098, 2469.8030, 2208.5598],\n",
      "        [ 454.9747, 2477.2930, 1829.6154],\n",
      "        [ 390.5557, 2472.4226, 1574.2288],\n",
      "        [ 492.3783, 2471.7637, 1995.1458],\n",
      "        [ 490.1115, 2475.5225, 1985.4451],\n",
      "        [ 568.4532, 2445.4392, 2352.2991],\n",
      "        [ 580.6846, 2470.4006, 2362.4729],\n",
      "        [ 522.5106, 2465.1782, 2142.4451],\n",
      "        [ 549.3834, 2486.1462, 2204.3066],\n",
      "        [ 314.6295, 2478.5276, 1286.9270],\n",
      "        [ 442.8535, 2487.6655, 1788.8458],\n",
      "        [ 547.4896, 2486.3696, 2208.5503],\n",
      "        [ 539.3368, 2475.3213, 2204.1250],\n",
      "        [ 592.3568, 2487.9893, 2387.4802],\n",
      "        [ 556.9440, 2474.7517, 2263.4739],\n",
      "        [ 387.1516, 2495.8704, 1539.4824],\n",
      "        [ 329.8708, 2491.9438, 1310.1213],\n",
      "        [ 317.4484, 2491.2847, 1282.6986],\n",
      "        [ 542.3884, 2472.6404, 2223.5200],\n",
      "        [ 504.5874, 2497.7808, 2022.2817],\n",
      "        [ 651.1112, 2500.6853, 2596.0508],\n",
      "        [ 539.6339, 2504.4387, 2155.6018],\n",
      "        [ 551.7325, 2515.1963, 2175.7024],\n",
      "        [ 533.3420, 2500.5845, 2127.9026],\n",
      "        [ 410.3481, 2491.0391, 1686.1899],\n",
      "        [ 487.4338, 2499.1436, 1954.1666],\n",
      "        [ 456.8596, 2496.5129, 1844.8677],\n",
      "        [ 564.3130, 2502.1206, 2271.2124],\n",
      "        [ 663.5911, 2483.9287, 2699.6165],\n",
      "        [ 659.9799, 2512.9565, 2631.1833],\n",
      "        [ 471.3991, 2522.5803, 1856.7098],\n",
      "        [ 634.5858, 2539.2810, 2477.1531],\n",
      "        [ 539.3535, 2528.7131, 2129.0745],\n",
      "        [ 728.9883, 2516.2629, 2913.6533],\n",
      "        [ 411.3448, 2495.5798, 1695.4423],\n",
      "        [ 633.7889, 2516.0898, 2516.9724],\n",
      "        [ 634.2308, 2524.6248, 2503.8000],\n",
      "        [ 542.7680, 2521.5750, 2147.3962],\n",
      "        [ 543.5471, 2497.8979, 2205.7078],\n",
      "        [ 535.7168, 2508.7507, 2151.2593],\n",
      "        [ 525.7819, 2528.1992, 2082.8630],\n",
      "        [ 364.1144, 2523.9543, 1453.6311],\n",
      "        [ 558.2000, 2532.7124, 2195.8279],\n",
      "        [ 539.8625, 2518.1455, 2140.2456],\n",
      "        [ 594.8632, 2544.1794, 2318.7869],\n",
      "        [ 363.2707, 2500.4395, 1488.6816],\n",
      "        [ 493.3372, 2514.8835, 1975.0292],\n",
      "        [ 502.2612, 2558.0369, 1920.2578],\n",
      "        [ 329.0259, 2514.7720, 1301.7772],\n",
      "        [ 492.7975, 2538.2756, 1901.1113],\n",
      "        [ 569.2338, 2558.6292, 2173.1907],\n",
      "        [ 642.3870, 2460.8608, 2652.9985],\n",
      "        [ 576.7635, 2493.3569, 2324.4995],\n",
      "        [ 423.6541, 2500.7075, 1684.6445],\n",
      "        [ 730.3975, 2470.4126, 2961.5186],\n",
      "        [ 531.6887, 2489.3130, 2124.7996],\n",
      "        [ 428.0489, 2491.3518, 1702.8325],\n",
      "        [ 582.5256, 2468.4736, 2365.6084],\n",
      "        [ 495.8958, 2485.0571, 1976.2256],\n",
      "        [ 642.3142, 2449.5947, 2631.7744],\n",
      "        [ 439.4699, 2426.7781, 1846.0117],\n",
      "        [ 633.0649, 2401.0288, 2712.4087],\n",
      "        [ 604.2350, 2490.6479, 2385.2537],\n",
      "        [ 587.8098, 2478.0247, 2331.9336]])\n",
      "data:  tensor([[[0.0013, 0.0012, 0.0013,  ..., 1.0000, 0.1435, 0.2609]],\n",
      "\n",
      "        [[0.0013, 0.0013, 0.0014,  ..., 1.0000, 0.1435, 0.3043]],\n",
      "\n",
      "        [[0.0012, 0.0012, 0.0012,  ..., 0.9977, 0.1435, 0.3043]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0067, 0.0066, 0.0066,  ..., 0.6050, 0.1435, 0.8261]],\n",
      "\n",
      "        [[0.0070, 0.0062, 0.0055,  ..., 0.6022, 0.1435, 0.8261]],\n",
      "\n",
      "        [[0.0076, 0.0076, 0.0076,  ..., 0.6014, 0.1435, 0.8261]]])\n",
      "output:  tensor([[0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431],\n",
      "        [0.9677, 6.0399, 4.8431]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[  457.0344,  2468.6914,  1804.9810],\n",
      "        [  626.0213,  2427.8503,  2573.3823],\n",
      "        [  456.2314,  2415.6460,  1896.0487],\n",
      "        [  634.7424,  2402.9780,  2663.1565],\n",
      "        [  383.2768,  2382.0996,  1667.2468],\n",
      "        [  609.2395,  2395.1194,  2547.6016],\n",
      "        [  740.8502,  2394.6111,  3078.7498],\n",
      "        [  580.5910,  2400.8779,  2393.8955],\n",
      "        [  307.2990,  2393.9744,  1254.7181],\n",
      "        [  394.7642,  2352.0664,  1707.5741],\n",
      "        [  434.1096,  2372.1296,  1819.6016],\n",
      "        [  317.5052,  2346.6431,  1363.8491],\n",
      "        [  527.5273,  2349.7595,  2261.8430],\n",
      "        [  490.7992,  2337.8513,  2108.0000],\n",
      "        [  544.2783,  2348.6313,  2307.6777],\n",
      "        [  430.0183,  2322.2119,  1868.4093],\n",
      "        [  407.4484,  2328.1101,  1752.1326],\n",
      "        [  712.0357,  2339.4490,  3020.7805],\n",
      "        [  414.1017,  2304.6401,  1806.5303],\n",
      "        [  427.7564,  2286.2302,  1888.9595],\n",
      "        [  636.3168,  2307.3826,  2739.9724],\n",
      "        [  277.5684,  2273.9534,  1237.5651],\n",
      "        [  561.0908,  2267.4351,  2485.2471],\n",
      "        [  525.0494,  2277.7705,  2287.3018],\n",
      "        [  325.4321,  2262.1121,  1445.2336],\n",
      "        [  517.4474,  2262.7432,  2280.9561],\n",
      "        [  370.3802,  2259.0569,  1640.3967],\n",
      "        [  661.7025,  2251.7285,  2949.3435],\n",
      "        [  493.2989,  2227.0129,  2238.7964],\n",
      "        [  576.2505,  2250.1367,  2552.4817],\n",
      "        [  428.0180,  2252.7507,  1892.8159],\n",
      "        [  687.2488,  2236.9773,  3073.9978],\n",
      "        [  416.4453,  2221.6396,  1908.1775],\n",
      "        [  595.0020,  2253.6501,  2631.2878],\n",
      "        [  377.4174,  2237.9661,  1711.7374],\n",
      "        [  505.0663,  2246.0146,  2267.3640],\n",
      "        [  379.4889,  2238.2732,  1715.2817],\n",
      "        [  511.4587,  2279.8850,  2224.4209],\n",
      "        [  318.6596,  2279.5891,  1390.8982],\n",
      "        [  593.4300,  2227.5769,  2707.7437],\n",
      "        [  430.2469,  2252.6838,  1930.4435],\n",
      "        [  337.6248,  2289.7715,  1460.1763],\n",
      "        [  385.8349,  2278.0364,  1707.6013],\n",
      "        [  458.8875,  2298.2949,  1991.1978],\n",
      "        [  395.6302,  2307.1646,  1708.7405],\n",
      "        [  313.3919,  2307.7344,  1355.1257],\n",
      "        [  265.6646,  2335.7288,  1123.3500],\n",
      "        [  367.3291,  2331.7522,  1577.0090],\n",
      "        [  347.1164,  2333.6567,  1488.2946],\n",
      "        [  322.0953,  2353.1055,  1372.0695],\n",
      "        [  342.6757,  2363.9580,  1464.9442],\n",
      "        [  427.8339,  2384.1382,  1804.9777],\n",
      "        [  260.3952,  2394.8569,  1078.8442],\n",
      "        [  309.1186,  2416.0706,  1283.4625],\n",
      "        [  392.1233,  2452.3484,  1602.4290],\n",
      "        [  472.5395,  2476.8071,  1913.2976],\n",
      "        [  283.6087,  2511.3088,  1131.7466],\n",
      "        [  261.5444,  2523.0056,  1037.9631],\n",
      "        [  277.6309,  2534.7847,  1099.9980],\n",
      "        [  335.2774,  2553.0938,  1308.0078],\n",
      "        [  271.8657,  2529.3499,  1081.9120],\n",
      "        [  258.4827,  2499.5850,  1039.0250],\n",
      "        [  160.4211,  2459.9055,   653.4436],\n",
      "        [  265.3469,  2399.6941,  1112.4033],\n",
      "        [  238.7593,  2354.8035,  1010.0034],\n",
      "        [  236.6113,  2281.6111,  1031.2075],\n",
      "        [  133.7965,  2204.7266,   604.2767],\n",
      "        [  156.5774,  2120.1233,   737.5280],\n",
      "        [  158.6335,  2012.2062,   793.9325],\n",
      "        [  176.2568,  1941.0691,   900.6042],\n",
      "        [  169.1472,  1842.3065,   912.3187],\n",
      "        [  127.8023,  1746.2361,   731.3701],\n",
      "        [  140.6365,  1658.6781,   848.8934],\n",
      "        [ 5470.3955,  8652.2715,  5271.6328],\n",
      "        [ 4162.2158, 12282.3486,  3389.3154],\n",
      "        [ 2547.8425, 12008.6875,  2122.7595],\n",
      "        [ 2630.1833, 11546.7617,  2285.0708],\n",
      "        [ 2338.7761, 11099.1016,  2111.4255],\n",
      "        [ 2557.4827, 10704.3857,  2391.2971],\n",
      "        [ 2663.2375, 10331.5322,  2583.4202],\n",
      "        [ 2415.7585,  9948.4961,  2432.3542],\n",
      "        [ 4632.2041, 11102.8828,  4094.1682],\n",
      "        [ 4502.9897, 12965.5215,  3476.3950],\n",
      "        [ 3638.7305, 12843.5176,  2835.4170],\n",
      "        [ 3863.2764, 12740.1357,  3033.3970],\n",
      "        [ 4311.1157, 12594.6836,  3424.8032],\n",
      "        [ 4229.2007, 12455.6943,  3397.4236],\n",
      "        [ 4255.6191, 12297.4463,  3461.3596],\n",
      "        [ 4408.7725, 12147.5537,  3629.9246],\n",
      "        [ 3959.4646, 12022.2715,  3296.6099],\n",
      "        [ 3870.5583, 11909.0625,  3250.6519],\n",
      "        [ 3678.4192, 11782.9941,  3124.0840],\n",
      "        [ 3875.4629, 11646.7480,  3329.5005],\n",
      "        [ 3718.8513, 11505.2949,  3233.7549],\n",
      "        [ 3312.1694, 11379.3418,  2912.0195],\n",
      "        [ 3379.8352, 11249.6582,  3006.1089],\n",
      "        [ 3022.8147, 11100.3359,  2728.4592],\n",
      "        [ 2722.8179, 10956.3809,  2486.4714],\n",
      "        [ 2929.3213, 10832.9805,  2708.1299],\n",
      "        [ 2713.3928, 10752.0752,  2526.2175],\n",
      "        [ 3061.4890, 10663.8916,  2871.3757],\n",
      "        [ 2746.7744, 10608.1309,  2591.5298],\n",
      "        [ 2697.0774, 10564.9551,  2556.8921],\n",
      "        [ 2588.8562, 10523.5107,  2461.2578],\n",
      "        [ 2260.0020, 10492.6680,  2155.4106],\n",
      "        [ 1952.2854, 10474.5879,  1867.0804],\n",
      "        [ 1915.1896, 10440.0918,  1836.6840],\n",
      "        [ 2099.6831, 10412.4658,  2019.9731],\n",
      "        [ 2042.0830, 10408.6562,  1965.2384],\n",
      "        [ 2513.2117, 10406.0596,  2417.5327],\n",
      "        [ 2245.2209, 10415.0859,  2158.3279],\n",
      "        [ 2225.6782, 10439.7227,  2133.0442],\n",
      "        [ 2025.8318, 10488.1553,  1933.8540],\n",
      "        [ 2559.6340, 10518.7471,  2435.0078],\n",
      "        [ 2677.1414, 10557.9678,  2537.3743],\n",
      "        [ 2191.6592, 10621.0059,  2065.7791],\n",
      "        [ 2251.5132, 10674.8223,  2111.4360],\n",
      "        [ 2426.5852, 10692.0703,  2271.7222],\n",
      "        [ 2631.6157, 10706.9326,  2459.6853],\n",
      "        [ 2746.6262, 10746.5732,  2556.9460],\n",
      "        [ 2313.9290, 10817.4922,  2142.0579],\n",
      "        [ 2199.4431, 10868.2207,  2025.0352],\n",
      "        [ 2239.2546, 10908.2900,  2055.2078],\n",
      "        [ 2226.7219, 10942.9648,  2037.1031],\n",
      "        [ 2553.1096, 10952.9287,  2333.4585],\n",
      "        [ 2323.2847, 10952.6445,  2123.8611],\n",
      "        [ 2206.5676, 10929.7051,  2020.2343],\n",
      "        [ 2471.4414, 10901.7100,  2269.8047],\n",
      "        [ 2310.8252, 10896.8174,  2124.0789],\n",
      "        [ 2820.9048, 10898.4990,  2591.2590],\n",
      "        [ 2185.3564, 10918.8857,  2003.4922],\n",
      "        [ 2616.4456, 10901.2295,  2403.0640],\n",
      "        [ 2078.3274, 10893.2314,  1909.2107],\n",
      "        [ 2239.1226, 10870.6602,  2062.1831],\n",
      "        [ 2272.5549, 10845.3916,  2097.4995],\n",
      "        [ 2467.8135, 10800.6016,  2287.1819],\n",
      "        [ 2115.8184, 10769.8652,  1967.0380],\n",
      "        [ 2632.7705, 10729.6328,  2455.1882],\n",
      "        [ 2059.4463, 10698.9854,  1927.2068],\n",
      "        [ 2065.4866, 10651.0664,  1941.5519],\n",
      "        [ 1691.7161, 10620.8496,  1594.4513],\n",
      "        [ 2147.6794, 10565.5137,  2035.8135],\n",
      "        [ 2145.7153, 10521.7793,  2041.0897],\n",
      "        [ 1952.5814, 10465.5723,  1868.9923],\n",
      "        [ 2230.0276, 10390.9844,  2148.6113],\n",
      "        [ 2204.4990, 10332.5713,  2137.2375],\n",
      "        [ 1663.6613, 10285.9932,  1620.7811],\n",
      "        [ 2163.2576, 10208.7686,  2122.5461],\n",
      "        [ 1917.5114, 10149.9648,  1892.2144],\n",
      "        [ 2058.5317, 10066.6230,  2048.7451]])\n",
      "data:  tensor([[[0.0053, 0.0056, 0.0056,  ..., 0.6014, 0.1435, 0.8261]],\n",
      "\n",
      "        [[0.0069, 0.0075, 0.0074,  ..., 0.5996, 0.1435, 0.8261]],\n",
      "\n",
      "        [[0.0057, 0.0049, 0.0051,  ..., 0.5979, 0.1435, 0.8261]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0261, 0.0243, 0.0246,  ..., 0.9644, 0.1477, 0.3043]],\n",
      "\n",
      "        [[0.0202, 0.0220, 0.0215,  ..., 0.9644, 0.1477, 0.3043]],\n",
      "\n",
      "        [[0.0236, 0.0236, 0.0237,  ..., 0.9644, 0.1477, 0.3043]]])\n",
      "output:  tensor([[1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962],\n",
      "        [1.0207, 6.0929, 4.8962]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[2026.0474, 9979.9697, 2033.6809],\n",
      "        [1962.1493, 9897.4775, 1986.3676],\n",
      "        [1887.3643, 9815.4131, 1927.0077],\n",
      "        [1868.6172, 9731.7842, 1922.4175],\n",
      "        [2005.0681, 9645.7627, 2082.1125],\n",
      "        [1899.5375, 9551.6074, 1993.6855],\n",
      "        [1701.0359, 9456.4814, 1802.1680],\n",
      "        [1739.6897, 9363.8906, 1860.0947],\n",
      "        [2044.7795, 9272.8633, 2208.7383],\n",
      "        [1776.9474, 9187.7686, 1935.9697],\n",
      "        [1523.2128, 9118.3584, 1673.8827],\n",
      "        [1978.4647, 9021.3154, 2193.9512],\n",
      "        [1539.6370, 8941.0078, 1726.4163],\n",
      "        [1500.1112, 8831.3418, 1701.0657],\n",
      "        [1012.1524, 8735.1650, 1160.5955],\n",
      "        [1274.1359, 8628.7559, 1477.7861],\n",
      "        [1251.2498, 8525.0840, 1469.2635],\n",
      "        [1019.0797, 8425.2266, 1211.8414],\n",
      "        [1146.9268, 8304.8311, 1384.0662],\n",
      "        [ 979.4707, 8181.9839, 1198.4397],\n",
      "        [ 943.8132, 8049.3901, 1174.7046],\n",
      "        [1218.6238, 7874.0054, 1550.6195],\n",
      "        [1019.9551, 7707.7422, 1325.9663],\n",
      "        [ 974.2468, 7549.0137, 1293.2007],\n",
      "        [ 939.7953, 7409.7617, 1269.1509],\n",
      "        [1046.5494, 7268.6055, 1444.0985],\n",
      "        [1104.9077, 7142.5801, 1550.0315],\n",
      "        [ 876.5967, 7026.9546, 1249.6787],\n",
      "        [1399.4294, 6902.2754, 2033.1304],\n",
      "        [1201.0394, 6799.9546, 1770.3071],\n",
      "        [1143.8108, 6705.8281, 1708.8998],\n",
      "        [ 644.9871, 6615.7510,  977.3181],\n",
      "        [ 985.5217, 6520.9536, 1513.8252],\n",
      "        [1111.4861, 6431.6743, 1729.1034],\n",
      "        [1147.2363, 6342.0620, 1811.2169],\n",
      "        [ 719.5988, 6275.2202, 1143.3927],\n",
      "        [1138.1925, 6189.2764, 1840.9308],\n",
      "        [ 804.6481, 6106.5161, 1321.1859],\n",
      "        [1016.8036, 6032.6982, 1688.9281],\n",
      "        [ 923.7142, 5960.2485, 1552.1339],\n",
      "        [1272.6793, 5906.3711, 2155.9905],\n",
      "        [ 906.5463, 5800.8892, 1572.3661],\n",
      "        [ 961.1877, 5748.7432, 1675.7612],\n",
      "        [ 888.3546, 5691.1958, 1561.2754],\n",
      "        [1185.9517, 5602.7271, 2124.5703],\n",
      "        [1159.4181, 5537.9019, 2104.2456],\n",
      "        [1040.5657, 5485.8394, 1903.7290],\n",
      "        [1266.8286, 5441.5410, 2335.8882],\n",
      "        [ 887.8481, 5407.6035, 1643.1722],\n",
      "        [1043.9310, 5333.1660, 1964.9484],\n",
      "        [ 913.5409, 5313.6387, 1710.1886],\n",
      "        [ 951.7437, 5236.3525, 1820.7153],\n",
      "        [ 699.7335, 5198.2314, 1348.8062],\n",
      "        [ 898.2410, 5145.9736, 1749.7601],\n",
      "        [1091.8379, 5093.8110, 2148.0798],\n",
      "        [ 907.8015, 5070.2515, 1785.5446],\n",
      "        [1001.7877, 4994.0986, 2017.8827],\n",
      "        [1059.4070, 4951.6489, 2141.0376],\n",
      "        [ 986.6906, 4915.9800, 2004.7278],\n",
      "        [ 901.1161, 4864.6938, 1851.7858],\n",
      "        [ 866.9072, 4836.9175, 1791.4237],\n",
      "        [ 357.9261, 4800.3213,  738.9158],\n",
      "        [1168.7578, 4726.9111, 2487.3550],\n",
      "        [ 623.7788, 4731.4409, 1312.1949],\n",
      "        [ 897.4930, 4682.5732, 1923.4945],\n",
      "        [ 729.2730, 4660.8975, 1565.1744],\n",
      "        [1035.4882, 4596.8975, 2273.7153],\n",
      "        [1391.6034, 4587.2065, 3037.6748],\n",
      "        [1033.7209, 4582.5430, 2240.2708],\n",
      "        [1344.5780, 4525.3364, 2975.0056],\n",
      "        [ 831.2770, 4488.2930, 1854.1591],\n",
      "        [ 694.2796, 4473.0503, 1548.3459],\n",
      "        [ 854.7598, 4435.2534, 1932.5060],\n",
      "        [ 797.4297, 4383.4536, 1835.2205],\n",
      "        [ 685.0405, 4378.1919, 1561.8163],\n",
      "        [ 704.9371, 4358.2910, 1604.6890],\n",
      "        [1124.8901, 4305.4243, 2612.8772],\n",
      "        [1112.3362, 4287.9194, 2586.3867],\n",
      "        [ 873.0218, 4250.9268, 2059.5264],\n",
      "        [1083.3328, 4260.0815, 2533.6858],\n",
      "        [ 762.6895, 4189.8940, 1826.7020],\n",
      "        [ 970.8375, 4170.9424, 2327.6877],\n",
      "        [ 891.5645, 4145.9863, 2160.7559],\n",
      "        [ 990.7676, 4135.7930, 2398.4724],\n",
      "        [ 531.6655, 4123.7451, 1285.5322],\n",
      "        [ 608.4177, 4102.4810, 1477.3972],\n",
      "        [ 980.5779, 4070.9734, 2407.4392],\n",
      "        [ 547.2552, 4032.3999, 1363.5168],\n",
      "        [1004.3687, 4027.0601, 2492.4653],\n",
      "        [ 865.6954, 4016.6265, 2147.4602],\n",
      "        [1199.3864, 4003.5676, 2983.9878],\n",
      "        [ 731.0349, 3956.4431, 1864.0269],\n",
      "        [ 687.7667, 3963.1399, 1717.5848],\n",
      "        [1144.0330, 3922.2373, 2921.1887],\n",
      "        [ 619.9041, 3877.1064, 1621.1683],\n",
      "        [ 611.1904, 3874.0232, 1579.4801],\n",
      "        [ 798.9620, 3865.3770, 2058.0173],\n",
      "        [1107.7263, 3849.7905, 2870.9404],\n",
      "        [ 982.3781, 3777.5117, 2632.9058],\n",
      "        [ 678.7136, 3802.4119, 1798.9949],\n",
      "        [ 825.1879, 3801.8867, 2159.4844],\n",
      "        [ 832.7523, 3741.5466, 2251.3027],\n",
      "        [1076.4742, 3736.6091, 2891.8843],\n",
      "        [ 481.7434, 3724.0920, 1303.4120],\n",
      "        [ 745.0364, 3707.2183, 2007.8728],\n",
      "        [ 907.0286, 3689.3223, 2467.1443],\n",
      "        [ 730.3620, 3685.2561, 1978.5016],\n",
      "        [ 670.0351, 3689.1492, 1800.6217],\n",
      "        [ 839.6673, 3635.0818, 2322.8655],\n",
      "        [ 885.7844, 3658.6187, 2405.4753],\n",
      "        [ 869.3920, 3587.5156, 2447.1772],\n",
      "        [ 867.6268, 3612.3374, 2386.6597],\n",
      "        [ 598.1012, 3591.5149, 1655.2214],\n",
      "        [ 522.8748, 3578.2212, 1461.6699],\n",
      "        [ 863.9942, 3566.2068, 2423.7480],\n",
      "        [ 534.0036, 3542.2061, 1527.5999],\n",
      "        [ 971.1064, 3560.4539, 2717.7087],\n",
      "        [ 660.1495, 3536.1401, 1863.0240],\n",
      "        [ 486.7744, 3511.5024, 1390.2225],\n",
      "        [ 669.5527, 3505.6658, 1907.1929],\n",
      "        [ 742.5389, 3491.7021, 2130.3425],\n",
      "        [ 798.4708, 3495.8577, 2278.2219],\n",
      "        [ 939.4358, 3452.5032, 2713.5640],\n",
      "        [ 735.8328, 3431.2781, 2143.7524],\n",
      "        [ 903.4041, 3387.7783, 2669.9009],\n",
      "        [ 895.8669, 3417.7222, 2602.6182],\n",
      "        [ 591.0694, 3399.5247, 1708.9978],\n",
      "        [ 899.1179, 3348.8979, 2701.0176],\n",
      "        [ 387.4938, 3369.7205, 1126.5133],\n",
      "        [ 639.1479, 3357.8066, 1887.7294],\n",
      "        [ 634.4849, 3361.9243, 1848.7278],\n",
      "        [ 709.0247, 3290.5964, 2178.2485],\n",
      "        [ 577.6346, 3326.5334, 1694.0529],\n",
      "        [ 714.8410, 3239.1765, 2242.2871],\n",
      "        [ 670.8770, 3302.0132, 1999.3577],\n",
      "        [ 710.3309, 3255.2234, 2200.2678],\n",
      "        [ 500.1947, 3254.6482, 1519.5616],\n",
      "        [ 914.5427, 3249.3420, 2807.0066],\n",
      "        [ 686.2716, 3234.6746, 2108.4255],\n",
      "        [ 590.7304, 3211.9583, 1825.7284],\n",
      "        [ 517.3472, 3194.7996, 1604.3265],\n",
      "        [ 682.2075, 3194.0679, 2120.8523],\n",
      "        [ 501.8773, 3176.8704, 1554.4673],\n",
      "        [ 661.2429, 3124.6851, 2136.1038],\n",
      "        [ 655.2746, 3132.3315, 2077.7805],\n",
      "        [ 710.2889, 3130.6726, 2259.4883],\n",
      "        [ 765.3094, 3132.6890, 2426.8071],\n",
      "        [ 633.4779, 3109.3696, 2036.5487],\n",
      "        [ 444.5927, 3087.1504, 1450.9415],\n",
      "        [ 498.9734, 3101.1367, 1592.4496]])\n",
      "data:  tensor([[[0.0239, 0.0233, 0.0243,  ..., 0.9644, 0.1477, 0.3043]],\n",
      "\n",
      "        [[0.0241, 0.0241, 0.0222,  ..., 0.9644, 0.1477, 0.3478]],\n",
      "\n",
      "        [[0.0229, 0.0230, 0.0250,  ..., 0.9641, 0.1477, 0.3478]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0064, 0.0064, 0.0065,  ..., 0.5907, 0.1477, 0.8696]],\n",
      "\n",
      "        [[0.0061, 0.0061, 0.0052,  ..., 0.5907, 0.1477, 0.8696]],\n",
      "\n",
      "        [[0.0058, 0.0059, 0.0062,  ..., 0.5907, 0.1477, 0.8696]]])\n",
      "output:  tensor([[1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492],\n",
      "        [1.0737, 6.1459, 4.9492]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 633.2141, 3031.4019, 2120.3289],\n",
      "        [ 685.2835, 3038.9255, 2272.2400],\n",
      "        [ 680.7668, 3018.4937, 2273.8096],\n",
      "        [ 572.7626, 3025.4531, 1881.2877],\n",
      "        [ 674.0590, 2989.2312, 2271.3447],\n",
      "        [ 666.4471, 2992.4319, 2223.1941],\n",
      "        [ 470.6411, 2999.5532, 1551.8917],\n",
      "        [1029.0751, 2946.7537, 3515.0583],\n",
      "        [ 638.5701, 2974.6196, 2131.3137],\n",
      "        [ 591.7308, 2944.4580, 2012.9332],\n",
      "        [ 717.9703, 2900.1038, 2513.0664],\n",
      "        [ 806.7289, 2933.2703, 2745.5967],\n",
      "        [ 770.7402, 2944.9438, 2603.0964],\n",
      "        [ 524.3668, 2928.2937, 1780.9056],\n",
      "        [ 661.7688, 2913.2458, 2251.4067],\n",
      "        [ 537.8497, 2885.9724, 1868.8992],\n",
      "        [ 353.9405, 2891.6975, 1208.5028],\n",
      "        [ 568.7001, 2863.5969, 1976.2273],\n",
      "        [ 719.7431, 2818.9634, 2575.0984],\n",
      "        [ 778.1439, 2840.8416, 2753.9661],\n",
      "        [ 223.2939, 2871.9192,  769.9274],\n",
      "        [ 803.9218, 2825.7273, 2891.6160],\n",
      "        [ 613.4817, 2894.9539, 2106.3662],\n",
      "        [ 457.3028, 2892.2615, 1594.0420],\n",
      "        [ 752.4911, 2921.7866, 2563.8020],\n",
      "        [ 411.4047, 2916.9438, 1404.8672],\n",
      "        [ 498.7881, 2918.3291, 1715.0125],\n",
      "        [ 656.7793, 2943.6648, 2217.9570],\n",
      "        [ 524.4064, 2909.8391, 1823.2653],\n",
      "        [ 337.1658, 2950.6077, 1143.0087],\n",
      "        [ 552.0632, 2960.3542, 1868.6355],\n",
      "        [ 455.3171, 2959.7400, 1544.9507],\n",
      "        [ 566.6369, 2974.7983, 1907.8278],\n",
      "        [ 347.4009, 2984.9976, 1158.7456],\n",
      "        [ 618.2188, 2982.3108, 2088.3010],\n",
      "        [ 476.2906, 2992.3591, 1610.4910],\n",
      "        [ 366.3788, 3022.4482, 1215.1698],\n",
      "        [ 370.9116, 3032.4519, 1237.7434],\n",
      "        [ 367.1992, 3062.9709, 1206.7742],\n",
      "        [ 498.6939, 3095.1489, 1611.5712],\n",
      "        [ 480.2516, 3115.2122, 1544.5555],\n",
      "        [ 379.4733, 3130.5107, 1221.6218],\n",
      "        [ 539.3714, 3141.8438, 1731.9530],\n",
      "        [ 529.9879, 3202.1055, 1664.2952],\n",
      "        [ 529.1957, 3255.6982, 1629.2614],\n",
      "        [ 485.7052, 3298.8462, 1473.7216],\n",
      "        [ 375.4984, 3355.5500, 1113.0837],\n",
      "        [ 457.5616, 3374.0779, 1365.8090],\n",
      "        [ 426.9128, 3440.2820, 1240.0131],\n",
      "        [ 560.5223, 3485.1392, 1603.5203],\n",
      "        [ 311.8988, 3508.7156,  893.5841],\n",
      "        [ 339.0271, 3560.5991,  954.0690],\n",
      "        [ 478.9921, 3617.0237, 1324.2028],\n",
      "        [ 375.6772, 3645.0015, 1036.3785],\n",
      "        [ 490.7320, 3689.1155, 1337.0516],\n",
      "        [ 407.5839, 3737.0225, 1096.5980],\n",
      "        [ 445.4349, 3777.9250, 1185.7676],\n",
      "        [ 420.7444, 3824.4072, 1099.4031],\n",
      "        [ 448.4500, 3845.9897, 1170.6888],\n",
      "        [ 487.3752, 3861.1096, 1266.1541],\n",
      "        [ 314.0445, 3882.8931,  811.7414],\n",
      "        [ 539.5915, 3884.5239, 1390.8734],\n",
      "        [ 423.0573, 3883.1667, 1092.6937],\n",
      "        [ 445.5442, 3867.7898, 1157.4535],\n",
      "        [ 348.5817, 3874.0847,  900.9005],\n",
      "        [ 450.0586, 3843.2751, 1177.0387],\n",
      "        [ 457.1720, 3824.2119, 1201.7313],\n",
      "        [ 285.2274, 3791.9390,  753.2138],\n",
      "        [ 605.5058, 3737.8770, 1622.3210],\n",
      "        [ 350.6958, 3691.0930,  951.0617],\n",
      "        [ 454.7018, 3620.5146, 1260.9758],\n",
      "        [ 288.0464, 3567.7095,  809.9274],\n",
      "        [ 332.0916, 3485.5134,  957.8737],\n",
      "        [ 245.3681, 3406.7188,  724.7126],\n",
      "        [ 248.3100, 3321.0652,  751.2256],\n",
      "        [ 375.3242, 3261.2334, 1149.7865],\n",
      "        [ 279.9578, 3163.8000,  885.1158],\n",
      "        [ 244.4564, 3086.8601,  794.6854],\n",
      "        [ 218.9121, 3020.3704,  726.3871],\n",
      "        [ 184.9834, 2967.1350,  623.0167],\n",
      "        [ 272.5697, 2901.1567,  940.3596],\n",
      "        [ 181.3341, 2849.8677,  636.3621],\n",
      "        [ 238.5055, 2796.5767,  853.7714],\n",
      "        [ 234.6597, 2732.0083,  865.0643],\n",
      "        [ 195.0048, 2673.7683,  730.7686],\n",
      "        [ 181.2248, 2611.4343,  691.1764],\n",
      "        [ 201.3547, 2546.1064,  789.9651],\n",
      "        [ 182.9157, 2474.1709,  740.8198],\n",
      "        [ 177.8508, 2423.7227,  734.9646],\n",
      "        [ 150.9695, 2355.3115,  644.7476],\n",
      "        [ 155.6187, 2298.3228,  678.4597],\n",
      "        [ 163.1862, 2260.1853,  718.9561],\n",
      "        [ 183.0170, 2199.5544,  832.5958],\n",
      "        [ 123.6825, 2156.7417,  578.9890],\n",
      "        [ 154.0399, 2120.6538,  719.8163],\n",
      "        [ 177.6457, 2042.6525,  865.1178],\n",
      "        [ 155.0659, 1967.6674,  789.0763],\n",
      "        [ 132.7561, 1912.4716,  690.3017],\n",
      "        [ 114.7903, 1849.1935,  622.5642],\n",
      "        [  92.7630, 1800.7225,  516.4448],\n",
      "        [ 188.0971, 1771.5776, 1055.0431],\n",
      "        [ 135.1165, 1745.8341,  770.6899],\n",
      "        [ 128.9273, 1709.8634,  756.7142],\n",
      "        [ 151.2496, 1700.8763,  888.2601],\n",
      "        [ 162.1217, 1694.1235,  951.9181],\n",
      "        [  95.7986, 1660.9066,  572.7001],\n",
      "        [ 125.5581, 1645.9711,  759.7767],\n",
      "        [ 111.2055, 1616.4628,  684.0003],\n",
      "        [ 130.8058, 1582.2517,  829.0923],\n",
      "        [ 110.0278, 1570.1368,  701.5701],\n",
      "        [ 141.0124, 1563.6632,  898.5903],\n",
      "        [ 108.2200, 1540.7795,  699.1666],\n",
      "        [  80.6398, 1525.7986,  529.6931],\n",
      "        [ 106.3155, 1496.9557,  717.8368],\n",
      "        [ 115.5369, 1518.4431,  753.8065],\n",
      "        [ 140.5417, 1534.1160,  907.6966],\n",
      "        [ 118.5290, 1511.9863,  789.7733],\n",
      "        [ 106.6935, 1519.0464,  694.9438],\n",
      "        [ 119.3140, 1510.8525,  788.6597],\n",
      "        [  92.6198, 1530.4741,  598.2429],\n",
      "        [ 105.4282, 1545.1306,  682.1522],\n",
      "        [  84.8173, 1544.8123,  554.4262],\n",
      "        [ 111.2568, 1564.1660,  707.4567],\n",
      "        [ 126.0193, 1566.2437,  801.6704],\n",
      "        [ 101.5331, 1559.3679,  653.6491],\n",
      "        [ 131.9997, 1549.2136,  855.4148],\n",
      "        [ 152.3428, 1560.1277,  976.5047],\n",
      "        [ 113.7717, 1577.9231,  713.9214],\n",
      "        [ 148.2159, 1567.9305,  938.0501],\n",
      "        [ 148.9271, 1561.8649,  968.2341],\n",
      "        [ 155.3785, 1570.2206,  990.7120],\n",
      "        [ 117.0329, 1580.5762,  740.5027],\n",
      "        [ 109.2563, 1594.3331,  684.3922],\n",
      "        [ 158.9066, 1596.1653,  987.2108],\n",
      "        [ 110.0513, 1610.2909,  675.6005],\n",
      "        [  89.4364, 1598.3771,  553.9895],\n",
      "        [ 150.2358, 1601.2535,  936.5716],\n",
      "        [ 141.0833, 1605.7947,  878.7993],\n",
      "        [ 158.6704, 1622.6740,  972.9000],\n",
      "        [ 120.7328, 1615.7590,  751.6277],\n",
      "        [ 166.9223, 1622.7856, 1027.6100],\n",
      "        [ 184.2849, 1594.9028, 1159.5708],\n",
      "        [ 187.1113, 1620.8308, 1145.3756],\n",
      "        [ 180.5749, 1641.0842, 1091.1880],\n",
      "        [ 117.5205, 1616.4182,  732.1993],\n",
      "        [ 172.4388, 1636.3417, 1048.1315],\n",
      "        [ 185.3980, 1621.4788, 1134.1262],\n",
      "        [ 151.3198, 1646.4625,  914.4050],\n",
      "        [ 139.5951, 1650.0652,  841.3627],\n",
      "        [ 177.3794, 1659.1417, 1057.9731]])\n",
      "data:  tensor([[[0.0074, 0.0073, 0.0076,  ..., 0.5895, 0.1477, 0.8696]],\n",
      "\n",
      "        [[0.0071, 0.0072, 0.0074,  ..., 0.5872, 0.1477, 0.8696]],\n",
      "\n",
      "        [[0.0081, 0.0077, 0.0077,  ..., 0.5872, 0.1477, 0.8696]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0018, 0.0019, 0.0018,  ..., 0.9786, 0.1519, 0.3478]],\n",
      "\n",
      "        [[0.0017, 0.0017, 0.0017,  ..., 0.9769, 0.1519, 0.3478]],\n",
      "\n",
      "        [[0.0021, 0.0021, 0.0021,  ..., 0.9751, 0.1519, 0.3478]]])\n",
      "output:  tensor([[1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021],\n",
      "        [1.1267, 6.1989, 5.0021]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 130.6695, 1665.9672,  780.6024],\n",
      "        [ 159.0576, 1672.7870,  953.0612],\n",
      "        [ 107.6233, 1657.4045,  653.5146],\n",
      "        [ 180.1718, 1686.1699, 1064.7222],\n",
      "        [ 184.4715, 1702.8761, 1089.2218],\n",
      "        [  90.5146, 1732.7697,  525.4691],\n",
      "        [ 171.6239, 1771.3429,  972.6762],\n",
      "        [ 138.4527, 1843.8538,  752.4937],\n",
      "        [ 208.2239, 1914.4265, 1092.6627],\n",
      "        [ 164.4846, 2006.8384,  814.9512],\n",
      "        [ 208.2142, 2113.1191,  981.0502],\n",
      "        [ 223.0832, 2203.0063,  999.6233],\n",
      "        [ 231.9769, 2248.5952, 1030.4690],\n",
      "        [ 218.8054, 2279.0752,  958.5358],\n",
      "        [ 192.9778, 2325.0884,  828.5226],\n",
      "        [ 221.5206, 2358.6294,  933.7618],\n",
      "        [ 293.0754, 2376.3689, 1219.7465],\n",
      "        [ 428.9032, 2379.3682, 1799.2131],\n",
      "        [ 186.8518, 2397.3647,  771.9461],\n",
      "        [ 254.8577, 2393.0415, 1066.1267],\n",
      "        [ 287.4203, 2399.5598, 1190.1697],\n",
      "        [ 402.4870, 2398.0964, 1677.6763],\n",
      "        [ 220.4179, 2394.3374,  918.1636],\n",
      "        [ 355.1858, 2400.4116, 1488.1667],\n",
      "        [ 238.2565, 2406.2178,  990.4570],\n",
      "        [ 419.9057, 2415.5679, 1748.2603],\n",
      "        [ 384.3334, 2426.0574, 1591.4960],\n",
      "        [ 278.0620, 2434.3296, 1150.2827],\n",
      "        [ 336.9542, 2447.7849, 1378.4506],\n",
      "        [ 526.9419, 2481.8398, 2105.9368],\n",
      "        [ 330.6172, 2472.8918, 1344.6509],\n",
      "        [ 355.6296, 2481.0076, 1449.9968],\n",
      "        [ 427.7689, 2506.3210, 1704.3582],\n",
      "        [ 431.9279, 2514.5986, 1717.8715],\n",
      "        [ 593.0730, 2512.7832, 2367.3069],\n",
      "        [ 488.9592, 2546.5251, 1912.7782],\n",
      "        [ 417.1258, 2563.4885, 1629.8190],\n",
      "        [ 347.7136, 2575.4189, 1352.6603],\n",
      "        [ 420.7854, 2590.0698, 1621.6743],\n",
      "        [ 259.2519, 2623.3091,  982.2715],\n",
      "        [ 590.1777, 2657.0061, 2202.3462],\n",
      "        [ 463.4380, 2674.1257, 1726.7222],\n",
      "        [ 355.6040, 2687.0952, 1313.4353],\n",
      "        [ 365.2949, 2700.9138, 1358.2119],\n",
      "        [ 427.6512, 2725.4622, 1552.5538],\n",
      "        [ 307.9599, 2728.3386, 1135.9720],\n",
      "        [ 581.0453, 2761.8347, 2091.7988],\n",
      "        [ 582.5808, 2775.0500, 2091.9614],\n",
      "        [ 345.1223, 2787.7405, 1227.2511],\n",
      "        [ 356.6139, 2789.0864, 1276.4521],\n",
      "        [ 443.9332, 2786.0256, 1616.2856],\n",
      "        [ 562.8539, 2789.8853, 2026.8257],\n",
      "        [ 471.1388, 2816.0085, 1671.6506],\n",
      "        [ 621.9773, 2802.5698, 2232.4646],\n",
      "        [ 561.4249, 2814.8020, 2014.4292],\n",
      "        [ 487.6775, 2839.5288, 1735.2985],\n",
      "        [ 611.2441, 2826.2131, 2183.2019],\n",
      "        [ 442.1394, 2870.8132, 1528.8314],\n",
      "        [ 696.4894, 2876.1194, 2413.6978],\n",
      "        [ 544.6263, 2874.6951, 1911.3378],\n",
      "        [ 625.7704, 2878.7056, 2183.6658],\n",
      "        [ 386.9767, 2898.1934, 1337.4126],\n",
      "        [ 592.8229, 2864.2002, 2101.4302],\n",
      "        [ 484.1702, 2915.6145, 1653.7349],\n",
      "        [ 545.2608, 2893.1443, 1889.4644],\n",
      "        [ 646.2006, 2917.0891, 2209.3723],\n",
      "        [ 898.5610, 2905.6555, 3091.5779],\n",
      "        [ 765.0431, 2926.4114, 2596.8936],\n",
      "        [ 662.5702, 2917.5305, 2254.2344],\n",
      "        [ 699.2632, 2920.0662, 2372.4548],\n",
      "        [ 726.3232, 2901.5894, 2503.8955],\n",
      "        [ 376.8735, 2892.6135, 1302.4370],\n",
      "        [ 742.2790, 2934.6946, 2488.0168],\n",
      "        [ 677.4240, 2881.8057, 2368.4270],\n",
      "        [ 560.5035, 2910.3977, 1921.1954],\n",
      "        [ 720.4229, 2887.3352, 2510.4902],\n",
      "        [ 737.0331, 2921.5408, 2524.9236],\n",
      "        [ 620.8184, 2895.2388, 2166.0171],\n",
      "        [ 697.0832, 2910.7217, 2392.5669],\n",
      "        [ 621.6245, 2939.3584, 2095.9373],\n",
      "        [ 605.0098, 2933.2424, 2054.9089],\n",
      "        [ 563.2252, 2929.1370, 1902.6338],\n",
      "        [ 533.2903, 2939.9729, 1777.1669],\n",
      "        [ 581.3340, 2914.0059, 1993.0236],\n",
      "        [ 806.9959, 2844.6399, 2878.8418],\n",
      "        [ 711.4117, 2912.8999, 2439.8840],\n",
      "        [ 499.6402, 2907.7056, 1723.8391],\n",
      "        [ 807.2049, 2903.9800, 2793.1721],\n",
      "        [ 566.2353, 2896.9846, 1964.1921],\n",
      "        [ 587.2301, 2906.8899, 2021.5059],\n",
      "        [ 487.7980, 2918.6084, 1669.3282],\n",
      "        [ 711.6016, 2929.5112, 2407.8948],\n",
      "        [ 376.7071, 2917.0669, 1290.3361],\n",
      "        [ 687.1483, 2914.7207, 2360.0623],\n",
      "        [ 333.9337, 2896.3894, 1154.9902],\n",
      "        [ 857.3257, 2906.7559, 2941.8901],\n",
      "        [ 453.3259, 2900.0645, 1567.5146],\n",
      "        [ 465.9786, 2908.5601, 1605.4945],\n",
      "        [ 694.6224, 2901.7234, 2391.4021],\n",
      "        [ 593.4639, 2917.5696, 2021.9805],\n",
      "        [ 456.2095, 2896.6406, 1580.5930],\n",
      "        [ 677.1373, 2874.1143, 2382.9385],\n",
      "        [ 520.1138, 2896.3445, 1803.1230],\n",
      "        [ 421.0172, 2900.1650, 1441.4689],\n",
      "        [ 499.7219, 2940.5815, 1663.5957],\n",
      "        [ 673.8376, 2944.8323, 2259.8682],\n",
      "        [ 607.7225, 2893.7976, 2108.4390],\n",
      "        [ 564.4738, 2912.4644, 1935.9867],\n",
      "        [ 666.2476, 2889.5134, 2309.6470],\n",
      "        [ 802.9052, 2894.6912, 2777.8479],\n",
      "        [ 405.5869, 2862.9937, 1441.1350],\n",
      "        [ 823.6559, 2881.8726, 2855.5742],\n",
      "        [ 830.2265, 2894.8701, 2852.1687],\n",
      "        [ 623.6686, 2875.9463, 2158.0481],\n",
      "        [ 624.9088, 2863.6304, 2175.1060],\n",
      "        [ 792.9691, 2835.5017, 2819.8435],\n",
      "        [ 708.2387, 2850.7502, 2484.1294],\n",
      "        [ 669.3651, 2813.6960, 2388.8608],\n",
      "        [ 555.1506, 2839.5400, 1938.2466],\n",
      "        [ 494.8825, 2829.2405, 1731.2145],\n",
      "        [ 841.4625, 2819.7842, 2963.0027],\n",
      "        [ 564.1829, 2806.4683, 2018.1299],\n",
      "        [ 530.7885, 2804.0725, 1882.7185],\n",
      "        [ 549.2789, 2791.2256, 1958.1809],\n",
      "        [ 622.9880, 2767.7498, 2261.9834],\n",
      "        [ 795.1584, 2722.5129, 2969.2300],\n",
      "        [ 795.3181, 2753.4956, 2897.8247],\n",
      "        [ 811.1135, 2791.3652, 2887.4766],\n",
      "        [ 834.5251, 2738.5210, 3058.2368],\n",
      "        [ 466.8579, 2768.0906, 1650.0870],\n",
      "        [ 513.9297, 2762.4883, 1840.8695],\n",
      "        [ 797.0502, 2696.5962, 2989.9810],\n",
      "        [ 461.6575, 2679.0354, 1775.4072],\n",
      "        [ 732.4027, 2708.4319, 2698.1218],\n",
      "        [ 758.1967, 2688.1790, 2843.5022],\n",
      "        [ 677.6243, 2673.4165, 2564.2896],\n",
      "        [ 500.2539, 2683.1855, 1871.0023],\n",
      "        [ 619.7433, 2667.0825, 2361.3928],\n",
      "        [ 544.5372, 2707.9180, 1989.4570],\n",
      "        [ 808.9457, 2670.1155, 3025.8623],\n",
      "        [ 625.0207, 2666.4067, 2355.1821],\n",
      "        [ 811.9355, 2668.5347, 3032.1304],\n",
      "        [ 792.5566, 2678.8567, 2932.4668],\n",
      "        [ 644.2752, 2655.9060, 2411.8621],\n",
      "        [ 579.4857, 2594.2756, 2264.2188],\n",
      "        [ 606.1836, 2630.4138, 2311.6235],\n",
      "        [ 624.2274, 2602.6428, 2413.0117],\n",
      "        [ 533.0825, 2590.4329, 2088.5127],\n",
      "        [ 380.7676, 2604.0950, 1454.9495],\n",
      "        [ 581.9908, 2609.6079, 2190.4626]])\n",
      "data:  tensor([[[0.0015, 0.0015, 0.0015,  ..., 0.9719, 0.1519, 0.3478]],\n",
      "\n",
      "        [[0.0017, 0.0017, 0.0017,  ..., 0.9696, 0.1519, 0.3913]],\n",
      "\n",
      "        [[0.0014, 0.0014, 0.0013,  ..., 0.9672, 0.1519, 0.3913]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0053, 0.0053, 0.0052,  ..., 0.5801, 0.1519, 0.9130]],\n",
      "\n",
      "        [[0.0053, 0.0053, 0.0061,  ..., 0.5801, 0.1519, 0.9130]],\n",
      "\n",
      "        [[0.0068, 0.0060, 0.0060,  ..., 0.5770, 0.1519, 0.9130]]])\n",
      "output:  tensor([[1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550],\n",
      "        [1.1796, 6.2518, 5.0550]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 629.8476, 2601.1738, 2418.2336],\n",
      "        [ 610.1677, 2601.8831, 2313.3728],\n",
      "        [ 529.7368, 2563.3936, 2084.4387],\n",
      "        [ 606.8961, 2596.6328, 2301.1125],\n",
      "        [ 383.7281, 2537.2422, 1550.1368],\n",
      "        [ 595.0215, 2565.5984, 2326.7522],\n",
      "        [ 685.1835, 2548.0781, 2686.1475],\n",
      "        [ 496.6505, 2550.6306, 1957.9412],\n",
      "        [ 694.0191, 2543.4587, 2724.4204],\n",
      "        [ 617.5493, 2509.3762, 2535.7141],\n",
      "        [ 472.6892, 2569.5376, 1846.8427],\n",
      "        [ 490.4133, 2563.4717, 1921.8199],\n",
      "        [ 597.9112, 2582.1550, 2313.4685],\n",
      "        [ 584.1356, 2577.7483, 2269.4883],\n",
      "        [ 466.8165, 2626.3027, 1760.5873],\n",
      "        [ 383.2129, 2629.5535, 1440.9000],\n",
      "        [ 490.8232, 2605.2959, 1897.4261],\n",
      "        [ 665.9678, 2620.8794, 2543.7905],\n",
      "        [ 504.5372, 2640.7749, 1903.8668],\n",
      "        [ 322.5412, 2652.8394, 1215.5372],\n",
      "        [ 335.9970, 2669.4788, 1249.1497],\n",
      "        [ 419.2877, 2669.2219, 1567.7480],\n",
      "        [ 570.1013, 2639.0657, 2183.2173],\n",
      "        [ 586.8173, 2719.1338, 2108.6311],\n",
      "        [ 435.9255, 2690.6365, 1599.9475],\n",
      "        [ 343.9326, 2674.1873, 1291.6544],\n",
      "        [ 496.8234, 2667.1494, 1877.4232],\n",
      "        [ 442.9978, 2697.4062, 1648.8148],\n",
      "        [ 542.7599, 2720.6362, 2007.0833],\n",
      "        [ 467.2582, 2731.6509, 1723.0217],\n",
      "        [ 451.0096, 2771.1292, 1624.8289],\n",
      "        [ 613.0963, 2731.9802, 2327.3452],\n",
      "        [ 452.3441, 2804.0498, 1622.2828],\n",
      "        [ 380.5605, 2820.2534, 1351.2566],\n",
      "        [ 308.7407, 2864.9709, 1070.2357],\n",
      "        [ 413.8329, 2863.4907, 1445.7562],\n",
      "        [ 292.7319, 2839.4563, 1041.5917],\n",
      "        [ 371.0826, 2843.9751, 1299.2592],\n",
      "        [ 437.1238, 2849.1838, 1511.0933],\n",
      "        [ 366.0704, 2776.9602, 1318.7146],\n",
      "        [ 193.5738, 2710.9287,  711.7432],\n",
      "        [ 348.8057, 2643.4392, 1321.7427],\n",
      "        [ 246.6158, 2553.8369,  978.9472],\n",
      "        [ 246.0253, 2464.1448, 1006.4832],\n",
      "        [ 327.3161, 2396.3760, 1353.8431],\n",
      "        [ 280.8835, 2311.0579, 1208.5825],\n",
      "        [ 211.0991, 2214.8198,  949.8675],\n",
      "        [ 295.0149, 2119.7156, 1379.1637],\n",
      "        [ 300.1937, 2012.6416, 1483.4468],\n",
      "        [ 150.1514, 1890.8446,  797.3796],\n",
      "        [ 185.6791, 1811.5194, 1017.4089],\n",
      "        [ 164.7039, 1668.8213,  961.6251],\n",
      "        [ 118.5855, 1616.1836,  739.2241],\n",
      "        [ 146.5964, 1548.7891,  934.0406],\n",
      "        [ 161.9787, 1462.3146, 1098.5267],\n",
      "        [ 103.8913, 1390.9152,  746.3253],\n",
      "        [ 110.9855, 1337.8586,  820.0375],\n",
      "        [ 148.3678, 1284.0256, 1148.9191],\n",
      "        [ 109.4592, 1240.8553,  878.5993],\n",
      "        [ 102.6229, 1196.6797,  856.9810],\n",
      "        [ 102.0613, 1174.1143,  859.1940],\n",
      "        [ 102.1178, 1128.0900,  902.1098],\n",
      "        [ 115.2080, 1099.3861, 1037.8929],\n",
      "        [ 103.3805, 1063.2144,  968.2834],\n",
      "        [ 150.4871, 1056.8301, 1411.1765],\n",
      "        [ 143.4269, 1023.4680, 1386.2927],\n",
      "        [ 115.6715, 1002.1707, 1139.4111],\n",
      "        [  98.7082,  986.5926,  987.8553],\n",
      "        [ 115.7545,  983.8168, 1120.8207],\n",
      "        [ 103.3373,  964.0608, 1055.5090],\n",
      "        [ 113.3305,  963.4185, 1113.1787],\n",
      "        [ 101.8419,  920.0140, 1049.9875],\n",
      "        [  95.9083,  903.8101, 1050.8861],\n",
      "        [  90.8195,  898.7048,  998.9054],\n",
      "        [  81.0544,  885.1099,  905.4938],\n",
      "        [ 102.4691,  863.0471, 1150.2311],\n",
      "        [  74.4142,  821.0107,  909.7574],\n",
      "        [  92.2639,  820.5583, 1073.0670],\n",
      "        [  86.3196,  795.8035, 1079.3716],\n",
      "        [  92.4170,  768.8256, 1556.4875],\n",
      "        [ 111.2170,  798.2221, 1321.2360],\n",
      "        [  73.5906,  796.6523,  918.8719],\n",
      "        [ 107.6741,  815.4419, 1216.6277],\n",
      "        [  72.4623,  824.9820,  867.8965],\n",
      "        [ 132.7288,  860.0980, 1390.5587],\n",
      "        [ 110.4984,  860.9527, 1249.7010],\n",
      "        [  96.4232,  871.8835, 1089.6506],\n",
      "        [ 124.8754,  867.1302, 1484.7423],\n",
      "        [ 103.1965,  887.7631, 1165.1450],\n",
      "        [  87.8119,  918.3715,  920.6085],\n",
      "        [  92.0484,  938.9932,  971.5918],\n",
      "        [  72.4314,  951.2310,  758.2184],\n",
      "        [  97.6295,  956.2354, 1051.8477],\n",
      "        [ 133.7637, 1001.1206, 1320.6824],\n",
      "        [ 133.2733, 1032.4663, 1177.0388],\n",
      "        [  92.5646, 1041.0066,  879.8835],\n",
      "        [  87.0608, 1054.9031,  822.3758],\n",
      "        [ 105.4595, 1073.0001,  973.3032],\n",
      "        [  95.7528, 1101.6035,  860.0480],\n",
      "        [ 141.7134, 1134.5803, 1235.4156],\n",
      "        [ 117.0601, 1134.7756, 1019.4955],\n",
      "        [  90.2310, 1142.0370,  785.7305],\n",
      "        [ 112.6731, 1172.3995,  943.5231],\n",
      "        [ 130.0566, 1191.8146, 1080.9845],\n",
      "        [  83.2506, 1192.6246,  700.4383],\n",
      "        [ 130.3872, 1210.9786, 1071.2616],\n",
      "        [ 118.8883, 1228.5337,  965.0052],\n",
      "        [  98.9433, 1240.6879,  786.1876],\n",
      "        [ 101.8008, 1271.1511,  793.9175],\n",
      "        [ 127.8466, 1296.3584,  980.4593],\n",
      "        [ 154.1917, 1299.3467, 1187.0970],\n",
      "        [ 177.3039, 1330.1058, 1320.2386],\n",
      "        [ 122.2115, 1321.0966,  913.2603],\n",
      "        [ 138.9359, 1322.6160, 1038.0021],\n",
      "        [  92.5093, 1307.8533,  710.6279],\n",
      "        [ 159.4844, 1319.6833, 1199.3636],\n",
      "        [ 101.2659, 1342.5615,  758.5792],\n",
      "        [ 142.0975, 1333.5242, 1069.1013],\n",
      "        [ 136.6247, 1344.6561, 1012.3480],\n",
      "        [ 113.5854, 1357.0337,  836.2533],\n",
      "        [ 169.9274, 1376.0300, 1222.7173],\n",
      "        [ 169.1702, 1345.7844, 1254.7979],\n",
      "        [ 130.5880, 1341.4556,  968.0093],\n",
      "        [ 118.8534, 1354.8105,  887.3091],\n",
      "        [ 147.6315, 1349.7670, 1087.0583],\n",
      "        [ 133.1561, 1368.0873,  972.0347],\n",
      "        [ 152.9968, 1374.1084, 1100.6124],\n",
      "        [  97.2030, 1373.8683,  701.6611],\n",
      "        [ 140.0735, 1363.8982, 1023.7582],\n",
      "        [ 206.5450, 1377.5939, 1489.5675],\n",
      "        [ 105.7184, 1368.0986,  771.6275],\n",
      "        [ 178.4317, 1394.3446, 1270.0065],\n",
      "        [ 114.6936, 1377.0632,  829.6243],\n",
      "        [ 157.4006, 1380.0625, 1140.1361],\n",
      "        [ 167.1210, 1393.9182, 1197.7648],\n",
      "        [ 140.0813, 1392.4790,  998.8210],\n",
      "        [ 148.8940, 1393.5068, 1059.3234],\n",
      "        [ 156.1671, 1384.0674, 1119.5404],\n",
      "        [ 124.8087, 1385.1956,  892.2514],\n",
      "        [  87.1010, 1409.2411,  611.5101],\n",
      "        [ 140.7288, 1384.5309, 1011.3759],\n",
      "        [ 183.8717, 1390.1389, 1307.6431],\n",
      "        [ 147.5919, 1418.1891, 1039.1174],\n",
      "        [ 133.6655, 1469.2349,  913.3240],\n",
      "        [ 136.6658, 1506.6019,  908.1785],\n",
      "        [ 156.1920, 1554.3466, 1007.7299],\n",
      "        [ 159.5583, 1634.1354,  974.5656],\n",
      "        [ 218.7814, 1705.0935, 1288.4558],\n",
      "        [ 144.3216, 1784.4800,  817.8716],\n",
      "        [ 313.6579, 1899.6194, 1642.1230]])\n",
      "data:  tensor([[[0.0065, 0.0073, 0.0065,  ..., 0.5765, 0.1519, 0.9130]],\n",
      "\n",
      "        [[0.0080, 0.0072, 0.0081,  ..., 0.5765, 0.1519, 0.9130]],\n",
      "\n",
      "        [[0.0062, 0.0065, 0.0062,  ..., 0.5742, 0.1519, 0.9130]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0025, 0.0025, 0.0026,  ..., 0.8873, 0.1561, 0.3913]],\n",
      "\n",
      "        [[0.0017, 0.0017, 0.0018,  ..., 0.8766, 0.1561, 0.3913]],\n",
      "\n",
      "        [[0.0036, 0.0036, 0.0033,  ..., 0.8670, 0.1561, 0.3913]]])\n",
      "Epoch 0, Iteration 40 Train Loss: 524.81\n",
      "output:  tensor([[1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079],\n",
      "        [1.2325, 6.3047, 5.1079]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 224.3447, 1955.4575, 1141.6768],\n",
      "        [ 317.3352, 2004.6992, 1572.3566],\n",
      "        [ 280.4454, 2056.0410, 1347.5156],\n",
      "        [ 332.8354, 2064.8772, 1601.7335],\n",
      "        [ 294.2329, 2090.6821, 1396.8928],\n",
      "        [ 157.5581, 2085.6665,  758.7507],\n",
      "        [ 338.5669, 2097.9880, 1617.2352],\n",
      "        [ 193.1861, 2114.6216,  913.9208],\n",
      "        [ 349.0844, 2117.9785, 1654.4933],\n",
      "        [ 352.7324, 2148.3413, 1623.7120],\n",
      "        [ 257.4153, 2155.2505, 1188.5278],\n",
      "        [ 343.4025, 2164.3940, 1572.8163],\n",
      "        [ 514.4583, 2149.7209, 2388.3032],\n",
      "        [ 254.9938, 2159.4285, 1168.8540],\n",
      "        [ 437.3427, 2156.0603, 2043.8029],\n",
      "        [ 441.2804, 2169.6179, 2038.6787],\n",
      "        [ 428.7230, 2174.2300, 1971.9714],\n",
      "        [ 368.0602, 2182.3682, 1686.6488],\n",
      "        [ 517.9822, 2201.3086, 2345.6895],\n",
      "        [ 626.5311, 2241.1833, 2762.9124],\n",
      "        [ 451.4269, 2227.8062, 2008.2299],\n",
      "        [ 346.4548, 2246.9475, 1518.4011],\n",
      "        [ 379.1762, 2222.4773, 1732.5496],\n",
      "        [ 473.3465, 2239.1055, 2110.8591],\n",
      "        [ 323.4361, 2273.7578, 1395.1841],\n",
      "        [ 461.3536, 2290.8550, 1963.9724],\n",
      "        [ 467.0250, 2250.9243, 2083.1785],\n",
      "        [ 513.8551, 2259.0847, 2279.8408],\n",
      "        [ 405.7751, 2268.4570, 1787.9510],\n",
      "        [ 603.3790, 2293.6812, 2618.4641],\n",
      "        [ 710.0778, 2313.0015, 3051.5955],\n",
      "        [ 432.6374, 2308.5220, 1889.9957],\n",
      "        [ 474.4087, 2321.1731, 2038.0927],\n",
      "        [ 440.2468, 2345.1741, 1884.6152],\n",
      "        [ 376.4510, 2343.6211, 1633.7345],\n",
      "        [ 555.5336, 2339.7393, 2404.9346],\n",
      "        [ 457.9839, 2405.4861, 1880.2616],\n",
      "        [ 351.6484, 2376.3914, 1522.3715],\n",
      "        [ 399.2762, 2415.9478, 1660.9492],\n",
      "        [ 503.7632, 2448.1423, 2035.9177],\n",
      "        [ 908.0117, 2465.9824, 3650.3550],\n",
      "        [ 478.3026, 2462.0393, 1920.9015],\n",
      "        [ 546.3729, 2451.7507, 2238.5334],\n",
      "        [ 540.4702, 2423.0803, 2257.1963],\n",
      "        [ 506.7582, 2455.5376, 2067.1685],\n",
      "        [ 464.1505, 2460.9165, 1894.5160],\n",
      "        [ 383.0399, 2469.6187, 1557.1599],\n",
      "        [ 594.1903, 2495.6323, 2369.3596],\n",
      "        [ 331.0527, 2477.8462, 1359.6954],\n",
      "        [ 573.8477, 2466.9543, 2353.9219],\n",
      "        [ 195.1027, 2491.1396,  786.4848],\n",
      "        [ 405.0332, 2517.5422, 1576.0100],\n",
      "        [ 662.7218, 2501.3555, 2644.1487],\n",
      "        [ 595.9080, 2494.2451, 2407.7510],\n",
      "        [ 647.4091, 2494.6025, 2597.5596],\n",
      "        [ 466.0678, 2499.3113, 1874.4788],\n",
      "        [ 540.7291, 2496.4458, 2155.5181],\n",
      "        [ 625.1589, 2480.9236, 2545.2969],\n",
      "        [ 669.7094, 2501.1265, 2687.8411],\n",
      "        [ 606.6090, 2465.0720, 2520.6321],\n",
      "        [ 537.3684, 2481.5437, 2197.2563],\n",
      "        [ 461.5486, 2508.9685, 1836.4976],\n",
      "        [ 568.0905, 2502.3162, 2258.6980],\n",
      "        [ 545.9882, 2552.2952, 2114.9460],\n",
      "        [ 403.6505, 2533.1479, 1577.8528],\n",
      "        [ 629.8496, 2514.0623, 2520.0737],\n",
      "        [ 562.6935, 2551.8706, 2184.5994],\n",
      "        [ 462.7957, 2519.5251, 1860.2732],\n",
      "        [ 484.8022, 2533.6506, 1921.5411],\n",
      "        [ 448.8410, 2538.0522, 1797.7649],\n",
      "        [ 501.3661, 2535.1140, 2005.6534],\n",
      "        [ 421.1738, 2562.4048, 1631.1797],\n",
      "        [ 642.3139, 2582.4065, 2482.3584],\n",
      "        [ 479.0809, 2555.9871, 1902.6241],\n",
      "        [ 475.7583, 2591.5891, 1815.5236],\n",
      "        [ 728.6613, 2643.0483, 2697.3713],\n",
      "        [ 421.7867, 2588.2378, 1632.1180],\n",
      "        [ 893.2498, 2584.3167, 3471.8042],\n",
      "        [ 565.4822, 2632.5417, 2113.4675],\n",
      "        [ 673.2220, 2613.2439, 2570.2078],\n",
      "        [ 526.9293, 2602.4451, 2063.4829],\n",
      "        [ 776.1669, 2646.1035, 2911.0930],\n",
      "        [ 654.2790, 2644.6624, 2458.8735],\n",
      "        [ 607.1829, 2654.4148, 2278.4585],\n",
      "        [ 528.1234, 2656.4868, 1968.0839],\n",
      "        [ 661.3973, 2657.0173, 2472.0420],\n",
      "        [ 333.8958, 2636.1055, 1273.0959],\n",
      "        [ 511.8997, 2638.7083, 1928.2019],\n",
      "        [ 601.7639, 2680.1411, 2201.5510],\n",
      "        [ 583.3140, 2624.3032, 2221.4766],\n",
      "        [ 553.3232, 2639.9763, 2092.4670],\n",
      "        [ 620.2181, 2595.8171, 2394.1099],\n",
      "        [ 533.1602, 2607.0386, 2064.4629],\n",
      "        [ 498.6428, 2617.0981, 1917.1460],\n",
      "        [ 698.8654, 2612.3113, 2678.4712],\n",
      "        [ 280.7228, 2605.2734, 1087.3228],\n",
      "        [ 635.6730, 2613.3501, 2441.1189],\n",
      "        [ 507.1357, 2614.3833, 1964.3087],\n",
      "        [ 671.0641, 2588.1594, 2624.6843],\n",
      "        [ 389.0442, 2595.0186, 1537.5475],\n",
      "        [ 566.1307, 2637.9709, 2127.4524],\n",
      "        [ 591.2279, 2630.9890, 2247.8262],\n",
      "        [ 774.6344, 2674.4331, 2859.9941],\n",
      "        [ 720.0503, 2655.5818, 2687.3542],\n",
      "        [ 552.5186, 2598.0234, 2148.6685],\n",
      "        [ 572.7705, 2623.7727, 2164.5034],\n",
      "        [ 707.5020, 2616.0254, 2703.4670],\n",
      "        [ 532.6096, 2596.0295, 2077.7676],\n",
      "        [ 660.0239, 2614.2996, 2529.9473],\n",
      "        [ 677.6302, 2609.2280, 2606.3577],\n",
      "        [ 655.3235, 2613.3948, 2511.4573],\n",
      "        [ 491.4772, 2596.1245, 1907.0239],\n",
      "        [ 473.0420, 2635.3345, 1753.8009],\n",
      "        [ 559.6312, 2608.4663, 2145.8440],\n",
      "        [ 326.5162, 2604.5027, 1233.5619],\n",
      "        [ 328.9439, 2574.7825, 1288.4598],\n",
      "        [ 558.4786, 2564.7507, 2187.9373],\n",
      "        [ 531.7894, 2529.2886, 2142.1965],\n",
      "        [ 654.3427, 2539.7000, 2595.9043],\n",
      "        [ 651.1856, 2524.8088, 2616.9561],\n",
      "        [ 392.7255, 2552.8594, 1529.3533],\n",
      "        [ 458.8111, 2543.4866, 1789.3428],\n",
      "        [ 392.3225, 2507.9463, 1590.0509],\n",
      "        [ 506.5634, 2517.0786, 2001.0687],\n",
      "        [ 674.1935, 2514.5984, 2675.3411],\n",
      "        [ 392.4702, 2504.0645, 1566.0679],\n",
      "        [ 474.5279, 2474.1987, 1925.7649],\n",
      "        [ 618.5422, 2476.2432, 2505.2266],\n",
      "        [ 408.5282, 2451.7339, 1697.1932],\n",
      "        [ 761.2809, 2403.7100, 3202.7241],\n",
      "        [ 353.4560, 2463.3071, 1425.1337],\n",
      "        [ 424.6368, 2449.3101, 1738.7507],\n",
      "        [ 526.3904, 2461.5811, 2121.8101],\n",
      "        [ 618.3401, 2427.2808, 2564.2976],\n",
      "        [ 442.7973, 2429.1575, 1819.0782],\n",
      "        [ 502.0513, 2415.3667, 2088.6458],\n",
      "        [ 599.4447, 2413.7637, 2476.0291],\n",
      "        [ 391.1357, 2432.1514, 1579.2206],\n",
      "        [ 565.1962, 2431.8831, 2284.2988],\n",
      "        [ 582.3041, 2369.0408, 2497.3574],\n",
      "        [ 651.4128, 2376.5869, 2743.5842],\n",
      "        [ 629.7357, 2403.1960, 2564.7046],\n",
      "        [ 348.5636, 2367.5159, 1462.5369],\n",
      "        [ 799.0748, 2344.5374, 3427.6082],\n",
      "        [ 486.0883, 2355.6355, 2059.0442],\n",
      "        [ 377.7492, 2342.2529, 1616.6864],\n",
      "        [ 568.6736, 2354.0271, 2402.9414],\n",
      "        [ 561.5490, 2336.9583, 2395.1345],\n",
      "        [ 437.8111, 2337.4548, 1837.5762],\n",
      "        [ 535.2845, 2326.0994, 2288.2534]])\n",
      "data:  tensor([[[0.0024, 0.0023, 0.0026,  ..., 0.8580, 0.1561, 0.3913]],\n",
      "\n",
      "        [[0.0039, 0.0041, 0.0038,  ..., 0.8497, 0.1561, 0.4348]],\n",
      "\n",
      "        [[0.0033, 0.0031, 0.0033,  ..., 0.8415, 0.1561, 0.4348]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0061, 0.0055, 0.0060,  ..., 0.5907, 0.1561, 0.9565]],\n",
      "\n",
      "        [[0.0055, 0.0061, 0.0060,  ..., 0.5907, 0.1561, 0.9565]],\n",
      "\n",
      "        [[0.0055, 0.0050, 0.0052,  ..., 0.5907, 0.1561, 0.9565]]])\n",
      "output:  tensor([[1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607],\n",
      "        [1.2853, 6.3575, 5.1607]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[  493.6033,  2307.1367,  2151.1843],\n",
      "        [  387.1499,  2294.2400,  1698.6254],\n",
      "        [  353.3293,  2277.1875,  1561.0150],\n",
      "        [  434.8257,  2282.1250,  1896.7489],\n",
      "        [  479.3969,  2274.3948,  2092.7898],\n",
      "        [  337.1313,  2245.9478,  1518.8654],\n",
      "        [  361.5846,  2236.9661,  1628.1962],\n",
      "        [  531.9061,  2254.4990,  2342.6582],\n",
      "        [  401.0864,  2257.8391,  1734.8591],\n",
      "        [  492.9909,  2227.2922,  2209.1670],\n",
      "        [  277.9404,  2245.8809,  1212.6938],\n",
      "        [  460.6831,  2192.2935,  2142.3662],\n",
      "        [  518.5589,  2224.4714,  2331.0271],\n",
      "        [  406.5067,  2238.2844,  1773.2706],\n",
      "        [  352.6078,  2198.1975,  1613.0250],\n",
      "        [  481.9519,  2186.3113,  2210.0193],\n",
      "        [  431.2115,  2202.9504,  1940.2629],\n",
      "        [  208.6052,  2180.9380,   960.3975],\n",
      "        [  659.3594,  2214.4568,  2951.3765],\n",
      "        [  498.7164,  2204.1179,  2229.3538],\n",
      "        [ 1364.1321,  4511.9648,  2825.3625],\n",
      "        [ 1399.5961,  5962.1646,  2354.3528],\n",
      "        [ 1243.8942,  5811.0771,  2141.9839],\n",
      "        [ 1492.5940,  5695.1226,  2627.0471],\n",
      "        [ 1184.6827,  5613.6582,  2114.7046],\n",
      "        [ 1077.1941,  5561.7354,  1937.5385],\n",
      "        [ 1159.2344,  5376.3359,  2167.3003],\n",
      "        [ 2643.3916,  8050.4512,  3192.3853],\n",
      "        [ 2866.6167, 10235.5684,  2817.4863],\n",
      "        [ 2155.9331, 10278.8271,  2104.3245],\n",
      "        [ 2066.4141,  9937.9844,  2080.1062],\n",
      "        [ 1907.9841,  9463.6924,  2017.8309],\n",
      "        [ 1261.7334,  8965.7900,  1409.4440],\n",
      "        [  811.3003,  8522.5986,   949.5894],\n",
      "        [  954.6661,  8073.5586,  1180.4420],\n",
      "        [ 1209.6422,  7775.0698,  1562.9307],\n",
      "        [  852.1050,  7639.8672,  1114.5710],\n",
      "        [ 1620.5803,  7442.9565,  2181.0232],\n",
      "        [ 1084.0864,  7341.8643,  1479.5758],\n",
      "        [ 1097.6580,  7235.3604,  1522.2623],\n",
      "        [ 1028.1675,  7184.6611,  1434.2281],\n",
      "        [ 1053.2640,  7107.2632,  1484.5924],\n",
      "        [  954.0316,  7066.7568,  1353.9427],\n",
      "        [ 1033.7135,  7032.5571,  1474.0514],\n",
      "        [ 1034.9016,  7013.4268,  1478.6129],\n",
      "        [ 1184.6632,  7004.0596,  1696.0874],\n",
      "        [  962.3532,  6999.2954,  1378.4651],\n",
      "        [ 1140.4191,  6975.0376,  1638.9390],\n",
      "        [ 1229.0063,  6934.9282,  1778.3173],\n",
      "        [  876.3661,  6958.5269,  1261.4779],\n",
      "        [  671.1292,  6958.0132,   965.2522],\n",
      "        [  994.5427,  6941.4238,  1434.8304],\n",
      "        [  972.7973,  6939.3018,  1405.0879],\n",
      "        [ 1148.0376,  6935.0566,  1659.0630],\n",
      "        [  991.6105,  6941.7646,  1431.5093],\n",
      "        [  899.5228,  6946.6689,  1297.0844],\n",
      "        [  802.4481,  6955.3936,  1156.0999],\n",
      "        [ 1076.8024,  6941.2676,  1554.3525],\n",
      "        [  752.9564,  6966.2461,  1082.3622],\n",
      "        [  925.2806,  6962.3809,  1331.5571],\n",
      "        [  977.4742,  6980.9077,  1401.9714],\n",
      "        [  948.0109,  6992.7271,  1357.8518],\n",
      "        [  869.4829,  7008.2041,  1242.8801],\n",
      "        [  908.7817,  7025.8848,  1296.2279],\n",
      "        [  908.8617,  7047.5146,  1292.2917],\n",
      "        [ 1011.6063,  7058.5293,  1437.1462],\n",
      "        [  982.1715,  7079.5029,  1389.7662],\n",
      "        [  938.0594,  7106.6934,  1321.7841],\n",
      "        [ 1022.8428,  7146.3613,  1432.9838],\n",
      "        [  982.2226,  7174.1377,  1370.8618],\n",
      "        [  796.9253,  7209.9575,  1107.1975],\n",
      "        [  884.8306,  7236.9355,  1225.6274],\n",
      "        [ 1043.7493,  7249.2905,  1443.3608],\n",
      "        [  885.2700,  7282.8315,  1216.3314],\n",
      "        [ 1047.6442,  7312.1387,  1435.6924],\n",
      "        [  829.7269,  7339.2339,  1133.0189],\n",
      "        [ 1110.8457,  7339.7256,  1516.8834],\n",
      "        [ 1002.4509,  7370.8311,  1361.3468],\n",
      "        [  940.1722,  7410.5659,  1270.8246],\n",
      "        [  957.6755,  7434.4834,  1289.9679],\n",
      "        [ 1012.5039,  7453.4458,  1361.5714],\n",
      "        [ 1124.1334,  7484.4399,  1505.8690],\n",
      "        [ 1068.4796,  7510.3564,  1425.6196],\n",
      "        [ 1043.8113,  7532.2627,  1388.6213],\n",
      "        [  867.7375,  7570.5293,  1148.0841],\n",
      "        [  970.9457,  7589.7988,  1281.5211],\n",
      "        [  825.5206,  7634.4268,  1082.9094],\n",
      "        [ 1075.5890,  7668.0796,  1404.7501],\n",
      "        [ 1030.9932,  7715.8804,  1338.0419],\n",
      "        [ 1254.9203,  7727.1519,  1625.3474],\n",
      "        [  769.4518,  7810.4092,   987.1176],\n",
      "        [ 1188.3629,  7815.8550,  1522.8870],\n",
      "        [  860.7980,  7882.7690,  1093.0037],\n",
      "        [  943.7151,  7943.8691,  1189.9283],\n",
      "        [ 1119.5317,  8033.4604,  1396.5347],\n",
      "        [  993.2773,  8131.8599,  1223.3777],\n",
      "        [ 1086.3416,  8263.0127,  1314.7554],\n",
      "        [ 1182.2510,  8274.8037,  1434.0162],\n",
      "        [  866.7355,  8304.2168,  1045.6514],\n",
      "        [ 1374.2181,  8279.4844,  1663.4730],\n",
      "        [ 1060.0990,  8311.7070,  1277.7594],\n",
      "        [ 1423.4493,  8318.3652,  1724.9481],\n",
      "        [ 1117.6716,  8327.7764,  1343.5746],\n",
      "        [  955.1370,  8328.1172,  1146.5490],\n",
      "        [ 1163.3787,  8361.1885,  1392.2915],\n",
      "        [ 1100.4155,  8321.6279,  1324.9907],\n",
      "        [  167.8965,  1794.7349,   930.8570],\n",
      "        [  225.5606,  1746.6718,  1327.4242],\n",
      "        [  257.1745,  1829.3036,  1385.9382],\n",
      "        [  190.7149,  1845.0715,  1028.2799],\n",
      "        [  315.1730,  1877.6293,  1622.5106],\n",
      "        [  259.8995,  1877.8472,  1362.5999],\n",
      "        [  276.4812,  1854.6058,  1501.5688],\n",
      "        [  370.4478,  1899.2283,  1923.8704],\n",
      "        [  286.9071,  1903.9425,  1486.4573],\n",
      "        [  326.4235,  1961.6127,  1614.7498],\n",
      "        [  278.2069,  1978.8328,  1407.7932],\n",
      "        [  256.4510,  2035.8773,  1253.9510],\n",
      "        [  291.0911,  2102.1323,  1377.2477],\n",
      "        [  360.1467,  2158.2275,  1660.4849],\n",
      "        [  336.9938,  2226.0520,  1492.6514],\n",
      "        [  309.3996,  2285.8057,  1341.8666],\n",
      "        [  268.4673,  2367.6108,  1128.4008],\n",
      "        [  268.3634,  2479.3096,  1077.8668],\n",
      "        [  407.9918,  2564.2537,  1586.4873],\n",
      "        [  477.3253,  2655.7812,  1773.9576],\n",
      "        [  425.2042,  2685.8608,  1552.6328],\n",
      "        [  320.9145,  2684.6152,  1188.9501],\n",
      "        [  461.8243,  2691.7371,  1691.5713],\n",
      "        [  513.3840,  2688.6482,  1891.8401],\n",
      "        [  497.0548,  2669.0708,  1869.8121],\n",
      "        [  493.7296,  2675.1592,  1837.5762],\n",
      "        [  441.4960,  2668.9761,  1641.8977],\n",
      "        [  355.9384,  2673.8857,  1327.0299],\n",
      "        [  385.9266,  2702.7405,  1409.5204],\n",
      "        [  422.0022,  2661.4468,  1594.0515],\n",
      "        [  460.2786,  2707.1418,  1697.6257],\n",
      "        [  497.1693,  2711.6660,  1824.5050],\n",
      "        [  569.0769,  2710.6326,  2102.3999],\n",
      "        [  418.3954,  2731.1929,  1516.7966],\n",
      "        [  441.1656,  2731.1926,  1616.1901],\n",
      "        [  413.5349,  2746.6870,  1501.5817],\n",
      "        [  579.1317,  2774.3240,  2068.8596],\n",
      "        [  495.9484,  2773.1958,  1773.0181],\n",
      "        [  587.4979,  2784.7744,  2094.4766],\n",
      "        [  482.7314,  2770.0957,  1732.2327],\n",
      "        [  480.0691,  2782.0208,  1715.6896],\n",
      "        [  783.4238,  2834.7253,  2694.3569],\n",
      "        [  658.7491,  2857.8213,  2267.8967],\n",
      "        [  511.2435,  2829.5757,  1817.5463]])\n",
      "data:  tensor([[[0.0057, 0.0063, 0.0056,  ..., 0.5907, 0.1561, 0.9565]],\n",
      "\n",
      "        [[0.0053, 0.0051, 0.0058,  ..., 0.5907, 0.1561, 0.9565]],\n",
      "\n",
      "        [[0.0040, 0.0038, 0.0038,  ..., 0.5907, 0.1561, 0.9565]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0088, 0.0076, 0.0072,  ..., 0.7269, 0.1941, 0.4348]],\n",
      "\n",
      "        [[0.0075, 0.0093, 0.0091,  ..., 0.7240, 0.1941, 0.4348]],\n",
      "\n",
      "        [[0.0061, 0.0059, 0.0065,  ..., 0.7206, 0.1941, 0.4348]]])\n",
      "output:  tensor([[1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135],\n",
      "        [1.3381, 6.4103, 5.2135]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 610.5671, 2877.1750, 2109.4500],\n",
      "        [ 383.3180, 2891.7085, 1299.4041],\n",
      "        [ 626.5419, 2906.0522, 2160.5911],\n",
      "        [ 588.0538, 2902.4382, 2019.6544],\n",
      "        [ 701.2591, 2919.3901, 2408.6526],\n",
      "        [ 464.7980, 2915.5083, 1609.1765],\n",
      "        [ 506.7613, 2942.6091, 1732.1107],\n",
      "        [ 754.1168, 3004.6750, 2476.3130],\n",
      "        [ 635.6743, 2937.0908, 2173.0857],\n",
      "        [ 780.7689, 3004.5132, 2585.4817],\n",
      "        [ 658.7296, 3016.2817, 2178.3386],\n",
      "        [ 688.2340, 3019.6331, 2264.1096],\n",
      "        [ 568.5490, 3051.7104, 1840.3748],\n",
      "        [ 666.3560, 3019.8789, 2201.2324],\n",
      "        [ 899.1942, 3051.8894, 2895.1577],\n",
      "        [ 607.9282, 3045.1421, 1990.2505],\n",
      "        [ 816.5287, 3093.4622, 2597.7451],\n",
      "        [ 811.9455, 3055.3635, 2709.4109],\n",
      "        [ 515.0319, 3074.0413, 1684.3291],\n",
      "        [ 438.1123, 3085.8604, 1407.1360],\n",
      "        [ 862.6172, 3099.9692, 2760.7791],\n",
      "        [ 980.5559, 3149.6633, 3056.3123],\n",
      "        [ 533.7347, 3106.4875, 1728.0303],\n",
      "        [ 638.4605, 3133.1416, 2046.3561],\n",
      "        [ 791.5485, 3160.2815, 2494.9795],\n",
      "        [ 738.0664, 3130.6001, 2411.3755],\n",
      "        [ 816.9401, 3172.5918, 2536.7681],\n",
      "        [ 688.4699, 3151.6240, 2202.8376],\n",
      "        [ 826.6029, 3132.4434, 2653.5171],\n",
      "        [ 486.0723, 3171.8098, 1514.5508],\n",
      "        [ 932.5136, 3142.0503, 2978.3787],\n",
      "        [ 675.6340, 3172.8096, 2108.0439],\n",
      "        [ 767.7505, 3180.5288, 2393.0820],\n",
      "        [ 690.3657, 3140.7600, 2215.8643],\n",
      "        [ 754.7395, 3171.9661, 2372.4827],\n",
      "        [ 663.4467, 3132.6331, 2143.5964],\n",
      "        [ 681.6114, 3202.6641, 2108.6726],\n",
      "        [ 656.7130, 3146.9600, 2108.2488],\n",
      "        [ 559.6855, 3194.3250, 1735.5848],\n",
      "        [ 861.3196, 3205.7195, 2668.1260],\n",
      "        [ 683.2006, 3181.5940, 2136.8237],\n",
      "        [ 668.8068, 3172.6365, 2112.1594],\n",
      "        [ 543.7394, 3195.5259, 1696.7740],\n",
      "        [ 694.7018, 3219.0295, 2130.8486],\n",
      "        [ 786.1666, 3245.5662, 2357.7585],\n",
      "        [ 962.1343, 3184.1201, 3031.8259],\n",
      "        [ 641.5130, 3220.3479, 1969.8820],\n",
      "        [ 756.0175, 3195.9504, 2372.2693],\n",
      "        [ 621.9357, 3194.4534, 1971.6641],\n",
      "        [ 516.6718, 3243.6504, 1569.9612],\n",
      "        [ 591.9259, 3230.7871, 1803.1366],\n",
      "        [ 849.4694, 3240.1541, 2580.1946],\n",
      "        [ 991.4594, 3235.4453, 3028.0378],\n",
      "        [ 926.6952, 3237.7576, 2833.4897],\n",
      "        [ 958.7926, 3219.5603, 2959.3335],\n",
      "        [ 738.8900, 3240.9414, 2249.9414],\n",
      "        [ 685.2451, 3196.4529, 2164.8960],\n",
      "        [ 716.6364, 3210.2212, 2218.3276],\n",
      "        [ 843.7970, 3208.2942, 2600.0037],\n",
      "        [ 914.3369, 3224.5425, 2805.5063],\n",
      "        [ 335.7697, 3185.4438, 1058.4694],\n",
      "        [ 449.0038, 3180.0374, 1405.3683],\n",
      "        [ 539.3399, 3205.2224, 1640.4645],\n",
      "        [ 952.8607, 3060.8706, 3381.9631],\n",
      "        [ 581.7229, 3181.4670, 1810.9971],\n",
      "        [ 673.1718, 3166.3640, 2105.0676],\n",
      "        [ 591.9947, 3150.3838, 1883.0922],\n",
      "        [ 657.3594, 3127.6284, 2120.0708],\n",
      "        [ 437.9776, 3145.6753, 1390.9452],\n",
      "        [ 662.3321, 3133.9011, 2134.9958],\n",
      "        [ 620.5307, 3114.3909, 2021.0564],\n",
      "        [ 651.4676, 3150.3894, 2061.5186],\n",
      "        [ 385.4836, 3111.7490, 1265.2284],\n",
      "        [1104.2273, 3186.3455, 3358.3179],\n",
      "        [ 824.7262, 3099.5615, 2682.1516],\n",
      "        [ 676.6656, 3109.1853, 2147.6687],\n",
      "        [ 591.4922, 3080.2134, 1934.0571],\n",
      "        [ 771.0469, 3069.2659, 2509.9907],\n",
      "        [ 881.6050, 3104.1306, 2766.5125],\n",
      "        [ 858.8647, 3045.3376, 2824.2227],\n",
      "        [ 780.4717, 2935.3538, 4329.0229],\n",
      "        [ 663.9777, 3048.9124, 2149.9504],\n",
      "        [ 941.2358, 3034.1106, 3087.7517],\n",
      "        [ 832.6285, 3041.6624, 2726.1548],\n",
      "        [ 971.5174, 3053.0063, 3172.7747],\n",
      "        [ 437.7022, 3040.5730, 1423.6758],\n",
      "        [ 666.0257, 2983.3330, 2270.7871],\n",
      "        [ 709.1791, 3011.2939, 2354.7598],\n",
      "        [ 585.1888, 2999.4414, 1956.5056],\n",
      "        [ 406.6173, 2982.7185, 1376.0974],\n",
      "        [ 831.1322, 2994.3252, 2776.8489],\n",
      "        [ 826.6991, 2968.7327, 2816.9644],\n",
      "        [ 797.0968, 2977.9319, 2666.4292],\n",
      "        [ 930.1409, 2967.3640, 3132.7898],\n",
      "        [ 586.1617, 2932.8960, 2023.7404],\n",
      "        [ 682.2108, 2965.6941, 2285.7273],\n",
      "        [ 660.9901, 2939.2021, 2223.8872],\n",
      "        [ 773.3306, 2909.2358, 2653.6931],\n",
      "        [ 631.5887, 2912.1069, 2140.2383],\n",
      "        [ 645.2176, 2882.7383, 2215.9478],\n",
      "        [1008.3073, 2867.2832, 3512.9355],\n",
      "        [ 710.1539, 2893.5686, 2425.9771],\n",
      "        [ 509.3386, 2846.7622, 1776.2350],\n",
      "        [ 582.3911, 2844.5447, 2032.6754],\n",
      "        [ 718.6669, 2820.6443, 2528.0942],\n",
      "        [ 865.4313, 2815.0144, 3055.1030],\n",
      "        [ 865.2769, 2811.9758, 3065.4504],\n",
      "        [ 762.1439, 2781.9856, 2742.6682],\n",
      "        [ 874.6317, 2780.1775, 3139.5378],\n",
      "        [ 512.9493, 2760.3882, 1840.1055],\n",
      "        [ 704.2416, 2760.1982, 2544.7002],\n",
      "        [ 490.1946, 2740.4924, 1769.0796],\n",
      "        [ 590.1147, 2648.5667, 3654.6018],\n",
      "        [ 545.7767, 2702.2097, 2038.7117],\n",
      "        [ 668.0494, 2697.8418, 2477.3245],\n",
      "        [ 651.0609, 2714.1125, 2391.2849],\n",
      "        [ 600.2126, 2684.9561, 2253.8464],\n",
      "        [ 517.1176, 2698.8025, 1886.7607],\n",
      "        [ 603.6636, 2715.3245, 2189.5259],\n",
      "        [ 476.6071, 2721.9543, 1697.1088],\n",
      "        [ 756.3195, 2716.2349, 2737.6709],\n",
      "        [ 574.2160, 2683.8838, 2131.8149],\n",
      "        [ 506.8702, 2708.8899, 1847.4066],\n",
      "        [ 669.7553, 2681.7500, 2511.3323],\n",
      "        [ 554.2000, 2675.7678, 2075.3748],\n",
      "        [ 839.8682, 2722.0271, 3049.0972],\n",
      "        [ 552.6199, 2702.9636, 2020.1061],\n",
      "        [ 539.7008, 2696.7637, 2000.4371],\n",
      "        [ 670.7424, 2710.5322, 2436.1533],\n",
      "        [ 465.1903, 2720.1895, 1672.6503],\n",
      "        [ 611.5525, 2611.0210, 3823.8877],\n",
      "        [ 274.9206, 2664.7646, 1064.8066],\n",
      "        [ 509.7075, 2693.3286, 1899.3834],\n",
      "        [ 344.8624, 2698.0430, 1290.2458],\n",
      "        [ 343.6094, 2727.1936, 1255.9475],\n",
      "        [ 436.5208, 2740.9619, 1585.5760],\n",
      "        [ 560.8937, 2638.7363, 3956.4253],\n",
      "        [ 480.6731, 2763.8792, 1736.5455],\n",
      "        [ 387.7252, 2750.4683, 1419.9308],\n",
      "        [ 484.5992, 2769.5483, 1749.8925],\n",
      "        [ 458.1367, 2788.9971, 1635.4282],\n",
      "        [ 328.3247, 2775.3149, 1189.3031],\n",
      "        [ 307.4651, 2815.5730, 1084.4852],\n",
      "        [ 313.1742, 2814.3384, 1100.2029],\n",
      "        [ 358.8386, 2871.3887, 1237.3059],\n",
      "        [ 280.1895, 2862.9602,  984.8925],\n",
      "        [ 433.2228, 2911.2134, 1481.3230],\n",
      "        [ 307.8427, 2954.4783, 1029.8811],\n",
      "        [ 266.7609, 2995.0457,  889.0817],\n",
      "        [ 350.7272, 3036.2444, 1162.8300]])\n",
      "data:  tensor([[[0.0074, 0.0072, 0.0072,  ..., 0.7179, 0.1941, 0.4348]],\n",
      "\n",
      "        [[0.0047, 0.0052, 0.0052,  ..., 0.7145, 0.1941, 0.4783]],\n",
      "\n",
      "        [[0.0073, 0.0068, 0.0062,  ..., 0.7116, 0.1941, 0.4783]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0036, 0.0034, 0.0035,  ..., 0.5547, 0.1941, 1.0000]],\n",
      "\n",
      "        [[0.0031, 0.0029, 0.0031,  ..., 0.5573, 0.1941, 1.0000]],\n",
      "\n",
      "        [[0.0037, 0.0041, 0.0039,  ..., 0.5613, 0.1941, 1.0000]]])\n",
      "output:  tensor([[1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663],\n",
      "        [1.3908, 6.4630, 5.2663]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[  182.1245,  3085.9050,   595.5414],\n",
      "        [  287.8348,  3139.7100,   912.1259],\n",
      "        [  352.2292,  3160.0076,  1113.7131],\n",
      "        [  310.9824,  3172.5417,   966.6432],\n",
      "        [  284.0266,  3084.4639,   931.7010],\n",
      "        [  463.9971,  3158.7119,  1456.5619],\n",
      "        [  249.3127,  3120.5405,   795.1826],\n",
      "        [  349.9186,  3076.7671,  1134.2058],\n",
      "        [  349.0082,  3043.8296,  1140.9694],\n",
      "        [  231.6518,  2991.5103,   767.0729],\n",
      "        [  319.5961,  2928.2656,  1089.0188],\n",
      "        [  333.5009,  2851.0574,  1170.0419],\n",
      "        [  241.2599,  2784.0425,   867.7627],\n",
      "        [  318.0768,  2709.0742,  1164.5873],\n",
      "        [  324.6808,  2623.6609,  1224.6816],\n",
      "        [  315.8885,  3867.8848,   722.8995],\n",
      "        [ 1493.4745,  7354.4043,  2025.3966],\n",
      "        [  799.2267,  7318.1094,  1093.1033],\n",
      "        [  554.1030,  7244.0459,   762.8732],\n",
      "        [ 1294.8566,  7248.7041,  1760.0360],\n",
      "        [  825.2771,  7212.1191,  1146.2765],\n",
      "        [ 1143.2894,  7219.2354,  1581.0017],\n",
      "        [  901.9103,  7240.4824,  1245.1736],\n",
      "        [  992.1970,  7255.9209,  1367.2316],\n",
      "        [ 1201.4962,  7240.9844,  1664.6725],\n",
      "        [ 1127.8557,  7264.3491,  1556.8135],\n",
      "        [ 1300.6185,  7281.1504,  1790.6023],\n",
      "        [ 1107.6758,  7323.7344,  1492.1318],\n",
      "        [  894.6497,  7321.6841,  1225.1605],\n",
      "        [  852.0225,  7325.0635,  1165.7471],\n",
      "        [  745.8471,  7358.7383,  1014.9752],\n",
      "        [  747.7455,  7162.1572,  1086.9021],\n",
      "        [ 1200.6444,  7384.5044,  1601.6617],\n",
      "        [  723.0262,  7384.2583,   980.0817],\n",
      "        [  913.7267,  7375.1431,  1248.7777],\n",
      "        [ 1042.8629,  7409.4102,  1409.6033],\n",
      "        [ 1138.5410,  7447.1401,  1520.3846],\n",
      "        [  693.4515,  7466.1699,   929.3517],\n",
      "        [  744.2134,  7467.2031,  1004.3082],\n",
      "        [ 1045.8065,  7484.6353,  1399.3751],\n",
      "        [ 1163.3131,  7558.6709,  1530.1731],\n",
      "        [  585.4728,  7730.5479,   755.7656],\n",
      "        [  501.8400,  7812.0791,   641.3141],\n",
      "        [ 1021.3735,  7594.5469,  1319.3734],\n",
      "        [  574.4506,  7334.5029,   782.4992],\n",
      "        [ 1391.5883,  9299.8027,  1483.0623],\n",
      "        [ 1286.2678, 10155.0527,  1338.6274],\n",
      "        [  993.0411, 10560.0176,   941.5012],\n",
      "        [ 1197.0388, 10588.7051,  1131.5299],\n",
      "        [ 1593.3524, 10548.3105,  1523.4072],\n",
      "        [  944.0010, 10524.8291,   896.6702],\n",
      "        [ 1098.4474, 10447.3750,  1051.5690],\n",
      "        [ 1102.9116, 10328.8789,  1068.6624],\n",
      "        [ 1380.8441, 10241.6621,  1346.9573],\n",
      "        [  764.7653, 10159.6221,   753.2593],\n",
      "        [ 1249.7174, 10088.1943,  1229.4608],\n",
      "        [ 1609.4476,  9998.8037,  1620.2839],\n",
      "        [ 1419.9689,  9949.5791,  1423.0497],\n",
      "        [  964.9207,  9891.0605,   976.0355],\n",
      "        [  748.5257,  9841.6738,   760.6172],\n",
      "        [  954.3381,  9783.1035,   984.0544],\n",
      "        [  578.3782,  9770.1348,   591.1075],\n",
      "        [  764.2811,  9749.9600,   783.4217],\n",
      "        [ 1404.6946,  9711.7939,  1448.0338],\n",
      "        [  906.5286,  9729.6621,   930.2678],\n",
      "        [  759.7276,  9728.4219,   781.7855],\n",
      "        [  759.8364,  9748.3623,   778.8592],\n",
      "        [  573.4991,  9742.1064,   588.6777],\n",
      "        [ 1002.5852,  9751.1162,  1027.6816],\n",
      "        [  819.2736,  9765.4814,   840.1217],\n",
      "        [  869.6415,  9780.5908,   890.3913],\n",
      "        [  699.0802,  9813.7070,   710.4695],\n",
      "        [  790.5082,  9842.7236,   802.9369],\n",
      "        [  900.1932,  9842.1094,   915.5276],\n",
      "        [ 1179.7463,  9780.0322,  1309.8424],\n",
      "        [  938.0975,  9882.0117,   948.6208],\n",
      "        [ 1126.0641,  9916.1006,  1116.6423],\n",
      "        [  617.0175,  9919.2666,   621.7366],\n",
      "        [  854.8422,  9957.5557,   857.9218],\n",
      "        [  910.6898,  9970.0107,   912.6462],\n",
      "        [ 1019.8721,  9995.0449,  1019.4404],\n",
      "        [  520.3736,  9992.6553,   520.4734],\n",
      "        [  851.7690,  9992.6494,   852.9149],\n",
      "        [  655.7692,  9999.1230,   656.5633],\n",
      "        [  695.8575,  9991.5156,   697.6794],\n",
      "        [  850.8032,  9973.4238,   853.1244],\n",
      "        [  808.7155,  9955.3770,   812.6233],\n",
      "        [  765.7620,  9941.1123,   770.7528],\n",
      "        [ 1007.9968,  9933.3145,  1013.9966],\n",
      "        [  953.5323,  9928.1035,   960.3679],\n",
      "        [  697.2444,  9893.7578,   705.1877],\n",
      "        [  746.9051,  9853.0732,   757.6798],\n",
      "        [  894.3378,  9806.4404,   911.4011],\n",
      "        [ 1147.2697,  9747.2285,  1178.9092],\n",
      "        [  889.3648,  9699.8525,   916.5583],\n",
      "        [  923.0638,  9617.4385,   960.0654],\n",
      "        [  908.5637,  9556.8916,   951.6671],\n",
      "        [  787.2418,  9489.5137,   830.2768],\n",
      "        [ 1111.0942,  9421.2979,  1178.5922],\n",
      "        [  708.9457,  9329.3389,   759.8140],\n",
      "        [  583.3389,  9251.8174,   629.8345],\n",
      "        [  871.6852,  9147.6260,   954.7229],\n",
      "        [ 1133.7639,  9078.8066,  1248.4709],\n",
      "        [  710.8289,  8997.4766,   791.0487],\n",
      "        [  910.3103,  8915.9619,  1022.3625],\n",
      "        [ 1060.5521,  8817.0820,  1202.6475],\n",
      "        [ 1160.8409,  8681.0088,  1337.6914],\n",
      "        [ 1071.9648,  8580.8721,  1247.5342],\n",
      "        [  995.1885,  8450.6016,  1174.3005],\n",
      "        [ 1086.5022,  8274.0605,  1314.2734],\n",
      "        [  760.5076,  8123.6157,   938.7567],\n",
      "        [  947.0491,  7978.9683,  1186.4709],\n",
      "        [  770.6837,  7807.3594,   988.6440],\n",
      "        [  921.1188,  7654.5181,  1203.4514],\n",
      "        [  793.1461,  7511.4067,  1054.2664],\n",
      "        [  689.9980,  7383.9512,   933.6615],\n",
      "        [  677.6070,  7250.6704,   938.8596],\n",
      "        [ 1123.5443,  7144.4233,  1571.2765],\n",
      "        [  812.4717,  7041.0469,  1152.1079],\n",
      "        [ 1012.9472,  6949.3242,  1454.6895],\n",
      "        [ 1369.7024,  6836.1211,  1998.6464],\n",
      "        [  892.3480,  6755.0195,  1320.0099],\n",
      "        [  869.5010,  6652.8945,  1308.4778],\n",
      "        [ 1111.3505,  6560.2197,  1696.6890],\n",
      "        [ 1109.0177,  6466.6016,  1713.7482],\n",
      "        [ 1105.6199,  6407.0322,  1721.3975],\n",
      "        [ 1139.6729,  6320.2168,  1799.7056],\n",
      "        [ 1026.2762,  6239.2441,  1639.7698],\n",
      "        [ 1062.8790,  6176.2954,  1714.4834],\n",
      "        [  771.4189,  6081.8896,  1272.3773],\n",
      "        [ 1091.7618,  6020.0806,  1808.3171],\n",
      "        [  968.3461,  5958.0425,  1620.4418],\n",
      "        [ 1032.5775,  5880.2979,  1757.8636],\n",
      "        [  962.8754,  5834.5249,  1645.9733],\n",
      "        [  616.2650,  5764.6392,  1069.3020],\n",
      "        [  918.9703,  5705.6621,  1604.6747],\n",
      "        [  975.9373,  5648.3496,  1722.3933],\n",
      "        [ 1676.4258,  5601.6494,  2981.6462],\n",
      "        [  853.8408,  5520.9053,  1543.5741],\n",
      "        [  968.4171,  5427.7002,  1798.9980],\n",
      "        [ 1431.3717,  5388.2554,  2658.4707],\n",
      "        [ 1278.6572,  5376.6543,  2362.8179],\n",
      "        [ 1321.9302,  5370.7280,  2436.5342],\n",
      "        [ 1250.2501,  5284.2480,  2365.5854],\n",
      "        [ 1013.5074,  5250.3330,  1921.0217],\n",
      "        [  582.8804,  5182.7148,  1130.3621],\n",
      "        [  824.6390,  5176.5542,  1585.3999],\n",
      "        [  938.6585,  5121.8389,  1828.6123],\n",
      "        [  842.0236,  5098.8154,  1644.8652],\n",
      "        [ 1271.4542,  5044.0664,  2508.3042]])\n",
      "data:  tensor([[[0.0024, 0.0021, 0.0021,  ..., 0.5657, 0.1941, 1.0000]],\n",
      "\n",
      "        [[0.0034, 0.0037, 0.0041,  ..., 0.5713, 0.1941, 1.0000]],\n",
      "\n",
      "        [[0.0039, 0.0034, 0.0033,  ..., 0.5780, 0.1941, 1.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0109, 0.0110, 0.0110,  ..., 0.6726, 0.1983, 0.4783]],\n",
      "\n",
      "        [[0.0099, 0.0101, 0.0101,  ..., 0.6694, 0.1983, 0.4783]],\n",
      "\n",
      "        [[0.0142, 0.0149, 0.0134,  ..., 0.6676, 0.1983, 0.4783]]])\n",
      "output:  tensor([[1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190],\n",
      "        [1.4435, 6.5157, 5.3190]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 821.3783, 4988.0103, 1647.4280],\n",
      "        [1108.6047, 4948.8599, 2239.9604],\n",
      "        [ 843.4735, 4894.9004, 1730.5238],\n",
      "        [1287.5391, 4897.8047, 2616.5930],\n",
      "        [ 862.1845, 4846.2563, 1783.9688],\n",
      "        [1274.9905, 4818.1221, 2649.2505],\n",
      "        [1215.7009, 4809.3081, 2512.6433],\n",
      "        [1180.6962, 4771.6450, 2469.8713],\n",
      "        [1411.7751, 4758.7593, 2952.2185],\n",
      "        [1303.7273, 4712.4839, 2757.4338],\n",
      "        [1118.2727, 4683.2998, 2380.9988],\n",
      "        [1268.4192, 4630.7681, 2725.3901],\n",
      "        [1175.2428, 4625.1821, 2528.9705],\n",
      "        [1164.9319, 4576.1978, 2544.7058],\n",
      "        [1097.4436, 4537.9316, 2417.9851],\n",
      "        [ 895.4954, 4504.4966, 1995.6558],\n",
      "        [1070.9746, 4486.1763, 2388.4768],\n",
      "        [1097.2345, 4434.7007, 2480.4011],\n",
      "        [ 902.4112, 4415.5254, 2052.5913],\n",
      "        [1121.2455, 4414.1460, 2515.8364],\n",
      "        [1247.6042, 4380.4600, 2843.5793],\n",
      "        [ 821.9077, 4360.2681, 1877.3595],\n",
      "        [ 862.0040, 4329.7549, 1989.9143],\n",
      "        [ 766.0619, 4337.0659, 1759.7938],\n",
      "        [ 912.2959, 4325.4595, 2102.2388],\n",
      "        [ 738.0225, 4319.1421, 1687.6311],\n",
      "        [1231.5104, 4307.0664, 2848.0349],\n",
      "        [1004.2836, 4275.6035, 2348.8145],\n",
      "        [ 927.9679, 4223.1948, 2215.4246],\n",
      "        [1767.8971, 4323.3037, 4055.9678],\n",
      "        [1113.0753, 4257.8975, 2602.4558],\n",
      "        [1181.1301, 4238.0913, 2764.9653],\n",
      "        [1322.2905, 4216.8608, 3128.0457],\n",
      "        [1048.3604, 4204.9526, 2476.7102],\n",
      "        [ 809.0089, 4197.0518, 1908.0629],\n",
      "        [ 873.7955, 4146.1821, 2116.7878],\n",
      "        [ 897.6331, 4098.3760, 2201.9031],\n",
      "        [ 824.5103, 4082.5798, 2037.0076],\n",
      "        [ 877.7079, 4100.8110, 2137.7537],\n",
      "        [ 947.1971, 4072.4814, 2328.7625],\n",
      "        [ 653.4030, 4075.5981, 1581.9379],\n",
      "        [1007.9171, 4039.6833, 2488.0854],\n",
      "        [1171.3092, 4023.8093, 2892.2771],\n",
      "        [ 790.7073, 3995.2620, 1973.8531],\n",
      "        [1228.2833, 3947.2546, 3128.7349],\n",
      "        [ 809.0428, 3949.2766, 2042.3490],\n",
      "        [ 938.6421, 3934.3579, 2381.7979],\n",
      "        [ 796.2734, 3923.8347, 2026.5813],\n",
      "        [1160.4852, 3917.7969, 2940.3894],\n",
      "        [ 819.2861, 3876.5984, 2108.3481],\n",
      "        [ 828.2057, 3837.8909, 2170.5322],\n",
      "        [1108.2571, 3778.9639, 2954.6851],\n",
      "        [1093.1906, 3760.2190, 2946.0422],\n",
      "        [1115.8881, 3779.0364, 2971.1125],\n",
      "        [ 833.3715, 3776.5342, 2200.6858],\n",
      "        [1053.0813, 3765.1455, 2796.6970],\n",
      "        [1085.4987, 3769.0664, 2855.1721],\n",
      "        [ 479.0839, 3728.7727, 1289.5259],\n",
      "        [ 672.8422, 3719.2830, 1818.4050],\n",
      "        [ 823.8949, 3758.7332, 2154.8416],\n",
      "        [ 821.1484, 3733.7996, 2166.8853],\n",
      "        [ 833.8625, 3704.8220, 2243.2041],\n",
      "        [ 762.3131, 3700.1973, 2052.2307],\n",
      "        [ 852.9766, 3692.9363, 2300.3638],\n",
      "        [ 982.3634, 3667.6562, 2672.5967],\n",
      "        [1258.7800, 3642.2759, 3447.7202],\n",
      "        [ 939.1446, 3626.8486, 2577.7080],\n",
      "        [ 458.5611, 3595.7820, 1300.8134],\n",
      "        [ 538.2216, 3619.5703, 1470.5564],\n",
      "        [ 732.2450, 3596.2737, 2035.8724],\n",
      "        [ 688.8778, 3605.6238, 1879.5439],\n",
      "        [ 690.2936, 3567.5195, 1945.9764],\n",
      "        [ 732.2301, 3545.3730, 2076.8083],\n",
      "        [ 944.5908, 3589.4817, 2597.9214],\n",
      "        [ 763.5942, 3553.9246, 2138.6580],\n",
      "        [ 724.8331, 3529.3928, 2063.5715],\n",
      "        [ 675.4495, 3531.0852, 1899.0775],\n",
      "        [ 805.6741, 3475.6829, 2325.5242],\n",
      "        [ 935.7378, 3461.8645, 2698.2651],\n",
      "        [ 610.7928, 3455.4634, 1772.9545],\n",
      "        [ 937.2040, 3461.2722, 2679.3530],\n",
      "        [ 648.1005, 3403.1665, 1922.6261],\n",
      "        [ 899.5548, 3421.6208, 2612.7874],\n",
      "        [1055.0339, 3361.9399, 3149.8879],\n",
      "        [ 950.9581, 3385.9072, 2787.3899],\n",
      "        [ 610.4526, 3381.5283, 1786.5044],\n",
      "        [ 836.8167, 3372.4797, 2471.8093],\n",
      "        [ 841.4745, 3343.2229, 2518.6445],\n",
      "        [ 874.4945, 3333.2583, 2619.1223],\n",
      "        [ 837.5982, 3330.9963, 2497.1033],\n",
      "        [ 532.0739, 3291.2109, 1618.8987],\n",
      "        [1035.8057, 3259.9265, 3197.1470],\n",
      "        [1049.6947, 3252.5984, 3244.9302],\n",
      "        [ 795.2208, 3253.5591, 2450.0022],\n",
      "        [ 988.8832, 3255.1118, 3035.3535],\n",
      "        [1135.2910, 3247.6050, 3476.9116],\n",
      "        [ 995.2570, 3285.3127, 2984.6531],\n",
      "        [ 818.9649, 3277.6492, 2455.0488],\n",
      "        [ 733.7596, 3238.8916, 2260.2649],\n",
      "        [ 874.9136, 3141.7991, 2847.5759],\n",
      "        [ 915.7297, 3227.2124, 2801.4331],\n",
      "        [ 822.0106, 3181.2380, 2582.2944],\n",
      "        [ 637.1916, 3148.5859, 2024.5052],\n",
      "        [1160.9167, 3192.6995, 3598.1521],\n",
      "        [1000.6687, 3138.4084, 3176.6655],\n",
      "        [ 554.0005, 3113.6926, 1791.4380],\n",
      "        [ 911.1112, 3128.0698, 2899.6509],\n",
      "        [ 702.6019, 3157.8125, 2185.4558],\n",
      "        [ 758.2365, 3095.2998, 2491.9915],\n",
      "        [ 519.3995, 3127.3438, 1679.7245],\n",
      "        [ 948.1505, 3164.9453, 2974.9482],\n",
      "        [ 748.1425, 3181.1265, 2315.9524],\n",
      "        [ 790.1945, 3176.8535, 2466.6072],\n",
      "        [ 816.0917, 3192.1523, 2540.6653],\n",
      "        [ 560.6935, 3184.8127, 1760.9132],\n",
      "        [ 643.5741, 3174.5969, 2042.0018],\n",
      "        [ 425.2755, 3220.7444, 1317.0903],\n",
      "        [ 630.5861, 3239.9863, 1936.2089],\n",
      "        [ 422.0713, 3239.4668, 1307.6254],\n",
      "        [ 684.5504, 3269.8630, 2084.5278],\n",
      "        [ 462.3611, 3281.3523, 1400.1735],\n",
      "        [ 538.3714, 3306.7498, 1621.3069],\n",
      "        [ 456.6385, 3296.7068, 1403.7407],\n",
      "        [ 727.2560, 3347.6858, 2169.7690],\n",
      "        [ 648.9219, 3359.7952, 1929.5568],\n",
      "        [ 586.8260, 3381.0925, 1731.0607],\n",
      "        [ 754.8967, 3380.6345, 2237.2271],\n",
      "        [ 277.5876, 3400.8206,  819.3780],\n",
      "        [ 396.2627, 3444.9517, 1140.4451],\n",
      "        [ 453.7411, 3440.2595, 1313.1860],\n",
      "        [ 621.4042, 3478.1685, 1779.3098],\n",
      "        [ 375.8974, 3482.3408, 1073.2811],\n",
      "        [ 426.3011, 3498.9241, 1221.4537],\n",
      "        [ 384.5209, 3532.5767, 1083.3341],\n",
      "        [ 420.8024, 3578.0706, 1166.0195],\n",
      "        [ 437.7317, 3601.4514, 1215.5801],\n",
      "        [ 531.4604, 3673.6555, 1440.4883],\n",
      "        [ 275.0639, 3738.4802,  732.5969],\n",
      "        [ 492.1058, 3792.8103, 1301.3448],\n",
      "        [ 350.6710, 3866.1758,  909.6315],\n",
      "        [ 228.5378, 3959.8054,  576.7513],\n",
      "        [ 457.1751, 4076.0171, 1118.5376],\n",
      "        [ 241.2169, 4162.3130,  577.0723],\n",
      "        [ 434.2912, 4264.4438, 1016.8410],\n",
      "        [ 314.5051, 4364.6582,  723.3860],\n",
      "        [ 367.9478, 4467.2080,  828.7759],\n",
      "        [ 478.6178, 4563.3345, 1059.2739],\n",
      "        [ 527.8558, 4664.6943, 1132.9789],\n",
      "        [ 398.0917, 4754.7769,  839.4022],\n",
      "        [ 385.3914, 4844.6309,  795.3777]])\n",
      "data:  tensor([[[0.0101, 0.0103, 0.0117,  ..., 0.6650, 0.1983, 0.4783]],\n",
      "\n",
      "        [[0.0114, 0.0114, 0.0101,  ..., 0.6619, 0.1983, 0.5217]],\n",
      "\n",
      "        [[0.0112, 0.0115, 0.0128,  ..., 0.6598, 0.1983, 0.5217]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0061, 0.0059, 0.0053,  ..., 0.6168, 0.2025, 0.0000]],\n",
      "\n",
      "        [[0.0043, 0.0039, 0.0043,  ..., 0.6265, 0.2025, 0.0000]],\n",
      "\n",
      "        [[0.0048, 0.0054, 0.0056,  ..., 0.6359, 0.2025, 0.0000]]])\n",
      "output:  tensor([[1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716],\n",
      "        [1.4962, 6.5684, 5.3716]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 480.6329, 4908.6743,  980.7491],\n",
      "        [ 362.3749, 4983.1455,  727.2236],\n",
      "        [ 318.5427, 5054.9468,  628.3798],\n",
      "        [ 535.6085, 5116.3091, 1048.1616],\n",
      "        [ 373.6388, 5192.5454,  719.4636],\n",
      "        [ 372.0710, 5256.3706,  708.9479],\n",
      "        [ 544.9114, 5325.0332, 1023.8029],\n",
      "        [ 639.7198, 5358.3394, 1198.0007],\n",
      "        [ 608.7979, 5431.5767, 1121.1171],\n",
      "        [ 505.3149, 5504.1211,  918.3428],\n",
      "        [ 544.7927, 5553.2285,  983.2387],\n",
      "        [ 621.2368, 5611.9658, 1109.9036],\n",
      "        [ 592.3774, 5671.3228, 1045.1925],\n",
      "        [ 528.3801, 5733.2266,  923.5672],\n",
      "        [ 492.0322, 5804.6372,  848.1232],\n",
      "        [ 490.1250, 5846.6289,  839.8229],\n",
      "        [ 500.9970, 5917.2910,  844.8481],\n",
      "        [ 757.2839, 5961.3882, 1269.5269],\n",
      "        [ 709.5390, 6002.0396, 1187.6763],\n",
      "        [ 602.7265, 6062.1772,  994.8882],\n",
      "        [ 392.3061, 6099.6572,  644.0457],\n",
      "        [ 519.1025, 6142.0229,  844.0945],\n",
      "        [ 538.8618, 6189.4717,  871.3969],\n",
      "        [ 721.6759, 6221.6665, 1162.9342],\n",
      "        [ 598.2361, 6269.6235,  956.5665],\n",
      "        [ 549.3154, 6311.3418,  871.0989],\n",
      "        [ 542.5179, 6360.4995,  852.2791],\n",
      "        [ 765.6883, 6378.8032, 1200.6487],\n",
      "        [ 449.3746, 6416.1421,  700.9990],\n",
      "        [ 379.0019, 6460.6362,  586.5794],\n",
      "        [ 656.5550, 6474.1699, 1014.7156],\n",
      "        [ 452.3741, 6489.6807,  696.2311],\n",
      "        [ 471.9660, 6501.6001,  726.8188],\n",
      "        [ 653.7436, 6516.1670, 1003.4232],\n",
      "        [ 377.1700, 6522.0820,  578.8129],\n",
      "        [ 675.3535, 6537.3418, 1032.8862],\n",
      "        [ 524.9663, 6542.6255,  801.2074],\n",
      "        [ 410.3784, 6528.9019,  629.3412],\n",
      "        [ 336.6600, 6494.2051,  517.7120],\n",
      "        [ 452.5002, 6458.8882,  701.6068],\n",
      "        [ 664.2247, 6401.5024, 1038.2029],\n",
      "        [ 364.4528, 6334.3037,  574.8854],\n",
      "        [ 594.2457, 6240.3105,  952.5195],\n",
      "        [ 249.9106, 6126.2944,  408.3870],\n",
      "        [ 508.6191, 5945.3467,  854.0040],\n",
      "        [ 421.3536, 5765.5161,  730.6674],\n",
      "        [ 426.5020, 5543.5879,  770.1059],\n",
      "        [ 443.8751, 5320.5649,  832.5153],\n",
      "        [ 549.9012, 5099.3013, 1074.7430],\n",
      "        [ 367.3690, 4873.3848,  757.5549],\n",
      "        [ 378.3128, 4671.5698,  811.9988],\n",
      "        [ 493.6895, 4482.2402, 1099.3209],\n",
      "        [ 381.2954, 4276.0000,  891.5400],\n",
      "        [ 503.7156, 4074.8049, 1234.1968],\n",
      "        [ 326.7471, 3863.6958,  844.8259],\n",
      "        [ 341.8031, 3648.0457,  929.3157],\n",
      "        [ 379.1582, 3470.1812, 1086.2170],\n",
      "        [ 363.2092, 3306.8000, 1091.7888],\n",
      "        [ 345.5292, 3168.6538, 1087.0161],\n",
      "        [ 357.5709, 3051.6379, 1166.5510],\n",
      "        [ 251.6090, 2932.7397,  854.6675],\n",
      "        [ 285.9544, 2848.2646, 1004.2462],\n",
      "        [ 343.9492, 2763.5383, 1241.5378],\n",
      "        [ 297.8437, 2667.6968, 1116.3037],\n",
      "        [ 244.5498, 2605.8264,  935.3170],\n",
      "        [ 294.7811, 2534.5669, 1156.2991],\n",
      "        [ 351.1409, 2479.2537, 1406.4254],\n",
      "        [ 302.9023, 2400.5317, 1255.7562],\n",
      "        [ 280.0743, 2341.8171, 1190.4336],\n",
      "        [ 307.6803, 2284.0239, 1338.6761],\n",
      "        [ 259.2560, 2218.8022, 1162.5663],\n",
      "        [ 317.2608, 2166.4604, 1459.2737],\n",
      "        [ 204.0877, 2099.3064,  965.2219],\n",
      "        [ 265.0121, 2053.8123, 1284.2640],\n",
      "        [ 306.0991, 2015.6523, 1508.6925],\n",
      "        [ 229.0411, 1954.6923, 1168.4470],\n",
      "        [ 222.3062, 1932.5347, 1148.5994],\n",
      "        [ 199.7953, 1911.5835, 1042.3367],\n",
      "        [ 246.3353, 1885.9404, 1299.0464],\n",
      "        [ 240.0962, 1866.1566, 1278.5090],\n",
      "        [ 185.4988, 1846.2220, 1000.2750],\n",
      "        [ 260.5211, 1858.2589, 1386.1079],\n",
      "        [ 275.2223, 1846.2484, 1476.0287],\n",
      "        [ 244.3758, 1849.8358, 1313.9929],\n",
      "        [ 239.0362, 1847.6240, 1282.2246],\n",
      "        [ 256.3618, 1839.4246, 1385.0015],\n",
      "        [ 318.8637, 1839.9049, 1715.6824],\n",
      "        [ 237.6095, 1852.3103, 1272.6960],\n",
      "        [ 261.3756, 1860.5153, 1395.7959],\n",
      "        [ 268.9367, 1876.3110, 1420.4327],\n",
      "        [ 231.1611, 1890.7495, 1217.1146],\n",
      "        [ 229.2718, 1934.2438, 1176.5575],\n",
      "        [ 280.2905, 1990.9868, 1395.1179],\n",
      "        [ 240.1886, 2039.6643, 1169.6436],\n",
      "        [ 217.3100, 2077.8804, 1050.0601],\n",
      "        [ 269.1323, 2141.5662, 1245.2495],\n",
      "        [ 308.6481, 2207.6479, 1389.5901],\n",
      "        [ 206.1671, 2254.7227,  921.3901],\n",
      "        [ 297.4051, 2362.6453, 1252.0924],\n",
      "        [ 397.2359, 2473.9753, 1596.3721],\n",
      "        [ 328.6321, 2555.6855, 1277.0066],\n",
      "        [ 477.7276, 2659.6426, 1785.8551],\n",
      "        [ 392.2869, 2650.9014, 1489.8168],\n",
      "        [ 407.0061, 2739.3083, 1473.5463],\n",
      "        [ 424.6556, 2772.1736, 1526.3013],\n",
      "        [ 365.1578, 2809.9089, 1296.4900],\n",
      "        [ 348.1763, 2833.8933, 1225.0801],\n",
      "        [ 530.2277, 2857.2183, 1847.0576],\n",
      "        [ 397.9442, 2886.2012, 1378.7383],\n",
      "        [ 454.3904, 2922.7861, 1548.6287],\n",
      "        [ 388.4367, 2943.7766, 1318.9330],\n",
      "        [ 456.3343, 2945.2400, 1561.6982],\n",
      "        [ 441.2422, 2960.0750, 1495.6140],\n",
      "        [ 639.6134, 3019.2041, 2109.1516],\n",
      "        [ 624.0063, 3015.4104, 2075.8916],\n",
      "        [ 693.6243, 3056.6091, 2264.7732],\n",
      "        [ 385.9162, 3061.6528, 1262.9635],\n",
      "        [ 491.4577, 3048.2141, 1628.8606],\n",
      "        [ 577.4983, 3116.6028, 1832.2394],\n",
      "        [ 908.4545, 3147.6248, 2863.3757],\n",
      "        [ 598.6931, 3114.6870, 1923.5900],\n",
      "        [ 595.3511, 3146.8762, 1877.6661],\n",
      "        [ 710.9861, 3160.1809, 2230.8665],\n",
      "        [ 446.6277, 3162.7612, 1407.6785],\n",
      "        [ 482.3493, 3167.4644, 1517.2731],\n",
      "        [ 447.1810, 3189.7783, 1393.8242],\n",
      "        [ 626.7617, 3171.9495, 1982.7979],\n",
      "        [ 904.4763, 3230.2063, 2789.0081],\n",
      "        [ 689.3386, 3207.0151, 2152.0520],\n",
      "        [ 829.8795, 3204.9653, 2603.0938],\n",
      "        [ 812.1168, 3274.1807, 2455.4648],\n",
      "        [ 850.8940, 3252.9614, 2611.5891],\n",
      "        [ 421.2183, 3251.4980, 1314.8749],\n",
      "        [ 661.1970, 3278.7551, 2008.7021],\n",
      "        [ 586.1439, 3285.9548, 1778.9786],\n",
      "        [ 592.7715, 3306.5264, 1777.6078],\n",
      "        [1119.2902, 3317.9819, 3355.6597],\n",
      "        [ 524.0041, 3333.4932, 1545.7968],\n",
      "        [ 969.0253, 3336.3025, 2887.2886],\n",
      "        [ 831.3333, 3291.2446, 2543.4741],\n",
      "        [ 677.3577, 3352.0703, 1997.5175],\n",
      "        [ 416.9553, 3328.1030, 1248.8367],\n",
      "        [ 734.5421, 3340.5923, 2182.5879],\n",
      "        [ 776.8687, 3314.8208, 2344.3779],\n",
      "        [ 672.4350, 3354.6455, 1979.5961],\n",
      "        [ 700.4871, 3329.4211, 2100.7356],\n",
      "        [ 784.0561, 3317.9917, 2359.3130],\n",
      "        [ 629.1561, 3311.9331, 1900.6603],\n",
      "        [ 640.0028, 3301.0300, 1957.9849],\n",
      "        [ 530.6790, 3337.1460, 1581.1969]])\n",
      "data:  tensor([[[0.0054, 0.0054, 0.0053,  ..., 0.6440, 0.2025, 0.0000]],\n",
      "\n",
      "        [[0.0044, 0.0044, 0.0039,  ..., 0.6521, 0.2025, 0.0000]],\n",
      "\n",
      "        [[0.0038, 0.0037, 0.0044,  ..., 0.6608, 0.2025, 0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0070, 0.0068, 0.0068,  ..., 0.6538, 0.2025, 0.5217]],\n",
      "\n",
      "        [[0.0081, 0.0082, 0.0073,  ..., 0.6512, 0.2025, 0.5217]],\n",
      "\n",
      "        [[0.0060, 0.0059, 0.0068,  ..., 0.6488, 0.2025, 0.5217]]])\n",
      "output:  tensor([[1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242],\n",
      "        [1.5488, 6.6210, 5.4242]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[1092.7123, 3364.8555, 3214.5549],\n",
      "        [1044.3370, 3308.3528, 3153.2068],\n",
      "        [ 551.1252, 3310.8774, 1666.5863],\n",
      "        [ 805.4915, 3341.8601, 2393.9658],\n",
      "        [ 549.4387, 3336.7830, 1623.2117],\n",
      "        [ 632.1786, 3332.0242, 1889.4788],\n",
      "        [ 588.6403, 3285.0557, 1822.5970],\n",
      "        [ 514.7443, 3311.0674, 1560.9487],\n",
      "        [ 700.1512, 3306.9285, 2126.6135],\n",
      "        [ 805.0834, 3271.3936, 2492.3259],\n",
      "        [ 892.7225, 3346.4124, 2639.7520],\n",
      "        [ 965.7017, 3345.4292, 2863.0571],\n",
      "        [ 962.3019, 3302.4880, 2924.4812],\n",
      "        [ 938.6746, 3346.4626, 2789.2996],\n",
      "        [ 868.6380, 3334.8391, 2587.1411],\n",
      "        [ 699.1602, 3279.0344, 2168.9932],\n",
      "        [ 730.5800, 3310.5925, 2217.2178],\n",
      "        [ 971.0642, 3279.3752, 2972.2073],\n",
      "        [ 810.2594, 3272.3206, 2498.2690],\n",
      "        [ 938.8232, 3324.6177, 2792.8933],\n",
      "        [ 675.5128, 3292.9202, 2040.3933],\n",
      "        [ 621.9501, 3274.7114, 1901.0238],\n",
      "        [ 626.2690, 3291.5684, 1888.0862],\n",
      "        [ 665.1985, 3255.0225, 2055.7932],\n",
      "        [ 830.1585, 3276.5544, 2539.2488],\n",
      "        [ 728.5126, 3247.6831, 2261.5093],\n",
      "        [ 927.1953, 3238.9531, 2875.0212],\n",
      "        [ 402.2897, 3242.1423, 1244.4208],\n",
      "        [ 842.6401, 3256.4746, 2583.8032],\n",
      "        [ 559.8068, 3262.2683, 1701.4091],\n",
      "        [ 497.5244, 3250.6379, 1518.6440],\n",
      "        [ 820.2419, 3235.3503, 2538.1335],\n",
      "        [ 749.6122, 3236.1770, 2306.5010],\n",
      "        [ 587.0786, 3220.3757, 1826.0598],\n",
      "        [ 751.4626, 3237.9978, 2305.4495],\n",
      "        [ 808.8241, 3214.9297, 2518.4756],\n",
      "        [ 704.9448, 3205.0994, 2194.7542],\n",
      "        [ 630.4619, 3204.0938, 1959.3456],\n",
      "        [ 752.5577, 3196.2632, 2329.2068],\n",
      "        [ 886.9246, 3166.9226, 2820.4226],\n",
      "        [ 854.7438, 3173.8040, 2684.5208],\n",
      "        [ 817.4393, 3166.1294, 2585.4260],\n",
      "        [ 803.2277, 3195.0735, 2464.2039],\n",
      "        [ 869.4599, 3130.4995, 2800.0374],\n",
      "        [ 603.1256, 3156.2432, 1904.3696],\n",
      "        [ 492.0734, 3128.4497, 1579.4110],\n",
      "        [ 609.1688, 3119.3677, 1966.6110],\n",
      "        [ 805.2101, 3155.5059, 2524.6406],\n",
      "        [ 888.2579, 3113.4692, 2847.3914],\n",
      "        [ 941.4993, 3120.1162, 3019.7913],\n",
      "        [ 759.5415, 3117.2620, 2413.7732],\n",
      "        [1008.3844, 3132.0691, 3184.8501],\n",
      "        [ 796.4244, 3106.5547, 2545.8457],\n",
      "        [ 689.3046, 3108.6882, 2193.0757],\n",
      "        [ 812.5153, 3094.4060, 2592.6541],\n",
      "        [ 699.8604, 3109.6379, 2191.5955],\n",
      "        [ 766.6588, 3020.5715, 2584.2415],\n",
      "        [ 512.1252, 3045.4158, 1684.4565],\n",
      "        [ 465.3226, 3038.1323, 1526.5125],\n",
      "        [ 963.1570, 3047.4656, 3128.5874],\n",
      "        [ 832.4333, 3029.3018, 2730.8276],\n",
      "        [ 606.5265, 2988.5332, 2049.0537],\n",
      "        [ 826.4324, 2978.0857, 2792.0515],\n",
      "        [ 607.0711, 3005.4570, 1994.9371],\n",
      "        [ 871.1640, 2978.9204, 2911.0586],\n",
      "        [ 439.4179, 2984.9136, 1455.2124],\n",
      "        [ 553.3373, 2948.9822, 1886.1854],\n",
      "        [1005.1792, 2940.2800, 3419.8259],\n",
      "        [ 536.5007, 2909.1858, 1866.4128],\n",
      "        [ 891.7243, 2933.9573, 3014.8853],\n",
      "        [ 640.2274, 2911.5596, 2190.1814],\n",
      "        [ 965.7278, 2904.2927, 3309.1091],\n",
      "        [ 768.0000, 2874.2820, 2685.9514],\n",
      "        [ 823.1478, 2845.8518, 2897.8767],\n",
      "        [1008.7498, 2932.6670, 3390.8601],\n",
      "        [ 766.2678, 2848.6836, 2681.3914],\n",
      "        [ 711.8830, 2809.3169, 2568.4487],\n",
      "        [1021.0657, 2867.4285, 3530.3491],\n",
      "        [ 245.9019, 2832.9717,  846.3068],\n",
      "        [ 836.8077, 2805.5413, 2973.4292],\n",
      "        [ 466.4622, 2807.8313, 1635.3765],\n",
      "        [ 521.9258, 2762.0137, 1911.0819],\n",
      "        [ 895.1431, 2780.1440, 3186.5754],\n",
      "        [ 899.4641, 2767.9622, 3246.4612],\n",
      "        [ 812.8668, 2765.0632, 2916.5452],\n",
      "        [ 619.0220, 2752.4680, 2231.5110],\n",
      "        [ 636.7452, 2775.4578, 2246.6560],\n",
      "        [ 669.5414, 2744.4583, 2433.2439],\n",
      "        [ 434.7900, 2747.0332, 1526.7600],\n",
      "        [ 673.8286, 2742.1404, 2439.4729],\n",
      "        [ 681.5692, 2739.7554, 2453.7622],\n",
      "        [ 685.3650, 2725.6240, 2531.8848],\n",
      "        [ 933.4809, 2667.2893, 3547.6721],\n",
      "        [ 572.5350, 2754.8921, 2075.2517],\n",
      "        [ 736.6693, 2747.9658, 2691.9995],\n",
      "        [ 701.2618, 2764.0857, 2538.4434],\n",
      "        [ 655.3729, 2765.5254, 2364.6333],\n",
      "        [ 413.2029, 2750.0103, 1520.0668],\n",
      "        [ 636.5612, 2780.2668, 2260.8774],\n",
      "        [ 457.9162, 2797.4980, 1621.4176],\n",
      "        [ 775.6478, 2785.2490, 2798.6711],\n",
      "        [ 629.9481, 2778.3735, 2298.6350],\n",
      "        [ 447.9655, 2852.4929, 1525.2880],\n",
      "        [ 355.5069, 2830.5195, 1249.9081],\n",
      "        [ 616.5798, 2832.8318, 2188.5498],\n",
      "        [ 752.1690, 2872.2544, 2597.0171],\n",
      "        [ 579.2186, 2851.9121, 2045.4042],\n",
      "        [ 427.2684, 2870.6458, 1501.1587],\n",
      "        [ 691.0846, 2897.5566, 2376.6523],\n",
      "        [ 607.0378, 2899.5452, 2106.6355],\n",
      "        [ 642.0795, 2898.6289, 2226.3232],\n",
      "        [ 574.6838, 2939.1294, 1946.8583],\n",
      "        [ 451.7225, 2928.9751, 1551.6710],\n",
      "        [ 724.7038, 2953.2607, 2449.9121],\n",
      "        [ 786.0378, 2999.8940, 2594.4663],\n",
      "        [ 515.8013, 2970.3916, 1742.1531],\n",
      "        [ 528.5246, 3004.8203, 1747.0046],\n",
      "        [ 477.2139, 3007.0713, 1574.7244],\n",
      "        [ 478.8615, 2990.7898, 1589.3466],\n",
      "        [ 396.2766, 2949.9541, 1356.0889],\n",
      "        [ 532.0800, 2973.4634, 1781.7620],\n",
      "        [ 331.8081, 2959.6562, 1121.8794],\n",
      "        [ 372.3640, 2954.6516, 1260.6158],\n",
      "        [ 411.5438, 2961.4993, 1394.4528],\n",
      "        [ 468.8850, 2943.1902, 1603.3857],\n",
      "        [ 216.5124, 2991.3928,  727.0444],\n",
      "        [ 230.9867, 3013.3271,  769.2081],\n",
      "        [ 209.8195, 3041.9641,  687.0547],\n",
      "        [ 446.3648, 3074.8792, 1447.7152],\n",
      "        [ 182.9320, 3075.1582,  590.7562],\n",
      "        [ 252.5693, 3069.4448,  826.5870],\n",
      "        [ 376.1820, 3062.3286, 1229.2448],\n",
      "        [ 401.3523, 3059.9158, 1308.1321],\n",
      "        [ 180.8228, 3031.5471,  595.4999],\n",
      "        [ 266.3036, 2984.3608,  888.3159],\n",
      "        [ 228.5267, 2936.7778,  778.5637],\n",
      "        [ 173.0423, 2884.1013,  599.8911],\n",
      "        [ 274.8738, 2809.4175,  978.8541],\n",
      "        [ 251.1114, 2735.6165,  914.9724],\n",
      "        [ 174.2959, 2659.6594,  657.2324],\n",
      "        [ 269.9669, 2658.8718, 1005.7565],\n",
      "        [ 397.6423, 3540.2959, 1115.9246],\n",
      "        [1175.7109, 5719.1680, 1974.0930],\n",
      "        [ 998.2715, 7386.6548, 1348.2930],\n",
      "        [ 748.3725, 7106.2744, 1052.0923],\n",
      "        [ 761.4850, 6833.9980, 1118.0803],\n",
      "        [ 478.4697, 6576.1216,  728.3428],\n",
      "        [ 656.6020, 6378.9092, 1028.9969],\n",
      "        [ 451.5097, 6434.0659,  701.4746],\n",
      "        [ 635.1575, 6535.1074,  970.1722]])\n",
      "data:  tensor([[[0.0125, 0.0125, 0.0114,  ..., 0.6466, 0.2025, 0.5217]],\n",
      "\n",
      "        [[0.0127, 0.0120, 0.0131,  ..., 0.6441, 0.2025, 0.5652]],\n",
      "\n",
      "        [[0.0058, 0.0064, 0.0064,  ..., 0.6423, 0.2025, 0.5652]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0083, 0.0078, 0.0071,  ..., 0.6626, 0.2068, 0.0435]],\n",
      "\n",
      "        [[0.0053, 0.0057, 0.0061,  ..., 0.6566, 0.2068, 0.0435]],\n",
      "\n",
      "        [[0.0068, 0.0067, 0.0063,  ..., 0.6555, 0.2068, 0.0435]]])\n",
      "output:  tensor([[1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768],\n",
      "        [1.6013, 6.6735, 5.4768]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 700.2421, 6605.9146, 1060.4364],\n",
      "        [ 681.0417, 6657.0107, 1025.7561],\n",
      "        [ 618.2584, 6734.3867,  917.6498],\n",
      "        [ 502.6166, 6770.8433,  742.4210],\n",
      "        [ 650.7099, 6820.9341,  954.3820],\n",
      "        [ 643.3099, 6873.1914,  939.2802],\n",
      "        [ 633.4230, 6934.3135,  913.8672],\n",
      "        [ 670.2668, 6991.7383,  959.1691],\n",
      "        [ 845.5805, 7053.7988, 1198.7893],\n",
      "        [ 476.1030, 7126.1309,  669.2362],\n",
      "        [ 589.1783, 7172.3730,  822.5016],\n",
      "        [ 588.4117, 7197.6973,  818.9718],\n",
      "        [ 624.6868, 7226.6304,  865.7249],\n",
      "        [ 632.0413, 7254.3262,  871.9621],\n",
      "        [ 621.8438, 7277.0620,  854.4062],\n",
      "        [ 579.5104, 7275.7886,  796.9136],\n",
      "        [ 698.0336, 7305.6875,  956.1155],\n",
      "        [ 519.6348, 7329.2749,  709.4705],\n",
      "        [ 571.4452, 7350.3545,  778.5223],\n",
      "        [ 548.8424, 7375.4727,  744.5552],\n",
      "        [ 559.0953, 7379.2598,  758.2574],\n",
      "        [ 629.8938, 7402.4336,  850.5910],\n",
      "        [ 628.7446, 7416.1182,  848.7731],\n",
      "        [ 658.2569, 7446.5923,  882.5468],\n",
      "        [ 376.3018, 7465.2930,  503.8714],\n",
      "        [ 724.5692, 7484.5347,  968.7694],\n",
      "        [ 474.2255, 7513.1885,  631.3815],\n",
      "        [ 683.2673, 7547.1982,  906.0349],\n",
      "        [ 632.8001, 7597.5796,  832.3002],\n",
      "        [ 532.9062, 7643.6768,  697.4905],\n",
      "        [ 513.0682, 7698.5654,  666.8057],\n",
      "        [ 616.4191, 7743.2549,  796.0229],\n",
      "        [ 673.1194, 7793.0996,  863.5596],\n",
      "        [ 440.5268, 7850.6582,  560.8255],\n",
      "        [ 497.8040, 7912.9194,  629.9794],\n",
      "        [ 484.5110, 7978.2808,  606.9111],\n",
      "        [ 678.3719, 8049.3901,  844.4658],\n",
      "        [ 789.6123, 8113.8857,  977.1595],\n",
      "        [ 695.9987, 8173.9966,  852.9254],\n",
      "        [ 706.0396, 8248.0938,  856.7125],\n",
      "        [ 647.2352, 8302.2783,  779.8767],\n",
      "        [ 563.1431, 8329.1953,  677.0663],\n",
      "        [ 484.1857, 8390.2393,  577.4275],\n",
      "        [ 766.3450, 8434.8506,  909.8858],\n",
      "        [ 653.9492, 8487.2705,  772.2155],\n",
      "        [ 765.4075, 8542.2744,  896.7482],\n",
      "        [ 482.7604, 8586.3232,  563.1390],\n",
      "        [ 884.0490, 8633.3584, 1023.8218],\n",
      "        [ 974.4768, 8649.3389, 1127.7515],\n",
      "        [ 577.5273, 8664.7207,  667.0941],\n",
      "        [ 695.3182, 8672.3457,  802.2996],\n",
      "        [ 677.3585, 8661.0625,  782.9015],\n",
      "        [ 981.7914, 8657.1914, 1134.7300],\n",
      "        [1075.2130, 8676.6240, 1239.0596],\n",
      "        [ 813.6216, 8670.1953,  937.9840],\n",
      "        [ 474.9292, 8660.8115,  549.0353],\n",
      "        [ 400.4589, 8667.8770,  461.2473],\n",
      "        [ 825.0765, 8654.2432,  953.1736],\n",
      "        [ 757.1770, 8617.3730,  879.2654],\n",
      "        [ 740.3603, 8598.4883,  861.4820],\n",
      "        [ 723.3432, 8562.0596,  844.8378],\n",
      "        [ 946.6850, 8538.5225, 1108.9420],\n",
      "        [ 879.7429, 8502.3398, 1034.2555],\n",
      "        [ 577.9805, 8454.0361,  684.9763],\n",
      "        [ 553.8040, 8428.1143,  656.8740],\n",
      "        [ 733.4906, 8380.4756,  876.3841],\n",
      "        [ 682.9875, 8332.7256,  821.0660],\n",
      "        [ 820.3323, 8298.3799,  988.6144],\n",
      "        [ 939.2937, 8244.9209, 1138.8684],\n",
      "        [1002.5743, 8178.1577, 1226.0966],\n",
      "        [ 870.9855, 8125.1738, 1070.9592],\n",
      "        [ 683.4647, 8065.5210,  847.3105],\n",
      "        [ 751.7303, 7997.8916,  940.3637],\n",
      "        [ 823.9209, 7955.3691, 1034.8300],\n",
      "        [ 632.7405, 7895.7612,  802.4439],\n",
      "        [1125.4894, 7822.3008, 1439.4453],\n",
      "        [ 971.2419, 7756.3115, 1253.2057],\n",
      "        [ 778.2640, 7686.9473, 1012.0300],\n",
      "        [1080.7003, 7601.0259, 1421.3577],\n",
      "        [1040.3989, 7487.5454, 1390.7778],\n",
      "        [ 867.1763, 7411.4321, 1169.9320],\n",
      "        [ 810.6071, 7332.5762, 1103.9041],\n",
      "        [ 752.8056, 7229.3062, 1042.2434],\n",
      "        [ 603.0959, 7130.6216,  845.5068],\n",
      "        [ 849.4819, 7011.9634, 1209.2844],\n",
      "        [ 724.5469, 6887.6245, 1053.4338],\n",
      "        [ 813.7127, 6754.2432, 1204.4114],\n",
      "        [1114.8262, 6645.8345, 1675.1805],\n",
      "        [ 799.4573, 6501.3096, 1228.2435],\n",
      "        [ 770.2404, 6383.7070, 1203.5508],\n",
      "        [ 891.8035, 6261.8486, 1423.3619],\n",
      "        [1152.4149, 6157.1318, 1871.6525],\n",
      "        [ 832.4763, 6065.5020, 1369.9386],\n",
      "        [ 740.9008, 5973.7715, 1241.4509],\n",
      "        [ 725.6981, 5876.6899, 1233.2266],\n",
      "        [ 572.3254, 5820.6284,  979.4571],\n",
      "        [1113.9353, 5665.3013, 1961.3005],\n",
      "        [ 615.5036, 5587.0542, 1097.4960],\n",
      "        [ 995.3234, 5534.7070, 1789.9142],\n",
      "        [ 795.8190, 5460.2974, 1460.0680],\n",
      "        [ 835.7605, 5390.2329, 1556.7935],\n",
      "        [1121.1989, 5355.7310, 2088.4634],\n",
      "        [ 781.9835, 5281.6396, 1481.6528],\n",
      "        [ 993.1915, 5213.9658, 1901.5192],\n",
      "        [ 824.3060, 5179.9893, 1585.1219],\n",
      "        [1102.8013, 5136.2383, 2138.2556],\n",
      "        [ 867.5741, 5072.6138, 1709.8063],\n",
      "        [ 607.3672, 5027.6113, 1202.6707],\n",
      "        [1345.2511, 4966.5020, 2703.2922],\n",
      "        [ 906.7715, 4910.3105, 1848.0680],\n",
      "        [1005.8837, 4876.0771, 2059.3455],\n",
      "        [ 591.9284, 4802.9463, 1242.9901],\n",
      "        [ 966.3386, 4786.4355, 2010.8054],\n",
      "        [ 927.0554, 4741.8970, 1953.0530],\n",
      "        [ 944.4510, 4718.7393, 1981.2389],\n",
      "        [1014.5906, 4644.5474, 2185.0015],\n",
      "        [1164.6315, 4609.5376, 2524.3789],\n",
      "        [ 669.9387, 4585.9834, 1451.4077],\n",
      "        [ 870.9916, 4559.1006, 1889.0948],\n",
      "        [ 977.0558, 4527.5034, 2136.7563],\n",
      "        [1194.2886, 4496.7051, 2643.9685],\n",
      "        [ 889.6080, 4446.8042, 1996.9254],\n",
      "        [ 946.8943, 4429.7515, 2126.1140],\n",
      "        [1064.6427, 4382.9116, 2416.3218],\n",
      "        [1034.6978, 4367.5293, 2350.5991],\n",
      "        [ 903.3329, 4320.9072, 2075.7061],\n",
      "        [ 820.2922, 4252.7812, 1936.9673],\n",
      "        [1144.8372, 4252.8984, 2675.9409],\n",
      "        [ 935.5673, 4179.9351, 2262.2517],\n",
      "        [ 864.6201, 4192.8711, 2054.0796],\n",
      "        [ 747.6010, 4141.7974, 1825.7540],\n",
      "        [1019.9077, 4167.0605, 2443.2903],\n",
      "        [1022.6380, 4151.0522, 2440.6687],\n",
      "        [ 569.9097, 4101.7661, 1383.1173],\n",
      "        [ 821.8217, 4102.6040, 1990.2695],\n",
      "        [ 786.4643, 4071.0793, 1920.7588],\n",
      "        [1081.4854, 4084.0713, 2629.3088],\n",
      "        [ 852.1306, 4011.7896, 2137.6431],\n",
      "        [ 563.8405, 3986.7720, 1431.3602],\n",
      "        [1047.4624, 3979.7119, 2642.8662],\n",
      "        [1065.0863, 4001.2263, 2650.7065],\n",
      "        [ 863.3026, 3945.5623, 2189.9167],\n",
      "        [ 969.6383, 3927.0967, 2478.5210],\n",
      "        [ 686.2965, 3911.5076, 1757.1052],\n",
      "        [ 981.5776, 3893.1367, 2531.1462],\n",
      "        [ 684.7854, 3902.3418, 1750.3657],\n",
      "        [ 731.4976, 3907.4917, 1852.1514],\n",
      "        [ 878.1975, 3892.0813, 2232.5793],\n",
      "        [ 932.8588, 3866.2986, 2393.5388],\n",
      "        [1166.4012, 3821.9666, 3052.7241]])\n",
      "data:  tensor([[[0.0086, 0.0086, 0.0084,  ..., 0.6605, 0.2068, 0.0435]],\n",
      "\n",
      "        [[0.0079, 0.0075, 0.0083,  ..., 0.6665, 0.2068, 0.0435]],\n",
      "\n",
      "        [[0.0072, 0.0074, 0.0071,  ..., 0.6713, 0.2068, 0.0435]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0100, 0.0093, 0.0096,  ..., 0.6370, 0.2068, 0.5652]],\n",
      "\n",
      "        [[0.0101, 0.0106, 0.0095,  ..., 0.6360, 0.2068, 0.5652]],\n",
      "\n",
      "        [[0.0146, 0.0149, 0.0153,  ..., 0.6335, 0.2068, 0.5652]]])\n",
      "output:  tensor([[1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293],\n",
      "        [1.6539, 6.7261, 5.5293]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 853.8336, 3804.6738, 2242.3247],\n",
      "        [ 848.3470, 3794.1506, 2239.9163],\n",
      "        [ 798.5557, 3747.2327, 2137.0359],\n",
      "        [ 792.8796, 3764.9219, 2093.2661],\n",
      "        [ 485.5934, 3751.3213, 1282.4318],\n",
      "        [ 896.1025, 3721.6958, 2399.3391],\n",
      "        [ 835.7949, 3687.4736, 2260.0347],\n",
      "        [ 908.8287, 3643.5491, 2499.3906],\n",
      "        [ 868.5545, 3650.1680, 2362.0286],\n",
      "        [ 758.8952, 3585.6277, 2132.5796],\n",
      "        [ 748.6169, 3571.5020, 2114.3262],\n",
      "        [ 830.2809, 3568.0278, 2325.7400],\n",
      "        [ 868.5705, 3574.4846, 2412.2009],\n",
      "        [ 813.0980, 3531.0518, 2306.4844],\n",
      "        [ 593.7388, 3494.5227, 1725.6323],\n",
      "        [1174.5458, 3554.9353, 3266.5159],\n",
      "        [ 935.2396, 3469.5947, 2711.7371],\n",
      "        [ 722.9815, 3482.9944, 2066.4331],\n",
      "        [ 665.0818, 3452.2852, 1924.0697],\n",
      "        [ 937.0247, 3421.9448, 2728.5198],\n",
      "        [ 603.9611, 3411.7012, 1769.7742],\n",
      "        [ 935.6769, 3402.5576, 2745.6167],\n",
      "        [ 872.8821, 3431.1553, 2504.5925],\n",
      "        [1133.3788, 3372.8435, 3365.7856],\n",
      "        [ 786.3746, 3337.4922, 2384.1924],\n",
      "        [ 798.9507, 3355.1201, 2373.0681],\n",
      "        [ 838.3459, 3361.5601, 2476.9277],\n",
      "        [ 489.9435, 3324.2209, 1480.0500],\n",
      "        [ 494.5987, 3306.1743, 1491.2078],\n",
      "        [ 898.8064, 3227.4077, 2826.2563],\n",
      "        [ 898.6235, 3238.7354, 2791.9890],\n",
      "        [ 663.7202, 3270.2373, 2009.9968],\n",
      "        [ 733.7126, 3282.7769, 2194.0322],\n",
      "        [ 348.7312, 3231.8984, 1090.2235],\n",
      "        [ 936.1114, 3237.5566, 2888.0884],\n",
      "        [ 494.4712, 3265.6404, 1478.5952],\n",
      "        [ 712.9702, 3239.7295, 2194.3071],\n",
      "        [ 676.3266, 3243.0640, 2111.8462],\n",
      "        [ 487.5201, 3268.7405, 1503.2043],\n",
      "        [ 686.6592, 3301.9182, 2081.3938],\n",
      "        [ 602.8124, 3311.1733, 1818.9961],\n",
      "        [ 761.6620, 3316.7029, 2298.4509],\n",
      "        [ 869.1146, 3337.1626, 2587.8494],\n",
      "        [ 733.9950, 3340.7485, 2200.3289],\n",
      "        [ 921.8739, 3363.2915, 2727.1641],\n",
      "        [ 801.7863, 3332.9792, 2426.5005],\n",
      "        [ 642.9342, 3342.2678, 1944.0907],\n",
      "        [ 600.4375, 3349.6853, 1814.1700],\n",
      "        [ 940.3661, 3376.8027, 2773.5684],\n",
      "        [ 396.8050, 3380.3943, 1163.7947],\n",
      "        [ 766.6239, 3340.9163, 2317.7480],\n",
      "        [ 877.6544, 3341.3352, 2662.5176],\n",
      "        [ 648.4736, 3402.4570, 1882.3472],\n",
      "        [ 775.6652, 3378.4729, 2296.8186],\n",
      "        [ 717.1774, 3384.4382, 2107.2322],\n",
      "        [ 955.0212, 3384.0977, 2793.4675],\n",
      "        [1097.0173, 3398.7092, 3205.7683],\n",
      "        [ 906.5136, 3398.4084, 2631.4946],\n",
      "        [ 867.1827, 3327.3992, 2606.3076],\n",
      "        [ 779.1216, 3339.5981, 2320.5752],\n",
      "        [ 722.7249, 3323.8022, 2186.6904],\n",
      "        [ 855.1169, 3340.3408, 2536.2791],\n",
      "        [ 962.3788, 3290.9482, 2948.9331],\n",
      "        [ 485.1536, 3307.9338, 1477.8353],\n",
      "        [ 943.0529, 3342.5750, 2803.0090],\n",
      "        [ 915.6431, 3344.1333, 2724.2468],\n",
      "        [ 819.7276, 3359.3538, 2402.4133],\n",
      "        [ 706.6889, 3278.3252, 2194.8545],\n",
      "        [ 529.6719, 3315.2173, 1590.5399],\n",
      "        [1136.9480, 3334.2358, 3395.8718],\n",
      "        [ 893.2437, 3288.6416, 2728.7754],\n",
      "        [ 480.1088, 3275.8953, 1481.2649],\n",
      "        [ 796.8162, 3249.6213, 2467.0200],\n",
      "        [ 696.2254, 3260.4460, 2151.8367],\n",
      "        [ 598.5160, 3298.1536, 1798.4397],\n",
      "        [ 523.7693, 3270.8965, 1592.8129],\n",
      "        [ 710.6916, 3268.1262, 2169.2800],\n",
      "        [ 818.5388, 3265.3555, 2496.0503],\n",
      "        [ 573.9253, 3242.0642, 1785.3691],\n",
      "        [ 492.9600, 3228.0444, 1548.2925],\n",
      "        [ 720.1073, 3227.2290, 2236.0520],\n",
      "        [ 528.0504, 3256.8435, 1611.3888],\n",
      "        [ 804.7299, 3186.6226, 2558.6716],\n",
      "        [ 685.6985, 3276.8003, 2064.3442],\n",
      "        [ 683.8104, 3237.7354, 2111.5737],\n",
      "        [ 544.6183, 3250.6770, 1667.2897],\n",
      "        [ 653.3658, 3264.3054, 1985.3575],\n",
      "        [ 813.6377, 3269.1313, 2494.9922],\n",
      "        [ 694.2141, 3262.0378, 2127.6914],\n",
      "        [ 944.3541, 3278.9060, 2865.9329],\n",
      "        [ 822.4798, 3253.5310, 2531.2576],\n",
      "        [ 848.4468, 3322.5325, 2521.4099],\n",
      "        [ 698.8951, 3250.4536, 2184.1853],\n",
      "        [ 481.3352, 3281.3245, 1470.6655],\n",
      "        [ 675.6826, 3311.1790, 2018.6754],\n",
      "        [ 597.4225, 3307.9451, 1813.0413],\n",
      "        [ 634.3168, 3327.0249, 1902.2760],\n",
      "        [ 589.7087, 3321.8081, 1774.5564],\n",
      "        [ 479.2207, 3355.9746, 1409.8517],\n",
      "        [ 818.9343, 3355.4941, 2435.4102],\n",
      "        [ 959.3249, 3378.4507, 2817.1182],\n",
      "        [ 639.8873, 3375.2725, 1887.6831],\n",
      "        [ 462.3821, 3354.7737, 1393.8464],\n",
      "        [ 696.8306, 3422.8721, 2017.3760],\n",
      "        [ 793.6592, 3422.8999, 2303.3567],\n",
      "        [ 663.5765, 3454.1062, 1893.3568],\n",
      "        [ 677.6831, 3435.2161, 1973.4645],\n",
      "        [ 721.9572, 3472.6555, 2063.9634],\n",
      "        [ 357.6993, 3434.0039, 1052.3998],\n",
      "        [ 484.3439, 3468.8184, 1394.9283],\n",
      "        [ 575.6151, 3491.5625, 1644.6262],\n",
      "        [ 523.2890, 3508.5981, 1487.1051],\n",
      "        [ 346.1782, 3530.1023,  976.5302],\n",
      "        [ 271.1471, 3539.7261,  770.4507],\n",
      "        [ 326.9585, 3567.0503,  915.2313],\n",
      "        [ 451.2157, 3589.8447, 1262.5970],\n",
      "        [ 168.5076, 3622.7041,  465.4901],\n",
      "        [ 312.3533, 3654.8989,  852.9365],\n",
      "        [ 368.5320, 3691.3220,  999.1652],\n",
      "        [ 359.7689, 3731.0403,  966.4855],\n",
      "        [ 311.2054, 3793.8491,  818.5025],\n",
      "        [ 382.3614, 3853.3347,  991.1429],\n",
      "        [ 340.1313, 3920.2041,  868.9887],\n",
      "        [ 309.8954, 3994.3850,  775.2935],\n",
      "        [ 242.4006, 4066.2244,  595.4767],\n",
      "        [ 456.1060, 4141.4121, 1102.1626],\n",
      "        [ 491.3894, 4207.7397, 1168.3372],\n",
      "        [ 449.4071, 4281.3506, 1045.3391],\n",
      "        [ 445.3680, 4339.9258, 1026.1321],\n",
      "        [ 428.0261, 4411.8052,  970.7609],\n",
      "        [ 360.8294, 4486.7798,  805.4703],\n",
      "        [ 446.8262, 4556.8384,  980.0649],\n",
      "        [ 466.9004, 4622.5073, 1009.6760],\n",
      "        [ 567.5346, 4685.9976, 1209.7352],\n",
      "        [ 415.9159, 4739.6401,  878.2296],\n",
      "        [ 441.6067, 4802.8403,  922.6176],\n",
      "        [ 451.4965, 4866.2075,  928.8418],\n",
      "        [ 514.9846, 4941.3213, 1041.0157],\n",
      "        [ 335.4408, 4990.5967,  672.0444],\n",
      "        [ 422.2737, 5043.8877,  839.6924],\n",
      "        [ 558.7152, 5096.1177, 1098.8701],\n",
      "        [ 323.5487, 5153.9663,  628.6594],\n",
      "        [ 527.2712, 5219.3838, 1007.5020],\n",
      "        [ 456.3936, 5248.7803,  871.0010],\n",
      "        [ 682.3860, 5305.2549, 1284.8257],\n",
      "        [ 301.8663, 5341.6055,  566.4880],\n",
      "        [ 440.1267, 5380.3799,  818.2642],\n",
      "        [ 502.8603, 5421.5005,  927.0477],\n",
      "        [ 492.8473, 5462.6372,  902.7981],\n",
      "        [ 549.8803, 5478.9077, 1005.8466]])\n",
      "data:  tensor([[[0.0099, 0.0088, 0.0083,  ..., 0.6320, 0.2068, 0.5652]],\n",
      "\n",
      "        [[0.0100, 0.0102, 0.0113,  ..., 0.6299, 0.2068, 0.6087]],\n",
      "\n",
      "        [[0.0093, 0.0091, 0.0077,  ..., 0.6294, 0.2068, 0.6087]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0052, 0.0052, 0.0053,  ..., 0.7954, 0.2110, 0.0870]],\n",
      "\n",
      "        [[0.0061, 0.0061, 0.0058,  ..., 0.8001, 0.2110, 0.0870]],\n",
      "\n",
      "        [[0.0064, 0.0057, 0.0061,  ..., 0.8056, 0.2110, 0.0870]]])\n",
      "output:  tensor([[1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818],\n",
      "        [1.7063, 6.7785, 5.5818]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 625.6489, 5506.0645, 1137.6545],\n",
      "        [ 497.9359, 5502.3784,  904.3085],\n",
      "        [ 315.9492, 5501.2441,  572.8772],\n",
      "        [ 399.7120, 5497.9155,  727.0634],\n",
      "        [ 383.5815, 5476.8135,  700.7457],\n",
      "        [ 423.3400, 5451.4834,  775.4035],\n",
      "        [ 414.0789, 5402.2583,  763.3009],\n",
      "        [ 411.0825, 5307.5605,  777.9005],\n",
      "        [ 302.7272, 5215.8872,  579.0750],\n",
      "        [ 353.0214, 5097.0278,  694.6698],\n",
      "        [ 358.8663, 4979.2300,  718.9185],\n",
      "        [ 398.1626, 4840.5645,  824.1180],\n",
      "        [ 334.6372, 4684.7910,  712.4396],\n",
      "        [ 393.2049, 4552.5820,  860.6224],\n",
      "        [ 299.2205, 4396.8530,  680.4703],\n",
      "        [ 292.9063, 4256.2998,  687.1654],\n",
      "        [ 312.9736, 4115.7300,  760.3965],\n",
      "        [ 320.0938, 3939.2341,  815.2006],\n",
      "        [ 327.7040, 3812.6667,  853.9282],\n",
      "        [ 331.9510, 3683.9993,  899.0268],\n",
      "        [ 305.3131, 3547.0486,  857.2344],\n",
      "        [ 394.9767, 3381.2881, 1163.0410],\n",
      "        [ 274.5615, 3226.5366,  852.6057],\n",
      "        [ 366.9061, 3107.7776, 1178.2018],\n",
      "        [ 229.0463, 2955.2490,  772.7490],\n",
      "        [ 300.8557, 2822.8062, 1063.2523],\n",
      "        [ 325.5037, 2688.9888, 1203.4927],\n",
      "        [ 254.4644, 2580.6863,  981.6365],\n",
      "        [ 251.8850, 2505.8127, 1000.0333],\n",
      "        [ 268.4444, 2446.2603, 1092.4507],\n",
      "        [ 290.7331, 2375.2126, 1220.3209],\n",
      "        [ 160.9783, 2286.4258,  701.6219],\n",
      "        [ 269.0714, 2229.1018, 1200.2345],\n",
      "        [ 311.5445, 2192.0479, 1413.2351],\n",
      "        [ 229.6155, 2126.6584, 1074.6483],\n",
      "        [ 226.3123, 2052.7400, 1096.0859],\n",
      "        [ 196.8151, 1976.9783,  988.7817],\n",
      "        [ 174.8383, 1924.4749,  900.6631],\n",
      "        [ 160.0085, 1872.2173,  852.7501],\n",
      "        [ 185.0414, 1831.7670, 1008.8539],\n",
      "        [ 228.5435, 1823.4277, 1247.5039],\n",
      "        [ 200.5146, 1793.8972, 1109.9849],\n",
      "        [ 222.2692, 1765.1486, 1250.6561],\n",
      "        [ 141.8953, 1770.5387,  799.1019],\n",
      "        [ 156.2906, 1747.2863,  889.1429],\n",
      "        [ 158.8181, 1763.4004,  897.5586],\n",
      "        [ 219.0184, 1798.1812, 1207.9348],\n",
      "        [ 175.2196, 1777.6323,  983.9358],\n",
      "        [ 134.0948, 1774.0911,  755.0875],\n",
      "        [ 172.6732, 1796.9916,  956.7967],\n",
      "        [ 144.5394, 1809.2460,  797.2546],\n",
      "        [ 224.7377, 1811.8824, 1233.2035],\n",
      "        [ 246.4246, 1829.2869, 1335.1063],\n",
      "        [ 202.2987, 1834.8221, 1095.3849],\n",
      "        [ 207.5455, 1832.2694, 1123.5325],\n",
      "        [ 192.3412, 1864.3247, 1033.5242],\n",
      "        [ 214.5653, 1886.1136, 1133.0443],\n",
      "        [ 219.7362, 1901.6414, 1149.9952],\n",
      "        [ 223.2421, 1908.3942, 1165.3306],\n",
      "        [ 246.9695, 1909.8129, 1290.9039],\n",
      "        [ 251.7473, 1921.0397, 1310.5746],\n",
      "        [ 253.7794, 1948.6879, 1290.5264],\n",
      "        [ 258.4701, 1953.2679, 1311.6000],\n",
      "        [ 242.1927, 1959.3840, 1225.4918],\n",
      "        [ 222.0592, 1968.0583, 1118.8762],\n",
      "        [ 195.5645, 1962.1880,  995.7093],\n",
      "        [ 234.2889, 2000.0465, 1160.6080],\n",
      "        [ 194.6087, 2030.9510,  952.2241],\n",
      "        [ 285.6637, 2086.2195, 1361.8590],\n",
      "        [ 232.4721, 2124.8894, 1084.5933],\n",
      "        [ 238.9322, 2154.1389, 1101.0219],\n",
      "        [ 250.8237, 2209.7983, 1127.5443],\n",
      "        [ 243.6135, 2286.9897, 1059.4500],\n",
      "        [ 279.0341, 2342.8894, 1177.1581],\n",
      "        [ 295.9502, 2392.3879, 1221.5118],\n",
      "        [ 308.3185, 2475.1484, 1241.9775],\n",
      "        [ 414.6349, 2474.8467, 1884.1970],\n",
      "        [ 352.3611, 2626.5542, 1331.3248],\n",
      "        [ 239.3478, 2655.3530,  907.1000],\n",
      "        [ 352.0366, 2704.9858, 1297.9231],\n",
      "        [ 313.2363, 2754.1211, 1127.2407],\n",
      "        [ 489.7692, 2744.0896, 1818.7732],\n",
      "        [ 399.5036, 2795.9509, 1425.9606],\n",
      "        [ 568.1773, 2847.1643, 1968.4385],\n",
      "        [ 314.3676, 2816.9858, 1118.2938],\n",
      "        [ 602.9700, 2894.0881, 2045.6775],\n",
      "        [ 728.6911, 2854.3418, 2530.2532],\n",
      "        [ 378.6816, 2883.7271, 1303.5674],\n",
      "        [ 368.3470, 2897.4563, 1260.0581],\n",
      "        [ 621.2903, 2916.1562, 2111.7324],\n",
      "        [ 375.5154, 2932.1309, 1281.3245],\n",
      "        [ 372.1470, 2960.8347, 1243.7605],\n",
      "        [ 623.1815, 2965.5991, 2106.1016],\n",
      "        [ 513.0370, 2985.0254, 1708.2019],\n",
      "        [ 384.2223, 2996.2019, 1289.4414],\n",
      "        [ 421.0747, 3034.2505, 1375.2466],\n",
      "        [ 421.2688, 3042.0923, 1389.8108],\n",
      "        [ 431.8939, 3046.3149, 1432.7041],\n",
      "        [ 593.8291, 3101.7734, 1900.0708],\n",
      "        [ 451.4371, 3096.7073, 1457.2988],\n",
      "        [ 672.4122, 3135.2139, 2135.6777],\n",
      "        [ 466.3164, 3124.9868, 1515.5142],\n",
      "        [ 674.5573, 3159.0359, 2119.4558],\n",
      "        [ 713.7610, 3178.9033, 2205.1001],\n",
      "        [ 764.9789, 3201.5359, 2369.3704],\n",
      "        [ 691.9878, 3222.7385, 2137.7339],\n",
      "        [ 336.2662, 3195.2356, 1065.5922],\n",
      "        [ 325.8991, 3223.6765, 1010.3053],\n",
      "        [ 800.6512, 3244.0972, 2452.2439],\n",
      "        [ 782.2961, 3253.6038, 2385.6465],\n",
      "        [ 522.1698, 3260.8369, 1587.4418],\n",
      "        [ 817.2332, 3271.0920, 2466.4319],\n",
      "        [ 651.9200, 3268.0479, 2002.4366],\n",
      "        [ 823.7937, 3293.6797, 2473.6462],\n",
      "        [ 778.8226, 3257.1116, 2403.5198],\n",
      "        [ 742.3552, 3316.8313, 2208.1057],\n",
      "        [ 773.1301, 3278.5374, 2389.7102],\n",
      "        [ 694.6366, 3300.1589, 2113.9302],\n",
      "        [ 954.6270, 3359.4824, 2803.3132],\n",
      "        [ 739.5062, 3308.6208, 2229.0901],\n",
      "        [ 740.8561, 3184.6396, 2390.2869],\n",
      "        [ 777.3464, 3328.2314, 2330.3120],\n",
      "        [ 844.3557, 3322.9419, 2531.2656],\n",
      "        [ 543.1191, 3317.0325, 1643.6469],\n",
      "        [ 568.9374, 3347.5461, 1688.9056],\n",
      "        [ 369.6629, 3317.7083, 1137.9763],\n",
      "        [ 806.3483, 3326.0754, 2441.5059],\n",
      "        [ 768.7355, 3329.6726, 2312.2615],\n",
      "        [ 688.9083, 3318.3115, 2096.8560],\n",
      "        [ 948.2740, 3353.8799, 2816.4856],\n",
      "        [ 618.8121, 3349.3501, 1846.7661],\n",
      "        [ 611.1230, 3350.7856, 1815.3030],\n",
      "        [ 865.2141, 3335.3193, 2599.0127],\n",
      "        [ 489.9886, 3356.4534, 1447.7257],\n",
      "        [ 775.0773, 3348.2275, 2261.6694],\n",
      "        [1129.4652, 3360.2922, 3356.8333],\n",
      "        [ 603.9238, 3357.5164, 1806.7191],\n",
      "        [ 799.0197, 3348.1606, 2392.1001],\n",
      "        [ 776.8188, 3254.1904, 2361.6389],\n",
      "        [ 669.6759, 3372.8762, 1979.1254],\n",
      "        [ 789.7221, 3376.9482, 2321.2756],\n",
      "        [ 615.3052, 3378.0654, 1808.1005],\n",
      "        [ 710.0880, 3352.6233, 2109.0901],\n",
      "        [ 671.4626, 3380.1709, 1954.7522],\n",
      "        [ 650.8543, 3356.2651, 1925.1976],\n",
      "        [ 671.9084, 3347.1831, 2004.1008],\n",
      "        [ 779.6691, 3324.0813, 2339.6182],\n",
      "        [ 898.5680, 3356.3320, 2660.0432],\n",
      "        [ 600.8502, 3325.3940, 1807.5708],\n",
      "        [ 921.9752, 3322.0427, 2780.5127]])\n",
      "data:  tensor([[[0.0072, 0.0079, 0.0075,  ..., 0.8099, 0.2110, 0.0870]],\n",
      "\n",
      "        [[0.0057, 0.0053, 0.0057,  ..., 0.8140, 0.2110, 0.0870]],\n",
      "\n",
      "        [[0.0042, 0.0045, 0.0041,  ..., 0.8166, 0.2110, 0.0870]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0105, 0.0098, 0.0103,  ..., 0.6335, 0.2110, 0.6087]],\n",
      "\n",
      "        [[0.0070, 0.0071, 0.0076,  ..., 0.6326, 0.2110, 0.6087]],\n",
      "\n",
      "        [[0.0107, 0.0107, 0.0099,  ..., 0.6299, 0.2110, 0.6087]]])\n",
      "Epoch 0, Iteration 50 Train Loss: 818.68\n",
      "output:  tensor([[1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342],\n",
      "        [1.7588, 6.8310, 5.6342]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 576.8680, 3336.8276, 1712.4496],\n",
      "        [ 713.2952, 3285.6533, 2277.8914],\n",
      "        [ 658.4909, 3298.4272, 2018.4722],\n",
      "        [ 884.6184, 3301.2368, 2695.3113],\n",
      "        [ 685.1437, 3325.2712, 2070.0737],\n",
      "        [ 884.7444, 3349.3501, 2592.1621],\n",
      "        [1308.5303, 3365.2856, 3849.7195],\n",
      "        [ 624.6302, 3294.7744, 1904.3290],\n",
      "        [ 695.8807, 3301.4658, 2099.5972],\n",
      "        [ 660.1886, 3245.9741, 2075.1626],\n",
      "        [ 811.5796, 3306.7666, 2429.3469],\n",
      "        [ 700.0195, 3287.5410, 2130.7769],\n",
      "        [ 538.9482, 3265.1880, 1660.9381],\n",
      "        [ 794.8224, 3300.3879, 2388.2356],\n",
      "        [ 839.7674, 3250.7161, 2582.6082],\n",
      "        [ 701.6244, 3276.7222, 2123.4478],\n",
      "        [ 466.3285, 3253.2100, 1446.0717],\n",
      "        [ 755.9969, 3299.4104, 2261.5420],\n",
      "        [ 550.6290, 3242.5332, 1723.9777],\n",
      "        [ 721.7983, 3236.3782, 2258.5632],\n",
      "        [ 642.9888, 3271.7678, 1962.4597],\n",
      "        [ 787.3905, 3268.6621, 2423.9910],\n",
      "        [ 844.9882, 3284.9214, 2559.0962],\n",
      "        [ 866.8140, 3280.6152, 2645.7490],\n",
      "        [ 892.9634, 3311.4695, 2664.6135],\n",
      "        [ 965.8516, 3270.2764, 2959.9658],\n",
      "        [ 383.4138, 3272.1140, 1168.1730],\n",
      "        [ 509.5501, 3249.2024, 1578.7950],\n",
      "        [ 706.1060, 3318.0381, 2097.1699],\n",
      "        [ 651.2349, 3244.6445, 2023.8136],\n",
      "        [ 775.4517, 3252.6318, 2373.5981],\n",
      "        [ 815.3087, 3270.6731, 2476.6680],\n",
      "        [ 645.7315, 3212.9583, 2041.0187],\n",
      "        [ 814.8370, 3244.5942, 2458.9714],\n",
      "        [ 562.7108, 3216.5327, 1754.0103],\n",
      "        [ 793.3543, 3210.7126, 2466.5259],\n",
      "        [ 833.7103, 3188.5496, 2616.3005],\n",
      "        [ 448.4027, 3188.7004, 1408.6600],\n",
      "        [ 651.6120, 3177.0491, 2051.9041],\n",
      "        [ 778.2688, 3166.0007, 2457.6140],\n",
      "        [ 492.2392, 3158.6001, 1562.1102],\n",
      "        [ 718.9365, 3149.7136, 2275.0090],\n",
      "        [ 779.2712, 3155.3550, 2448.6948],\n",
      "        [ 723.4879, 3137.0291, 2298.6096],\n",
      "        [ 605.5543, 3131.4324, 1915.7838],\n",
      "        [ 549.8500, 3099.6118, 1777.3367],\n",
      "        [ 395.1593, 3092.8870, 1283.4027],\n",
      "        [ 745.1454, 3082.4253, 2413.1990],\n",
      "        [ 859.7931, 3115.8059, 2721.9421],\n",
      "        [ 624.6113, 3043.0195, 2067.1284],\n",
      "        [ 524.0408, 3021.9072, 1755.8613],\n",
      "        [ 602.4504, 3031.0557, 1988.9630],\n",
      "        [ 584.5936, 3025.2800, 1936.7323],\n",
      "        [ 423.2568, 3025.4084, 1386.4434],\n",
      "        [1004.5551, 3054.2407, 3261.9702],\n",
      "        [ 501.2725, 2930.9131, 1667.0850],\n",
      "        [ 884.5864, 3006.6973, 2928.7202],\n",
      "        [ 762.6089, 2934.3816, 2645.2170],\n",
      "        [ 451.0332, 2946.5356, 1551.4910],\n",
      "        [ 481.8488, 2953.1658, 1654.7689],\n",
      "        [ 624.8673, 2950.1272, 2118.1396],\n",
      "        [ 851.4922, 2916.8770, 2849.2747],\n",
      "        [ 351.3427, 2827.9224, 1243.9860],\n",
      "        [ 369.3302, 2931.3601, 1260.7520],\n",
      "        [ 503.5638, 2934.3816, 1707.2659],\n",
      "        [ 725.5823, 2926.9922, 2487.8704],\n",
      "        [ 310.1656, 2906.1472, 1077.8612],\n",
      "        [ 715.7801, 2957.8074, 2365.6875],\n",
      "        [ 553.8856, 2912.3413, 1897.5112],\n",
      "        [ 530.6906, 2877.0244, 1867.5768],\n",
      "        [ 414.7867, 2889.6028, 1429.2140],\n",
      "        [ 569.3071, 2875.8513, 1979.9500],\n",
      "        [ 709.6597, 2865.2837, 2481.4001],\n",
      "        [ 629.7211, 2913.0452, 2121.5603],\n",
      "        [ 501.7758, 2850.8171, 1759.4961],\n",
      "        [ 456.1582, 2841.8694, 1612.6267],\n",
      "        [ 457.6241, 2865.1497, 1560.9688],\n",
      "        [ 523.8923, 2834.3401, 1838.9679],\n",
      "        [ 517.3339, 2843.3718, 1812.2620],\n",
      "        [ 436.3195, 2819.6501, 1570.3300],\n",
      "        [ 720.4556, 2871.5842, 2479.6594],\n",
      "        [ 685.8188, 2847.0750, 2387.5098],\n",
      "        [ 389.7825, 2823.4260, 1390.7062],\n",
      "        [ 463.9214, 2808.6216, 1670.5873],\n",
      "        [ 584.7642, 2824.3420, 2073.3105],\n",
      "        [ 328.7702, 2838.4343, 1151.3534],\n",
      "        [ 395.2588, 2823.9568, 1419.2234],\n",
      "        [ 523.7437, 2826.2244, 1862.6735],\n",
      "        [ 543.5153, 2856.3582, 1878.9330],\n",
      "        [ 532.3392, 2830.0281, 1876.9285],\n",
      "        [ 629.7744, 2864.8257, 2173.6416],\n",
      "        [ 418.1333, 2850.0745, 1469.8977],\n",
      "        [ 618.3005, 2869.8582, 2130.7043],\n",
      "        [ 473.3678, 2746.7820, 1849.9672],\n",
      "        [ 615.0421, 2858.5588, 2126.7339],\n",
      "        [ 476.9828, 2722.2505, 1756.3843],\n",
      "        [ 337.3088, 2849.7615, 1160.4591],\n",
      "        [ 760.6749, 2862.4463, 2646.4998],\n",
      "        [ 629.2180, 2841.6682, 2205.9358],\n",
      "        [ 454.6195, 2817.0642, 1627.4641],\n",
      "        [ 380.7400, 2825.4646, 1356.9055],\n",
      "        [ 410.6339, 2823.3311, 1460.9618],\n",
      "        [ 357.8137, 2848.2759, 1246.3558],\n",
      "        [ 366.5765, 2827.2966, 1290.6816],\n",
      "        [ 291.2990, 2754.9590, 1068.1907],\n",
      "        [ 291.8613, 2691.9658, 1331.4441],\n",
      "        [ 308.3398, 2724.3225, 1128.0262],\n",
      "        [ 294.5584, 2650.3765, 1130.4553],\n",
      "        [ 372.0336, 2601.5422, 1450.5601],\n",
      "        [ 230.2718, 2522.6753,  918.0561],\n",
      "        [ 357.3828, 2475.5896, 1416.7229],\n",
      "        [ 247.7098, 2403.1514, 1024.0983],\n",
      "        [ 256.5653, 2335.0195, 1096.5654],\n",
      "        [ 220.4645, 2266.6196,  972.3878],\n",
      "        [ 193.2607, 2217.9924,  865.9568],\n",
      "        [ 312.2437, 2180.2231, 1421.8771],\n",
      "        [ 310.7046, 2130.0208, 1450.5442],\n",
      "        [ 142.4015, 2055.6606,  692.7840],\n",
      "        [ 259.2611, 2018.6852, 1274.6301],\n",
      "        [ 228.9091, 1942.2422, 1181.4553],\n",
      "        [ 209.9628, 1891.5986, 1102.3330],\n",
      "        [ 201.6112, 1833.1129, 1095.7465],\n",
      "        [ 220.1808, 1780.0450, 1232.5144],\n",
      "        [ 184.1323, 1760.2334, 1038.4539],\n",
      "        [ 236.8200, 1724.5477, 1364.0762],\n",
      "        [ 193.0304, 1704.0432, 1119.3015],\n",
      "        [ 268.5433, 1711.8184, 1555.3412],\n",
      "        [ 201.7046, 1688.9011, 1180.6697],\n",
      "        [ 168.2816, 1673.4462, 1004.6421],\n",
      "        [ 199.6343, 1674.8816, 1185.8463],\n",
      "        [ 156.6598, 1669.8713,  930.2208],\n",
      "        [ 194.5532, 1678.2328, 1145.6503],\n",
      "        [ 166.0993, 1665.4923,  999.6009],\n",
      "        [ 202.9342, 1687.3428, 1194.4708],\n",
      "        [ 180.0771, 1696.1400, 1056.1320],\n",
      "        [ 186.5745, 1673.5579, 1112.3420],\n",
      "        [ 184.7733, 1647.1998, 1118.1893],\n",
      "        [ 163.3389, 1638.8942,  989.4295],\n",
      "        [ 148.8623, 1623.7631,  912.4955],\n",
      "        [ 196.8715, 1610.7322, 1208.5554],\n",
      "        [ 183.7582, 1590.8423, 1146.2900],\n",
      "        [ 122.1051, 1565.8137,  772.5764],\n",
      "        [ 142.0937, 1543.2092,  908.1220],\n",
      "        [ 189.0344, 1520.6326, 1230.5721],\n",
      "        [ 141.7004, 1475.3845,  952.2902],\n",
      "        [ 181.1560, 1451.5457, 1236.5955],\n",
      "        [ 146.5777, 1441.8660, 1005.3159],\n",
      "        [ 132.5981, 1419.9094,  929.2180],\n",
      "        [ 159.7239, 1440.4529, 1095.2373],\n",
      "        [ 149.4944, 1419.2552, 1041.8289]])\n",
      "data:  tensor([[[0.0068, 0.0071, 0.0072,  ..., 0.6299, 0.2110, 0.6087]],\n",
      "\n",
      "        [[0.0083, 0.0084, 0.0087,  ..., 0.6268, 0.2110, 0.6522]],\n",
      "\n",
      "        [[0.0067, 0.0061, 0.0064,  ..., 0.6263, 0.2110, 0.6522]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0015, 0.0015, 0.0015,  ..., 0.8743, 0.2152, 0.1304]],\n",
      "\n",
      "        [[0.0019, 0.0019, 0.0021,  ..., 0.8818, 0.2152, 0.1304]],\n",
      "\n",
      "        [[0.0018, 0.0017, 0.0017,  ..., 0.8882, 0.2152, 0.1304]]])\n",
      "output:  tensor([[1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866],\n",
      "        [1.8112, 6.8834, 5.6866]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 152.0019, 1428.0085, 1053.3438],\n",
      "        [ 158.8481, 1356.3912, 1229.7102],\n",
      "        [ 166.8093, 1427.1708, 1161.4907],\n",
      "        [ 195.2295, 1453.4893, 1307.6510],\n",
      "        [ 180.9742, 1440.4529, 1210.3693],\n",
      "        [ 137.9467, 1415.9214,  970.3708],\n",
      "        [ 147.0185, 1444.9827, 1005.5964],\n",
      "        [ 147.1849, 1459.5721,  997.5129],\n",
      "        [ 145.6909, 1517.7562,  956.0764],\n",
      "        [ 153.6820, 1659.6946,  920.5002],\n",
      "        [ 191.5253, 1748.2804, 1083.6693],\n",
      "        [ 169.9222, 1625.0869, 1042.2275],\n",
      "        [ 148.4968, 1593.5624,  925.1789],\n",
      "        [ 228.0010, 1677.0320, 1336.8645],\n",
      "        [ 128.0048, 1688.7168,  755.0797],\n",
      "        [ 163.1590, 1633.6104,  992.3594],\n",
      "        [ 155.1079, 1618.8088,  948.8199],\n",
      "        [ 175.4791, 1603.5212, 1085.1400],\n",
      "        [ 171.2043, 1582.2238, 1072.8406],\n",
      "        [ 187.1754, 1603.0690, 1160.1508],\n",
      "        [ 183.3786, 1619.4120, 1125.9104],\n",
      "        [ 165.6028, 1631.2700, 1009.3701],\n",
      "        [ 132.0220, 1647.4457,  797.1908],\n",
      "        [ 210.6820, 1683.3491, 1243.6090],\n",
      "        [ 186.1045, 1702.6750, 1085.1217],\n",
      "        [ 181.2661, 1708.5621, 1056.9813],\n",
      "        [ 147.2859, 1728.3011,  846.9090],\n",
      "        [ 176.5154, 1739.8185, 1007.9241],\n",
      "        [ 137.7649, 1742.9966,  788.9178],\n",
      "        [ 179.0886, 1719.7944, 1038.6689],\n",
      "        [ 238.7208, 1785.5862, 1325.2427],\n",
      "        [ 193.8242, 1751.5424, 1094.1836],\n",
      "        [ 232.6917, 1730.3119, 1331.0035],\n",
      "        [ 194.4662, 1748.6156, 1103.8961],\n",
      "        [ 272.2491, 1773.3147, 1523.1545],\n",
      "        [ 226.8098, 1788.2671, 1261.7047],\n",
      "        [ 148.6402, 1792.7466,  825.1281],\n",
      "        [ 251.8998, 1774.2417, 1408.9628],\n",
      "        [ 179.8898, 1745.8452, 1025.2487],\n",
      "        [ 193.2278, 1730.6359, 1108.5126],\n",
      "        [ 189.8399, 1722.6263, 1098.0344],\n",
      "        [ 163.0613, 1727.0835,  940.0243],\n",
      "        [ 187.2141, 1741.5891, 1071.2888],\n",
      "        [ 212.3988, 1746.9287, 1212.1729],\n",
      "        [ 199.1624, 1746.5433, 1131.8843],\n",
      "        [ 194.6815, 1735.1825, 1112.5995],\n",
      "        [ 170.6784, 1751.5369,  967.2406],\n",
      "        [ 221.1894, 1759.3564, 1251.4872],\n",
      "        [ 256.7913, 1777.3251, 1407.1138],\n",
      "        [ 233.3360, 1780.2853, 1294.2092],\n",
      "        [ 278.3098, 1797.3546, 1490.8309],\n",
      "        [ 306.1339, 1771.3485, 1657.4808],\n",
      "        [ 281.9854, 1750.2466, 1897.1632],\n",
      "        [ 198.7347, 1800.6947, 1090.2704],\n",
      "        [ 210.8024, 1895.5587, 1106.0272],\n",
      "        [ 300.9592, 1977.6262, 1496.6306],\n",
      "        [ 220.1068, 2006.8163, 1092.1393],\n",
      "        [ 221.3405, 2044.8086, 1080.2289],\n",
      "        [ 252.7759, 2063.6260, 1216.7083],\n",
      "        [ 281.0504, 2098.2839, 1329.3052],\n",
      "        [ 322.0581, 2141.0298, 1493.7882],\n",
      "        [ 364.8956, 2185.3818, 1660.2677],\n",
      "        [ 251.6683, 2211.4517, 1142.0927],\n",
      "        [ 581.4539, 2290.9500, 2447.2156],\n",
      "        [ 601.9070, 2303.1880, 2562.8835],\n",
      "        [ 514.7413, 2328.8755, 2221.2185],\n",
      "        [ 420.8076, 2360.4893, 1772.5529],\n",
      "        [ 373.5178, 2388.0425, 1552.6422],\n",
      "        [ 311.4100, 2409.6753, 1279.2021],\n",
      "        [ 440.5663, 2375.3467, 1887.4131],\n",
      "        [ 392.1738, 2452.5940, 1566.5396],\n",
      "        [ 404.9089, 2463.0725, 1629.6230],\n",
      "        [ 403.1807, 2455.9119, 1633.2908],\n",
      "        [ 470.2439, 2434.6367, 1938.5095],\n",
      "        [ 530.8776, 2452.0076, 2161.5669],\n",
      "        [ 501.7218, 2468.1665, 2021.4485],\n",
      "        [ 502.2734, 2473.7966, 2017.7202],\n",
      "        [ 358.1861, 2449.7009, 1475.2926],\n",
      "        [ 401.5612, 2491.2068, 1590.4491],\n",
      "        [ 296.2534, 2456.7664, 1213.7177],\n",
      "        [ 452.2560, 2472.3501, 1826.2256],\n",
      "        [ 535.0537, 2505.1367, 2123.3052],\n",
      "        [ 502.0036, 2510.9290, 1985.2548],\n",
      "        [ 314.3977, 2518.1509, 1248.9122],\n",
      "        [ 434.0335, 2525.2671, 1721.8564],\n",
      "        [ 475.1756, 2561.0811, 1840.0129],\n",
      "        [ 413.5858, 2560.4834, 1620.9604],\n",
      "        [ 480.1287, 2586.3442, 1848.5386],\n",
      "        [ 304.8360, 2599.2300, 1172.6055],\n",
      "        [ 581.3019, 2632.5698, 2191.2751],\n",
      "        [ 438.5697, 2671.7910, 1614.3275],\n",
      "        [ 519.6458, 2688.7488, 1921.5339],\n",
      "        [ 324.4067, 2683.6694, 1216.1772],\n",
      "        [ 750.6212, 2715.0898, 2762.1165],\n",
      "        [ 468.7608, 2749.8203, 1693.0133],\n",
      "        [ 629.5592, 2766.4092, 2257.0090],\n",
      "        [ 644.6049, 2794.3369, 2288.6150],\n",
      "        [ 487.8675, 2782.6799, 1768.4509],\n",
      "        [ 839.6313, 2828.4138, 2959.9795],\n",
      "        [ 611.2194, 2845.9468, 2127.7959],\n",
      "        [ 628.2362, 2834.8818, 2216.1128],\n",
      "        [ 752.3051, 2871.8298, 2601.9316],\n",
      "        [ 489.8628, 2892.8035, 1674.5272],\n",
      "        [ 542.3879, 2898.5342, 1856.2844],\n",
      "        [ 475.7858, 2912.6709, 1617.7714],\n",
      "        [ 622.5270, 2903.0974, 2141.2798],\n",
      "        [ 714.0955, 2906.8286, 2458.5254],\n",
      "        [ 646.6097, 2938.8503, 2185.7100],\n",
      "        [ 534.1161, 2919.7646, 1851.4989],\n",
      "        [ 708.2871, 2961.1921, 2379.5464],\n",
      "        [ 593.1701, 2971.8828, 1976.0200],\n",
      "        [ 613.5598, 2964.7222, 2065.9446],\n",
      "        [ 565.9220, 2966.5374, 1907.7966],\n",
      "        [ 635.5582, 2977.6023, 2125.9260],\n",
      "        [ 622.6130, 2940.7327, 2147.7666],\n",
      "        [ 608.6204, 2985.4163, 2031.9991],\n",
      "        [ 628.3415, 2980.2388, 2109.5566],\n",
      "        [ 423.4956, 3002.6868, 1395.7456],\n",
      "        [ 616.7695, 3007.5405, 2031.1715],\n",
      "        [ 683.9120, 2986.6677, 2300.0305],\n",
      "        [ 686.0803, 2989.0190, 2303.6516],\n",
      "        [ 593.7889, 3013.0254, 1960.7708],\n",
      "        [ 703.3793, 2986.1370, 2370.5955],\n",
      "        [ 716.5624, 3028.0168, 2354.4031],\n",
      "        [ 450.2844, 2922.0435, 1563.5658],\n",
      "        [ 641.0718, 3015.0261, 2070.2808],\n",
      "        [ 602.4963, 3027.8662, 1991.5459],\n",
      "        [ 628.2798, 3080.0010, 1999.7631],\n",
      "        [ 362.7431, 3040.2490, 1192.7996],\n",
      "        [ 557.7067, 3049.1917, 1819.3076],\n",
      "        [ 697.5352, 3022.5713, 2318.0237],\n",
      "        [ 659.0195, 3033.1724, 2172.7637],\n",
      "        [ 580.3730, 3047.9517, 1900.2971],\n",
      "        [ 959.7085, 3071.6733, 3100.0264],\n",
      "        [ 834.3392, 3056.5420, 2706.6196],\n",
      "        [ 692.0820, 3063.1943, 2243.7786],\n",
      "        [ 388.9948, 3036.6912, 1298.2913],\n",
      "        [ 617.2016, 3045.8345, 2020.9186],\n",
      "        [ 573.4183, 3096.0259, 1819.7911],\n",
      "        [ 444.4680, 3069.8914, 1441.1088],\n",
      "        [ 557.3257, 3073.9075, 1793.8846],\n",
      "        [ 685.8535, 3067.6741, 2242.6338],\n",
      "        [ 477.8356, 3065.9590, 1567.0120],\n",
      "        [ 957.7159, 3062.3398, 3120.5969],\n",
      "        [ 648.0877, 3062.6248, 2126.5466],\n",
      "        [ 519.0730, 3101.7175, 1605.2196],\n",
      "        [ 627.3629, 3026.2966, 2107.7556],\n",
      "        [ 716.8945, 3043.0701, 2371.8142],\n",
      "        [ 511.6336, 3050.5488, 1684.5370],\n",
      "        [ 777.7012, 3071.8687, 2513.9517]])\n",
      "data:  tensor([[[0.0017, 0.0016, 0.0016,  ..., 0.8937, 0.2152, 0.1304]],\n",
      "\n",
      "        [[0.0019, 0.0019, 0.0019,  ..., 0.8991, 0.2152, 0.1304]],\n",
      "\n",
      "        [[0.0020, 0.0021, 0.0021,  ..., 0.9057, 0.2152, 0.1304]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0083, 0.0082, 0.0083,  ..., 0.6335, 0.2152, 0.6522]],\n",
      "\n",
      "        [[0.0059, 0.0053, 0.0053,  ..., 0.6312, 0.2152, 0.6522]],\n",
      "\n",
      "        [[0.0084, 0.0092, 0.0081,  ..., 0.6299, 0.2152, 0.6522]]])\n",
      "output:  tensor([[1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390],\n",
      "        [1.8635, 6.9357, 5.7390]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 792.5435, 3053.7271, 2589.4873],\n",
      "        [ 888.0240, 3043.9858, 2914.6021],\n",
      "        [ 584.9421, 3041.2712, 1927.3845],\n",
      "        [ 631.9314, 3016.0305, 2133.0352],\n",
      "        [ 707.1434, 3077.3201, 2261.8359],\n",
      "        [ 632.5743, 3028.5923, 2109.6436],\n",
      "        [ 731.0937, 3030.6196, 2429.6799],\n",
      "        [ 442.9029, 3046.0247, 1438.4790],\n",
      "        [ 888.9668, 3071.1299, 2869.8706],\n",
      "        [ 608.2198, 3055.7712, 1991.5626],\n",
      "        [ 570.9397, 3055.9724, 1855.1317],\n",
      "        [ 814.3870, 3052.3586, 2660.6240],\n",
      "        [ 750.2642, 3047.1306, 2452.3157],\n",
      "        [ 793.3727, 2990.0635, 2689.6729],\n",
      "        [ 703.3318, 3022.9285, 2319.1987],\n",
      "        [ 476.5406, 3036.8030, 1569.7699],\n",
      "        [ 547.9910, 3036.8198, 1779.8867],\n",
      "        [ 756.8743, 3015.0642, 2514.8145],\n",
      "        [ 713.1952, 3032.2339, 2320.9050],\n",
      "        [ 617.9067, 3006.5798, 2059.3972],\n",
      "        [ 310.3679, 3004.4294, 1038.9111],\n",
      "        [ 737.1929, 3023.6658, 2418.8884],\n",
      "        [ 627.7800, 2999.4248, 2098.3879],\n",
      "        [ 530.4955, 3008.4734, 1767.5085],\n",
      "        [ 508.3884, 3011.8972, 1689.5989],\n",
      "        [ 492.8814, 3020.9568, 1621.9569],\n",
      "        [ 813.6705, 2976.7700, 2753.2529],\n",
      "        [ 637.0132, 2987.8293, 2127.4446],\n",
      "        [ 522.7723, 2999.2739, 1739.9774],\n",
      "        [ 803.8296, 2997.1404, 2679.9448],\n",
      "        [ 379.9605, 3007.8979, 1231.7654],\n",
      "        [ 528.8745, 2955.9585, 1815.3794],\n",
      "        [ 550.4217, 3002.0332, 1810.4896],\n",
      "        [ 624.4761, 2944.4805, 2140.4622],\n",
      "        [ 697.7213, 2933.9797, 2402.6379],\n",
      "        [ 699.2249, 2985.3884, 2302.7903],\n",
      "        [ 844.8895, 2988.2371, 2795.1621],\n",
      "        [ 764.7499, 2915.0837, 2641.9060],\n",
      "        [ 369.8083, 2933.4658, 1259.3977],\n",
      "        [ 550.1726, 2921.7251, 1886.4429],\n",
      "        [ 575.6721, 2908.6047, 1981.4204],\n",
      "        [ 842.9284, 2925.9551, 2855.4443],\n",
      "        [ 616.4136, 2867.9814, 2176.1047],\n",
      "        [ 396.9714, 2861.3962, 1412.1517],\n",
      "        [ 538.7750, 2853.7273, 1912.0754],\n",
      "        [ 436.1631, 2855.0063, 1537.0184],\n",
      "        [ 606.0632, 2859.4636, 2103.5525],\n",
      "        [ 544.2653, 2813.3105, 1955.4978],\n",
      "        [ 422.8153, 2808.0044, 1528.7240],\n",
      "        [ 568.7997, 2806.8315, 2041.6154],\n",
      "        [ 644.8647, 2804.5078, 2291.4756],\n",
      "        [ 685.2228, 2793.2085, 2443.9070],\n",
      "        [ 741.7498, 2787.3940, 2659.2881],\n",
      "        [ 610.0028, 2750.3345, 2239.8271],\n",
      "        [ 682.8185, 2745.5754, 2495.9084],\n",
      "        [ 447.3471, 2751.6245, 1619.6284],\n",
      "        [ 654.3026, 2737.3088, 2387.7065],\n",
      "        [ 561.0455, 2714.5481, 2068.4167],\n",
      "        [ 653.2498, 2714.2576, 2403.7197],\n",
      "        [ 591.1326, 2700.5396, 2185.1211],\n",
      "        [ 518.4604, 2696.8086, 1922.7345],\n",
      "        [ 592.1669, 2703.0474, 2172.9160],\n",
      "        [ 609.8331, 2715.7715, 2202.6370],\n",
      "        [ 523.5794, 2699.3445, 1940.5513],\n",
      "        [ 543.1887, 2682.0012, 2030.1915],\n",
      "        [ 666.3509, 2648.0417, 2556.5239],\n",
      "        [ 517.8531, 2663.1282, 1956.9159],\n",
      "        [ 538.9229, 2652.1860, 2038.4202],\n",
      "        [ 588.2114, 2654.6885, 2232.8269],\n",
      "        [ 683.6100, 2671.5564, 2542.5461],\n",
      "        [ 435.7827, 2675.2095, 1618.0137],\n",
      "        [ 329.5378, 2672.3496, 1213.9701],\n",
      "        [ 423.4423, 2633.6252, 1629.5370],\n",
      "        [ 404.9976, 2650.8735, 1536.6115],\n",
      "        [ 661.9559, 2635.3513, 2529.0159],\n",
      "        [ 437.4314, 2636.5767, 1664.1910],\n",
      "        [ 861.9124, 2666.4792, 3217.8755],\n",
      "        [ 451.1339, 2667.2500, 1664.9619],\n",
      "        [ 639.6429, 2664.9321, 2358.2515],\n",
      "        [ 432.6984, 2647.6060, 1613.2896],\n",
      "        [ 426.1564, 2621.4324, 1639.4696],\n",
      "        [ 234.8277, 2631.9331,  897.9028],\n",
      "        [ 407.3095, 2644.8020, 1536.4347],\n",
      "        [ 437.6321, 2641.4172, 1647.7417],\n",
      "        [ 711.5323, 2649.4771, 2673.6331],\n",
      "        [ 565.8217, 2653.4988, 2120.2300],\n",
      "        [ 428.5936, 2640.1326, 1624.4249],\n",
      "        [ 377.6352, 2631.8828, 1446.9313],\n",
      "        [ 319.8559, 2645.1650, 1206.5280],\n",
      "        [ 458.1170, 2638.0771, 1731.8557],\n",
      "        [ 350.0255, 2631.2852, 1340.3235],\n",
      "        [ 506.3780, 2645.3718, 1909.9119],\n",
      "        [ 355.2083, 2650.6892, 1330.0573],\n",
      "        [ 248.1842, 2634.6084,  950.5302],\n",
      "        [ 323.2185, 2642.0149, 1219.5792],\n",
      "        [ 286.4741, 2635.2117, 1092.3088],\n",
      "        [ 310.6533, 2638.0715, 1179.9114],\n",
      "        [ 204.8164, 2633.5806,  774.1783],\n",
      "        [ 444.9622, 2619.0474, 1689.4235],\n",
      "        [ 290.5268, 2588.0535, 1124.1921],\n",
      "        [ 197.4319, 2547.2905,  782.9699],\n",
      "        [ 277.3827, 2522.5579, 1092.6979],\n",
      "        [ 351.0326, 2452.4990, 1435.5920],\n",
      "        [ 298.1801, 2386.7522, 1255.4137],\n",
      "        [ 289.2917, 2324.0552, 1245.7162],\n",
      "        [ 262.9080, 2259.8555, 1155.2738],\n",
      "        [ 326.2805, 3115.4802, 1091.7131],\n",
      "        [1368.7080, 6519.8760, 2095.8984],\n",
      "        [ 872.9629, 6628.4121, 1319.9607],\n",
      "        [ 588.7750, 6421.4482,  916.3441],\n",
      "        [1131.0627, 6413.7070, 1761.1913],\n",
      "        [ 813.9549, 6411.4951, 1263.4877],\n",
      "        [ 638.7417, 6421.4482,  991.4816],\n",
      "        [ 846.0019, 7024.0947, 1222.2067],\n",
      "        [ 935.2877, 8291.2588, 1128.7433],\n",
      "        [1297.5266, 8598.1592, 1485.8759],\n",
      "        [1075.2080, 8658.7393, 1243.5876],\n",
      "        [1198.2880, 8673.8311, 1383.0226],\n",
      "        [1143.4882, 8639.6475, 1326.2046],\n",
      "        [1292.7080, 8592.6572, 1506.9166],\n",
      "        [1203.3048, 8544.0742, 1411.2960],\n",
      "        [1050.9805, 8489.6494, 1240.3676],\n",
      "        [ 965.4566, 8439.1562, 1144.8068],\n",
      "        [ 782.0277, 8377.3809,  934.7545],\n",
      "        [ 965.8226, 8311.0371, 1163.7435],\n",
      "        [ 888.2038, 8260.2139, 1074.8969],\n",
      "        [1205.4958, 8208.8613, 1470.9373],\n",
      "        [ 707.5319, 8239.5195,  858.3719],\n",
      "        [ 941.3669, 8253.0869, 1142.1842],\n",
      "        [ 846.8380, 8220.4902, 1030.2540],\n",
      "        [ 991.4502, 8140.9922, 1219.7570],\n",
      "        [ 801.5275, 8119.4263,  986.8850],\n",
      "        [1014.4116, 8102.4185, 1254.6354],\n",
      "        [ 716.6888, 8097.6260,  885.1525],\n",
      "        [ 674.8501, 8044.4023,  840.3016],\n",
      "        [ 828.2443, 8008.1577, 1035.0164],\n",
      "        [ 881.8474, 7979.5098, 1107.2866],\n",
      "        [ 701.6356, 7932.5581,  886.0328],\n",
      "        [ 985.3445, 7893.0630, 1249.8357],\n",
      "        [ 621.9149, 7862.1250,  791.0081],\n",
      "        [ 784.0580, 7838.9619, 1002.4616],\n",
      "        [ 617.9992, 7893.3706,  781.7336],\n",
      "        [ 734.5323, 7882.2554,  933.7180],\n",
      "        [ 640.3013, 7896.4927,  812.1779],\n",
      "        [ 795.2784, 7967.9146,  996.3012],\n",
      "        [ 633.8873, 8012.8330,  792.0215],\n",
      "        [ 569.4013, 8076.2061,  706.5707],\n",
      "        [ 904.1591, 8098.2017, 1112.8367],\n",
      "        [ 640.1638, 8077.6304,  792.2015],\n",
      "        [ 687.9086, 8044.6816,  854.1003]])\n",
      "data:  tensor([[[0.0094, 0.0094, 0.0094,  ..., 0.6299, 0.2152, 0.6522]],\n",
      "\n",
      "        [[0.0098, 0.0089, 0.0090,  ..., 0.6263, 0.2152, 0.6957]],\n",
      "\n",
      "        [[0.0068, 0.0067, 0.0070,  ..., 0.6263, 0.2152, 0.6957]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0100, 0.0105, 0.0103,  ..., 0.8048, 0.2194, 0.1739]],\n",
      "\n",
      "        [[0.0077, 0.0072, 0.0074,  ..., 0.8099, 0.2194, 0.1739]],\n",
      "\n",
      "        [[0.0083, 0.0082, 0.0081,  ..., 0.8114, 0.2194, 0.1739]]])\n",
      "output:  tensor([[1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913],\n",
      "        [1.9158, 6.9880, 5.7913]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[1400.1486, 8027.1318, 1706.8644],\n",
      "        [ 972.6550, 7968.4785, 1229.0146],\n",
      "        [ 778.0820, 7939.8975,  974.8740],\n",
      "        [ 924.0717, 7899.2520, 1170.3158],\n",
      "        [ 663.7188, 7890.1475,  839.2168],\n",
      "        [ 592.5850, 7871.4194,  753.7732],\n",
      "        [ 936.1790, 7857.2378, 1190.7146],\n",
      "        [1069.9049, 7818.4580, 1366.1688],\n",
      "        [1261.9689, 7786.9443, 1600.7521],\n",
      "        [ 863.0405, 7792.2617, 1101.9849],\n",
      "        [ 848.1774, 7817.2178, 1082.4250],\n",
      "        [ 741.7429, 7800.3384,  952.0630],\n",
      "        [ 939.4421, 7828.8916, 1210.1439],\n",
      "        [ 713.8666, 7871.0620,  906.3558],\n",
      "        [ 543.2835, 7884.6792,  688.5714],\n",
      "        [ 988.1323, 7923.0962, 1241.3274],\n",
      "        [ 558.6390, 7936.8032,  704.2986],\n",
      "        [ 692.7232, 7965.3564,  870.8416],\n",
      "        [1009.8269, 8004.2197, 1258.0864],\n",
      "        [ 714.5145, 8032.7729,  887.3528],\n",
      "        [ 731.4479, 8048.1108,  909.5201],\n",
      "        [1311.4828, 8085.6396, 1601.0483],\n",
      "        [ 627.5027, 8102.8472,  774.3677],\n",
      "        [ 964.9802, 8128.8882, 1186.5720],\n",
      "        [ 754.9825, 8119.2422,  930.7768],\n",
      "        [1034.9242, 8143.0083, 1262.5205],\n",
      "        [ 970.6029, 8120.2642, 1183.8778],\n",
      "        [ 920.8324, 8060.4102, 1139.9089],\n",
      "        [ 717.8662, 8039.1909,  892.7620],\n",
      "        [ 543.0684, 8043.1064,  673.7708],\n",
      "        [ 880.7311, 8050.3730, 1092.6627],\n",
      "        [ 633.4689, 8039.8501,  788.6293],\n",
      "        [ 803.6130, 8067.9673,  994.5846],\n",
      "        [ 659.5143, 7995.9033,  826.0928],\n",
      "        [ 573.0922, 7951.0186,  717.6217],\n",
      "        [ 506.7118, 7925.0737,  637.0568],\n",
      "        [ 722.5084, 7923.8896,  908.0161],\n",
      "        [ 864.5442, 7910.1270, 1088.2390],\n",
      "        [ 856.2621, 7879.1890, 1084.3195],\n",
      "        [ 951.1212, 7839.7832, 1213.9072],\n",
      "        [ 829.5078, 7834.8789, 1044.4532],\n",
      "        [ 978.5704, 7787.2515, 1256.2438],\n",
      "        [ 869.0715, 7739.2891, 1121.2239],\n",
      "        [ 659.3163, 7680.2783,  857.1108],\n",
      "        [ 689.0921, 7611.4482,  904.7969],\n",
      "        [ 772.0620, 7528.9619, 1024.6567],\n",
      "        [1034.0035, 7431.2607, 1392.5527],\n",
      "        [ 840.3479, 7366.3237, 1139.5275],\n",
      "        [ 635.7159, 7267.3931,  874.8250],\n",
      "        [ 829.2295, 7185.8843, 1153.7800],\n",
      "        [1243.4159, 7099.6665, 1749.7355],\n",
      "        [ 721.0797, 7013.6948, 1029.3013],\n",
      "        [ 873.5966, 6912.7480, 1268.6036],\n",
      "        [ 627.4716, 6847.4258,  916.9863],\n",
      "        [ 971.8718, 6765.1841, 1441.8529],\n",
      "        [1204.1514, 6705.2749, 1793.0383],\n",
      "        [ 822.3983, 6595.5986, 1249.8412],\n",
      "        [ 802.3002, 6530.5444, 1226.3125],\n",
      "        [ 975.1330, 6424.3140, 1521.5859],\n",
      "        [ 864.2720, 6388.0190, 1346.9314],\n",
      "        [ 939.9186, 6312.8555, 1484.2445],\n",
      "        [ 968.9819, 6227.5479, 1554.3302],\n",
      "        [ 787.8937, 6132.4717, 1289.5959],\n",
      "        [ 981.4785, 6082.2749, 1613.0472],\n",
      "        [ 991.7208, 6019.4438, 1640.9108],\n",
      "        [ 781.3372, 5961.1538, 1301.5909],\n",
      "        [1250.3915, 5846.6621, 2146.4500],\n",
      "        [1306.2466, 5809.3008, 2254.0281],\n",
      "        [1166.3112, 5768.2363, 2016.0680],\n",
      "        [1117.1309, 5703.6626, 1952.9462],\n",
      "        [ 671.4157, 5615.4565, 1201.0900],\n",
      "        [1215.1877, 5604.6763, 2153.9846],\n",
      "        [ 624.8870, 5538.3599, 1119.7516],\n",
      "        [ 882.5049, 5442.8926, 1626.1975],\n",
      "        [ 738.3549, 5395.8521, 1371.9973],\n",
      "        [1409.6418, 5396.8179, 2599.3137],\n",
      "        [ 917.0495, 5342.3652, 1703.9941],\n",
      "        [ 786.8206, 5249.9868, 1497.8190],\n",
      "        [ 639.7046, 5210.7207, 1227.5497],\n",
      "        [1160.5277, 5170.8291, 2236.7942],\n",
      "        [1080.7557, 5158.5298, 2075.6646],\n",
      "        [1398.3107, 5123.5200, 2712.4326],\n",
      "        [ 887.9155, 4881.6851, 2072.4502],\n",
      "        [1387.8177, 5032.7612, 2729.3022],\n",
      "        [1129.3464, 4953.8438, 2277.6333],\n",
      "        [1090.9958, 4879.9087, 2250.2346],\n",
      "        [1266.6488, 4903.8735, 2558.4817],\n",
      "        [1330.9452, 4824.5229, 2762.4153],\n",
      "        [ 770.3414, 4820.2280, 1594.5167],\n",
      "        [1264.3824, 4785.8770, 2631.1379],\n",
      "        [ 925.3002, 4761.1221, 1934.5447],\n",
      "        [1303.1904, 4735.9873, 2732.2507],\n",
      "        [1423.8351, 4727.0508, 2984.1560],\n",
      "        [1074.5629, 4686.0811, 2272.0991],\n",
      "        [ 833.5584, 4591.2173, 1827.1985],\n",
      "        [ 964.7353, 4605.1196, 2084.2668],\n",
      "        [ 897.1807, 4588.2344, 1943.7611],\n",
      "        [ 799.0265, 4541.7520, 1753.4247],\n",
      "        [ 888.8021, 4480.0044, 1990.2034],\n",
      "        [1179.2550, 4447.7705, 2651.5352],\n",
      "        [ 857.7946, 4437.9678, 1926.0319],\n",
      "        [1353.1882, 4432.1089, 3041.7920],\n",
      "        [1022.7923, 4416.7095, 2301.1462],\n",
      "        [1017.0063, 4407.6055, 2274.7104],\n",
      "        [1090.3073, 4357.7993, 2494.7070],\n",
      "        [ 894.8777, 4346.8350, 2033.9430],\n",
      "        [1078.3401, 4344.1035, 2458.6877],\n",
      "        [ 791.8870, 4225.1606, 1835.6084],\n",
      "        [1046.4697, 4282.1050, 2441.1770],\n",
      "        [ 544.1155, 4262.7290, 1273.6951],\n",
      "        [ 747.4670, 4261.6787, 1730.1820],\n",
      "        [ 750.2428, 4217.5977, 1782.9539],\n",
      "        [ 461.9446, 4199.8921, 1095.6787],\n",
      "        [ 745.2383, 4186.5034, 1781.2943],\n",
      "        [ 892.4651, 4163.5083, 2135.3958],\n",
      "        [ 917.9900, 4109.9546, 2253.3047],\n",
      "        [ 759.2225, 4120.1201, 1841.3650],\n",
      "        [1232.1465, 4127.6104, 2969.9692],\n",
      "        [ 848.8357, 4087.8105, 2071.2488],\n",
      "        [ 745.9333, 4057.4675, 1848.1522],\n",
      "        [ 887.9580, 4036.7566, 2200.1433],\n",
      "        [1165.2469, 4023.0442, 2875.1375],\n",
      "        [ 886.6195, 4007.5110, 2217.9626],\n",
      "        [1001.9006, 4005.7461, 2500.6555],\n",
      "        [ 878.0873, 3952.8850, 2240.8420],\n",
      "        [ 376.7996, 3949.6843,  964.4444],\n",
      "        [ 941.5256, 3944.1436, 2397.3943],\n",
      "        [ 932.1893, 3920.5002, 2376.5059],\n",
      "        [ 621.2692, 3919.7461, 1588.3627],\n",
      "        [ 711.8917, 3919.2156, 1826.2479],\n",
      "        [ 862.6054, 3934.4136, 2163.2703],\n",
      "        [ 465.6832, 3897.7952, 1182.1476],\n",
      "        [1206.4379, 3908.8657, 3068.1687],\n",
      "        [1112.3051, 3857.2947, 2885.4490],\n",
      "        [ 702.0902, 3825.8149, 1855.7427],\n",
      "        [ 559.9529, 3844.1296, 1445.1415],\n",
      "        [ 620.5699, 3802.9927, 1633.0071],\n",
      "        [ 911.0881, 3785.1245, 2409.9727],\n",
      "        [ 879.2562, 3758.9121, 2363.5571],\n",
      "        [ 795.0020, 3765.8770, 2122.5103],\n",
      "        [ 734.5645, 3761.5818, 1961.4934],\n",
      "        [ 719.1926, 3777.2883, 1899.8590],\n",
      "        [ 575.5529, 3739.7537, 1548.9203],\n",
      "        [ 837.9161, 3782.8625, 2181.2263],\n",
      "        [ 506.0740, 3770.4626, 1334.9398],\n",
      "        [1007.4120, 3774.1211, 2664.1897],\n",
      "        [ 546.1185, 3776.1936, 1435.1180],\n",
      "        [ 707.7891, 3774.0820, 1855.8622],\n",
      "        [ 780.5750, 3791.0789, 2039.2844],\n",
      "        [ 758.7631, 3772.2000, 1999.5281]])\n",
      "data:  tensor([[[0.0162, 0.0169, 0.0162,  ..., 0.8130, 0.2194, 0.1739]],\n",
      "\n",
      "        [[0.0114, 0.0093, 0.0101,  ..., 0.8149, 0.2194, 0.1739]],\n",
      "\n",
      "        [[0.0090, 0.0112, 0.0109,  ..., 0.8149, 0.2194, 0.1739]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0082, 0.0090, 0.0084,  ..., 0.5943, 0.2194, 0.6957]],\n",
      "\n",
      "        [[0.0082, 0.0074, 0.0080,  ..., 0.5943, 0.2194, 0.6957]],\n",
      "\n",
      "        [[0.0097, 0.0109, 0.0103,  ..., 0.5943, 0.2194, 0.6957]]])\n",
      "output:  tensor([[1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436],\n",
      "        [1.9681, 7.0403, 5.8436]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 599.2560, 3778.3550, 1560.3920],\n",
      "        [ 908.0026, 3767.7236, 2396.0752],\n",
      "        [ 979.6417, 3753.3601, 2610.6501],\n",
      "        [ 649.8516, 3738.0334, 1753.8191],\n",
      "        [ 598.9878, 3759.3589, 1575.8148],\n",
      "        [ 855.0059, 3758.0295, 2266.1577],\n",
      "        [ 734.8911, 3724.8181, 1972.3817],\n",
      "        [1027.3373, 3781.5610, 2679.5879],\n",
      "        [ 785.6539, 3699.1694, 2166.2346],\n",
      "        [ 798.6917, 3694.0420, 2177.7466],\n",
      "        [ 732.1190, 3733.4478, 1954.0527],\n",
      "        [ 749.1692, 3718.2383, 1994.4711],\n",
      "        [ 730.7024, 3701.8896, 1970.6285],\n",
      "        [ 837.5712, 3674.3923, 2295.6987],\n",
      "        [1111.0005, 3667.2207, 3035.4299],\n",
      "        [ 797.4166, 3683.2397, 2163.6213],\n",
      "        [ 811.5350, 3687.1216, 2190.0969],\n",
      "        [ 884.5187, 3632.5347, 2459.7139],\n",
      "        [ 937.4429, 3669.9463, 2545.0076],\n",
      "        [1117.5687, 3685.1836, 3022.3906],\n",
      "        [ 592.3824, 3625.2234, 1656.9756],\n",
      "        [ 773.9684, 3653.9326, 2125.4617],\n",
      "        [ 718.1212, 3633.8025, 1995.6205],\n",
      "        [ 880.0080, 3633.3892, 2432.6655],\n",
      "        [ 445.7677, 3647.9172, 1212.4733],\n",
      "        [ 633.4130, 3642.7170, 1722.3311],\n",
      "        [ 759.7525, 3635.7742, 2011.2961],\n",
      "        [ 579.4742, 3635.2939, 1574.2998],\n",
      "        [1237.2188, 3596.6592, 3445.4187],\n",
      "        [ 661.7265, 3628.5635, 1817.5272],\n",
      "        [ 733.4307, 3628.3232, 2016.6104],\n",
      "        [ 706.5702, 3624.7988, 1930.3234],\n",
      "        [ 787.4107, 3609.0754, 2174.8230],\n",
      "        [ 248.7095, 3591.9451,  702.5532],\n",
      "        [ 532.9275, 3602.0044, 1488.5175],\n",
      "        [ 627.9390, 3602.9114, 1751.6285],\n",
      "        [ 548.1445, 3590.3306, 1536.7142],\n",
      "        [ 776.5767, 3614.0020, 2139.2866],\n",
      "        [ 705.3030, 3609.0254, 1944.9735],\n",
      "        [ 636.2595, 3485.0886, 2117.4060],\n",
      "        [1080.0685, 3536.0732, 3204.5676],\n",
      "        [ 707.6232, 3590.7664, 1976.0797],\n",
      "        [ 584.6006, 3600.2002, 1613.1541],\n",
      "        [ 693.0535, 3584.4993, 1955.7894],\n",
      "        [ 648.2952, 3438.9414, 1942.2482],\n",
      "        [ 689.8891, 3566.3857, 1939.9498],\n",
      "        [ 642.7068, 3554.0193, 1819.8064],\n",
      "        [ 839.3972, 3578.9084, 2333.7087],\n",
      "        [ 631.1495, 3553.2261, 1792.5995],\n",
      "        [ 601.8748, 3573.6746, 1692.1942],\n",
      "        [ 895.5699, 3590.5317, 2484.6560],\n",
      "        [ 783.0980, 3579.1987, 2177.8508],\n",
      "        [ 865.4520, 3581.3044, 2410.0657],\n",
      "        [ 889.0970, 3568.2400, 2505.3525],\n",
      "        [ 586.1642, 3617.0237, 1574.4088],\n",
      "        [ 605.2870, 3607.1990, 1653.2983],\n",
      "        [ 769.1850, 3610.6226, 2112.1418],\n",
      "        [ 508.4301, 3607.4446, 1397.7428],\n",
      "        [ 503.9035, 3614.6499, 1384.2908],\n",
      "        [ 527.7851, 3590.0459, 1487.0477],\n",
      "        [ 993.2466, 3660.6519, 2662.2356],\n",
      "        [ 545.2338, 3612.8347, 1507.6853],\n",
      "        [ 633.5542, 3631.1382, 1734.9755],\n",
      "        [ 914.5826, 3630.9873, 2493.2180],\n",
      "        [ 468.7350, 3616.3926, 1302.7181],\n",
      "        [ 821.3900, 3677.5647, 2193.9138],\n",
      "        [ 585.4822, 3649.5425, 1615.3042],\n",
      "        [ 865.0744, 3683.2063, 2337.1716],\n",
      "        [ 834.8054, 3667.9971, 2281.8179],\n",
      "        [ 882.3487, 3702.2358, 2366.0083],\n",
      "        [ 815.2346, 3724.6951, 2171.1362],\n",
      "        [ 938.4490, 3713.2170, 2533.9622],\n",
      "        [ 645.8336, 3753.7678, 1717.6979],\n",
      "        [ 625.5392, 3778.6455, 1649.6259],\n",
      "        [ 412.5928, 3812.3147, 1073.5442],\n",
      "        [ 577.6533, 3814.6831, 1516.6796],\n",
      "        [ 522.6563, 3821.5588, 1369.7313],\n",
      "        [ 473.2386, 3836.9634, 1236.4585],\n",
      "        [ 277.7414, 3847.4810,  725.3395],\n",
      "        [ 448.9391, 3861.2661, 1161.9910],\n",
      "        [ 504.2384, 3879.3855, 1301.9583],\n",
      "        [ 541.1877, 3913.0269, 1374.6761],\n",
      "        [ 333.5812, 3936.4187,  847.7996],\n",
      "        [ 479.6396, 4001.9534, 1199.8727],\n",
      "        [ 453.9304, 4051.1616, 1118.5167],\n",
      "        [ 482.4208, 4087.2886, 1178.6527],\n",
      "        [ 576.1238, 4122.2090, 1397.9565],\n",
      "        [ 416.7322, 4162.7930, 1003.6367],\n",
      "        [ 412.2294, 4229.0425,  973.6089],\n",
      "        [ 458.4824, 4287.5396, 1071.0658],\n",
      "        [ 304.3431, 4350.9517,  698.8815],\n",
      "        [ 407.0485, 4428.9136,  910.1185],\n",
      "        [ 550.8819, 4482.7915, 1229.4863],\n",
      "        [ 372.9729, 4382.8838, 1059.4531],\n",
      "        [ 425.0815, 4667.8330,  909.8604],\n",
      "        [ 518.8303, 4745.5947, 1096.2014],\n",
      "        [ 600.2088, 4836.4648, 1241.5018],\n",
      "        [ 407.6174, 4721.6216, 1053.5032],\n",
      "        [ 503.7015, 5003.1191, 1022.8995],\n",
      "        [ 375.1197, 5113.7344,  731.9539],\n",
      "        [ 496.0069, 5201.3984,  951.8950],\n",
      "        [ 369.6445, 5170.5249,  716.8739],\n",
      "        [ 464.8431, 5362.0874,  866.7399],\n",
      "        [ 537.9520, 5424.9297,  988.2289],\n",
      "        [ 587.7181, 5506.5898, 1064.9205],\n",
      "        [ 613.5506, 5574.9282, 1120.2217],\n",
      "        [ 537.8630, 5692.7598,  941.5769],\n",
      "        [ 533.3284, 5800.2075,  920.7137],\n",
      "        [ 595.8273, 5897.0601, 1006.4694],\n",
      "        [ 601.0917, 5991.8345,  999.6598],\n",
      "        [ 527.1387, 6076.8569,  866.3360],\n",
      "        [ 691.3067, 6156.7856, 1118.2650],\n",
      "        [ 654.5864, 6221.0464, 1052.8873],\n",
      "        [ 633.3126, 6307.1860, 1003.6885],\n",
      "        [ 791.0450, 6355.0601, 1248.5990],\n",
      "        [ 606.3320, 6431.5249,  943.6871],\n",
      "        [ 545.7512, 6530.7676,  836.9214],\n",
      "        [ 718.0114, 6601.9102, 1092.6379],\n",
      "        [ 660.7199, 6689.9150,  988.2266],\n",
      "        [ 584.0263, 6762.8950,  863.7109],\n",
      "        [ 414.9016, 6790.9844,  620.7193],\n",
      "        [ 712.5491, 6867.2485, 1037.1163],\n",
      "        [ 601.2330, 6911.8936,  871.4241],\n",
      "        [ 814.1241, 7000.2002, 1153.9425],\n",
      "        [ 745.1423, 7035.9248, 1065.7499],\n",
      "        [ 676.4862, 7101.4707,  953.0493],\n",
      "        [ 706.9523, 7164.4189,  988.4064],\n",
      "        [ 845.9790, 7244.6997, 1162.2833],\n",
      "        [ 686.3011, 7264.0088,  946.7032],\n",
      "        [ 825.6651, 7315.4229, 1119.0204],\n",
      "        [ 717.5181, 7356.3589,  976.1302],\n",
      "        [ 540.6262, 7411.2251,  729.4454],\n",
      "        [ 588.8906, 7445.4756,  792.1655],\n",
      "        [ 812.9470, 7488.1040, 1081.4794],\n",
      "        [ 693.9232, 7488.7129,  925.8314],\n",
      "        [ 518.5314, 7517.9307,  689.9312],\n",
      "        [ 748.1224, 7547.2935,  983.6347],\n",
      "        [ 681.4666, 7560.6763,  904.0662],\n",
      "        [ 726.2758, 7576.0811,  960.3218],\n",
      "        [ 466.6572, 7584.0122,  616.0740],\n",
      "        [ 633.4193, 7596.3789,  835.2368],\n",
      "        [ 579.5904, 7595.5020,  763.4189],\n",
      "        [ 615.3643, 7576.8237,  812.3270],\n",
      "        [ 699.1313, 7547.0479,  928.6610],\n",
      "        [ 484.8839, 7530.9668,  645.0629],\n",
      "        [ 663.1992, 7518.5508,  882.3748],\n",
      "        [ 485.6947, 7500.7832,  646.7828],\n",
      "        [ 652.8516, 7463.1870,  875.9481],\n",
      "        [ 300.1186, 7436.8628,  403.4439],\n",
      "        [ 673.0722, 7395.0332,  910.1263]])\n",
      "data:  tensor([[[0.0070, 0.0062, 0.0068,  ..., 0.5943, 0.2194, 0.6957]],\n",
      "\n",
      "        [[0.0106, 0.0108, 0.0108,  ..., 0.5943, 0.2194, 0.7391]],\n",
      "\n",
      "        [[0.0105, 0.0106, 0.0097,  ..., 0.5943, 0.2194, 0.7391]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0081, 0.0081, 0.0074,  ..., 0.9515, 0.2236, 0.2174]],\n",
      "\n",
      "        [[0.0037, 0.0036, 0.0044,  ..., 0.9555, 0.2236, 0.2174]],\n",
      "\n",
      "        [[0.0078, 0.0072, 0.0068,  ..., 0.9594, 0.2236, 0.2174]]])\n",
      "output:  tensor([[2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958],\n",
      "        [2.0204, 7.0926, 5.8958]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 425.8836, 7325.0801,  579.8772],\n",
      "        [ 671.5875, 7225.2007,  928.0460],\n",
      "        [ 504.6195, 7130.9731,  707.2822],\n",
      "        [ 360.0108, 6991.2856,  512.8527],\n",
      "        [ 444.7710, 6812.1143,  654.3280],\n",
      "        [ 524.5383, 6609.7354,  795.3426],\n",
      "        [ 609.4344, 6332.2593,  963.0081],\n",
      "        [ 468.1728, 6028.9224,  775.8154],\n",
      "        [ 612.1264, 5732.8633, 1074.1592],\n",
      "        [ 395.1184, 5434.4419,  731.2992],\n",
      "        [ 543.2578, 5171.2593, 1046.2792],\n",
      "        [ 546.4550, 4871.2456, 1131.6033],\n",
      "        [ 545.2149, 4628.7852, 1166.2617],\n",
      "        [ 570.9172, 4351.7114, 1306.3126],\n",
      "        [ 425.8318, 4074.8274, 1040.4047],\n",
      "        [ 361.2577, 3804.4417,  945.1751],\n",
      "        [ 522.5019, 3574.6802, 1471.6464],\n",
      "        [ 532.7534, 3418.8728, 1540.8074],\n",
      "        [ 487.1894, 3245.8845, 1493.5811],\n",
      "        [ 450.0896, 3075.8289, 1436.5272],\n",
      "        [ 397.8597, 2943.0393, 1344.1283],\n",
      "        [ 331.1429, 2802.3127, 1177.4984],\n",
      "        [ 319.3105, 2691.6475, 1176.2725],\n",
      "        [ 312.5299, 2592.7676, 1198.0469],\n",
      "        [ 452.7240, 2507.6560, 1798.6755],\n",
      "        [ 374.9121, 2403.8103, 1564.3241],\n",
      "        [ 424.4045, 2364.4438, 1728.6373],\n",
      "        [ 378.7238, 2292.1396, 1630.4865],\n",
      "        [ 332.0501, 2243.6855, 1476.0516],\n",
      "        [ 331.0575, 2204.8274, 1481.5382],\n",
      "        [ 292.4250, 2184.5186, 1327.2378],\n",
      "        [ 242.9420, 2156.9038, 1120.0663],\n",
      "        [ 233.0919, 2119.7937, 1090.7548],\n",
      "        [ 249.7545, 2106.6287, 1176.5560],\n",
      "        [ 272.0818, 2085.3088, 1333.9464],\n",
      "        [ 303.3493, 2116.4761, 1438.9139],\n",
      "        [ 288.9790, 2142.3201, 1327.3311],\n",
      "        [ 316.6207, 2158.5740, 1440.1909],\n",
      "        [ 344.5544, 2245.0261, 1509.7598],\n",
      "        [ 304.0999, 2331.0371, 1307.2900],\n",
      "        [ 271.6452, 2416.8188, 1113.8899],\n",
      "        [ 283.0913, 2477.9521, 1131.5029],\n",
      "        [ 395.4921, 2479.9573, 1555.1738],\n",
      "        [ 208.9517, 2472.9253,  847.9128],\n",
      "        [ 434.3514, 2495.2227, 1736.5219],\n",
      "        [ 396.5479, 2515.2173, 1547.3998],\n",
      "        [ 247.2376, 2470.7637, 1000.9448],\n",
      "        [ 412.4016, 2485.2021, 1641.6371],\n",
      "        [ 420.4890, 2487.6042, 1657.9850],\n",
      "        [ 496.9993, 2514.5483, 1916.5270],\n",
      "        [ 388.5901, 2493.1895, 1545.5416],\n",
      "        [ 410.5623, 2515.1741, 1620.7357],\n",
      "        [ 493.8253, 2520.8989, 1959.2954],\n",
      "        [ 343.5824, 2523.1055, 1359.1266],\n",
      "        [ 560.3955, 2580.4460, 2147.9563],\n",
      "        [ 347.4956, 2555.5403, 1356.2284],\n",
      "        [ 420.8296, 2578.0945, 1621.0823],\n",
      "        [ 472.8544, 2569.4761, 1835.2905],\n",
      "        [ 410.9625, 2580.9543, 1616.5262],\n",
      "        [ 350.7958, 2607.5916, 1345.0110],\n",
      "        [ 408.4514, 2625.1802, 1557.2690],\n",
      "        [ 573.8224, 2637.5242, 2168.6499],\n",
      "        [ 408.5033, 2658.5200, 1535.1821],\n",
      "        [ 487.5425, 2670.1155, 1819.4296],\n",
      "        [ 674.5142, 2701.3774, 2480.9602],\n",
      "        [ 355.1640, 2692.4910, 1331.3422],\n",
      "        [ 415.0211, 2728.8525, 1522.7698],\n",
      "        [ 453.6408, 2755.7466, 1631.1168],\n",
      "        [ 376.1327, 2738.6660, 1393.6075],\n",
      "        [ 474.3234, 2773.8604, 1712.1740],\n",
      "        [ 654.2385, 2799.5537, 2363.5015],\n",
      "        [ 512.8201, 2839.1660, 1783.9735],\n",
      "        [ 536.1633, 2852.1074, 1855.5913],\n",
      "        [ 404.9789, 2844.0754, 1418.2069],\n",
      "        [ 581.8027, 2889.3965, 1988.5369],\n",
      "        [ 528.1364, 2907.2251, 1796.0632],\n",
      "        [ 629.3969, 2935.7559, 2114.6792],\n",
      "        [ 749.1249, 2931.2332, 2546.6465],\n",
      "        [ 527.0055, 2851.3311, 1804.6191],\n",
      "        [ 615.1559, 2968.4141, 2051.3574],\n",
      "        [ 639.2785, 2983.4224, 2116.0120],\n",
      "        [ 608.1877, 2981.0483, 2029.4474],\n",
      "        [ 377.3510, 2999.9497, 1239.4301],\n",
      "        [ 348.1526, 2989.1252, 1176.7496],\n",
      "        [ 654.2589, 3009.8640, 2163.0042],\n",
      "        [ 789.2602, 3018.8064, 2596.7773],\n",
      "        [ 538.4485, 3028.3577, 1769.1831],\n",
      "        [ 637.0747, 3043.6675, 2075.9570],\n",
      "        [ 700.6662, 3027.0061, 2312.8772],\n",
      "        [ 607.0575, 3041.2156, 2005.5786],\n",
      "        [ 458.3600, 3038.4451, 1526.9578],\n",
      "        [ 731.9064, 3049.3201, 2414.2100],\n",
      "        [ 706.4028, 3104.2756, 2250.8240],\n",
      "        [ 742.6854, 3081.4143, 2400.7156],\n",
      "        [ 774.7474, 3109.2522, 2473.1414],\n",
      "        [ 588.0544, 3103.8679, 1899.0640],\n",
      "        [ 510.1769, 3099.8855, 1652.8738],\n",
      "        [ 729.3167, 3130.9575, 2308.0225],\n",
      "        [ 968.9026, 3153.2214, 3047.7930],\n",
      "        [ 649.0972, 3143.6533, 2052.0737],\n",
      "        [ 747.1158, 3153.6123, 2366.3335],\n",
      "        [ 636.9462, 3180.9644, 1972.4360],\n",
      "        [ 523.2048, 3149.5293, 1670.0231],\n",
      "        [ 508.3483, 3164.0906, 1604.6698],\n",
      "        [ 885.2380, 3192.0461, 2755.4143],\n",
      "        [ 600.8563, 3188.8958, 1853.4293],\n",
      "        [ 649.6245, 3169.4304, 2061.5959],\n",
      "        [ 680.1228, 3207.9255, 2101.1052],\n",
      "        [ 697.7040, 3181.5398, 2195.1365],\n",
      "        [ 716.0551, 3204.5669, 2209.0461],\n",
      "        [ 463.0697, 3195.6711, 1430.5573],\n",
      "        [ 612.1896, 3216.4268, 1878.7838],\n",
      "        [ 613.4509, 3211.9751, 1904.7209],\n",
      "        [ 603.5535, 3218.0745, 1868.8092],\n",
      "        [ 400.9016, 3195.1572, 1265.5812],\n",
      "        [ 877.2872, 3230.4519, 2690.5974],\n",
      "        [ 849.0489, 3244.0750, 2584.3284],\n",
      "        [ 655.9764, 3220.9119, 2038.9172],\n",
      "        [ 502.5059, 3229.9771, 1546.6371],\n",
      "        [ 609.6268, 3215.7451, 1889.7950],\n",
      "        [ 562.1357, 3237.5454, 1724.9958],\n",
      "        [ 664.4141, 3233.9373, 2029.0253],\n",
      "        [ 617.8456, 3206.4456, 1952.8689],\n",
      "        [ 606.1645, 3235.5681, 1857.8665],\n",
      "        [ 926.2352, 3242.7288, 2833.1179],\n",
      "        [ 705.5194, 3194.9897, 2201.9407],\n",
      "        [ 670.8625, 3216.1643, 2072.8804],\n",
      "        [ 780.0803, 3206.2612, 2437.0234],\n",
      "        [ 636.1790, 3183.6174, 2029.3623],\n",
      "        [ 647.5891, 3220.0852, 2018.0548],\n",
      "        [ 580.5366, 3218.1694, 1801.5736],\n",
      "        [ 819.8552, 3253.0564, 2489.1099],\n",
      "        [ 842.8602, 3224.4641, 2595.0496],\n",
      "        [ 523.3605, 3206.2556, 1654.4121],\n",
      "        [ 617.5054, 3226.5029, 1907.8715],\n",
      "        [ 487.6659, 3132.0635, 1697.7859],\n",
      "        [ 857.8087, 3228.4634, 2624.1401],\n",
      "        [ 626.2067, 3203.5464, 1924.7014],\n",
      "        [ 685.6723, 3177.3340, 2170.3347],\n",
      "        [ 636.0280, 3193.0906, 2006.8596],\n",
      "        [ 604.4122, 3197.3857, 1891.5668],\n",
      "        [ 715.8751, 3225.1570, 2188.3530],\n",
      "        [ 738.8915, 3203.7068, 2291.6165],\n",
      "        [ 482.2766, 3184.0085, 1510.4115],\n",
      "        [ 776.7420, 3200.3293, 2412.1873],\n",
      "        [ 842.9812, 3165.7886, 2669.3728],\n",
      "        [ 889.7411, 3213.2878, 2737.8828],\n",
      "        [ 667.7200, 3171.7205, 2092.0720],\n",
      "        [ 634.1798, 3172.6421, 1987.7833],\n",
      "        [ 796.4447, 3150.6965, 2543.6353]])\n",
      "data:  tensor([[[0.0050, 0.0057, 0.0060,  ..., 0.9626, 0.2236, 0.2174]],\n",
      "\n",
      "        [[0.0076, 0.0076, 0.0077,  ..., 0.9644, 0.2236, 0.2174]],\n",
      "\n",
      "        [[0.0054, 0.0048, 0.0043,  ..., 0.9644, 0.2236, 0.2174]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0086, 0.0085, 0.0085,  ..., 0.6157, 0.2236, 0.7391]],\n",
      "\n",
      "        [[0.0075, 0.0076, 0.0076,  ..., 0.6144, 0.2236, 0.7391]],\n",
      "\n",
      "        [[0.0093, 0.0084, 0.0084,  ..., 0.6121, 0.2236, 0.7391]]])\n",
      "output:  tensor([[2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480],\n",
      "        [2.0726, 7.1448, 5.9480]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 618.0750, 3133.8564, 1994.5532],\n",
      "        [ 581.1378, 3129.9409, 1870.9896],\n",
      "        [ 642.4039, 3144.2063, 2038.0823],\n",
      "        [ 682.4079, 3135.8669, 2175.8196],\n",
      "        [ 510.9723, 3114.3462, 1642.6394],\n",
      "        [ 612.1140, 3077.9343, 2025.8953],\n",
      "        [ 736.7494, 3112.4248, 2352.7659],\n",
      "        [ 301.2694, 3100.6228,  986.2978],\n",
      "        [ 440.4985, 3111.3135, 1411.3256],\n",
      "        [ 763.8378, 3112.5757, 2435.0642],\n",
      "        [ 850.1746, 3083.6541, 2757.7397],\n",
      "        [ 694.5264, 3076.8455, 2247.2686],\n",
      "        [ 802.9474, 3060.8484, 2629.8481],\n",
      "        [ 564.6332, 3047.7280, 1864.1002],\n",
      "        [ 680.2218, 3072.3267, 2203.9001],\n",
      "        [ 575.8678, 3040.9753, 1910.2224],\n",
      "        [ 644.9733, 3061.3511, 2096.1741],\n",
      "        [ 610.3510, 3007.6580, 2058.6077],\n",
      "        [ 589.0280, 3001.5864, 1988.1426],\n",
      "        [ 883.7628, 3043.4441, 2882.9851],\n",
      "        [ 592.0592, 3026.6372, 1942.0396],\n",
      "        [ 726.9370, 3024.2131, 2382.8096],\n",
      "        [ 749.7288, 3009.7190, 2482.2993],\n",
      "        [ 655.4869, 3013.5281, 2172.4314],\n",
      "        [ 461.0549, 3015.4832, 1514.8130],\n",
      "        [ 809.9656, 2999.6260, 2685.7954],\n",
      "        [ 368.2450, 3008.6772, 1194.5835],\n",
      "        [ 487.9864, 2983.9697, 1621.6774],\n",
      "        [ 684.8711, 2975.4407, 2291.9712],\n",
      "        [ 345.5479, 2961.5889, 1159.6227],\n",
      "        [ 697.0086, 2941.8999, 2371.8208],\n",
      "        [ 473.4526, 2951.0657, 1609.8983],\n",
      "        [ 371.3643, 2935.8340, 1304.7727],\n",
      "        [ 744.3552, 2951.9985, 2533.3665],\n",
      "        [ 590.7747, 2954.0874, 1989.9836],\n",
      "        [ 477.6033, 2933.9685, 1639.7578],\n",
      "        [ 772.2844, 2946.1838, 2608.6870],\n",
      "        [ 676.1576, 2927.4668, 2316.9075],\n",
      "        [ 474.5544, 2925.8359, 1635.0187],\n",
      "        [ 517.7538, 2931.7007, 1752.8782],\n",
      "        [ 601.2231, 2938.6602, 2027.9896],\n",
      "        [ 519.6685, 2924.5344, 1782.1946],\n",
      "        [ 881.8433, 2962.9851, 2955.2148],\n",
      "        [ 540.9587, 2934.7112, 1840.6949],\n",
      "        [ 360.9491, 2942.9666, 1222.3820],\n",
      "        [ 507.3793, 2945.2456, 1725.4635],\n",
      "        [ 757.5386, 2984.8914, 2517.2925],\n",
      "        [ 532.8978, 2946.0273, 1810.2594],\n",
      "        [ 545.3397, 2959.3098, 1839.9691],\n",
      "        [ 394.3385, 2965.4539, 1324.3652],\n",
      "        [ 480.9644, 2966.7329, 1615.3656],\n",
      "        [ 414.9661, 2978.9651, 1383.7850],\n",
      "        [ 405.9879, 2996.1572, 1352.5066],\n",
      "        [ 331.5288, 2970.4585, 1131.6390],\n",
      "        [ 652.2963, 3009.6184, 2152.9712],\n",
      "        [ 362.3453, 2991.1917, 1210.0166],\n",
      "        [ 286.8352, 2990.7114,  956.5848],\n",
      "        [ 386.2021, 2988.4158, 1289.5011],\n",
      "        [ 428.1246, 3015.6060, 1401.6807],\n",
      "        [ 506.3217, 2991.3511, 1693.1624],\n",
      "        [ 338.8335, 3020.3704, 1114.9335],\n",
      "        [ 387.3757, 3013.2322, 1284.0498],\n",
      "        [ 414.6143, 2983.3667, 1398.5603],\n",
      "        [ 366.7848, 3011.4226, 1217.3016],\n",
      "        [ 462.4174, 3000.0168, 1544.2870],\n",
      "        [ 355.4254, 3006.4736, 1186.1962],\n",
      "        [ 390.9110, 3020.5994, 1292.4702],\n",
      "        [ 354.1461, 3011.8301, 1176.4143],\n",
      "        [ 445.0970, 3027.4641, 1465.1768],\n",
      "        [ 534.2505, 3036.6130, 1755.5461],\n",
      "        [ 393.3533, 3040.8525, 1298.7438],\n",
      "        [ 305.3903, 3061.0496,  992.8160],\n",
      "        [ 306.2468, 3068.0984,  994.9852],\n",
      "        [ 455.8906, 3054.8384, 1495.7629],\n",
      "        [ 447.6334, 3070.3047, 1460.8303],\n",
      "        [ 408.9493, 3076.1248, 1337.3505],\n",
      "        [ 252.8298, 3117.5244,  802.3977],\n",
      "        [ 294.0453, 3120.4009,  933.8694],\n",
      "        [ 313.3461, 3121.1326, 1002.2800],\n",
      "        [ 296.7819, 3109.6992,  952.6477],\n",
      "        [ 495.9316, 3110.8889, 1591.1884],\n",
      "        [ 390.7674, 3119.7529, 1252.1658],\n",
      "        [ 359.7126, 3136.8391, 1142.2057],\n",
      "        [ 671.1462, 4553.4312, 1453.0726],\n",
      "        [ 781.4526, 6084.9673, 1276.4154],\n",
      "        [ 659.8806, 5758.5454, 1141.9396],\n",
      "        [ 858.7213, 5723.4351, 1498.3960],\n",
      "        [ 365.7601, 5728.5737,  638.5306],\n",
      "        [ 524.9394, 5778.9492,  910.9899],\n",
      "        [ 682.6786, 5858.7886, 1167.5244],\n",
      "        [ 584.1583, 5946.8882,  983.0421],\n",
      "        [ 856.3885, 6011.8252, 1429.4347],\n",
      "        [ 687.3666, 6105.5776, 1126.6851],\n",
      "        [ 702.2139, 6180.1104, 1138.8582],\n",
      "        [ 743.2388, 6249.6777, 1191.7638],\n",
      "        [ 776.3422, 6313.2798, 1233.1580],\n",
      "        [ 697.4269, 6386.6396, 1095.1785],\n",
      "        [ 722.9766, 6447.2085, 1123.3158],\n",
      "        [ 655.5108, 6502.9126, 1009.1988],\n",
      "        [ 548.3444, 6558.0918,  836.8076],\n",
      "        [ 682.3572, 6600.4800, 1035.3853],\n",
      "        [ 564.2775, 6662.1719,  847.9001],\n",
      "        [ 748.1124, 6684.5977, 1121.1034],\n",
      "        [ 615.1664, 6727.8350,  915.6574],\n",
      "        [ 720.1974, 6749.8643, 1066.5800],\n",
      "        [ 654.4310, 6785.8066,  965.1248],\n",
      "        [ 601.7980, 6804.1885,  886.6598],\n",
      "        [ 557.1851, 6823.8604,  817.5800],\n",
      "        [ 539.1520, 6864.7070,  785.8458],\n",
      "        [ 481.2763, 6892.4448,  699.2886],\n",
      "        [ 472.1539, 6925.1538,  681.9928],\n",
      "        [ 607.8744, 6959.0630,  874.7994],\n",
      "        [ 551.1624, 6971.3848,  791.4391],\n",
      "        [ 828.6284, 6973.0098, 1190.0422],\n",
      "        [ 704.3876, 7034.0034, 1003.1524],\n",
      "        [ 504.5882, 7058.4570,  716.2952],\n",
      "        [ 910.7461, 7041.5049, 1295.5890],\n",
      "        [ 645.3881, 7016.1523,  920.6445],\n",
      "        [ 812.8041, 7021.7715, 1158.0046],\n",
      "        [ 652.3007, 7056.9712,  922.7269],\n",
      "        [ 367.0050, 7111.3516,  515.8831],\n",
      "        [ 884.9313, 7171.5073, 1228.6746],\n",
      "        [ 959.5969, 7212.1694, 1324.8495],\n",
      "        [ 680.7527, 7242.1860,  938.4915],\n",
      "        [ 760.0825, 7213.9097, 1053.6346],\n",
      "        [ 953.7995, 7241.7505, 1318.0359],\n",
      "        [ 722.0073, 7291.8970,  990.5670],\n",
      "        [ 749.8582, 7305.8604, 1027.7988],\n",
      "        [ 627.9285, 7330.9897,  857.1252],\n",
      "        [ 607.4085, 7377.8184,  823.9588],\n",
      "        [ 602.2045, 7391.6313,  815.7613],\n",
      "        [ 732.8275, 7440.0576,  985.6271],\n",
      "        [ 704.3581, 7479.1338,  941.9186],\n",
      "        [ 723.9843, 7554.7446,  959.7433],\n",
      "        [ 578.1352, 7619.3521,  759.7137],\n",
      "        [ 666.0547, 7698.4033,  865.7385],\n",
      "        [ 588.3174, 7765.8145,  758.7960],\n",
      "        [ 679.5100, 7840.8223,  867.2822],\n",
      "        [ 504.3486, 7898.7886,  638.1147],\n",
      "        [ 566.3259, 7931.2959,  714.4072],\n",
      "        [1116.5865, 7956.9727, 1405.7714],\n",
      "        [ 871.6359, 7998.2549, 1091.0558],\n",
      "        [ 709.9892, 8054.6738,  879.9360],\n",
      "        [ 543.3838, 8087.3101,  671.0732],\n",
      "        [ 840.1120, 8102.8765, 1036.5873],\n",
      "        [ 903.3023, 8123.9897, 1113.1235],\n",
      "        [ 969.0761, 8152.8164, 1178.3357],\n",
      "        [ 776.1246, 8123.9731,  956.2239],\n",
      "        [ 737.2074, 8103.4912,  910.0635],\n",
      "        [1047.6438, 8091.0635, 1301.8625]])\n",
      "data:  tensor([[[0.0069, 0.0076, 0.0075,  ..., 0.6121, 0.2236, 0.7391]],\n",
      "\n",
      "        [[0.0068, 0.0070, 0.0059,  ..., 0.6118, 0.2236, 0.7826]],\n",
      "\n",
      "        [[0.0072, 0.0073, 0.0078,  ..., 0.6085, 0.2236, 0.7826]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0088, 0.0080, 0.0090,  ..., 0.9346, 0.2278, 0.2609]],\n",
      "\n",
      "        [[0.0082, 0.0088, 0.0088,  ..., 0.9324, 0.2278, 0.2609]],\n",
      "\n",
      "        [[0.0120, 0.0121, 0.0122,  ..., 0.9324, 0.2278, 0.2609]]])\n",
      "output:  tensor([[2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002],\n",
      "        [2.1247, 7.1969, 6.0002]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 881.7756, 8102.1113, 1077.5369],\n",
      "        [ 888.6098, 8056.9302, 1102.7784],\n",
      "        [ 495.1590, 8042.3130,  615.2752],\n",
      "        [ 869.0187, 8006.4600, 1083.9634],\n",
      "        [ 784.7026, 7963.0718,  984.6504],\n",
      "        [1144.4762, 7942.4556, 1435.9011],\n",
      "        [ 781.3410, 7921.3110,  987.4938],\n",
      "        [ 710.7312, 7917.9800,  896.7261],\n",
      "        [ 630.5667, 7893.2754,  800.8068],\n",
      "        [ 965.6859, 7894.9287, 1221.4131],\n",
      "        [ 768.2260, 7873.1006,  974.6167],\n",
      "        [1016.1303, 7838.1968, 1293.5074],\n",
      "        [1035.6642, 7821.1558, 1320.5588],\n",
      "        [ 758.8556, 7774.6953,  975.1608],\n",
      "        [ 762.7465, 7748.8682,  982.2805],\n",
      "        [ 813.0197, 7699.9785, 1057.5612],\n",
      "        [1261.6361, 7671.0122, 1643.4215],\n",
      "        [ 834.0226, 7594.2451, 1100.1573],\n",
      "        [ 734.7173, 7545.2715,  974.3937],\n",
      "        [ 823.9368, 7480.9937, 1102.9840],\n",
      "        [ 620.5080, 7435.2988,  834.7364],\n",
      "        [ 873.8828, 7376.5449, 1183.4596],\n",
      "        [ 705.0461, 7278.2070,  970.6201],\n",
      "        [ 760.1073, 7202.5903, 1055.1824],\n",
      "        [ 704.3625, 7095.9912,  990.1400],\n",
      "        [1116.6799, 6996.5195, 1596.0645],\n",
      "        [ 880.3049, 6894.6230, 1273.4321],\n",
      "        [1005.1959, 6749.8418, 1490.5825],\n",
      "        [ 802.6516, 6669.8184, 1200.4408],\n",
      "        [ 749.7704, 6555.5337, 1140.2206],\n",
      "        [1136.2937, 6447.3257, 1759.3191],\n",
      "        [ 982.6098, 6330.1089, 1555.2528],\n",
      "        [1013.2195, 6236.9541, 1623.4243],\n",
      "        [ 941.2715, 6180.1104, 1516.9536],\n",
      "        [1180.8839, 6093.0884, 1933.3512],\n",
      "        [1182.8497, 6020.8569, 1960.0674],\n",
      "        [ 559.8967, 5919.2178,  950.8912],\n",
      "        [ 809.1595, 5860.9922, 1379.2247],\n",
      "        [ 904.4754, 5803.6929, 1556.1642],\n",
      "        [ 842.6857, 5725.6582, 1474.1556],\n",
      "        [1011.0060, 5690.8159, 1769.1208],\n",
      "        [ 891.5822, 5624.6948, 1584.6312],\n",
      "        [ 732.7235, 5563.6733, 1313.2753],\n",
      "        [ 611.6752, 5515.8228, 1102.1515],\n",
      "        [ 639.1422, 5434.3804, 1177.7015],\n",
      "        [ 908.7026, 5386.5127, 1682.2107],\n",
      "        [ 962.3691, 5345.7163, 1794.5768],\n",
      "        [1046.8317, 5295.3687, 1965.7283],\n",
      "        [1093.0511, 5219.8530, 2097.8596],\n",
      "        [ 972.6187, 5220.0542, 1849.7878],\n",
      "        [1084.4950, 5147.0518, 2089.5405],\n",
      "        [ 815.6473, 5098.5361, 1596.4550],\n",
      "        [ 893.9498, 4924.5981, 1966.9822],\n",
      "        [1530.8307, 5061.2808, 3002.4585],\n",
      "        [ 755.3492, 4955.7876, 1522.2544],\n",
      "        [1174.6989, 4899.0449, 2381.7812],\n",
      "        [1010.4199, 4866.2856, 2069.8833],\n",
      "        [ 996.4611, 4793.9595, 2099.7239],\n",
      "        [ 789.1492, 4793.6802, 1637.3561],\n",
      "        [ 833.8027, 4750.4819, 1742.0160],\n",
      "        [1100.0874, 4721.1968, 2307.7830],\n",
      "        [ 603.0946, 4665.1636, 1297.3839],\n",
      "        [ 946.0119, 4621.1050, 2050.5281],\n",
      "        [1211.1510, 4565.9541, 2659.8792],\n",
      "        [1293.0900, 4586.2236, 2810.0059],\n",
      "        [ 833.1646, 4537.5742, 1828.4011],\n",
      "        [ 967.2180, 4460.3994, 2178.2820],\n",
      "        [ 742.9005, 4448.5078, 1672.7323],\n",
      "        [ 780.9967, 4441.0513, 1754.1362],\n",
      "        [ 877.8683, 4422.4033, 1963.7524],\n",
      "        [ 777.6078, 4378.1084, 1766.2992],\n",
      "        [1029.4165, 4347.1255, 2363.5669],\n",
      "        [ 620.9864, 4300.0229, 1458.9216],\n",
      "        [ 626.0590, 4282.1831, 1471.5483],\n",
      "        [ 824.8499, 4257.8247, 1944.5138],\n",
      "        [ 478.0802, 4246.2515, 1130.2681],\n",
      "        [ 952.1322, 4254.6299, 2226.8269],\n",
      "        [ 813.4500, 4213.1631, 1925.8685],\n",
      "        [ 745.9018, 4190.3911, 1778.6606],\n",
      "        [ 661.8235, 4145.5176, 1602.7333],\n",
      "        [ 798.1346, 4136.2061, 1934.7892],\n",
      "        [1157.9948, 4151.5386, 2754.3391],\n",
      "        [ 841.8206, 4122.8848, 2034.9882],\n",
      "        [ 835.9891, 4094.4102, 2041.2075],\n",
      "        [ 778.2183, 4089.1987, 1835.2300],\n",
      "        [ 604.4710, 4054.2168, 1489.3931],\n",
      "        [ 645.7581, 4009.0916, 1624.1995],\n",
      "        [ 789.7062, 4035.3770, 1938.7621],\n",
      "        [ 666.2957, 3990.8494, 1675.7325],\n",
      "        [ 782.6408, 3975.3999, 1968.1930],\n",
      "        [ 708.6350, 3988.3528, 1752.6823],\n",
      "        [ 879.1012, 3946.0315, 2231.7476],\n",
      "        [ 857.0499, 3963.0168, 2146.8411],\n",
      "        [ 853.2072, 3912.5183, 2187.6504],\n",
      "        [ 798.8309, 3911.7253, 2046.2501],\n",
      "        [ 610.3638, 3906.0896, 1550.5208],\n",
      "        [1034.2091, 3890.2156, 2649.9106],\n",
      "        [ 968.3238, 3854.0886, 2500.7695],\n",
      "        [ 858.2313, 3873.1353, 2195.0049],\n",
      "        [ 631.9578, 3812.7561, 1672.5834],\n",
      "        [ 279.9043, 3806.1541,  740.2041],\n",
      "        [ 829.9914, 3756.1138, 2243.7817],\n",
      "        [ 831.9907, 3737.5652, 2250.1975],\n",
      "        [ 847.3707, 3799.4460, 2220.3564],\n",
      "        [ 714.4259, 3749.6064, 1913.2592],\n",
      "        [ 742.9043, 3758.2249, 1956.7916],\n",
      "        [ 560.9559, 3743.9932, 1494.5927],\n",
      "        [ 877.8168, 3695.8574, 2397.1777],\n",
      "        [ 773.6221, 3728.0967, 2059.3948],\n",
      "        [ 951.2109, 3721.8801, 2539.8096],\n",
      "        [ 507.9230, 3687.9707, 1381.6444],\n",
      "        [ 600.3975, 3704.1516, 1606.5730],\n",
      "        [ 766.7175, 3676.0454, 2072.1921],\n",
      "        [ 761.7207, 3638.8127, 2106.5281],\n",
      "        [ 641.2065, 3662.3892, 1746.3816],\n",
      "        [ 707.1046, 3643.4038, 1938.0260],\n",
      "        [ 650.7858, 3625.9382, 1797.6445],\n",
      "        [ 562.6131, 3612.2815, 1564.2006],\n",
      "        [ 718.2383, 3597.5918, 1995.3043],\n",
      "        [ 825.2297, 3605.5845, 2281.1697],\n",
      "        [ 705.1689, 3573.1553, 1973.8986],\n",
      "        [ 720.2058, 3576.6406, 2011.6785],\n",
      "        [ 827.6707, 3540.9829, 2357.0845],\n",
      "        [ 779.8027, 3529.5437, 2216.4802],\n",
      "        [ 646.3508, 3545.1497, 1817.3511],\n",
      "        [ 613.5286, 3568.8823, 1704.6863],\n",
      "        [1236.9965, 3550.8245, 3476.8892],\n",
      "        [ 898.6031, 3588.8281, 2465.1350],\n",
      "        [ 763.0152, 3530.7893, 2171.8904],\n",
      "        [ 780.7753, 3506.9839, 2251.2151],\n",
      "        [ 860.9858, 3546.4175, 2418.9548],\n",
      "        [ 852.8268, 3567.8323, 2369.8938],\n",
      "        [1024.4930, 3517.2390, 2929.8337],\n",
      "        [1023.0757, 3538.5195, 2897.8926],\n",
      "        [ 640.2281, 3513.6475, 1899.2670],\n",
      "        [ 847.1052, 3573.2561, 2362.0854],\n",
      "        [ 763.9679, 3527.6682, 2185.9417],\n",
      "        [ 755.0475, 3505.8167, 2186.7122],\n",
      "        [ 837.5141, 3556.0413, 2347.5066],\n",
      "        [ 696.5377, 3525.4663, 1995.7650],\n",
      "        [ 626.9955, 3557.8398, 1737.8689],\n",
      "        [ 726.5292, 3506.8220, 2090.2996],\n",
      "        [ 688.2409, 3519.1826, 1961.8455],\n",
      "        [ 621.1640, 3538.3577, 1747.1768],\n",
      "        [ 791.2078, 3505.2412, 2270.5679],\n",
      "        [ 858.6084, 3507.7100, 2441.1555],\n",
      "        [ 685.3416, 3521.2214, 1928.5834],\n",
      "        [ 659.6682, 3508.3469, 1877.2759],\n",
      "        [ 390.7547, 3504.8279, 1101.0641],\n",
      "        [ 616.9106, 3510.0840, 1750.8284]])\n",
      "data:  tensor([[[0.0111, 0.0112, 0.0111,  ..., 0.9324, 0.2278, 0.2609]],\n",
      "\n",
      "        [[0.0104, 0.0103, 0.0100,  ..., 0.9324, 0.2278, 0.2609]],\n",
      "\n",
      "        [[0.0053, 0.0051, 0.0053,  ..., 0.9324, 0.2278, 0.2609]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0074, 0.0077, 0.0087,  ..., 0.6192, 0.2278, 0.7826]],\n",
      "\n",
      "        [[0.0047, 0.0046, 0.0039,  ..., 0.6192, 0.2278, 0.7826]],\n",
      "\n",
      "        [[0.0073, 0.0069, 0.0076,  ..., 0.6192, 0.2278, 0.7826]]])\n",
      "output:  tensor([[2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523],\n",
      "        [2.1769, 7.2491, 6.0523]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 622.3024, 3476.1074, 1810.1487],\n",
      "        [ 668.5460, 3490.8809, 1921.3604],\n",
      "        [ 983.5730, 3500.7839, 2806.3755],\n",
      "        [ 669.1060, 3519.6069, 1885.8365],\n",
      "        [ 678.9028, 3484.3628, 1958.9814],\n",
      "        [ 566.0836, 3497.6226, 1602.1398],\n",
      "        [ 710.0780, 3504.0293, 2031.2662],\n",
      "        [ 797.1811, 3492.4058, 2245.1399],\n",
      "        [ 911.7571, 3472.3037, 2624.4629],\n",
      "        [ 672.5989, 3486.7478, 1930.0652],\n",
      "        [ 887.8760, 3469.6562, 2548.7776],\n",
      "        [ 732.5914, 3463.7803, 2109.9041],\n",
      "        [ 535.4104, 3453.7542, 1554.7349],\n",
      "        [ 453.8930, 3460.9819, 1311.2605],\n",
      "        [ 575.0176, 3437.4224, 1685.8793],\n",
      "        [ 625.7712, 3462.4116, 1783.7831],\n",
      "        [ 585.1133, 3443.6614, 1698.2551],\n",
      "        [ 897.0731, 3389.4485, 2683.1074],\n",
      "        [ 892.8608, 3449.3982, 2580.9568],\n",
      "        [ 815.5912, 3400.3457, 2454.5518],\n",
      "        [ 716.5195, 3451.3860, 2058.9495],\n",
      "        [ 471.1376, 3429.5188, 1356.5151],\n",
      "        [ 922.8301, 3454.0837, 2656.5547],\n",
      "        [ 883.5692, 3444.6499, 2549.5293],\n",
      "        [ 632.4425, 3421.4197, 1845.7130],\n",
      "        [ 554.6069, 3425.1790, 1614.3037],\n",
      "        [ 386.4912, 3409.5286, 1138.1548],\n",
      "        [ 547.7368, 3429.0833, 1580.1278],\n",
      "        [ 855.4423, 3448.8557, 2459.3083],\n",
      "        [ 831.0593, 3444.5828, 2398.2732],\n",
      "        [ 438.5013, 3438.7405, 1273.3221],\n",
      "        [ 486.5287, 3407.5288, 1450.8588],\n",
      "        [ 583.7679, 3450.4141, 1682.4592],\n",
      "        [ 706.5566, 3436.0483, 2054.2629],\n",
      "        [ 597.1362, 3434.3279, 1732.5432],\n",
      "        [ 737.0991, 3450.5815, 2114.5325],\n",
      "        [ 425.9967, 3446.3757, 1232.3024],\n",
      "        [ 836.0219, 3466.2209, 2398.9263],\n",
      "        [ 480.6539, 3458.1333, 1378.1606],\n",
      "        [ 649.6284, 3466.5115, 1863.6461],\n",
      "        [ 469.6179, 3439.5112, 1366.6722],\n",
      "        [ 610.7326, 3448.5374, 1767.1788],\n",
      "        [ 711.3665, 3481.0002, 2021.9360],\n",
      "        [ 501.5638, 3456.0442, 1450.2788],\n",
      "        [ 689.6459, 3477.8892, 1993.2650],\n",
      "        [ 806.8983, 3511.2847, 2300.0994],\n",
      "        [ 959.7779, 3567.1287, 2664.4351],\n",
      "        [ 602.5906, 3510.3743, 1728.8778],\n",
      "        [ 886.0056, 3565.0061, 2477.1484],\n",
      "        [ 475.3301, 3570.1501, 1334.4919],\n",
      "        [ 434.0852, 3606.6682, 1200.0640],\n",
      "        [ 442.8187, 3632.2856, 1222.5050],\n",
      "        [ 466.8443, 3678.8384, 1261.3390],\n",
      "        [ 384.9544, 3683.0442, 1045.8785],\n",
      "        [ 437.4142, 3708.9778, 1178.2721],\n",
      "        [ 433.2812, 3742.9597, 1160.3922],\n",
      "        [ 366.9965, 3791.3582,  960.2238],\n",
      "        [ 406.0908, 3819.3916, 1053.2004],\n",
      "        [ 409.4404, 3831.7803, 1072.2289],\n",
      "        [ 367.8836, 3882.2954,  949.2094],\n",
      "        [ 342.8830, 3924.7844,  874.8281],\n",
      "        [ 574.2347, 3980.0266, 1406.0941],\n",
      "        [ 471.5069, 3984.3647, 1181.7142],\n",
      "        [ 270.4884, 4019.9163,  671.7814],\n",
      "        [ 398.8623, 4050.8601,  976.7501],\n",
      "        [ 258.2272, 4085.5234,  629.5756],\n",
      "        [ 599.4326, 4135.4746, 1440.5056],\n",
      "        [ 247.3748, 4162.9160,  593.3364],\n",
      "        [ 477.8247, 4206.1309, 1129.8307],\n",
      "        [ 418.5812, 4256.2051,  980.4194],\n",
      "        [ 404.1412, 4102.0508, 1114.7559],\n",
      "        [ 287.2123, 4351.9180,  660.0828],\n",
      "        [ 339.2034, 4232.1147, 1086.7229],\n",
      "        [ 585.8180, 4481.2275, 1306.8088],\n",
      "        [ 660.4699, 4517.8965, 1435.9075],\n",
      "        [ 476.2898, 4557.9443, 1044.2684],\n",
      "        [ 497.0175, 4587.4692, 1088.7911],\n",
      "        [ 506.6199, 4637.4204, 1087.8453],\n",
      "        [ 618.1299, 4672.3799, 1310.7561],\n",
      "        [ 558.8108, 4685.8911, 1187.2452],\n",
      "        [ 402.3320, 4692.8564,  858.3448],\n",
      "        [ 637.3412, 4751.6602, 1359.9137],\n",
      "        [ 453.1944, 4808.5542,  954.5056],\n",
      "        [ 508.0153, 4856.8237, 1045.6378],\n",
      "        [ 418.3693, 4841.9536,  868.0705],\n",
      "        [ 418.8263, 4865.5820,  853.9832],\n",
      "        [ 395.0142, 4846.5801,  816.0146],\n",
      "        [ 250.5000, 4840.2798,  517.3323],\n",
      "        [ 453.9255, 4858.7231,  936.4561],\n",
      "        [ 426.7432, 4898.1899,  870.0746],\n",
      "        [ 587.2857, 4933.9204, 1181.9810],\n",
      "        [ 460.9651, 4972.1978,  926.2376],\n",
      "        [ 622.5836, 5001.5942, 1243.2458],\n",
      "        [ 848.1144, 5086.8062, 1646.3684],\n",
      "        [ 548.9955, 5163.6460, 1054.7666],\n",
      "        [ 510.5184, 5154.5361,  990.7996],\n",
      "        [ 595.9218, 5179.5146, 1151.3225],\n",
      "        [ 769.5233, 5265.0005, 1464.6128],\n",
      "        [ 415.3920, 5339.5947,  776.6685],\n",
      "        [ 494.3213, 5412.2227,  911.1747],\n",
      "        [ 650.7133, 5528.9707, 1171.4080],\n",
      "        [ 650.1823, 5620.9863, 1153.2781],\n",
      "        [ 754.7156, 5686.0293, 1340.2325],\n",
      "        [ 486.2370, 5785.8755,  839.7131],\n",
      "        [ 585.8947, 5864.0947, 1000.8914],\n",
      "        [ 690.6865, 5943.9058, 1149.8621],\n",
      "        [ 572.7911, 6017.4497,  952.7824],\n",
      "        [ 459.4889, 6075.6562,  756.9741],\n",
      "        [ 540.1586, 5902.2153, 1060.7070],\n",
      "        [ 553.0781, 6145.4468,  900.2982],\n",
      "        [ 522.9581, 6171.3467,  848.6553],\n",
      "        [ 353.0283, 6202.8936,  568.6151],\n",
      "        [ 535.7194, 6192.2700,  865.1839],\n",
      "        [ 723.3824, 6196.2021, 1155.4188],\n",
      "        [ 603.4606, 6199.2017,  973.6009],\n",
      "        [ 427.3242, 6184.9980,  690.9080],\n",
      "        [ 739.4086, 6149.3584, 1186.2211],\n",
      "        [ 549.5379, 6079.9009,  902.0356],\n",
      "        [ 528.7401, 5976.6924,  885.5532],\n",
      "        [ 519.6797, 5804.3521,  889.5339],\n",
      "        [ 620.8730, 5527.2559, 1347.2368],\n",
      "        [ 673.6165, 5431.2915, 1232.8274],\n",
      "        [ 561.0102, 5248.5342, 1062.1204],\n",
      "        [ 518.6378, 5069.9604, 1022.7719],\n",
      "        [ 390.8189, 4839.0117,  804.6968],\n",
      "        [ 440.1066, 4576.0806,  976.8616],\n",
      "        [ 473.6432, 4395.3789, 1067.3090],\n",
      "        [ 435.4658, 4189.7769, 1038.1915],\n",
      "        [ 493.9919, 4019.5310, 1222.4822],\n",
      "        [ 389.2704, 3814.3591, 1018.0016],\n",
      "        [ 274.5584, 3612.8123,  758.8924],\n",
      "        [ 255.1036, 3434.7693,  740.7839],\n",
      "        [ 277.8659, 3292.7302,  842.5473],\n",
      "        [ 275.8231, 3179.8530,  862.3837],\n",
      "        [ 437.5932, 3125.0425, 1390.4209],\n",
      "        [ 357.3289, 2954.7856, 1200.3070],\n",
      "        [ 248.8055, 2838.0095,  875.0997],\n",
      "        [ 369.7158, 2754.7581, 1337.1146],\n",
      "        [ 326.9102, 2681.6606, 1214.6831],\n",
      "        [ 272.8039, 2629.9614, 1033.2988],\n",
      "        [ 184.9860, 2605.6421,  712.5948],\n",
      "        [ 461.8005, 2740.7273, 1678.4579],\n",
      "        [ 381.0888, 2740.0959, 1382.2251],\n",
      "        [ 224.9897, 2561.4385,  871.9109],\n",
      "        [ 204.6227, 2502.5955,  819.8544],\n",
      "        [ 218.3532, 2534.8181,  858.9518],\n",
      "        [ 273.2008, 2518.8745, 1081.5054],\n",
      "        [ 307.6828, 2487.2800, 1234.6078],\n",
      "        [ 238.1924, 2477.3657,  964.1879],\n",
      "        [ 187.4069, 2479.8625,  751.2198]])\n",
      "data:  tensor([[[0.0072, 0.0067, 0.0066,  ..., 0.6192, 0.2278, 0.7826]],\n",
      "\n",
      "        [[0.0078, 0.0079, 0.0079,  ..., 0.6192, 0.2278, 0.8261]],\n",
      "\n",
      "        [[0.0112, 0.0115, 0.0115,  ..., 0.6192, 0.2278, 0.8261]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0033, 0.0033, 0.0035,  ..., 0.9201, 0.2321, 0.3043]],\n",
      "\n",
      "        [[0.0031, 0.0031, 0.0030,  ..., 0.9152, 0.2321, 0.3043]],\n",
      "\n",
      "        [[0.0018, 0.0019, 0.0020,  ..., 0.9094, 0.2321, 0.3043]]])\n",
      "output:  tensor([[2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044],\n",
      "        [2.2290, 7.3012, 6.1044]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 253.1603, 2439.4517, 1036.5458],\n",
      "        [ 302.1143, 2373.7549, 1275.1495],\n",
      "        [ 262.6549, 2335.1086, 1122.3916],\n",
      "        [ 252.7681, 2323.1504, 1082.2594],\n",
      "        [ 219.6720, 2339.8286,  941.9218],\n",
      "        [ 291.5109, 2362.6621, 1242.7375],\n",
      "        [ 320.7257, 2387.2773, 1298.8441],\n",
      "        [ 404.7186, 2392.9802, 1679.8002],\n",
      "        [ 309.9384, 2397.6104, 1285.0328],\n",
      "        [ 353.7574, 2396.5381, 1460.6121],\n",
      "        [ 311.9526, 2367.1584, 1332.1165],\n",
      "        [ 324.8593, 2383.2615, 1362.3171],\n",
      "        [ 245.9695, 2392.5557, 1023.4084],\n",
      "        [ 210.3703, 2386.7188,  884.0341],\n",
      "        [ 243.2426, 2405.8828, 1009.3821],\n",
      "        [ 243.3276, 2409.3513, 1007.1275],\n",
      "        [ 357.3464, 2420.9243, 1475.4988],\n",
      "        [ 426.4955, 2434.5195, 1746.0836],\n",
      "        [ 295.2404, 2440.4011, 1202.1631],\n",
      "        [ 293.4313, 2449.9912, 1202.8069],\n",
      "        [ 380.9406, 2481.1750, 1543.2682],\n",
      "        [ 496.9247, 2532.3831, 1950.9156],\n",
      "        [ 396.1946, 2543.1294, 1548.9808],\n",
      "        [ 315.1099, 2521.6809, 1255.0043],\n",
      "        [ 496.5282, 2573.0396, 1883.4713],\n",
      "        [ 345.2899, 2556.0820, 1349.7526],\n",
      "        [ 415.6213, 2579.9878, 1577.2798],\n",
      "        [ 469.1767, 2574.8828, 1806.9039],\n",
      "        [ 460.8217, 2592.5676, 1767.7650],\n",
      "        [ 438.6279, 2595.7615, 1609.4696],\n",
      "        [ 440.8158, 2622.8901, 1658.1084],\n",
      "        [ 313.0770, 2610.3171, 1188.2482],\n",
      "        [ 369.2254, 2635.2676, 1394.2736],\n",
      "        [ 362.9229, 2561.5615, 1560.1744],\n",
      "        [ 272.9841, 2662.5862, 1032.2655],\n",
      "        [ 411.6050, 2675.8740, 1540.3453],\n",
      "        [ 365.4572, 2685.3022, 1365.7139],\n",
      "        [ 630.2758, 2694.4849, 2350.5210],\n",
      "        [ 272.0817, 2735.1978,  999.0249],\n",
      "        [ 611.3536, 2765.6721, 2201.9619],\n",
      "        [ 398.1776, 2773.4470, 1441.7181],\n",
      "        [ 421.9680, 2808.8701, 1487.9895],\n",
      "        [ 477.6624, 2836.8032, 1664.8297],\n",
      "        [ 475.1535, 2812.5344, 1709.3236],\n",
      "        [ 551.8752, 2862.5969, 1918.1323],\n",
      "        [ 497.3783, 2898.6960, 1691.8541],\n",
      "        [ 575.2206, 2883.7327, 1995.8102],\n",
      "        [ 423.4897, 2889.2341, 1482.4744],\n",
      "        [ 525.2662, 2946.5916, 1721.8539],\n",
      "        [ 522.7455, 2958.1257, 1744.1320],\n",
      "        [ 518.2934, 2941.5256, 1792.7852],\n",
      "        [ 452.7243, 2969.7434, 1514.0378],\n",
      "        [ 570.4164, 2963.7502, 1927.6799],\n",
      "        [ 655.5001, 2996.8833, 2178.0095],\n",
      "        [ 558.5066, 2998.9502, 1848.7330],\n",
      "        [ 676.9408, 3042.9807, 2138.5400],\n",
      "        [ 685.4681, 2997.4253, 2300.2969],\n",
      "        [ 610.6141, 2988.4495, 2071.2808],\n",
      "        [ 703.5508, 2989.8735, 2524.2146],\n",
      "        [ 782.6042, 3038.8591, 2588.2068],\n",
      "        [ 435.9318, 3064.5796, 1417.4302],\n",
      "        [ 363.6263, 3056.3130, 1199.0612],\n",
      "        [ 836.6193, 3090.1611, 2694.2739],\n",
      "        [ 779.3252, 3089.4016, 2522.4062],\n",
      "        [ 522.7714, 3077.4263, 1693.1246],\n",
      "        [ 730.2411, 3104.9292, 2329.5376],\n",
      "        [ 474.6902, 3106.0798, 1512.4039],\n",
      "        [ 603.7688, 3076.9624, 1966.2948],\n",
      "        [ 610.7022, 3112.9722, 1952.0062],\n",
      "        [ 596.0030, 3144.0554, 1872.3303],\n",
      "        [ 670.3951, 3158.2148, 2102.6689],\n",
      "        [ 538.4903, 3132.0354, 1710.3010],\n",
      "        [ 729.8611, 3100.4219, 2378.8726],\n",
      "        [ 711.3206, 3125.0762, 2275.3938],\n",
      "        [ 809.2113, 3151.4507, 2566.2197],\n",
      "        [ 744.9092, 3173.5862, 2338.7092],\n",
      "        [ 785.9573, 3145.4128, 2457.0308],\n",
      "        [ 821.4120, 3189.4209, 2549.4324],\n",
      "        [ 457.0085, 3159.6057, 1438.3882],\n",
      "        [ 686.0831, 3149.4958, 2182.3362],\n",
      "        [ 889.0850, 3152.1433, 2818.5059],\n",
      "        [ 836.5421, 3139.5034, 2690.3545],\n",
      "        [ 501.3834, 3165.9675, 1568.2516],\n",
      "        [ 660.0737, 3189.4880, 2054.2180],\n",
      "        [ 614.1151, 3180.2998, 1903.7561],\n",
      "        [ 411.4607, 3175.8091, 1290.3145],\n",
      "        [ 623.8510, 3191.0239, 1921.1149],\n",
      "        [ 677.5773, 3173.3962, 2151.5005],\n",
      "        [ 714.0392, 3207.2163, 2202.0090],\n",
      "        [ 810.3994, 3212.4331, 2496.8325],\n",
      "        [ 641.5893, 3133.5940, 2071.7737],\n",
      "        [ 758.7201, 3164.8169, 2417.5229],\n",
      "        [ 493.5785, 3218.7156, 1509.5811],\n",
      "        [ 846.2971, 3170.4189, 2658.8801],\n",
      "        [ 660.1090, 3214.9688, 2037.6991],\n",
      "        [ 742.7440, 3210.9138, 2286.1772],\n",
      "        [ 632.6994, 3181.7129, 1991.5841],\n",
      "        [ 423.1033, 3190.5715, 1313.4681],\n",
      "        [ 775.2090, 3160.9629, 2472.9548],\n",
      "        [ 669.1031, 3198.7542, 2091.1018],\n",
      "        [ 705.0854, 3217.0913, 2171.7075],\n",
      "        [ 514.9473, 3197.5310, 1604.1880],\n",
      "        [ 730.2641, 3215.3096, 2249.5271],\n",
      "        [ 584.7296, 3197.6260, 1821.4210],\n",
      "        [ 797.7184, 3170.7988, 2519.6450],\n",
      "        [ 787.6975, 3186.8459, 2456.4382],\n",
      "        [ 535.3384, 3175.5186, 1676.0726],\n",
      "        [ 749.7258, 3169.2014, 2360.4287],\n",
      "        [ 850.8132, 3131.9463, 2739.3965],\n",
      "        [ 835.0082, 3148.4402, 2659.4951],\n",
      "        [ 943.2928, 3149.1494, 2997.5442],\n",
      "        [ 649.9576, 3170.8938, 2015.0139],\n",
      "        [ 885.8300, 3071.6787, 2921.4802],\n",
      "        [ 827.0339, 3122.2888, 2646.4048],\n",
      "        [ 483.3987, 3103.9741, 1577.6439],\n",
      "        [ 586.1262, 3103.2087, 1901.5319],\n",
      "        [ 478.3860, 3086.6924, 1559.1395],\n",
      "        [ 618.5484, 3101.9019, 1954.1652],\n",
      "        [ 678.3467, 3034.9150, 2474.3467],\n",
      "        [ 569.8869, 3078.3813, 1864.7592],\n",
      "        [ 712.5689, 3095.0317, 2271.4666],\n",
      "        [ 548.1178, 3074.7898, 1766.6489],\n",
      "        [ 530.6891, 3089.6641, 1700.9341],\n",
      "        [ 730.0577, 3083.1289, 2364.2791],\n",
      "        [ 884.8699, 3064.7136, 2897.4434],\n",
      "        [ 894.2230, 3013.7388, 3010.5173],\n",
      "        [ 773.3936, 3079.7610, 2479.4060],\n",
      "        [ 826.5936, 3069.9695, 2670.5276],\n",
      "        [ 802.8154, 3044.5054, 2635.3882],\n",
      "        [ 681.8914, 3016.5723, 2271.0474],\n",
      "        [ 752.0874, 3033.6135, 2480.7542],\n",
      "        [ 604.9774, 3035.5964, 1991.2446],\n",
      "        [ 689.3384, 3049.2642, 2259.3591],\n",
      "        [ 530.6216, 3025.8889, 1762.1479],\n",
      "        [ 865.2559, 3051.5598, 2838.2354],\n",
      "        [ 776.0529, 3065.6519, 2537.9980],\n",
      "        [ 576.9556, 2959.3267, 1993.4968],\n",
      "        [ 772.3716, 3105.6555, 2452.1736],\n",
      "        [ 511.2388, 3048.2700, 1683.9131],\n",
      "        [ 536.0354, 3061.5803, 1760.8949],\n",
      "        [ 412.3821, 3075.6108, 1341.9597],\n",
      "        [ 489.1714, 3075.9348, 1593.9678],\n",
      "        [ 523.7648, 3101.1421, 1661.7229],\n",
      "        [ 713.2322, 3116.8262, 2241.3201],\n",
      "        [ 526.3133, 3061.6750, 1729.5073],\n",
      "        [ 567.8659, 3060.0273, 1860.1736],\n",
      "        [ 567.9953, 3040.1709, 1886.4762],\n",
      "        [ 684.0812, 3070.9023, 2231.7986],\n",
      "        [ 469.5344, 3066.8418, 1528.3893],\n",
      "        [ 834.6723, 3079.9006, 2656.4290]])\n",
      "data:  tensor([[[0.0033, 0.0034, 0.0030,  ..., 0.9050, 0.2321, 0.3043]],\n",
      "\n",
      "        [[0.0036, 0.0034, 0.0035,  ..., 0.8995, 0.2321, 0.3043]],\n",
      "\n",
      "        [[0.0031, 0.0030, 0.0029,  ..., 0.8968, 0.2321, 0.3043]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0072, 0.0071, 0.0074,  ..., 0.6147, 0.2321, 0.8261]],\n",
      "\n",
      "        [[0.0063, 0.0064, 0.0067,  ..., 0.6157, 0.2321, 0.8261]],\n",
      "\n",
      "        [[0.0076, 0.0076, 0.0076,  ..., 0.6157, 0.2321, 0.8261]]])\n",
      "Epoch 0, Iteration 60 Train Loss: 897.75\n",
      "output:  tensor([[2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565],\n",
      "        [2.2810, 7.3533, 6.1565]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 743.0538, 3029.9441, 2464.2905],\n",
      "        [ 608.2106, 3048.9458, 1980.1388],\n",
      "        [ 536.2344, 3029.5530, 1778.2018],\n",
      "        [ 438.2016, 3028.6479, 1454.5575],\n",
      "        [ 489.5692, 3024.7495, 1613.5093],\n",
      "        [ 532.1473, 3033.0774, 1728.4708],\n",
      "        [ 510.4235, 2995.2415, 1714.5304],\n",
      "        [ 385.7585, 3006.0437, 1260.3314],\n",
      "        [ 735.1928, 2981.4036, 2474.8113],\n",
      "        [ 641.4742, 2939.9729, 2204.5664],\n",
      "        [ 760.3879, 3013.6121, 2472.3096],\n",
      "        [ 615.9608, 2967.3865, 2056.2129],\n",
      "        [ 826.5859, 2932.9575, 2816.3845],\n",
      "        [ 538.8113, 2935.1191, 1848.4144],\n",
      "        [ 724.2922, 2909.1130, 2507.2161],\n",
      "        [ 723.0734, 2926.8694, 2481.1738],\n",
      "        [ 494.5899, 2933.6055, 1681.7256],\n",
      "        [ 592.7412, 2876.3152, 2224.9429],\n",
      "        [ 753.7044, 2923.2166, 2582.0149],\n",
      "        [ 440.7850, 2935.5771, 1499.5708],\n",
      "        [ 543.7217, 2914.2517, 1865.0833],\n",
      "        [ 465.4493, 2902.8853, 1629.9521],\n",
      "        [ 529.7458, 2926.2605, 1805.0718],\n",
      "        [ 506.4850, 2925.6907, 1720.2726],\n",
      "        [ 586.6666, 2934.6443, 1979.5538],\n",
      "        [ 410.2360, 2929.0532, 1395.1172],\n",
      "        [ 502.1960, 2934.3091, 1679.5493],\n",
      "        [ 474.6066, 2919.8315, 1614.5658],\n",
      "        [ 476.4997, 2927.4949, 1628.3730],\n",
      "        [ 487.5426, 2944.8992, 1643.2567],\n",
      "        [ 472.9135, 2927.2156, 1628.2704],\n",
      "        [ 535.7504, 2931.8740, 1850.3302],\n",
      "        [ 742.0421, 2942.0674, 2516.0713],\n",
      "        [ 380.5659, 2950.3228, 1288.3755],\n",
      "        [ 602.4958, 2962.6443, 2005.0225],\n",
      "        [ 716.6605, 2948.3792, 2422.8359],\n",
      "        [ 308.7858, 2917.4521, 1075.7054],\n",
      "        [ 502.7090, 2950.4346, 1694.3419],\n",
      "        [ 595.8089, 2928.3606, 2035.4893],\n",
      "        [ 507.2764, 2944.7092, 1707.6689],\n",
      "        [ 579.4070, 2927.5117, 1974.4005],\n",
      "        [ 542.4426, 2913.3335, 1871.7021],\n",
      "        [ 403.1252, 2931.9856, 1373.0590],\n",
      "        [ 608.3739, 2919.3345, 2085.8320],\n",
      "        [ 475.6869, 2936.3591, 1607.7704],\n",
      "        [ 679.1126, 2909.2471, 2341.1565],\n",
      "        [ 310.5645, 2920.3176, 1057.3350],\n",
      "        [ 205.3830, 2930.5278,  692.4200],\n",
      "        [ 369.7254, 2929.7012, 1252.9594],\n",
      "        [ 301.5665, 2909.1465, 1037.6923],\n",
      "        [ 285.4589, 2874.2539,  999.7761],\n",
      "        [ 341.8345, 2858.5420, 1195.1511],\n",
      "        [ 219.0289, 2845.2595,  767.5541],\n",
      "        [ 339.8388, 2807.3733, 1203.5978],\n",
      "        [ 192.6053, 2765.8059,  691.2809],\n",
      "        [ 271.8429, 2697.3447, 1012.2850],\n",
      "        [ 287.6635, 2637.9875, 1089.4045],\n",
      "        [ 340.2144, 2613.6516, 1293.8970],\n",
      "        [ 263.6861, 2553.1775, 1028.6177],\n",
      "        [ 266.6238, 2628.2744, 1009.6386],\n",
      "        [ 753.1641, 5287.8730, 1420.2909],\n",
      "        [ 516.6074, 5903.0308,  878.5731],\n",
      "        [ 812.2319, 5554.1333, 1458.2349],\n",
      "        [ 647.0834, 5545.6323, 1164.0568],\n",
      "        [ 766.4606, 5823.4712, 1326.0255],\n",
      "        [ 713.6381, 6909.4863, 1032.8550],\n",
      "        [ 910.6512, 7220.1626, 1259.9155],\n",
      "        [ 965.2340, 7341.9204, 1316.7740],\n",
      "        [ 875.9601, 7433.0532, 1180.9001],\n",
      "        [ 950.9756, 7493.7397, 1270.6885],\n",
      "        [ 764.2007, 7523.6333, 1017.9588],\n",
      "        [ 834.7966, 7543.6909, 1108.4092],\n",
      "        [ 992.9099, 7547.1147, 1317.9260],\n",
      "        [ 733.5010, 7562.6704,  970.9833],\n",
      "        [ 836.8862, 7560.6792, 1108.6040],\n",
      "        [ 923.1853, 7554.9902, 1225.5812],\n",
      "        [ 805.2410, 7569.6465, 1065.1960],\n",
      "        [ 759.1116, 7576.3770, 1004.0366],\n",
      "        [1189.1296, 7569.3130, 1571.0817],\n",
      "        [1019.0982, 7529.8667, 1368.2186],\n",
      "        [1189.8723, 7549.1201, 1548.0288],\n",
      "        [ 691.8329, 7515.5513,  922.5453],\n",
      "        [ 859.4636, 7527.3979, 1128.9042],\n",
      "        [ 906.7368, 7536.9043, 1200.7706],\n",
      "        [ 636.8671, 7553.3423,  844.3168],\n",
      "        [ 714.3417, 7586.4419,  932.4100],\n",
      "        [ 686.2455, 7594.4072,  905.1993],\n",
      "        [1015.6163, 7626.0210, 1321.0178],\n",
      "        [ 639.5983, 7633.0142,  838.4129],\n",
      "        [ 726.1840, 7676.1060,  939.8369],\n",
      "        [ 494.4766, 7687.7964,  643.5461],\n",
      "        [1069.7639, 7733.7930, 1366.2158],\n",
      "        [1362.8182, 7767.8086, 1727.1993],\n",
      "        [1096.8079, 7794.9985, 1399.6541],\n",
      "        [ 597.2642, 7844.5029,  762.9528],\n",
      "        [ 781.6110, 7880.5239,  994.0148],\n",
      "        [1041.0604, 7961.0107, 1298.4434],\n",
      "        [ 832.4544, 8004.8179, 1040.1427],\n",
      "        [ 558.9333, 8066.2134,  693.3297],\n",
      "        [ 674.1141, 8131.1338,  828.8525],\n",
      "        [ 824.2292, 8187.0107, 1007.7521],\n",
      "        [1014.0580, 8263.3760, 1226.8605],\n",
      "        [ 635.1011, 8332.0498,  762.5236],\n",
      "        [1045.5798, 8359.2119, 1262.3198],\n",
      "        [ 811.8661, 8427.1699,  964.0086],\n",
      "        [ 735.8267, 8481.1875,  867.9077],\n",
      "        [1235.9650, 8539.8174, 1438.3743],\n",
      "        [ 771.7750, 8587.8428,  898.6667],\n",
      "        [ 645.7704, 8611.1338,  750.1429],\n",
      "        [ 771.3597, 8672.6416,  890.0814],\n",
      "        [ 809.1942, 8721.3467,  926.6669],\n",
      "        [ 881.6402, 8752.7822, 1007.7378],\n",
      "        [ 888.7083, 8794.7793, 1010.0638],\n",
      "        [ 941.8727, 8824.4434, 1066.6755],\n",
      "        [ 902.2108, 8834.9785, 1020.8950],\n",
      "        [ 778.5971, 8855.8516,  879.2997],\n",
      "        [ 799.6963, 8865.6758,  901.9672],\n",
      "        [1224.9625, 8852.1650, 1389.5820],\n",
      "        [1586.7666, 8863.0674, 1794.6486],\n",
      "        [ 966.5125, 8872.7861, 1087.9872],\n",
      "        [1035.3602, 8868.2676, 1168.6023],\n",
      "        [1146.1938, 8855.5107, 1295.9906],\n",
      "        [ 596.0659, 8863.1963,  672.3463],\n",
      "        [1099.7483, 8833.5312, 1249.6166],\n",
      "        [1102.3339, 8836.2686, 1230.2487],\n",
      "        [ 720.3378, 8789.0654,  820.6846],\n",
      "        [ 724.7047, 8765.6172,  826.5215],\n",
      "        [ 977.0857, 8734.4277, 1115.2443],\n",
      "        [ 522.2389, 8705.0488,  599.9612],\n",
      "        [ 869.4324, 8685.3984, 1002.2498],\n",
      "        [ 602.5659, 8659.7334,  695.4249],\n",
      "        [1329.3031, 8599.5273, 1557.8641],\n",
      "        [ 867.2551, 8566.2207, 1018.0740],\n",
      "        [ 784.2301, 8543.8682,  911.0680],\n",
      "        [1348.6826, 8471.6025, 1582.5958],\n",
      "        [1244.8345, 8446.2500, 1472.3792],\n",
      "        [ 971.3583, 8392.0039, 1159.4091],\n",
      "        [ 803.5500, 8340.1182,  964.1367],\n",
      "        [1053.1892, 8291.5156, 1257.5337],\n",
      "        [1352.9712, 8231.1191, 1630.5096],\n",
      "        [1005.9805, 8144.3320, 1231.9551],\n",
      "        [ 779.0587, 8049.7251,  968.8858],\n",
      "        [ 580.9135, 7962.7979,  730.9175],\n",
      "        [ 579.7263, 7893.9902,  734.5089],\n",
      "        [ 948.8024, 7818.3403, 1214.5159],\n",
      "        [ 758.2506, 7731.9106,  984.0911],\n",
      "        [ 924.3440, 7667.0630, 1206.0031],\n",
      "        [ 869.2521, 7600.1377, 1143.1968],\n",
      "        [ 784.5145, 7525.5601, 1038.7478],\n",
      "        [ 714.2119, 7418.5591,  963.6422]])\n",
      "data:  tensor([[[0.0108, 0.0107, 0.0106,  ..., 0.6157, 0.2321, 0.8261]],\n",
      "\n",
      "        [[0.0071, 0.0071, 0.0070,  ..., 0.6157, 0.2321, 0.8696]],\n",
      "\n",
      "        [[0.0056, 0.0051, 0.0054,  ..., 0.6157, 0.2321, 0.8696]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0104, 0.0104, 0.0096,  ..., 0.8989, 0.2363, 0.3478]],\n",
      "\n",
      "        [[0.0098, 0.0097, 0.0101,  ..., 0.8924, 0.2363, 0.3478]],\n",
      "\n",
      "        [[0.0075, 0.0064, 0.0065,  ..., 0.8848, 0.2363, 0.3478]]])\n",
      "output:  tensor([[2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085],\n",
      "        [2.3331, 7.4053, 6.2085]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 459.0530, 7313.3062,  628.0692],\n",
      "        [ 709.4665, 7220.0508,  979.4435],\n",
      "        [1095.2325, 7071.2754, 1564.2811],\n",
      "        [ 871.3832, 6970.1836, 1248.6519],\n",
      "        [ 823.2175, 6842.3374, 1203.2051],\n",
      "        [ 779.4448, 6730.1304, 1159.6227],\n",
      "        [1177.2725, 6628.1396, 1773.8108],\n",
      "        [ 734.3434, 6501.8853, 1129.5104],\n",
      "        [ 941.5308, 6398.8940, 1468.1060],\n",
      "        [ 886.1501, 6285.9331, 1411.9813],\n",
      "        [ 700.9731, 6208.4121, 1131.3285],\n",
      "        [ 783.2343, 6125.0039, 1278.3156],\n",
      "        [ 916.3992, 6042.8359, 1516.0677],\n",
      "        [1036.7833, 5985.6797, 1724.2272],\n",
      "        [ 846.0264, 5879.4214, 1441.6257],\n",
      "        [ 789.2265, 5833.3130, 1348.1250],\n",
      "        [1107.6307, 5747.6875, 1930.6578],\n",
      "        [ 859.1503, 5698.7197, 1505.8125],\n",
      "        [ 739.9655, 5625.3301, 1317.9003],\n",
      "        [ 955.5407, 5588.7188, 1699.0947],\n",
      "        [1090.9470, 5512.4263, 1974.0038],\n",
      "        [ 833.1417, 5441.9658, 1537.7793],\n",
      "        [ 816.2780, 5385.1333, 1521.5232],\n",
      "        [ 893.9537, 5340.3096, 1673.0081],\n",
      "        [ 720.9637, 5299.0664, 1352.3735],\n",
      "        [ 867.0179, 5247.6294, 1648.4875],\n",
      "        [1225.1920, 5185.8931, 2364.1763],\n",
      "        [ 908.3431, 5134.5122, 1766.1455],\n",
      "        [1164.8833, 5081.2656, 2291.5918],\n",
      "        [ 973.7929, 5053.8633, 1922.5527],\n",
      "        [1199.1527, 4989.8706, 2408.0813],\n",
      "        [ 758.5573, 4956.8989, 1527.5896],\n",
      "        [ 695.5967, 4926.1401, 1406.4740],\n",
      "        [ 771.3876, 4871.8042, 1584.5374],\n",
      "        [ 849.9702, 4829.5894, 1756.1255],\n",
      "        [ 885.7434, 4775.3091, 1840.6105],\n",
      "        [1077.2432, 4762.3843, 2266.2900],\n",
      "        [ 966.7631, 4729.0669, 2030.5764],\n",
      "        [ 877.0140, 4701.1396, 1858.4496],\n",
      "        [ 919.1208, 4652.7803, 1967.0197],\n",
      "        [ 989.8632, 4652.1602, 2109.1680],\n",
      "        [ 489.1779, 4588.3462, 1072.2393],\n",
      "        [ 784.3824, 4572.8354, 1714.1234],\n",
      "        [ 496.4359, 4516.7847, 1105.4321],\n",
      "        [1112.2480, 4543.7461, 2422.2737],\n",
      "        [1155.7296, 4443.0396, 2613.1721],\n",
      "        [1103.0989, 4435.0859, 2496.7336],\n",
      "        [ 849.3654, 4397.6577, 1938.5876],\n",
      "        [1132.7075, 4394.5352, 2522.0732],\n",
      "        [ 999.5786, 4378.7749, 2275.4375],\n",
      "        [ 837.9984, 4353.4541, 1896.4757],\n",
      "        [1320.5121, 4327.1460, 3000.7698],\n",
      "        [ 934.5401, 4282.9033, 2175.6626],\n",
      "        [1017.9490, 4279.2729, 2369.9949],\n",
      "        [ 530.4014, 4242.1074, 1253.3400],\n",
      "        [ 846.1691, 4221.9155, 2005.1293],\n",
      "        [ 840.5175, 4205.8740, 2003.5042],\n",
      "        [ 883.1429, 4164.6533, 2139.0068],\n",
      "        [ 791.6996, 4175.9805, 1889.1100],\n",
      "        [ 727.0823, 4150.3823, 1749.1045],\n",
      "        [1205.6161, 4153.8174, 2881.8091],\n",
      "        [ 995.0987, 4114.1489, 2417.9221],\n",
      "        [1043.0599, 4069.4766, 2575.4417],\n",
      "        [ 841.6904, 4088.9253, 2061.1648],\n",
      "        [1205.0642, 4090.1594, 2908.5054],\n",
      "        [ 956.1550, 4052.5469, 2351.5127],\n",
      "        [ 783.0264, 4045.6096, 1922.7328],\n",
      "        [ 908.1107, 4054.4739, 2214.3647],\n",
      "        [ 882.9235, 3994.2510, 2214.4080],\n",
      "        [ 790.1024, 3984.2585, 1984.3999],\n",
      "        [1015.3196, 3983.4651, 2537.6091],\n",
      "        [ 836.1097, 3956.4861, 2115.9294],\n",
      "        [ 923.3484, 3949.1929, 2344.8369],\n",
      "        [ 858.2693, 3932.1851, 2190.5486],\n",
      "        [ 604.0281, 3954.3206, 1522.7085],\n",
      "        [ 520.0823, 3920.0479, 1339.8910],\n",
      "        [ 795.6307, 3942.4290, 2013.0135],\n",
      "        [ 841.2936, 3950.3154, 2125.5015],\n",
      "        [1035.6448, 3929.8894, 2632.3137],\n",
      "        [ 951.8420, 3915.6184, 2436.1343],\n",
      "        [ 866.8277, 3920.2822, 2197.2349],\n",
      "        [ 706.5503, 3910.9675, 1804.3778],\n",
      "        [1200.6621, 3862.0815, 3113.8767],\n",
      "        [ 912.0414, 3895.8960, 2329.4155],\n",
      "        [ 925.5543, 3867.2090, 2385.2822],\n",
      "        [ 721.9821, 3840.7339, 1881.5482],\n",
      "        [ 838.2963, 3831.6125, 2187.0107],\n",
      "        [ 831.1821, 3792.5645, 2207.0112],\n",
      "        [ 957.2075, 3742.0103, 2590.7131],\n",
      "        [ 978.6191, 3798.1277, 2562.7825],\n",
      "        [ 851.5071, 3681.3855, 2358.9915],\n",
      "        [ 478.7216, 3763.5312, 1268.8099],\n",
      "        [1118.3889, 3730.5432, 2998.8857],\n",
      "        [ 887.5945, 3745.1938, 2359.0259],\n",
      "        [ 683.6323, 3732.4871, 1795.1949],\n",
      "        [ 661.2870, 3690.1992, 1818.3508],\n",
      "        [ 867.1276, 3735.1624, 2295.2478],\n",
      "        [1152.6611, 3746.5903, 3043.7046],\n",
      "        [ 518.4622, 3701.9900, 1376.4248],\n",
      "        [1195.4946, 3668.2371, 3256.8638],\n",
      "        [ 912.7845, 3666.8521, 2487.6328],\n",
      "        [ 936.3613, 3677.1514, 2529.6995],\n",
      "        [ 796.5432, 3628.7253, 2213.7681],\n",
      "        [ 557.6325, 3636.4221, 1541.7904],\n",
      "        [ 901.7681, 3667.7065, 2438.5417],\n",
      "        [ 647.5177, 3647.9060, 1775.2726],\n",
      "        [ 926.8900, 3629.4402, 2561.4863],\n",
      "        [ 592.0891, 3648.4868, 1615.6198],\n",
      "        [ 743.4624, 3611.2261, 2066.7017],\n",
      "        [ 684.3439, 3588.2808, 1935.1915],\n",
      "        [ 877.6764, 3615.6553, 2425.2515],\n",
      "        [ 908.9445, 3631.7861, 2489.5481],\n",
      "        [ 812.4125, 3610.3604, 2239.0408],\n",
      "        [ 742.7866, 3594.8604, 2077.1892],\n",
      "        [ 959.0374, 3595.2751, 2666.9338],\n",
      "        [ 707.6387, 3599.2842, 1957.7572],\n",
      "        [ 690.4039, 3611.7122, 1898.9031],\n",
      "        [ 989.5616, 3649.8721, 2654.0837],\n",
      "        [ 662.6962, 3593.1570, 1853.7280],\n",
      "        [ 539.2918, 3558.8899, 1542.5122],\n",
      "        [ 922.0592, 3624.8435, 2523.3486],\n",
      "        [ 465.5307, 3613.5664, 1267.0438],\n",
      "        [ 761.0848, 3596.0112, 2090.4407],\n",
      "        [ 909.3126, 3563.0232, 2558.9648],\n",
      "        [ 629.9938, 3565.9221, 1754.7664],\n",
      "        [ 785.2834, 3555.2983, 2206.9954],\n",
      "        [ 877.3922, 3512.6646, 2516.4282],\n",
      "        [ 664.6189, 3534.1294, 1890.2275],\n",
      "        [ 991.8516, 3490.3391, 2868.5769],\n",
      "        [1020.7408, 3532.1633, 2890.6448],\n",
      "        [ 654.4759, 3550.2939, 1828.9940],\n",
      "        [ 411.5557, 3520.9924, 1174.2394],\n",
      "        [ 319.1552, 3532.5432,  899.8026],\n",
      "        [ 921.3979, 3525.7959, 2602.8899],\n",
      "        [ 831.7399, 3513.6641, 2371.2886],\n",
      "        [ 529.6953, 3508.6819, 1512.8239],\n",
      "        [ 630.4969, 3521.4001, 1783.4701],\n",
      "        [ 976.3541, 3537.2573, 2741.3960],\n",
      "        [ 653.5644, 3504.3755, 1869.7668],\n",
      "        [ 736.1541, 3522.5173, 2064.5508],\n",
      "        [ 458.9750, 3480.3245, 1331.8528],\n",
      "        [ 664.1934, 3481.5757, 1924.6281],\n",
      "        [ 755.8798, 3497.2651, 2162.4998],\n",
      "        [ 558.9589, 3481.8435, 1605.9374],\n",
      "        [ 801.4998, 3517.3672, 2247.7617],\n",
      "        [ 531.6362, 3470.4827, 1533.4814],\n",
      "        [ 935.7911, 3423.3132, 2757.6914],\n",
      "        [ 977.4503, 3478.4563, 2794.5579],\n",
      "        [ 881.0295, 3439.1426, 2578.4180],\n",
      "        [ 557.1516, 3461.0320, 1621.0353]])\n",
      "data:  tensor([[[0.0055, 0.0064, 0.0070,  ..., 0.8769, 0.2363, 0.3478]],\n",
      "\n",
      "        [[0.0088, 0.0080, 0.0080,  ..., 0.8691, 0.2363, 0.3478]],\n",
      "\n",
      "        [[0.0129, 0.0128, 0.0128,  ..., 0.8614, 0.2363, 0.3478]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0115, 0.0105, 0.0094,  ..., 0.6228, 0.2363, 0.8696]],\n",
      "\n",
      "        [[0.0111, 0.0119, 0.0136,  ..., 0.6228, 0.2363, 0.8696]],\n",
      "\n",
      "        [[0.0065, 0.0066, 0.0066,  ..., 0.6228, 0.2363, 0.8696]]])\n",
      "output:  tensor([[2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605],\n",
      "        [2.3851, 7.4573, 6.2605]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 747.9978, 3370.4858, 2343.6101],\n",
      "        [ 398.4236, 3457.2451, 1164.7673],\n",
      "        [ 924.9362, 3436.6069, 2696.7412],\n",
      "        [ 800.3471, 3490.2388, 2275.2441],\n",
      "        [ 708.7253, 3471.4771, 2040.7542],\n",
      "        [ 406.4828, 3471.5776, 1164.5203],\n",
      "        [ 849.0190, 3466.0312, 2451.6543],\n",
      "        [ 595.9674, 3451.0842, 1738.2391],\n",
      "        [ 920.7888, 3484.3350, 2606.1887],\n",
      "        [ 790.3055, 3452.5088, 2306.0947],\n",
      "        [ 720.7935, 3509.2292, 2026.0115],\n",
      "        [ 851.2966, 3482.3911, 2441.3579],\n",
      "        [ 731.7634, 3542.0608, 2046.7975],\n",
      "        [ 864.6591, 3526.3936, 2417.0698],\n",
      "        [ 812.9832, 3500.0132, 2335.7727],\n",
      "        [ 822.7213, 3537.9890, 2317.7217],\n",
      "        [ 541.7623, 3539.2402, 1528.5479],\n",
      "        [ 556.8975, 3538.3577, 1574.8262],\n",
      "        [ 498.9557, 3546.2332, 1408.6011],\n",
      "        [ 370.9735, 3536.5144, 1066.9232],\n",
      "        [ 735.6347, 3592.5425, 2034.8838],\n",
      "        [ 602.4005, 3574.9817, 1685.4100],\n",
      "        [ 605.3354, 3608.8467, 1675.8289],\n",
      "        [ 448.9069, 3617.5376, 1238.7847],\n",
      "        [ 612.7072, 3634.2996, 1692.2787],\n",
      "        [ 496.2972, 3639.4104, 1361.2646],\n",
      "        [ 410.8458, 3648.7102, 1131.8439],\n",
      "        [ 287.5147, 3661.7800,  779.9674],\n",
      "        [ 594.7358, 3661.4841, 1627.1964],\n",
      "        [ 362.8755, 3684.9319,  988.1771],\n",
      "        [ 579.9364, 3687.7869, 1581.4177],\n",
      "        [ 539.9033, 3727.7727, 1451.3741],\n",
      "        [ 432.4461, 3763.7937, 1144.8083],\n",
      "        [ 311.3567, 3790.2075,  815.4831],\n",
      "        [ 423.4456, 3795.4355, 1120.5562],\n",
      "        [ 534.7380, 3818.0510, 1404.8593],\n",
      "        [ 331.3220, 3856.5295,  859.1757],\n",
      "        [ 363.7177, 3891.2712,  932.7444],\n",
      "        [ 417.7159, 3901.7888, 1073.9650],\n",
      "        [ 343.3521, 3933.4307,  875.5674],\n",
      "        [ 301.9301, 3984.6160,  753.8192],\n",
      "        [ 367.2108, 4004.9082,  917.9334],\n",
      "        [ 347.7220, 4040.6721,  859.6050],\n",
      "        [ 272.9825, 4080.5190,  668.5727],\n",
      "        [ 473.0910, 4123.6387, 1150.4358],\n",
      "        [ 302.8089, 4179.5664,  724.9102],\n",
      "        [ 599.1159, 4227.4785, 1411.9749],\n",
      "        [ 442.6610, 4091.0532, 1204.9912],\n",
      "        [ 382.4987, 4316.7515,  886.0798],\n",
      "        [ 602.6640, 4343.0591, 1385.9398],\n",
      "        [ 539.5923, 4372.1431, 1244.4120],\n",
      "        [ 426.7203, 4433.3711,  961.0005],\n",
      "        [ 355.6428, 4460.6841,  795.0160],\n",
      "        [ 428.2427, 4468.7104,  955.9036],\n",
      "        [ 461.1629, 4487.9360, 1028.7155],\n",
      "        [ 513.3118, 4545.9805, 1125.7701],\n",
      "        [ 572.0005, 4568.4395, 1242.4117],\n",
      "        [ 427.6037, 4595.5068,  933.0807],\n",
      "        [ 520.3342, 4529.0171, 1383.0275],\n",
      "        [ 500.9674, 4677.0942, 1073.8491],\n",
      "        [ 677.2515, 4740.3721, 1396.1823],\n",
      "        [ 666.5861, 4756.5308, 1401.0935],\n",
      "        [ 358.9791, 4791.9878,  747.2910],\n",
      "        [ 604.0051, 4857.7993, 1211.2590],\n",
      "        [ 486.5693, 4837.7104, 1009.6091],\n",
      "        [ 472.3581, 4879.1548,  971.3217],\n",
      "        [ 648.2079, 4946.9512, 1292.2312],\n",
      "        [ 433.0549, 4993.7856,  860.1722],\n",
      "        [ 501.0315, 5034.2471,  991.9372],\n",
      "        [ 712.3179, 5079.8359, 1379.5850],\n",
      "        [ 776.2297, 5123.7544, 1506.9766],\n",
      "        [ 599.3474, 5166.4946, 1160.6606],\n",
      "        [ 619.2574, 5214.7588, 1184.2316],\n",
      "        [ 545.2013, 5250.8745, 1040.1108],\n",
      "        [ 399.0359, 5302.0991,  753.5453],\n",
      "        [ 524.3931, 5320.0063,  986.6133],\n",
      "        [ 467.7906, 5355.2622,  879.5584],\n",
      "        [ 509.6404, 5395.7622,  946.9175],\n",
      "        [ 518.5619, 5436.7041,  947.4967],\n",
      "        [ 482.4706, 5461.5093,  885.2123],\n",
      "        [ 707.5547, 5494.6646, 1275.3903],\n",
      "        [ 545.1830, 5496.5581,  992.3149],\n",
      "        [ 585.6612, 5510.1030, 1055.1274],\n",
      "        [ 612.8928, 5504.0312, 1115.6051],\n",
      "        [ 618.9754, 5572.4204, 1074.4067],\n",
      "        [ 613.4431, 5387.0884, 1186.7953],\n",
      "        [ 749.9193, 5564.4106, 1347.5616],\n",
      "        [ 562.7475, 5567.2817, 1009.4792],\n",
      "        [ 278.6053, 5547.4419,  503.3400],\n",
      "        [ 350.1152, 5525.0215,  633.9578],\n",
      "        [ 502.7119, 5477.4277,  918.9324],\n",
      "        [ 471.0750, 5415.1162,  869.3591],\n",
      "        [ 607.4775, 5314.9961, 1148.5836],\n",
      "        [ 430.5720, 5219.2720,  822.0021],\n",
      "        [ 433.6317, 5082.7588,  852.8069],\n",
      "        [ 370.5302, 4951.8892,  755.7919],\n",
      "        [ 352.5540, 4836.9341,  728.0759],\n",
      "        [ 478.5263, 4708.7861, 1017.1581],\n",
      "        [ 350.9539, 4571.5396,  764.5063],\n",
      "        [ 355.5765, 4416.9722,  803.7894],\n",
      "        [ 373.6543, 4227.3501,  881.9325],\n",
      "        [ 348.8355, 4032.9304,  867.7875],\n",
      "        [ 337.3079, 3898.0688,  861.3267],\n",
      "        [ 345.7539, 3759.4202,  917.9294],\n",
      "        [ 238.2672, 3602.8813,  654.1415],\n",
      "        [ 263.3129, 3424.8716,  762.3920],\n",
      "        [ 299.1589, 3233.5574,  919.7833],\n",
      "        [ 216.9032, 3070.1260,  705.1207],\n",
      "        [ 289.5508, 2944.4580,  979.5997],\n",
      "        [ 246.8732, 2832.3237,  870.5461],\n",
      "        [ 272.3152, 2735.0413,  991.9490],\n",
      "        [ 252.0909, 2673.9917,  935.1974],\n",
      "        [ 250.3492, 2600.9558,  962.6472],\n",
      "        [ 277.9218, 2540.0964, 1091.8597],\n",
      "        [ 277.9840, 2480.0132, 1086.9683],\n",
      "        [ 347.0312, 2401.5427, 1418.2444],\n",
      "        [ 272.6602, 2324.9099, 1164.4821],\n",
      "        [ 259.6099, 2258.9675, 1140.0818],\n",
      "        [ 249.7266, 2196.5271, 1133.0897],\n",
      "        [ 227.3063, 2157.0881, 1045.5255],\n",
      "        [ 192.5764, 2117.3977,  906.4315],\n",
      "        [ 211.8680, 2091.9390, 1010.5364],\n",
      "        [ 311.8316, 2060.9785, 1472.8229],\n",
      "        [ 300.7982, 2059.5542, 1409.9768],\n",
      "        [ 305.0893, 2046.1379, 1463.1311],\n",
      "        [ 301.6579, 2006.3503, 1486.5436],\n",
      "        [ 168.6332, 2010.2845,  833.8338],\n",
      "        [ 237.6990, 2002.8727, 1181.3883],\n",
      "        [ 226.9805, 2006.4641, 1122.9343],\n",
      "        [ 167.0993, 1996.3936,  835.4160],\n",
      "        [ 198.5255, 2010.3350,  985.8047],\n",
      "        [ 215.4470, 2017.0432, 1063.0756],\n",
      "        [ 178.4475, 2021.6790,  886.5482],\n",
      "        [ 184.9058, 2053.4158,  899.7127],\n",
      "        [ 178.4166, 2081.9575,  854.1130],\n",
      "        [ 334.2412, 2093.5029, 1594.0380],\n",
      "        [ 187.6706, 2116.5374,  877.9111],\n",
      "        [ 209.0802, 2148.1345,  968.6124],\n",
      "        [ 241.0678, 2187.8027, 1099.6666],\n",
      "        [ 367.9677, 2224.2815, 1647.1061],\n",
      "        [ 301.9456, 2290.5535, 1306.2146],\n",
      "        [ 338.0612, 2349.3352, 1435.6199],\n",
      "        [ 318.7478, 2427.1411, 1300.9702],\n",
      "        [ 389.0013, 2486.8779, 1500.5778],\n",
      "        [ 470.0045, 2549.3179, 1820.6085],\n",
      "        [ 261.9195, 2589.8130, 1010.2201],\n",
      "        [ 284.0048, 2628.5315, 1078.0070],\n",
      "        [ 331.9056, 2648.8682, 1250.6554],\n",
      "        [ 347.0892, 2667.5127, 1265.2538],\n",
      "        [ 538.3749, 2698.0876, 1988.1107]])\n",
      "data:  tensor([[[0.0079, 0.0079, 0.0070,  ..., 0.6228, 0.2363, 0.8696]],\n",
      "\n",
      "        [[0.0051, 0.0050, 0.0059,  ..., 0.6228, 0.2363, 0.9130]],\n",
      "\n",
      "        [[0.0102, 0.0102, 0.0102,  ..., 0.6228, 0.2363, 0.9130]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0039, 0.0034, 0.0037,  ..., 0.8347, 0.2405, 0.3913]],\n",
      "\n",
      "        [[0.0040, 0.0044, 0.0045,  ..., 0.8289, 0.2405, 0.3913]],\n",
      "\n",
      "        [[0.0062, 0.0058, 0.0055,  ..., 0.8234, 0.2405, 0.3913]]])\n",
      "output:  tensor([[2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125],\n",
      "        [2.4371, 7.5093, 6.3125]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 486.8377, 2723.2056, 1782.2831],\n",
      "        [ 294.4182, 2737.2642, 1063.4723],\n",
      "        [ 565.6413, 2752.8311, 2056.6233],\n",
      "        [ 563.1863, 2765.9290, 2033.5774],\n",
      "        [ 279.7070, 2792.7617,  991.8034],\n",
      "        [ 356.8398, 2786.6177, 1284.6735],\n",
      "        [ 484.2009, 2784.8359, 1762.6522],\n",
      "        [ 323.9000, 2826.9893, 1147.0554],\n",
      "        [ 384.7006, 2855.3525, 1344.7577],\n",
      "        [ 579.1329, 2877.5940, 2006.1729],\n",
      "        [ 466.2752, 2860.5583, 1661.7426],\n",
      "        [ 419.9692, 2900.2878, 1453.8287],\n",
      "        [ 477.1699, 2898.5452, 1657.3134],\n",
      "        [ 443.2657, 2930.2317, 1482.0536],\n",
      "        [ 502.2241, 2962.4543, 1679.1262],\n",
      "        [ 432.0116, 2939.3584, 1490.6732],\n",
      "        [ 610.0862, 2982.9587, 2033.8768],\n",
      "        [ 533.7382, 2991.0522, 1781.6974],\n",
      "        [ 559.0793, 3005.2336, 1870.2550],\n",
      "        [ 399.8988, 3025.7380, 1319.3759],\n",
      "        [ 701.6712, 3031.6924, 2328.8484],\n",
      "        [ 673.5664, 3055.7769, 2202.1462],\n",
      "        [ 470.8685, 3071.3604, 1533.5308],\n",
      "        [ 825.7496, 3100.6731, 2634.2041],\n",
      "        [ 586.2880, 3112.1177, 1861.7748],\n",
      "        [ 847.5563, 3126.0596, 2695.0903],\n",
      "        [ 539.2803, 3104.5884, 1739.2446],\n",
      "        [ 739.6151, 3127.9192, 2355.3892],\n",
      "        [ 823.1294, 3139.6653, 2612.9985],\n",
      "        [ 624.2837, 3112.4807, 2021.8788],\n",
      "        [ 381.3640, 3147.9541, 1208.7251],\n",
      "        [ 754.2199, 3150.8977, 2402.7424],\n",
      "        [ 507.9716, 3182.6511, 1578.9265],\n",
      "        [ 486.2764, 3178.7749, 1530.9369],\n",
      "        [ 494.2740, 3194.1182, 1535.5215],\n",
      "        [ 569.0966, 3206.9707, 1763.9491],\n",
      "        [ 470.4782, 3192.6604, 1480.3416],\n",
      "        [ 506.5002, 3197.4807, 1589.4860],\n",
      "        [ 481.1693, 3211.9106, 1503.9777],\n",
      "        [ 522.6850, 3190.8564, 1662.3011],\n",
      "        [ 545.3687, 3217.1697, 1686.4114],\n",
      "        [ 627.7529, 3218.6721, 1958.5538],\n",
      "        [ 454.1018, 3221.8669, 1419.7412],\n",
      "        [ 766.7886, 3261.2615, 2326.0068],\n",
      "        [ 661.4041, 3228.5083, 2050.1904],\n",
      "        [ 746.0596, 3226.5308, 2316.2041],\n",
      "        [ 691.5049, 3272.4436, 2085.8264],\n",
      "        [ 499.5569, 3228.7258, 1559.3275],\n",
      "        [ 606.9830, 3216.7954, 1912.0142],\n",
      "        [ 653.5468, 3252.2522, 2012.6426],\n",
      "        [ 845.3862, 3254.8103, 2597.4250],\n",
      "        [ 558.7671, 3262.7249, 1707.6873],\n",
      "        [ 528.5496, 3285.8936, 1600.7815],\n",
      "        [ 963.1980, 3237.1936, 3005.5623],\n",
      "        [ 554.5965, 3314.3403, 1661.6122],\n",
      "        [ 544.6154, 3294.1489, 1668.8846],\n",
      "        [ 406.5057, 3337.0732, 1199.1375],\n",
      "        [ 609.4695, 3308.2354, 1849.9200],\n",
      "        [ 747.0035, 3314.7036, 2254.1108],\n",
      "        [ 551.3644, 3303.7559, 1680.3427],\n",
      "        [ 677.0913, 3327.5669, 2028.0000],\n",
      "        [ 610.0072, 3296.7739, 1871.5823],\n",
      "        [ 436.3681, 3307.6602, 1325.2311],\n",
      "        [ 600.8272, 3331.9626, 1795.9861],\n",
      "        [ 762.6512, 3302.2200, 2327.2534],\n",
      "        [ 940.6711, 3394.1682, 2740.9729],\n",
      "        [ 483.3974, 3342.7314, 1441.3031],\n",
      "        [ 687.6270, 3325.5449, 2068.8772],\n",
      "        [1097.2986, 3367.7041, 3239.7551],\n",
      "        [ 465.6292, 3333.6719, 1395.4739],\n",
      "        [ 631.8157, 3314.4019, 1939.3220],\n",
      "        [ 439.6909, 3355.5698, 1293.8578],\n",
      "        [ 591.3882, 3339.4192, 1752.7795],\n",
      "        [ 767.2186, 3319.8364, 2299.0359],\n",
      "        [ 640.3777, 3341.4468, 1894.5837],\n",
      "        [ 718.6965, 3279.1628, 2214.6621],\n",
      "        [ 658.5760, 3349.8530, 1942.1127],\n",
      "        [ 552.1979, 3305.7107, 1675.2855],\n",
      "        [ 528.2994, 3316.8987, 1585.6375],\n",
      "        [ 719.4837, 3309.2017, 2156.7664],\n",
      "        [ 891.5047, 3287.9602, 2706.6235],\n",
      "        [ 476.8779, 3249.1689, 1492.8632],\n",
      "        [ 856.5032, 3237.2104, 2667.7761],\n",
      "        [ 584.1293, 3248.8059, 1802.1367],\n",
      "        [ 654.3715, 3146.2729, 2124.2786],\n",
      "        [ 646.6041, 3224.7825, 1986.2640],\n",
      "        [ 686.2868, 3219.6553, 2136.1128],\n",
      "        [ 538.4848, 3210.6848, 1682.2585],\n",
      "        [ 557.8723, 3183.8467, 1769.8013],\n",
      "        [ 597.4708, 3189.2925, 1877.7490],\n",
      "        [ 619.1144, 3191.4373, 1922.7432],\n",
      "        [ 941.6417, 3173.5972, 2938.7314],\n",
      "        [ 483.4499, 3148.2112, 1542.8156],\n",
      "        [ 810.6592, 3146.8037, 2552.1233],\n",
      "        [ 507.2203, 3138.6599, 1600.0852],\n",
      "        [ 731.8568, 3086.9216, 2381.1255],\n",
      "        [ 668.1719, 3081.9951, 2181.4478],\n",
      "        [ 713.4051, 3101.5723, 2293.8489],\n",
      "        [ 680.2857, 3109.3250, 2150.6577],\n",
      "        [ 716.4944, 3033.7756, 2386.2415],\n",
      "        [ 773.9335, 3057.5083, 2525.1484],\n",
      "        [ 625.5729, 3043.7346, 2059.2170],\n",
      "        [ 637.2380, 3081.7605, 2037.5278],\n",
      "        [ 578.7720, 3069.7258, 1844.4131],\n",
      "        [ 836.0828, 3056.5532, 2678.4509],\n",
      "        [ 674.8222, 3006.9875, 2240.9592],\n",
      "        [ 804.8708, 3014.5559, 2657.5498],\n",
      "        [ 759.5947, 3010.1377, 2500.2151],\n",
      "        [ 603.3881, 3002.8320, 1996.5089],\n",
      "        [ 654.0500, 2972.9888, 2202.8313],\n",
      "        [ 866.1413, 2982.6292, 2893.1646],\n",
      "        [ 560.4339, 2964.6328, 1878.1658],\n",
      "        [ 581.7335, 2955.0481, 1964.5352],\n",
      "        [ 766.7263, 2944.5698, 2598.5188],\n",
      "        [ 654.6826, 2951.5125, 2204.5559],\n",
      "        [ 525.4185, 2921.9207, 1801.9010],\n",
      "        [ 577.7004, 2899.0591, 2000.4807],\n",
      "        [ 482.6741, 2925.4060, 1639.2432],\n",
      "        [ 681.6999, 2891.4963, 2366.0850],\n",
      "        [ 574.2042, 2903.1589, 1967.6410],\n",
      "        [ 370.7697, 2892.5186, 1287.7827],\n",
      "        [ 543.4001, 2904.1084, 1854.9318],\n",
      "        [ 870.6106, 2891.8428, 2966.5623],\n",
      "        [ 561.4897, 2883.1907, 1933.5983],\n",
      "        [ 326.5527, 2877.4990, 1127.5308],\n",
      "        [ 443.3413, 2868.2607, 1536.7764],\n",
      "        [ 695.3677, 2845.5835, 2422.8311],\n",
      "        [ 550.6003, 2844.9971, 1924.8975],\n",
      "        [ 861.3237, 2823.9622, 3050.8943],\n",
      "        [ 391.4297, 2844.9636, 1360.0228],\n",
      "        [ 419.6151, 2843.3828, 1454.0461],\n",
      "        [ 434.6210, 2815.6790, 1553.0302],\n",
      "        [ 763.8954, 2808.6357, 2721.6721],\n",
      "        [ 631.0812, 2755.7634, 2333.2427],\n",
      "        [ 747.6202, 2801.5476, 2677.1970],\n",
      "        [ 311.3557, 2801.4583, 1096.3855],\n",
      "        [ 774.9321, 2763.5710, 2827.1362],\n",
      "        [ 841.2393, 2824.0908, 2908.0337],\n",
      "        [ 616.1263, 2764.2925, 2242.0920],\n",
      "        [ 644.5036, 2760.6619, 2346.0925],\n",
      "        [ 509.6528, 2772.5645, 1818.9484],\n",
      "        [ 462.8806, 2746.8823, 1702.8175],\n",
      "        [ 605.0533, 2733.4438, 2240.6611],\n",
      "        [ 487.0458, 2743.6206, 1786.2344],\n",
      "        [ 545.1103, 2745.7932, 1958.5792],\n",
      "        [ 550.4666, 2738.9846, 1986.3221],\n",
      "        [ 545.8049, 2703.1033, 2031.3379],\n",
      "        [ 535.1336, 2696.4399, 1996.0603],\n",
      "        [ 582.8098, 2697.0598, 2162.7473],\n",
      "        [ 588.2621, 2702.1873, 2163.8779]])\n",
      "data:  tensor([[[0.0054, 0.0060, 0.0057,  ..., 0.8170, 0.2405, 0.3913]],\n",
      "\n",
      "        [[0.0037, 0.0035, 0.0040,  ..., 0.8125, 0.2405, 0.3913]],\n",
      "\n",
      "        [[0.0066, 0.0069, 0.0062,  ..., 0.8070, 0.2405, 0.3913]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0063, 0.0058, 0.0050,  ..., 0.6299, 0.2405, 0.9130]],\n",
      "\n",
      "        [[0.0062, 0.0067, 0.0071,  ..., 0.6299, 0.2405, 0.9130]],\n",
      "\n",
      "        [[0.0075, 0.0076, 0.0080,  ..., 0.6299, 0.2405, 0.9130]]])\n",
      "output:  tensor([[2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644],\n",
      "        [2.4890, 7.5612, 6.3644]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 452.9163, 2686.8384, 1688.2836],\n",
      "        [ 619.3439, 2684.0681, 2308.9468],\n",
      "        [ 354.2009, 2697.9758, 1297.4460],\n",
      "        [ 648.4854, 2655.7551, 2465.4243],\n",
      "        [ 498.5200, 2657.7212, 1888.8097],\n",
      "        [ 407.1249, 2672.7629, 1531.2565],\n",
      "        [ 425.2499, 2689.0056, 1568.2643],\n",
      "        [ 593.7682, 2694.0437, 2172.2156],\n",
      "        [ 463.7734, 2676.2427, 1736.4117],\n",
      "        [ 473.1421, 2674.3381, 1764.3292],\n",
      "        [ 549.6282, 2676.4438, 2056.0774],\n",
      "        [ 599.2550, 2722.9207, 2169.9172],\n",
      "        [ 277.9887, 2672.7686, 1044.9719],\n",
      "        [ 514.8811, 2646.9021, 1968.0458],\n",
      "        [ 656.7993, 2688.9944, 2441.1252],\n",
      "        [ 561.2743, 2701.4053, 2028.6182],\n",
      "        [ 407.7800, 2660.3799, 1551.8534],\n",
      "        [ 491.4485, 2675.5444, 1834.2614],\n",
      "        [ 667.5726, 2701.7405, 2414.5891],\n",
      "        [ 646.2610, 2676.0039, 2387.7429],\n",
      "        [ 277.1067, 2661.7092, 1031.4258],\n",
      "        [ 461.4812, 2647.3767, 1743.1257],\n",
      "        [ 431.3652, 2625.8838, 1632.5022],\n",
      "        [ 478.6920, 2605.3013, 1822.0919],\n",
      "        [ 384.8510, 2584.9758, 1501.4613],\n",
      "        [ 451.9717, 2580.4236, 1714.2699],\n",
      "        [ 355.3111, 2539.6050, 1400.1910],\n",
      "        [ 445.8238, 2533.8464, 1680.1107],\n",
      "        [ 381.7047, 2469.0881, 1544.6733],\n",
      "        [ 216.0819, 2424.3428,  887.6762],\n",
      "        [ 281.9794, 2370.3086, 1184.0842],\n",
      "        [ 290.8326, 2313.7107, 1249.1450],\n",
      "        [ 316.5338, 2266.1338, 1393.2203],\n",
      "        [ 211.6563, 2205.5869,  953.6627],\n",
      "        [ 270.6203, 2164.2544, 1235.9821],\n",
      "        [ 321.2701, 2106.4668, 1493.2424],\n",
      "        [ 198.5384, 2033.1908,  977.7633],\n",
      "        [ 213.1888, 1978.2295, 1079.1614],\n",
      "        [ 265.7159, 1936.4946, 1365.4343],\n",
      "        [ 212.6933, 1895.6034, 1121.0717],\n",
      "        [ 201.1820, 1854.3936, 1080.4280],\n",
      "        [ 193.0343, 1790.2610, 1081.4564],\n",
      "        [ 193.8915, 1755.0388, 1097.4592],\n",
      "        [ 220.2028, 1734.3390, 1256.2216],\n",
      "        [ 123.0784, 1662.9955,  749.7001],\n",
      "        [ 172.6492, 1649.0654, 1034.9462],\n",
      "        [ 210.7968, 1624.0870, 1289.9353],\n",
      "        [ 128.2861, 1602.7504,  803.1949],\n",
      "        [ 208.5161, 1619.6689, 1274.9783],\n",
      "        [ 150.2941, 1578.3585,  945.3060],\n",
      "        [ 129.7646, 1575.0520,  822.6434],\n",
      "        [ 169.4509, 1587.9824, 1059.5568],\n",
      "        [ 143.2298, 1581.9727,  906.1804],\n",
      "        [ 154.7014, 1594.7521,  964.5949],\n",
      "        [ 129.3209, 1586.2286,  809.2376],\n",
      "        [ 147.9331, 1582.8214,  920.9822],\n",
      "        [ 124.6793, 1565.5343,  786.0793],\n",
      "        [ 103.6038, 1538.9363,  675.4715],\n",
      "        [ 160.8752, 1537.0988, 1043.1699],\n",
      "        [ 152.5027, 1552.4700,  981.5897],\n",
      "        [ 181.5832, 1595.6123, 1120.7402],\n",
      "        [ 276.6299, 1617.1945, 1628.2831],\n",
      "        [ 137.2617, 1551.4366,  877.9589],\n",
      "        [ 112.7545, 1517.7057,  739.3897],\n",
      "        [ 151.3676, 1524.8777,  985.5323],\n",
      "        [ 125.2986, 1538.3499,  811.0580],\n",
      "        [ 107.7857, 1543.8572,  697.2214],\n",
      "        [ 141.4782, 1533.3005,  911.3588],\n",
      "        [ 145.3934, 1481.9475,  977.1874],\n",
      "        [ 147.3424, 1491.3311,  980.9149],\n",
      "        [ 135.6910, 1499.1285,  902.6212],\n",
      "        [  88.8276, 1499.4135,  596.3494],\n",
      "        [ 130.5097, 1514.3992,  859.7867],\n",
      "        [ 107.4799, 1506.8867,  710.9929],\n",
      "        [ 155.9665, 1479.8138, 1049.2817],\n",
      "        [ 158.9924, 1451.2106, 1089.2284],\n",
      "        [ 123.5689, 1403.3093,  874.1479],\n",
      "        [ 145.6244, 1369.2601, 1059.3209],\n",
      "        [ 116.7091, 1350.9901,  865.2883],\n",
      "        [ 130.1859, 1349.7222,  957.2778],\n",
      "        [ 114.7811, 1359.5184,  842.3284],\n",
      "        [ 173.2137, 1379.4648, 1201.8788],\n",
      "        [  98.9229, 1394.0208,  704.2415],\n",
      "        [ 104.5378, 1402.4548,  742.1184],\n",
      "        [  88.4542, 1396.3116,  631.3359],\n",
      "        [ 124.2018, 1428.9636,  855.0515],\n",
      "        [ 102.8179, 1436.3420,  714.7505],\n",
      "        [ 198.5394, 1477.4624, 1306.9158],\n",
      "        [ 149.0337, 1516.8735,  968.8610],\n",
      "        [  95.8736, 1543.9576,  606.7278],\n",
      "        [ 164.3897, 1564.6184, 1048.2477],\n",
      "        [ 140.0097, 1590.0770,  876.1872],\n",
      "        [ 164.1917, 1614.1339, 1002.9229],\n",
      "        [ 159.1106, 1630.7898,  969.9986],\n",
      "        [ 146.3973, 1648.9089,  893.2719],\n",
      "        [ 173.6061, 1662.7443, 1030.4899],\n",
      "        [ 123.5114, 1662.9901,  739.5356],\n",
      "        [ 306.3221, 1692.7159, 1710.0150],\n",
      "        [ 178.9837, 1697.6312, 1050.3373],\n",
      "        [ 207.5868, 1731.1442, 1139.2573],\n",
      "        [ 194.5567, 1746.7891, 1100.1384],\n",
      "        [ 183.7664, 1748.0961, 1042.8688],\n",
      "        [ 205.0467, 1763.0485, 1150.9695],\n",
      "        [ 163.5382, 1753.3969,  932.9501],\n",
      "        [ 305.4362, 1802.2866, 1676.5570],\n",
      "        [ 198.6161, 1789.5013, 1100.5295],\n",
      "        [ 178.2170, 1798.1310,  989.3522],\n",
      "        [ 134.6854, 1800.2980,  751.9138],\n",
      "        [ 218.7672, 1839.8156, 1131.5984],\n",
      "        [ 201.8740, 1845.3954, 1082.9301],\n",
      "        [ 206.7760, 1851.0535, 1111.5852],\n",
      "        [ 196.7662, 1846.8309, 1059.9695],\n",
      "        [ 174.6121, 1848.7578,  940.6463],\n",
      "        [ 194.6677, 1858.7446, 1039.3301],\n",
      "        [ 163.0001, 1864.2576,  863.5795],\n",
      "        [ 202.1558, 1872.1528, 1069.1019],\n",
      "        [ 144.2825, 1838.5867,  789.8457],\n",
      "        [ 209.2558, 1870.0330, 1103.8715],\n",
      "        [ 189.0694, 1858.1135, 1015.9280],\n",
      "        [ 143.7568, 1855.7397,  767.4792],\n",
      "        [ 190.3771, 1872.2224, 1011.0048],\n",
      "        [ 189.9989, 1874.3505, 1016.3908],\n",
      "        [ 224.7139, 1896.9327, 1182.1508],\n",
      "        [ 229.7665, 1876.3557, 1230.0804],\n",
      "        [ 137.1208, 1901.0828,  720.2618],\n",
      "        [ 198.5531, 1929.4904, 1021.7156],\n",
      "        [ 190.5887, 1931.5460,  989.9623],\n",
      "        [ 245.9168, 1976.2579, 1232.7213],\n",
      "        [ 173.5700, 1986.0771,  874.2919],\n",
      "        [ 266.5364, 2017.7468, 1324.6465],\n",
      "        [ 241.7688, 2090.8274, 1152.5900],\n",
      "        [ 193.4152, 2134.2715,  902.0062],\n",
      "        [ 283.6096, 2150.3799, 1317.7738],\n",
      "        [ 201.9202, 2154.2339,  938.5401],\n",
      "        [ 176.7661, 2180.0332,  813.8373],\n",
      "        [ 231.4163, 2186.8923, 1054.7554],\n",
      "        [ 228.7681, 2189.0259, 1042.5039],\n",
      "        [ 269.0818, 2184.4570, 1223.1029],\n",
      "        [ 266.5830, 2162.9082, 1236.2227],\n",
      "        [ 483.5404, 2199.8730, 2093.4043],\n",
      "        [ 418.6994, 2192.3325, 1885.7896],\n",
      "        [ 407.6038, 2135.9973, 1916.1207],\n",
      "        [ 210.5783, 2169.2310,  971.5759],\n",
      "        [ 220.9636, 2176.6821, 1005.7852],\n",
      "        [ 304.5034, 2148.4475, 1431.2535],\n",
      "        [ 276.2815, 2143.1301, 1290.1033],\n",
      "        [ 259.1666, 2127.7097, 1233.5288],\n",
      "        [ 398.6809, 2129.8142, 1868.4188],\n",
      "        [ 462.2986, 2135.0869, 2157.9565],\n",
      "        [ 258.4768, 2126.0886, 1214.3861]])\n",
      "data:  tensor([[[0.0053, 0.0049, 0.0044,  ..., 0.6330, 0.2405, 0.9130]],\n",
      "\n",
      "        [[0.0072, 0.0070, 0.0069,  ..., 0.6335, 0.2405, 0.9565]],\n",
      "\n",
      "        [[0.0042, 0.0047, 0.0046,  ..., 0.6335, 0.2405, 0.9565]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0048, 0.0048, 0.0045,  ..., 0.7638, 0.2447, 0.4348]],\n",
      "\n",
      "        [[0.0049, 0.0050, 0.0049,  ..., 0.7590, 0.2447, 0.4348]],\n",
      "\n",
      "        [[0.0035, 0.0035, 0.0039,  ..., 0.7535, 0.2447, 0.4348]]])\n",
      "output:  tensor([[2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164],\n",
      "        [2.5409, 7.6131, 6.4164]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 460.2066, 2117.2747, 2161.9070],\n",
      "        [ 377.2641, 2125.0442, 1757.7498],\n",
      "        [ 487.9787, 2102.6912, 2329.6035],\n",
      "        [ 315.3807, 2127.2056, 1466.5789],\n",
      "        [ 370.4356, 2140.6277, 1707.2466],\n",
      "        [ 368.9529, 2095.8823, 1799.0714],\n",
      "        [ 448.8989, 2130.9648, 2108.4956],\n",
      "        [ 358.0081, 2150.4805, 1637.8993],\n",
      "        [ 436.9910, 2117.5540, 2086.7666],\n",
      "        [ 321.0301, 2151.6030, 1485.9802],\n",
      "        [ 281.6946, 2171.3923, 1283.5646],\n",
      "        [ 339.0585, 2165.2988, 1591.0226],\n",
      "        [ 454.9550, 2172.7944, 2120.5073],\n",
      "        [ 440.0276, 2219.5674, 1961.5643],\n",
      "        [ 443.4410, 2193.0083, 2042.0096],\n",
      "        [ 514.8368, 2236.0110, 2280.9163],\n",
      "        [ 379.3349, 2249.2488, 1666.3600],\n",
      "        [ 454.4688, 2255.1248, 1998.6821],\n",
      "        [ 574.0527, 2261.5703, 2517.8447],\n",
      "        [ 471.6988, 2284.3313, 2034.5994],\n",
      "        [ 541.4496, 2276.7183, 2370.2061],\n",
      "        [ 498.2133, 2312.5156, 2121.8005],\n",
      "        [ 526.7103, 2315.0291, 2242.6172],\n",
      "        [ 245.2341, 2298.0994, 1080.7538],\n",
      "        [ 580.3679, 2323.1167, 2468.0674],\n",
      "        [ 289.8466, 2298.5239, 1267.7848],\n",
      "        [ 635.6141, 2260.0173, 2836.1243],\n",
      "        [ 565.9205, 2355.6577, 2361.6023],\n",
      "        [ 600.0204, 2322.6533, 2592.9314],\n",
      "        [ 658.6490, 2369.3931, 2747.9902],\n",
      "        [ 497.4516, 2340.9680, 2152.3252],\n",
      "        [ 621.9444, 2392.0640, 2570.4182],\n",
      "        [ 632.8071, 2344.6936, 2739.4070],\n",
      "        [ 572.7802, 2385.5347, 2403.4709],\n",
      "        [ 474.3605, 2382.5020, 2009.0520],\n",
      "        [ 655.3228, 2405.0840, 2726.6614],\n",
      "        [ 367.3794, 2365.2649, 1735.1946],\n",
      "        [ 280.6385, 2413.6074, 1170.6051],\n",
      "        [ 575.3527, 2405.8713, 2429.4082],\n",
      "        [ 610.6427, 2415.4282, 2549.8562],\n",
      "        [ 613.4672, 2399.6604, 2591.7568],\n",
      "        [ 618.6608, 2432.9109, 2547.6477],\n",
      "        [ 285.3559, 2452.6724, 1137.3086],\n",
      "        [ 506.2045, 2447.3494, 2039.6731],\n",
      "        [ 397.2281, 2429.1128, 1632.6941],\n",
      "        [ 468.0493, 2456.6772, 1886.7096],\n",
      "        [ 592.2496, 2479.5999, 2362.5225],\n",
      "        [ 504.1011, 2435.7593, 2075.0654],\n",
      "        [ 484.9804, 2457.0457, 1960.8848],\n",
      "        [ 538.7288, 2449.4829, 2188.2646],\n",
      "        [ 369.3038, 2455.7332, 1511.7833],\n",
      "        [ 585.1974, 2459.0117, 2361.3162],\n",
      "        [ 367.7082, 2446.1819, 1496.1453],\n",
      "        [ 486.2277, 2444.5566, 1993.9867],\n",
      "        [ 362.1380, 2423.9910, 1527.1266],\n",
      "        [ 549.4582, 2474.0422, 2195.7842],\n",
      "        [ 565.1346, 2471.7019, 2285.2061],\n",
      "        [ 465.3920, 2469.1328, 1889.1641],\n",
      "        [ 352.4176, 2491.0781, 1399.2024],\n",
      "        [ 627.6600, 2461.6760, 2575.8218],\n",
      "        [ 519.6353, 2474.6121, 2118.9524],\n",
      "        [ 250.6255, 2499.4768,  998.4927],\n",
      "        [ 885.6757, 2506.2314, 3529.1797],\n",
      "        [ 597.6946, 2409.5969, 3019.5266],\n",
      "        [ 420.5165, 2519.3799, 1667.6251],\n",
      "        [ 595.6113, 2502.9919, 2410.5925],\n",
      "        [ 709.8872, 2542.6155, 2764.7991],\n",
      "        [ 599.7548, 2511.4985, 2406.2427],\n",
      "        [ 424.9578, 2526.0713, 1678.6959],\n",
      "        [ 813.6714, 2558.9475, 3140.6294],\n",
      "        [ 434.7118, 2528.8416, 1725.2731],\n",
      "        [ 556.8472, 2530.2493, 2218.3950],\n",
      "        [ 772.2280, 2554.3115, 3019.3340],\n",
      "        [ 498.5943, 2566.1638, 1926.2269],\n",
      "        [ 514.4114, 2541.3252, 2050.5640],\n",
      "        [ 542.2499, 2533.8630, 2169.9263],\n",
      "        [ 706.2842, 2581.6245, 2715.8550],\n",
      "        [ 393.6693, 2531.8857, 1603.1523],\n",
      "        [ 491.1082, 2547.2793, 1938.6992],\n",
      "        [ 556.5685, 2532.4612, 2217.0471],\n",
      "        [ 734.6018, 2503.6287, 2988.5479],\n",
      "        [ 412.4692, 2578.0723, 1600.0844],\n",
      "        [ 422.9582, 2574.3130, 1649.5398],\n",
      "        [ 589.7181, 2569.1355, 2317.3330],\n",
      "        [ 630.9161, 2590.3098, 2433.5364],\n",
      "        [ 746.1773, 2588.4500, 2894.4773],\n",
      "        [ 690.3033, 2586.3220, 2675.4382],\n",
      "        [ 618.3350, 2573.8550, 2430.2527],\n",
      "        [ 455.0228, 2588.2490, 1780.1512],\n",
      "        [ 631.6382, 2594.4377, 2439.8608],\n",
      "        [ 782.1308, 2594.8230, 3027.8245],\n",
      "        [ 796.8564, 2583.5627, 3100.5854],\n",
      "        [ 718.5901, 2553.0603, 2879.4353],\n",
      "        [ 715.8595, 2615.3721, 2738.4907],\n",
      "        [ 577.0447, 2600.0679, 2239.4329],\n",
      "        [ 514.6151, 2675.7053, 1870.7698],\n",
      "        [ 720.8065, 2644.3496, 2717.7446],\n",
      "        [ 637.8322, 2634.7090, 2425.3701],\n",
      "        [ 515.8755, 2669.1660, 1915.9989],\n",
      "        [ 683.1392, 2670.7800, 2567.1614],\n",
      "        [ 704.3436, 2667.2390, 2641.4976],\n",
      "        [ 529.0161, 2700.7966, 1941.6714],\n",
      "        [ 668.7926, 2654.1912, 2561.4973],\n",
      "        [ 840.7841, 2620.0471, 3295.9717],\n",
      "        [ 678.1239, 2616.8635, 2685.3970],\n",
      "        [ 466.1141, 2660.4414, 1760.3093],\n",
      "        [ 719.3620, 2596.6663, 2824.5947],\n",
      "        [ 375.7403, 2632.6648, 1436.1696],\n",
      "        [ 591.3936, 2608.0718, 2297.8193],\n",
      "        [ 491.1258, 2531.5227, 3348.6279],\n",
      "        [ 545.8782, 2609.5183, 2103.8728],\n",
      "        [ 643.4762, 2594.9795, 2472.5081],\n",
      "        [ 772.5869, 2603.5532, 2969.6777],\n",
      "        [ 545.7755, 2602.1011, 2090.2319],\n",
      "        [ 615.8066, 2605.7483, 2337.7476],\n",
      "        [ 589.9907, 2553.7808, 2340.5039],\n",
      "        [ 565.9935, 2576.4802, 2181.8669],\n",
      "        [ 464.5643, 2552.2114, 1852.6692],\n",
      "        [ 570.9366, 2576.1174, 2190.3391],\n",
      "        [ 742.3277, 2500.0483, 3006.5708],\n",
      "        [ 708.4811, 2508.5776, 2873.1443],\n",
      "        [ 934.5065, 2563.3320, 3577.8408],\n",
      "        [ 562.2240, 2510.7278, 2272.2417],\n",
      "        [ 387.7299, 2521.9268, 1540.3221],\n",
      "        [ 693.0237, 2487.5762, 2821.5029],\n",
      "        [ 578.6318, 2534.7734, 2253.0266],\n",
      "        [ 513.3845, 2502.1206, 2056.7300],\n",
      "        [ 278.3422, 2519.7039, 1078.1582],\n",
      "        [ 666.0192, 2503.0288, 2667.9336],\n",
      "        [ 493.8029, 2512.3420, 1983.7690],\n",
      "        [ 522.4227, 2502.8020, 2128.1980],\n",
      "        [ 618.9736, 2534.1479, 2463.4460],\n",
      "        [ 397.2031, 2521.9492, 1597.7089],\n",
      "        [ 565.1067, 2518.6423, 2277.3306],\n",
      "        [ 528.1120, 2527.7468, 2130.1587],\n",
      "        [ 533.6262, 2572.1292, 2063.1685],\n",
      "        [ 386.9222, 2581.0994, 1484.4634],\n",
      "        [ 614.0751, 2619.1980, 2315.9468],\n",
      "        [ 211.4737, 2604.3184,  804.4220],\n",
      "        [ 557.3423, 2609.0327, 2131.0205],\n",
      "        [ 732.5731, 2631.4246, 2760.6443],\n",
      "        [ 514.4466, 2615.4949, 1981.8300],\n",
      "        [ 746.4750, 2609.1274, 2863.3870],\n",
      "        [ 437.6337, 2638.2615, 1657.2664],\n",
      "        [ 493.8122, 2633.6868, 1895.6121],\n",
      "        [ 430.6103, 2631.3074, 1653.6185],\n",
      "        [ 677.9642, 2630.2686, 2586.6965],\n",
      "        [ 564.6545, 2660.2627, 2114.6521],\n",
      "        [ 366.2194, 2649.8569, 1376.9338],\n",
      "        [ 721.4603, 2642.0649, 2752.1362]])\n",
      "data:  tensor([[[0.0053, 0.0049, 0.0049,  ..., 0.7485, 0.2447, 0.4348]],\n",
      "\n",
      "        [[0.0044, 0.0046, 0.0046,  ..., 0.7428, 0.2447, 0.4348]],\n",
      "\n",
      "        [[0.0053, 0.0054, 0.0049,  ..., 0.7384, 0.2447, 0.4348]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0064, 0.0070, 0.0063,  ..., 0.5904, 0.2447, 0.9565]],\n",
      "\n",
      "        [[0.0044, 0.0045, 0.0044,  ..., 0.5919, 0.2447, 0.9565]],\n",
      "\n",
      "        [[0.0084, 0.0077, 0.0081,  ..., 0.5945, 0.2447, 0.9565]]])\n",
      "output:  tensor([[2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682],\n",
      "        [2.5928, 7.6650, 6.4682]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 531.6733, 2616.2712, 2065.4573],\n",
      "        [ 470.2420, 2659.6426, 1760.2393],\n",
      "        [ 498.8227, 2632.4692, 1922.1528],\n",
      "        [ 478.4247, 2662.3906, 1803.6377],\n",
      "        [ 366.1145, 2656.3193, 1386.0872],\n",
      "        [ 362.2876, 2659.9890, 1361.6528],\n",
      "        [ 369.2823, 2658.6931, 1401.0927],\n",
      "        [ 263.2256, 2655.8835,  995.5573],\n",
      "        [ 421.2121, 2638.6299, 1598.4395],\n",
      "        [ 394.4072, 2641.9590, 1486.8733],\n",
      "        [ 419.3649, 2591.1758, 1624.3167],\n",
      "        [ 335.1552, 2568.4819, 1308.1876],\n",
      "        [ 305.0326, 2452.4836, 1252.8594],\n",
      "        [ 389.2775, 2549.8542, 1527.7679],\n",
      "        [ 251.2765, 2520.6030, 1004.6843],\n",
      "        [ 296.4419, 2502.9585, 1153.4861],\n",
      "        [ 259.9822, 2470.8752, 1052.4181],\n",
      "        [ 263.5433, 2430.6096, 1090.6233],\n",
      "        [ 623.1712, 3882.7981, 1351.4230],\n",
      "        [1088.5850, 6788.5215, 1608.8993],\n",
      "        [ 851.9951, 6588.2368, 1288.6392],\n",
      "        [ 910.0392, 6697.1426, 1352.7152],\n",
      "        [1040.0353, 6758.1528, 1541.9736],\n",
      "        [1056.1884, 6836.4561, 1545.9943],\n",
      "        [ 751.1320, 6768.9829, 1105.5157],\n",
      "        [1034.3041, 6644.4214, 1548.1531],\n",
      "        [1311.1857, 6854.0449, 1900.8859],\n",
      "        [1061.6125, 7438.2983, 1431.3715],\n",
      "        [1267.6475, 7614.8613, 1668.3732],\n",
      "        [1199.3568, 7705.3457, 1556.5441],\n",
      "        [1344.7513, 7737.5073, 1742.1882],\n",
      "        [1028.0157, 7755.6660, 1328.3748],\n",
      "        [ 835.1535, 7755.0400, 1077.9368],\n",
      "        [1101.1892, 7537.7085, 1688.7488],\n",
      "        [ 922.4397, 7710.9150, 1198.1593],\n",
      "        [1007.1271, 7682.4009, 1314.3865],\n",
      "        [ 643.3467, 7659.2827,  841.6927],\n",
      "        [ 770.3709, 7631.4277, 1010.8367],\n",
      "        [ 767.0291, 7589.1284, 1013.2482],\n",
      "        [ 709.7142, 7548.4775,  941.7952],\n",
      "        [ 998.3254, 7506.7876, 1330.9845],\n",
      "        [ 772.2499, 7471.1797, 1034.7177],\n",
      "        [ 570.4750, 7422.4688,  769.2001],\n",
      "        [ 776.3140, 7373.7466, 1054.1555],\n",
      "        [ 576.4966, 7342.4341,  785.0572],\n",
      "        [ 727.4122, 7307.2056,  996.4070],\n",
      "        [ 559.6588, 7279.8491,  770.6715],\n",
      "        [ 783.4463, 7265.5278, 1079.6703],\n",
      "        [ 654.0590, 7246.3916,  903.3765],\n",
      "        [ 744.7907, 7228.6860, 1032.3507],\n",
      "        [ 642.8648, 7215.1411,  892.4561],\n",
      "        [ 799.7786, 7195.6196, 1113.0343],\n",
      "        [ 383.9396, 7171.0044,  535.5434],\n",
      "        [ 482.5624, 7153.1367,  674.1794],\n",
      "        [ 541.7051, 7140.3735,  759.3058],\n",
      "        [ 677.9908, 7147.2495,  948.6918],\n",
      "        [ 610.2022, 7159.3530,  853.8008],\n",
      "        [ 853.9233, 7154.1084, 1195.9048],\n",
      "        [ 468.0325, 7203.2383,  650.2762],\n",
      "        [ 491.0406, 7247.7715,  677.5412],\n",
      "        [ 523.5532, 7287.6016,  718.5305],\n",
      "        [ 512.2299, 7347.0088,  697.5734],\n",
      "        [ 719.3851, 7387.0850,  975.7950],\n",
      "        [ 594.5479, 7446.6318,  799.3329],\n",
      "        [ 661.8187, 7498.6045,  884.5526],\n",
      "        [ 733.2379, 7564.5469,  970.4105],\n",
      "        [ 962.0238, 7635.8457, 1261.7972],\n",
      "        [ 665.4758, 7678.7983,  867.2345],\n",
      "        [ 777.9573, 7734.6196, 1006.5411],\n",
      "        [ 579.7628, 7785.7715,  744.8771],\n",
      "        [ 955.9185, 7832.3433, 1221.3828],\n",
      "        [ 794.8090, 7886.7461, 1008.2278],\n",
      "        [ 937.9348, 7838.2026, 1199.7509],\n",
      "        [ 700.7458, 7847.5249,  894.3920],\n",
      "        [ 667.4502, 7905.8877,  844.9645],\n",
      "        [ 810.2011, 7968.1880, 1018.0694],\n",
      "        [ 774.1245, 8042.1680,  962.7921],\n",
      "        [ 899.8672, 8105.7964, 1110.2113],\n",
      "        [ 884.3948, 8150.2808, 1087.2799],\n",
      "        [ 845.6755, 8196.6514, 1033.0621],\n",
      "        [ 935.8418, 8235.1016, 1137.8433],\n",
      "        [ 734.3891, 8274.5137,  888.5915],\n",
      "        [ 939.9612, 8281.5625, 1136.9790],\n",
      "        [1217.2233, 8267.9336, 1474.2998],\n",
      "        [ 651.3146, 8280.8809,  786.1486],\n",
      "        [ 794.9216, 8238.6992,  966.9643],\n",
      "        [ 739.9086, 8252.8252,  896.6989],\n",
      "        [ 924.3215, 8234.0020, 1124.2360],\n",
      "        [ 727.1558, 8199.9248,  887.2101],\n",
      "        [ 860.9076, 8199.6396, 1051.6135],\n",
      "        [ 933.0291, 8228.4551, 1135.2573],\n",
      "        [ 936.3490, 8224.1602, 1140.1194],\n",
      "        [ 884.9567, 8216.1895, 1080.1261],\n",
      "        [ 653.5519, 8211.7598,  794.9491],\n",
      "        [ 697.0528, 8194.2109,  851.7191],\n",
      "        [ 889.7388, 8164.2500, 1090.5110],\n",
      "        [ 866.3017, 8121.0293, 1068.4728],\n",
      "        [1082.7072, 8069.6934, 1344.4755],\n",
      "        [ 888.4277, 8035.0967, 1108.4043],\n",
      "        [ 869.3494, 7957.3745, 1092.5480],\n",
      "        [ 648.1281, 7892.9014,  820.7236],\n",
      "        [ 793.6948, 7842.1792, 1012.7884],\n",
      "        [1319.6675, 7818.9102, 1666.9463],\n",
      "        [1054.8003, 7776.9966, 1358.2239],\n",
      "        [1247.4482, 7734.1172, 1611.2087],\n",
      "        [ 801.9940, 7644.9224, 1049.6736],\n",
      "        [ 834.7078, 7563.6587, 1103.2947],\n",
      "        [ 890.5706, 7518.2266, 1183.2915],\n",
      "        [ 662.9659, 7440.4082,  891.7923],\n",
      "        [ 912.0945, 7345.0483, 1241.8015],\n",
      "        [ 958.2413, 7279.5811, 1316.2874],\n",
      "        [ 795.5306, 7224.0107, 1104.0778],\n",
      "        [1047.8845, 7171.4180, 1463.9261],\n",
      "        [ 710.9863, 7095.9355, 1003.1053],\n",
      "        [1091.3053, 6998.8818, 1570.7937],\n",
      "        [ 789.3451, 6943.0273, 1127.1149],\n",
      "        [ 863.1244, 6848.8779, 1255.6239],\n",
      "        [1216.3278, 6700.4268, 1814.0912],\n",
      "        [ 892.9548, 6569.9053, 1360.5835],\n",
      "        [1195.6578, 6488.0776, 1823.0167],\n",
      "        [ 661.6464, 6351.1045, 1039.9777],\n",
      "        [ 561.0081, 6231.6758,  901.7768],\n",
      "        [1000.8956, 6136.9570, 1629.2112],\n",
      "        [ 811.3781, 6053.0015, 1337.3018],\n",
      "        [ 839.5253, 5963.4434, 1406.3265],\n",
      "        [1073.5157, 5884.5542, 1808.1793],\n",
      "        [ 842.5653, 5779.3628, 1460.4265],\n",
      "        [ 744.7313, 5719.4526, 1298.5166],\n",
      "        [ 666.5358, 5636.1895, 1186.8286],\n",
      "        [ 649.6958, 5578.0674, 1164.7952],\n",
      "        [ 695.4723, 5517.4033, 1257.8888],\n",
      "        [1066.2765, 5442.9097, 1961.1875],\n",
      "        [ 612.1976, 5384.2397, 1139.9401],\n",
      "        [ 697.8853, 5312.8901, 1319.7646],\n",
      "        [ 996.5158, 5279.9976, 1881.8064],\n",
      "        [ 764.0859, 5192.9253, 1480.7471],\n",
      "        [ 750.6636, 5151.4028, 1460.1300],\n",
      "        [ 689.1464, 5105.1719, 1355.3068],\n",
      "        [1002.1342, 5027.3213, 2005.8708],\n",
      "        [ 956.4618, 5025.9058, 1895.3698],\n",
      "        [ 696.7861, 4950.9673, 1409.8365],\n",
      "        [ 627.1469, 4910.5171, 1271.5616],\n",
      "        [1174.1895, 4896.4639, 2382.2192],\n",
      "        [ 768.0875, 4836.5093, 1586.9193],\n",
      "        [ 973.7096, 4752.5874, 2062.8162],\n",
      "        [ 701.1183, 4748.4487, 1469.9192],\n",
      "        [1013.7925, 4664.1973, 2194.2722],\n",
      "        [ 571.0701, 4658.7734, 1229.2617],\n",
      "        [1101.2661, 4668.0176, 2338.3267],\n",
      "        [ 946.6876, 4621.0103, 2030.9387]])\n",
      "data:  tensor([[[0.0062, 0.0062, 0.0059,  ..., 0.5979, 0.2447, 0.9565]],\n",
      "\n",
      "        [[0.0049, 0.0055, 0.0062,  ..., 0.5987, 0.2447, 1.0000]],\n",
      "\n",
      "        [[0.0064, 0.0064, 0.0057,  ..., 0.6014, 0.2447, 1.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0055, 0.0062, 0.0067,  ..., 0.7098, 0.2489, 0.4783]],\n",
      "\n",
      "        [[0.0130, 0.0121, 0.0123,  ..., 0.7077, 0.2489, 0.4783]],\n",
      "\n",
      "        [[0.0107, 0.0107, 0.0109,  ..., 0.7046, 0.2489, 0.4783]]])\n",
      "output:  tensor([[2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201],\n",
      "        [2.6447, 7.7169, 6.5201]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[1118.2531, 4541.2939, 2461.4451],\n",
      "        [ 764.2392, 4515.5112, 1687.5435],\n",
      "        [1047.5526, 4500.1289, 2309.9753],\n",
      "        [ 934.7310, 4467.4092, 2079.1123],\n",
      "        [ 628.4501, 4399.7856, 1438.5865],\n",
      "        [ 732.2198, 4397.8584, 1656.1239],\n",
      "        [ 951.7152, 4399.7241, 2136.8599],\n",
      "        [1193.5946, 4308.9937, 2779.8489],\n",
      "        [ 504.2649, 4293.5327, 1182.7800],\n",
      "        [1214.1044, 4267.1807, 2855.2837],\n",
      "        [ 723.4057, 4243.8389, 1702.0933],\n",
      "        [ 913.5038, 4188.3691, 2219.8298],\n",
      "        [ 770.2296, 4211.6440, 1811.4790],\n",
      "        [ 646.9842, 4169.1943, 1548.4852],\n",
      "        [1142.0835, 4159.6206, 2722.4009],\n",
      "        [ 762.8738, 4153.1021, 1804.6693],\n",
      "        [1028.7333, 4089.5339, 2487.0449],\n",
      "        [ 688.3738, 4053.1333, 1698.4104],\n",
      "        [ 924.4651, 4014.6995, 2303.7900],\n",
      "        [1205.9894, 4022.9268, 2972.9312],\n",
      "        [ 979.1661, 3990.0852, 2431.1440],\n",
      "        [ 969.8069, 3946.2493, 2446.0015],\n",
      "        [1220.0314, 3922.0977, 3106.2900],\n",
      "        [ 413.9229, 3878.6816, 1075.7183],\n",
      "        [ 968.9109, 3875.7268, 2493.3682],\n",
      "        [1393.5314, 3820.0339, 3662.4590],\n",
      "        [ 734.7369, 3811.9517, 1927.1128],\n",
      "        [ 706.9394, 3809.0640, 1843.8878],\n",
      "        [ 746.4553, 3727.1694, 2040.2819],\n",
      "        [ 741.3703, 3750.9080, 1968.9061],\n",
      "        [ 924.3210, 3718.9868, 2482.2627],\n",
      "        [ 653.9065, 3697.0750, 1756.8152],\n",
      "        [ 589.9026, 3667.6169, 1621.4647],\n",
      "        [ 705.6877, 3637.2991, 1981.5017],\n",
      "        [ 779.8965, 3635.9529, 2137.6885],\n",
      "        [ 943.8165, 3617.6157, 2610.3655],\n",
      "        [1092.0625, 3599.1836, 3025.3723],\n",
      "        [ 981.7520, 3560.8840, 2762.9951],\n",
      "        [ 719.7180, 3563.4031, 2011.0594],\n",
      "        [ 857.0787, 3544.9653, 2412.5784],\n",
      "        [ 907.5288, 3500.7114, 2618.6538],\n",
      "        [ 542.2596, 3502.1077, 1545.6309],\n",
      "        [ 787.8943, 3486.2227, 2264.2871],\n",
      "        [ 987.5399, 3475.0740, 2854.4878],\n",
      "        [ 873.1011, 3433.6187, 2561.0688],\n",
      "        [1231.9445, 3469.2429, 3519.3931],\n",
      "        [ 667.2432, 3395.9724, 1983.8208],\n",
      "        [ 829.8979, 3396.9275, 2447.7253],\n",
      "        [ 820.8961, 3381.2490, 2435.4014],\n",
      "        [ 769.7904, 3342.8433, 2315.1287],\n",
      "        [1026.2173, 3338.6150, 3079.9404],\n",
      "        [ 699.5744, 3341.8936, 2086.6565],\n",
      "        [ 878.1367, 3295.8076, 2687.0845],\n",
      "        [ 511.4474, 3316.6926, 1529.9014],\n",
      "        [ 801.6639, 3307.6770, 2415.5840],\n",
      "        [ 781.2254, 3329.1699, 2298.7339],\n",
      "        [ 554.4134, 3294.1658, 1657.8528],\n",
      "        [ 692.7573, 3234.0657, 2177.7810],\n",
      "        [ 584.0352, 3226.6819, 1830.4819],\n",
      "        [ 686.3032, 3218.4150, 2121.3718],\n",
      "        [ 976.8932, 3199.9775, 3057.2412],\n",
      "        [ 954.0909, 3206.7358, 2957.6494],\n",
      "        [1023.9820, 3179.9033, 3249.4688],\n",
      "        [ 540.8064, 3188.0022, 1684.5386],\n",
      "        [ 701.8244, 3187.9519, 2166.5667],\n",
      "        [ 818.0161, 3152.7297, 2605.9014],\n",
      "        [ 467.2746, 3141.5310, 1479.1156],\n",
      "        [ 895.6608, 3062.4236, 2980.2419],\n",
      "        [ 529.1254, 3094.6240, 1724.2225],\n",
      "        [ 443.0061, 2940.1235, 1818.1676],\n",
      "        [ 493.8653, 3078.8562, 1619.1042],\n",
      "        [ 605.1107, 3058.7483, 1994.3062],\n",
      "        [ 573.2066, 3063.6467, 1849.3601],\n",
      "        [ 387.3301, 3026.8162, 1296.4009],\n",
      "        [ 674.0794, 3075.1584, 2151.9834],\n",
      "        [ 706.1468, 2991.4431, 2395.1741],\n",
      "        [1084.1864, 2937.6213, 3711.2175],\n",
      "        [ 945.4320, 3034.3174, 3060.5886],\n",
      "        [ 918.2502, 2987.4888, 3074.2224],\n",
      "        [ 801.7606, 2943.3074, 2726.2686],\n",
      "        [1119.7095, 2911.8723, 3872.0356],\n",
      "        [ 601.3688, 2916.9329, 2077.3105],\n",
      "        [ 585.8461, 2893.8088, 2074.7109],\n",
      "        [1052.1494, 2893.9595, 3661.5239],\n",
      "        [ 794.2332, 2946.1282, 2629.4324],\n",
      "        [ 672.6299, 2922.1440, 2298.1108],\n",
      "        [ 757.2193, 2954.4114, 2552.4912],\n",
      "        [ 733.7130, 2947.9216, 2518.0933],\n",
      "        [ 722.9890, 2980.5737, 2406.8530],\n",
      "        [ 696.7504, 2982.2607, 2343.4985],\n",
      "        [ 845.1268, 2987.7288, 2846.7827],\n",
      "        [ 284.4668, 3001.1284,  953.4173],\n",
      "        [ 776.8462, 3005.6582, 2596.4355],\n",
      "        [ 744.4908, 3020.0688, 2462.4790],\n",
      "        [ 751.2956, 3010.8640, 2523.4626],\n",
      "        [ 758.4036, 3022.7888, 2528.0457],\n",
      "        [ 759.2057, 3060.1558, 2455.0530],\n",
      "        [ 819.5381, 3059.4688, 2658.1226],\n",
      "        [ 942.7148, 3061.0942, 3088.8462],\n",
      "        [ 302.3062, 3071.5503,  983.5725],\n",
      "        [ 657.5722, 3096.5286, 2094.7639],\n",
      "        [ 602.0925, 3084.1680, 1967.0109],\n",
      "        [ 523.1105, 3095.2217, 1682.5255],\n",
      "        [ 745.1080, 3104.0688, 2403.6208],\n",
      "        [ 601.9746, 3118.5242, 1916.3033],\n",
      "        [ 411.2716, 3086.7036, 1365.5435],\n",
      "        [ 528.6227, 3138.3862, 1634.9805],\n",
      "        [ 648.6809, 3115.8098, 2076.6365],\n",
      "        [ 464.9427, 3104.7783, 1506.6594],\n",
      "        [ 702.6812, 3100.8406, 2253.7356],\n",
      "        [ 574.6800, 3080.2749, 1907.2631],\n",
      "        [ 559.9826, 3094.8752, 1832.1996],\n",
      "        [ 734.3962, 3123.0818, 2354.4827],\n",
      "        [ 686.2048, 3158.3711, 2133.5371],\n",
      "        [ 723.0416, 3057.4971, 2416.7051],\n",
      "        [ 623.9090, 3103.6780, 2015.5470],\n",
      "        [ 613.8599, 3077.3816, 2028.3983],\n",
      "        [ 620.3287, 3097.3218, 2014.7550],\n",
      "        [ 391.7985, 3090.9431, 1288.2384],\n",
      "        [ 521.2156, 3068.9138, 1745.6766],\n",
      "        [ 441.7262, 3116.6367, 1418.4388],\n",
      "        [ 442.2269, 3118.1165, 1415.4052],\n",
      "        [ 705.5585, 3145.9937, 2216.6035],\n",
      "        [ 657.8807, 3129.4719, 2088.9541],\n",
      "        [ 413.7065, 3104.1138, 1345.2843],\n",
      "        [ 669.0887, 3096.9866, 2204.7122],\n",
      "        [ 628.3750, 3111.5425, 2034.7610],\n",
      "        [ 854.6521, 3137.0569, 2722.7100],\n",
      "        [ 542.6328, 3168.6150, 1692.0929],\n",
      "        [ 632.0586, 3147.4404, 2002.5760],\n",
      "        [ 594.9978, 3004.4797, 2174.8301],\n",
      "        [ 797.0273, 3148.2390, 2543.5881],\n",
      "        [ 841.8016, 3151.5176, 2681.2209],\n",
      "        [ 536.1584, 3169.6763, 1686.7030],\n",
      "        [ 577.2734, 3175.7029, 1815.1276],\n",
      "        [ 522.1949, 3183.5728, 1643.1196],\n",
      "        [ 739.7972, 3206.2947, 2306.5527],\n",
      "        [ 600.7328, 3184.9524, 1909.4529],\n",
      "        [ 766.2185, 3238.2937, 2358.6348],\n",
      "        [ 299.5892, 3141.9050, 1060.0212],\n",
      "        [ 571.4265, 3199.6033, 1793.9570],\n",
      "        [ 353.4443, 3233.7139, 1080.7098],\n",
      "        [ 500.5954, 3216.5105, 1554.0555],\n",
      "        [ 535.0404, 3200.9438, 1684.5043],\n",
      "        [ 243.5341, 3209.9812,  768.6838],\n",
      "        [ 391.3396, 3252.0510, 1198.5543],\n",
      "        [ 406.7775, 3260.1780, 1256.8835],\n",
      "        [ 507.7775, 3310.8328, 1526.3683],\n",
      "        [ 573.9583, 3350.0371, 1685.7518],\n",
      "        [ 496.6938, 3369.2456, 1472.5170]])\n",
      "data:  tensor([[[0.0132, 0.0144, 0.0144,  ..., 0.7022, 0.2489, 0.4783]],\n",
      "\n",
      "        [[0.0093, 0.0090, 0.0089,  ..., 0.6996, 0.2489, 0.4783]],\n",
      "\n",
      "        [[0.0119, 0.0109, 0.0101,  ..., 0.6975, 0.2489, 0.4783]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0060, 0.0059, 0.0059,  ..., 0.5907, 0.2489, 1.0000]],\n",
      "\n",
      "        [[0.0064, 0.0063, 0.0059,  ..., 0.5919, 0.2489, 1.0000]],\n",
      "\n",
      "        [[0.0061, 0.0061, 0.0066,  ..., 0.5961, 0.2489, 1.0000]]])\n",
      "output:  tensor([[2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719],\n",
      "        [2.6965, 7.7687, 6.5719]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 552.7604, 3387.5884, 1595.9714],\n",
      "        [ 315.4558, 3377.8250,  961.8841],\n",
      "        [ 365.9402, 3420.8223, 1092.1169],\n",
      "        [ 412.9339, 3446.5881, 1200.2051],\n",
      "        [ 382.5572, 3500.5105, 1097.9113],\n",
      "        [ 322.7990, 3545.4121,  908.8773],\n",
      "        [ 485.6763, 3474.3535, 1358.8079],\n",
      "        [ 415.4466, 3598.2957, 1165.3839],\n",
      "        [ 361.6625, 3696.5276,  978.6492],\n",
      "        [ 493.1334, 3759.3533, 1312.5009],\n",
      "        [ 326.2516, 3814.9680,  856.3206],\n",
      "        [ 400.3705, 3876.9836, 1034.3447],\n",
      "        [ 423.5359, 3937.7202, 1075.0793],\n",
      "        [ 456.8961, 3969.0828, 1152.8975],\n",
      "        [ 499.8176, 3998.5071, 1252.5117],\n",
      "        [ 446.2989, 4059.5452, 1101.0354],\n",
      "        [ 469.2806, 4098.9902, 1149.8215],\n",
      "        [ 523.0582, 4159.3691, 1261.3390],\n",
      "        [ 502.8055, 4216.2739, 1190.9983],\n",
      "        [ 389.3153, 4265.9067,  913.8546],\n",
      "        [ 477.0970, 4305.3179, 1110.0820],\n",
      "        [ 452.0218, 4350.7114, 1041.5591],\n",
      "        [ 412.1654, 4404.9409,  935.4761],\n",
      "        [ 382.2285, 4434.3540,  863.4720],\n",
      "        [ 264.9064, 4473.0337,  591.5527],\n",
      "        [ 521.3186, 4499.9111, 1162.3105],\n",
      "        [ 440.2499, 4529.7319,  974.2383],\n",
      "        [ 420.7331, 4546.4995,  929.7945],\n",
      "        [ 442.8784, 4594.1660,  962.0833],\n",
      "        [ 444.0446, 4613.7935,  958.1597],\n",
      "        [ 520.1814, 4620.9766, 1128.3442],\n",
      "        [ 626.9244, 4649.6807, 1347.6246],\n",
      "        [ 531.0052, 4667.6934, 1140.4977],\n",
      "        [ 390.0104, 4682.3276,  835.8445],\n",
      "        [ 446.8530, 4704.2173,  953.7073],\n",
      "        [ 554.1902, 4715.9243, 1177.6378],\n",
      "        [ 278.4288, 4749.5586,  586.2773],\n",
      "        [ 500.6882, 4758.1509, 1056.2125],\n",
      "        [ 377.1609, 4798.8242,  787.2631],\n",
      "        [ 532.5635, 4823.3613, 1104.8361],\n",
      "        [ 512.1809, 4841.2573, 1062.4088],\n",
      "        [ 563.0355, 4895.5928, 1141.7294],\n",
      "        [ 565.1205, 4925.6929, 1149.5977],\n",
      "        [ 602.6174, 4936.2886, 1243.6163],\n",
      "        [ 711.0070, 4998.6562, 1400.1281],\n",
      "        [ 569.4362, 5022.5845, 1135.4469],\n",
      "        [ 540.1992, 5070.1396, 1068.5851],\n",
      "        [ 529.3683, 5113.2256, 1024.1891],\n",
      "        [ 484.7408, 5132.5854,  946.1090],\n",
      "        [ 500.2177, 5164.5957,  966.7515],\n",
      "        [ 495.6733, 5166.7798,  960.1356],\n",
      "        [ 528.1936, 5218.5625, 1016.9731],\n",
      "        [ 659.9664, 5234.9170, 1261.8768],\n",
      "        [ 387.3647, 5240.6030,  741.0818],\n",
      "        [ 656.2784, 5275.4731, 1241.8781],\n",
      "        [ 402.5788, 5270.8818,  765.3004],\n",
      "        [ 551.2439, 5257.7114, 1051.1776],\n",
      "        [ 443.1302, 5257.2085,  841.4991],\n",
      "        [ 584.9095, 5239.7651, 1109.5977],\n",
      "        [ 518.0728, 5183.5361, 1002.7158],\n",
      "        [ 387.3343, 5141.7676,  755.2686],\n",
      "        [ 431.2501, 5054.2432,  854.9933],\n",
      "        [ 443.6046, 4979.4590,  891.7997],\n",
      "        [ 336.9925, 4861.3037,  694.0244],\n",
      "        [ 390.6076, 4722.6938,  824.4088],\n",
      "        [ 364.4752, 4577.7393,  799.1346],\n",
      "        [ 281.6619, 4450.3008,  631.4643],\n",
      "        [ 366.6310, 4283.1606,  856.9985],\n",
      "        [ 242.4604, 4112.7144,  587.8094],\n",
      "        [ 163.2258, 3947.1990,  414.0001],\n",
      "        [ 223.7451, 3763.1064,  589.0991],\n",
      "        [ 143.0229, 3584.6782,  399.9953],\n",
      "        [ 304.7249, 3416.7839,  896.9332],\n",
      "        [ 297.8431, 3263.5012,  906.1679],\n",
      "        [ 182.5209, 3111.3188,  586.8406],\n",
      "        [ 224.7175, 2996.0791,  747.4463],\n",
      "        [ 255.1060, 2873.4163,  888.8952],\n",
      "        [ 197.2241, 2758.1316,  717.7953],\n",
      "        [ 232.0107, 2653.6997,  872.3082],\n",
      "        [ 157.6402, 2564.0190,  618.0600],\n",
      "        [ 149.9512, 2494.9937,  598.6420],\n",
      "        [ 245.0540, 2408.9883, 1014.7363],\n",
      "        [ 264.4852, 2353.0830, 1119.3883],\n",
      "        [ 182.2140, 2283.2253,  795.8382],\n",
      "        [ 234.0860, 2220.2712, 1052.0659],\n",
      "        [ 183.8354, 2146.1069,  857.7704],\n",
      "        [ 196.3329, 2097.1782,  934.5768],\n",
      "        [ 187.4188, 2043.0323,  916.5154],\n",
      "        [ 257.4325, 1990.6628, 1290.4243],\n",
      "        [ 176.0578, 1938.3157,  901.3075],\n",
      "        [ 147.5563, 1886.7057,  779.2234],\n",
      "        [ 168.5771, 1846.7469,  904.1428],\n",
      "        [ 146.2840, 1787.2896,  829.8531],\n",
      "        [ 156.0929, 1771.7843,  878.4194],\n",
      "        [ 212.1687, 1763.3557, 1193.2120],\n",
      "        [ 111.3505, 1752.3523,  632.8975],\n",
      "        [ 261.6629, 1738.9471, 1546.1161],\n",
      "        [ 250.3039, 1740.8184, 1456.3748],\n",
      "        [ 314.6963, 1779.9893, 1668.7588],\n",
      "        [ 164.7073, 1787.9738,  917.8435],\n",
      "        [ 190.5459, 1810.2068, 1045.1367],\n",
      "        [ 177.2364, 1826.8906,  968.3234],\n",
      "        [ 191.2183, 1873.8143,  971.2939],\n",
      "        [ 221.3524, 1927.8651, 1134.9995],\n",
      "        [ 140.1268, 1985.1108,  703.6708],\n",
      "        [ 290.2823, 2078.5840, 1386.4092],\n",
      "        [ 242.2728, 2156.6357, 1120.8398],\n",
      "        [ 275.1132, 2259.0513, 1190.4031],\n",
      "        [ 178.6918, 2287.7717,  779.4632],\n",
      "        [ 218.6660, 2354.7085,  915.4933],\n",
      "        [ 185.2453, 2381.9768,  778.4962],\n",
      "        [ 246.3165, 2408.7871, 1030.6777],\n",
      "        [ 240.9727, 2468.4849,  970.2925],\n",
      "        [ 334.9109, 2494.0886, 1341.6060],\n",
      "        [ 279.0920, 2512.8728, 1110.3107],\n",
      "        [ 273.8178, 2524.9094, 1093.8123],\n",
      "        [ 303.6513, 2557.3445, 1188.2954],\n",
      "        [ 450.8578, 2597.0237, 1725.0039],\n",
      "        [ 299.5828, 2615.5396, 1134.5229],\n",
      "        [ 265.6919, 2610.6189, 1026.7433],\n",
      "        [ 314.9059, 2635.1167, 1203.6984],\n",
      "        [ 360.1207, 2659.2908, 1355.0979],\n",
      "        [ 411.6744, 2653.6887, 1568.7479],\n",
      "        [ 311.9156, 2689.5920, 1167.5253],\n",
      "        [ 473.4555, 2690.1506, 1758.4419],\n",
      "        [ 392.8263, 2715.8438, 1456.5404],\n",
      "        [ 373.0080, 2753.3057, 1353.1486],\n",
      "        [ 436.6016, 2771.1348, 1562.4647],\n",
      "        [ 395.6088, 2781.3728, 1413.7402],\n",
      "        [ 413.1529, 2792.6165, 1454.6930],\n",
      "        [ 385.1548, 2813.4373, 1360.0687],\n",
      "        [ 445.6979, 2802.5417, 1598.1519],\n",
      "        [ 621.1734, 2827.3525, 2181.7195],\n",
      "        [ 384.4285, 2822.0408, 1377.5608],\n",
      "        [ 521.6451, 2829.0396, 1868.0955],\n",
      "        [ 435.3036, 2867.5291, 1488.7303],\n",
      "        [ 498.3630, 2864.1890, 1738.0743],\n",
      "        [ 601.3824, 2872.1147, 2084.2402],\n",
      "        [ 747.1502, 2818.8572, 2678.7715],\n",
      "        [ 493.7057, 2877.6667, 1717.0780],\n",
      "        [ 640.4958, 2913.2129, 2128.7788],\n",
      "        [ 439.3558, 2861.3403, 1558.5486],\n",
      "        [ 592.1923, 2902.8853, 2021.6260],\n",
      "        [ 678.8600, 2882.3193, 2367.8042],\n",
      "        [ 570.1166, 2914.5754, 1929.5449],\n",
      "        [ 565.6023, 2904.4937, 1950.7601],\n",
      "        [ 675.7580, 2898.1208, 2333.6689],\n",
      "        [ 423.2979, 2923.1382, 1439.5034],\n",
      "        [ 696.7422, 2909.2527, 2398.0972],\n",
      "        [ 437.2416, 2917.3293, 1509.5758]])\n",
      "data:  tensor([[[0.0064, 0.0062, 0.0058,  ..., 0.6000, 0.2489, 1.0000]],\n",
      "\n",
      "        [[0.0037, 0.0039, 0.0043,  ..., 0.6043, 0.2532, 0.0000]],\n",
      "\n",
      "        [[0.0043, 0.0043, 0.0042,  ..., 0.6082, 0.2532, 0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0048, 0.0048, 0.0040,  ..., 0.6902, 0.2532, 0.5217]],\n",
      "\n",
      "        [[0.0083, 0.0076, 0.0084,  ..., 0.6868, 0.2532, 0.5217]],\n",
      "\n",
      "        [[0.0051, 0.0058, 0.0058,  ..., 0.6841, 0.2532, 0.5217]]])\n",
      "output:  tensor([[2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237],\n",
      "        [2.7483, 7.8205, 6.6237]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 707.4700, 2917.7537, 2414.4163],\n",
      "        [ 707.9818, 2899.1541, 2453.3328],\n",
      "        [ 940.6548, 2907.3032, 3259.9253],\n",
      "        [ 845.7397, 2957.6399, 2781.1562],\n",
      "        [ 707.9937, 2952.4285, 2365.6780],\n",
      "        [ 606.6619, 2927.8972, 2038.9410],\n",
      "        [ 945.8643, 2909.6436, 3233.9453],\n",
      "        [ 629.2499, 2918.9658, 2140.7849],\n",
      "        [ 506.5204, 2935.2698, 1696.7311],\n",
      "        [ 320.2623, 2900.1260, 1144.1742],\n",
      "        [ 514.6086, 2926.5955, 1751.8409],\n",
      "        [ 559.1840, 2932.7173, 1900.5863],\n",
      "        [ 709.4510, 2896.0073, 2467.4255],\n",
      "        [ 624.6860, 2949.6023, 2078.0369],\n",
      "        [ 556.8797, 2907.1133, 1919.1600],\n",
      "        [ 497.7056, 2933.9795, 1688.8866],\n",
      "        [ 631.0328, 2926.4561, 2147.1157],\n",
      "        [ 814.7101, 2886.2852, 2802.4553],\n",
      "        [ 805.0831, 2935.6274, 2727.6404],\n",
      "        [ 609.6380, 2941.6150, 2050.4763],\n",
      "        [ 757.5549, 2925.3667, 2578.0586],\n",
      "        [1000.3998, 2948.8147, 3387.6360],\n",
      "        [ 844.9074, 2917.0332, 2898.9856],\n",
      "        [ 725.7812, 2934.8174, 2456.4404],\n",
      "        [ 488.8384, 2942.7937, 1631.5349],\n",
      "        [ 899.9159, 2960.0806, 2995.0708],\n",
      "        [ 584.8483, 2947.5579, 1961.7659],\n",
      "        [ 635.7480, 2909.6270, 2188.7625],\n",
      "        [ 546.1189, 2915.2515, 1897.3949],\n",
      "        [ 719.3086, 2916.2849, 2487.5669],\n",
      "        [ 600.2416, 2927.4446, 2049.1111],\n",
      "        [ 455.5465, 2805.4966, 1980.8954],\n",
      "        [ 720.0060, 2925.2605, 2467.8579],\n",
      "        [ 587.2911, 2907.7390, 2037.2946],\n",
      "        [ 647.6740, 2940.2410, 2168.6616],\n",
      "        [ 590.7523, 2964.3813, 1930.4442],\n",
      "        [ 930.2512, 2912.4922, 3188.9670],\n",
      "        [ 711.1079, 2895.1047, 2463.0017],\n",
      "        [1260.2378, 2980.4565, 4187.6836],\n",
      "        [ 862.9302, 2919.4014, 2946.5227],\n",
      "        [ 681.6061, 2913.3635, 2326.9651],\n",
      "        [ 578.5986, 2930.6897, 1911.6859],\n",
      "        [ 752.7729, 2874.8406, 2626.7971],\n",
      "        [ 983.2646, 2816.5503, 3532.3032],\n",
      "        [ 407.6621, 2837.5181, 1472.8086],\n",
      "        [ 791.7788, 2874.8015, 2752.5403],\n",
      "        [ 838.6904, 2883.3694, 2885.4316],\n",
      "        [ 755.4979, 2864.9653, 2616.3618],\n",
      "        [ 715.4282, 2865.9763, 2467.9026],\n",
      "        [ 887.2041, 2887.9553, 3025.1963],\n",
      "        [ 440.2404, 2850.9849, 1518.5182],\n",
      "        [ 939.7539, 2818.6729, 3338.3679],\n",
      "        [ 728.0175, 2802.5364, 2621.5178],\n",
      "        [ 912.5358, 2844.0754, 3199.9304],\n",
      "        [ 548.9833, 2800.8884, 1970.5408],\n",
      "        [ 761.2868, 2766.3870, 2804.4819],\n",
      "        [ 896.6799, 2781.6521, 3237.5588],\n",
      "        [ 570.6606, 2794.7781, 2066.6760],\n",
      "        [ 672.5383, 2787.1426, 2423.0493],\n",
      "        [ 873.2001, 2789.1702, 3127.7126],\n",
      "        [ 907.4581, 2792.0188, 3251.6562],\n",
      "        [ 664.8932, 2764.6331, 2471.2454],\n",
      "        [ 832.7466, 2848.0134, 2909.9180],\n",
      "        [ 842.1038, 2863.4460, 2891.2214],\n",
      "        [ 665.5634, 2861.0386, 2317.1770],\n",
      "        [ 826.1773, 2879.9680, 2862.2415],\n",
      "        [ 614.2711, 2876.1753, 2124.8730],\n",
      "        [ 700.5214, 2861.9268, 2478.0916],\n",
      "        [ 772.5572, 2883.7661, 2653.1697],\n",
      "        [ 594.2909, 2929.5449, 1959.7607],\n",
      "        [ 612.4550, 2828.0227, 2227.2839],\n",
      "        [ 661.1920, 2842.4390, 2347.3831],\n",
      "        [ 544.6426, 2852.4202, 1936.2438],\n",
      "        [ 640.2872, 2855.2019, 2250.4304],\n",
      "        [ 433.3678, 2857.2292, 1529.0347],\n",
      "        [ 793.8625, 2871.6008, 2755.6821],\n",
      "        [ 653.8871, 2828.3748, 2357.7896],\n",
      "        [ 618.0255, 2909.0293, 2055.9412],\n",
      "        [ 344.7073, 2816.9692, 1257.6722],\n",
      "        [ 760.3771, 2850.0029, 2636.7747],\n",
      "        [ 829.1504, 2825.2021, 2953.5474],\n",
      "        [ 491.3030, 2800.9109, 1771.9569],\n",
      "        [ 712.6400, 2828.5872, 2505.8418],\n",
      "        [ 688.2026, 2794.1301, 2484.5979],\n",
      "        [ 582.9718, 2817.0586, 2062.3296],\n",
      "        [ 687.1422, 2814.4053, 2414.2227],\n",
      "        [ 706.7643, 2791.1475, 2514.5107],\n",
      "        [ 672.6911, 2794.5825, 2371.3347],\n",
      "        [ 439.2150, 2781.7805, 1568.8951],\n",
      "        [ 611.4576, 2747.8264, 2253.5627],\n",
      "        [ 522.6339, 2760.6953, 1875.2993],\n",
      "        [ 512.7650, 2752.9373, 1858.7906],\n",
      "        [ 802.0955, 2762.1475, 2867.2754],\n",
      "        [ 270.0648, 2740.4480,  976.4690],\n",
      "        [ 707.5083, 2754.9592, 2541.5623],\n",
      "        [ 500.2066, 2688.3801, 1877.3739],\n",
      "        [ 485.0901, 2705.0190, 1768.7378],\n",
      "        [ 540.5365, 2718.6758, 1961.6337],\n",
      "        [ 808.8093, 2618.9189, 3146.4097],\n",
      "        [ 472.0888, 2705.2986, 1705.4631],\n",
      "        [ 511.5601, 2646.0364, 1947.6572],\n",
      "        [ 884.0037, 2711.3923, 3233.1733],\n",
      "        [ 698.3708, 2646.2429, 2631.4780],\n",
      "        [ 575.2266, 2639.4062, 2188.5596],\n",
      "        [ 850.0297, 2571.7773, 3390.2771],\n",
      "        [ 472.4554, 2606.3628, 1855.1086],\n",
      "        [ 581.4318, 2650.1641, 2165.6873],\n",
      "        [ 763.0880, 2645.4053, 2858.5649],\n",
      "        [ 447.3418, 2624.3926, 1685.9781],\n",
      "        [ 463.6311, 2623.7949, 1750.4476],\n",
      "        [ 677.3299, 2589.7905, 2622.2729],\n",
      "        [ 417.9098, 2598.2412, 1608.5734],\n",
      "        [ 902.8916, 2526.3701, 3614.4294],\n",
      "        [ 602.1835, 2626.0012, 2254.6375],\n",
      "        [ 575.6486, 2581.1667, 2252.2905],\n",
      "        [ 805.6508, 2601.0789, 3097.6147],\n",
      "        [ 694.0298, 2568.5713, 2730.7974],\n",
      "        [ 227.0470, 2603.9497,  858.0867],\n",
      "        [ 402.4693, 2568.4036, 1592.3969],\n",
      "        [ 546.5411, 2602.9331, 2087.1050],\n",
      "        [ 507.5958, 2620.1643, 1908.1401],\n",
      "        [ 365.5129, 2588.7795, 1405.6503],\n",
      "        [ 525.8506, 2498.2891, 2056.6399],\n",
      "        [ 567.3967, 2541.8950, 2282.7429],\n",
      "        [ 691.7547, 2547.1006, 2753.7224],\n",
      "        [ 524.0668, 2606.6140, 1991.8533],\n",
      "        [ 583.8630, 2608.1333, 2195.3159],\n",
      "        [ 415.1559, 2572.3916, 1608.7622],\n",
      "        [ 563.6390, 2592.5833, 2154.8848],\n",
      "        [ 389.6776, 2570.0457, 1500.7714],\n",
      "        [ 376.3822, 2547.4524, 1485.8368],\n",
      "        [ 313.6878, 2500.1042, 1348.5487],\n",
      "        [ 503.8379, 2514.1125, 2026.6641],\n",
      "        [ 687.8155, 2571.6543, 2633.7207],\n",
      "        [ 456.7065, 2510.0408, 1814.8792],\n",
      "        [ 318.8455, 2506.0249, 1281.2025],\n",
      "        [ 400.7759, 2500.9197, 1602.0586],\n",
      "        [ 425.6763, 2498.0488, 1708.2600],\n",
      "        [ 270.7150, 2477.2148, 1091.7904],\n",
      "        [ 140.4037, 2466.1445,  569.9670],\n",
      "        [ 171.9459, 2442.5625,  709.8528],\n",
      "        [ 297.7389, 2978.4568,  976.3694],\n",
      "        [1024.2928, 5576.5869, 1843.2689],\n",
      "        [ 643.6992, 5300.8813, 1215.6829],\n",
      "        [ 682.7648, 5255.0078, 1296.3960],\n",
      "        [ 581.6434, 5299.1694, 1095.7516],\n",
      "        [ 515.8337, 5342.2871,  965.4346],\n",
      "        [ 587.7160, 5309.6841, 1106.7290],\n",
      "        [ 573.5175, 5274.4790, 1088.0988],\n",
      "        [ 596.7066, 5208.0059, 1148.5787]])\n",
      "data:  tensor([[[0.0080, 0.0078, 0.0076,  ..., 0.6820, 0.2532, 0.5217]],\n",
      "\n",
      "        [[0.0081, 0.0083, 0.0085,  ..., 0.6792, 0.2532, 0.5217]],\n",
      "\n",
      "        [[0.0114, 0.0106, 0.0097,  ..., 0.6762, 0.2532, 0.5217]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0062, 0.0068, 0.0066,  ..., 0.6121, 0.2574, 0.0000]],\n",
      "\n",
      "        [[0.0068, 0.0066, 0.0066,  ..., 0.6121, 0.2574, 0.0000]],\n",
      "\n",
      "        [[0.0072, 0.0073, 0.0079,  ..., 0.6145, 0.2574, 0.0000]]])\n",
      "Epoch 0, Iteration 70 Train Loss: 955.90\n",
      "output:  tensor([[2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755],\n",
      "        [2.8001, 7.8723, 6.6755]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 787.6798, 5749.9663, 1382.5382],\n",
      "        [ 898.8109, 6049.0913, 1488.4761],\n",
      "        [ 758.0743, 6198.9277, 1223.0956],\n",
      "        [ 735.1242, 6279.8452, 1172.8284],\n",
      "        [ 658.7313, 6346.8711, 1038.2268],\n",
      "        [ 690.3103, 6380.6855, 1084.6007],\n",
      "        [ 744.7349, 6415.5166, 1162.4484],\n",
      "        [ 593.3065, 6443.2095,  920.6754],\n",
      "        [ 734.5640, 6440.1152, 1142.4288],\n",
      "        [ 731.4839, 6453.1182, 1134.1271],\n",
      "        [ 578.7817, 6440.2656,  900.5453],\n",
      "        [ 825.8844, 6436.2109, 1285.0820],\n",
      "        [ 831.5183, 6417.3149, 1299.9030],\n",
      "        [ 730.1010, 6436.1885, 1135.9392],\n",
      "        [ 651.9890, 6453.5425, 1010.8090],\n",
      "        [ 773.7579, 6445.8735, 1195.5449],\n",
      "        [ 964.1255, 6423.2471, 1494.4525],\n",
      "        [ 851.3809, 6377.7529, 1336.8110],\n",
      "        [ 566.5620, 6347.8149,  892.9301],\n",
      "        [ 831.7908, 6309.5933, 1320.4562],\n",
      "        [ 645.7878, 6286.7876, 1028.5197],\n",
      "        [ 598.8702, 6261.2510,  957.5877],\n",
      "        [ 629.1366, 6248.8735, 1008.5471],\n",
      "        [ 713.8901, 6248.9741, 1142.6255],\n",
      "        [ 557.7404, 6238.7021,  896.4297],\n",
      "        [ 503.9005, 6252.3535,  806.6365],\n",
      "        [ 437.4546, 6271.6680,  698.2609],\n",
      "        [ 558.8251, 6262.7925,  901.9441],\n",
      "        [ 786.5520, 6302.0815, 1249.2856],\n",
      "        [ 459.6176, 6316.1284,  728.0489],\n",
      "        [ 965.6885, 6367.1406, 1473.3303],\n",
      "        [ 618.7756, 6366.7607,  973.0950],\n",
      "        [ 587.2229, 6387.1533,  919.9666],\n",
      "        [ 715.6562, 6403.0161, 1128.9871],\n",
      "        [ 580.6857, 6443.6953,  899.8018],\n",
      "        [ 470.0631, 6472.8013,  725.1388],\n",
      "        [ 574.7673, 6484.2070,  892.9580],\n",
      "        [ 379.6675, 6538.5762,  580.5256],\n",
      "        [ 702.9508, 6575.4741, 1070.2954],\n",
      "        [ 749.0105, 6619.7109, 1132.1602],\n",
      "        [ 778.5942, 6660.7085, 1172.2820],\n",
      "        [ 424.9389, 6709.8887,  634.4940],\n",
      "        [ 624.2333, 6773.8872,  921.8210],\n",
      "        [ 706.6671, 6812.5781, 1039.4797],\n",
      "        [ 627.0521, 6878.0342,  912.3936],\n",
      "        [ 698.7871, 6923.6899, 1011.7314],\n",
      "        [ 808.4860, 6984.7285, 1159.0070],\n",
      "        [ 569.4919, 7035.0762,  811.8099],\n",
      "        [ 627.7595, 7090.1602,  886.8757],\n",
      "        [ 774.2460, 7148.7856, 1084.2399],\n",
      "        [ 769.0352, 7185.5771, 1071.3055],\n",
      "        [ 790.8174, 7231.4341, 1096.9813],\n",
      "        [ 811.7751, 7281.2119, 1117.0876],\n",
      "        [ 599.9547, 7336.9604,  818.9557],\n",
      "        [1077.9192, 7138.3403, 1612.0421],\n",
      "        [ 827.1176, 7259.3726, 1158.2021],\n",
      "        [ 740.7062, 7456.8921,  993.4516],\n",
      "        [ 796.2751, 7472.5708, 1067.6753],\n",
      "        [ 837.8304, 7503.4976, 1118.0388],\n",
      "        [ 845.5095, 7520.1646, 1126.6951],\n",
      "        [ 818.1187, 7552.8560, 1085.1440],\n",
      "        [ 686.8155, 7585.8223,  906.8354],\n",
      "        [ 890.9785, 7624.7808, 1169.1736],\n",
      "        [ 713.4781, 7638.5044,  935.0079],\n",
      "        [ 640.2598, 7661.4438,  836.6674],\n",
      "        [ 888.7437, 7689.5278, 1156.7771],\n",
      "        [ 760.8058, 7711.0039,  988.9506],\n",
      "        [ 884.9060, 7736.9990, 1145.3269],\n",
      "        [ 819.1605, 7765.8428, 1056.6075],\n",
      "        [ 832.5880, 7768.5737, 1073.0082],\n",
      "        [ 927.1016, 7775.7231, 1195.0826],\n",
      "        [ 642.8012, 7797.1436,  824.9266],\n",
      "        [ 721.3694, 7783.7778,  927.8047],\n",
      "        [ 630.4482, 7791.3291,  809.1746],\n",
      "        [ 724.5612, 7771.6235,  933.4862],\n",
      "        [ 797.2463, 7756.4365, 1030.1001],\n",
      "        [ 980.1595, 7717.1426, 1273.8815],\n",
      "        [ 865.2676, 7694.1250, 1125.8848],\n",
      "        [ 732.4409, 7655.8696,  957.4548],\n",
      "        [ 739.2260, 7595.8701,  974.7784],\n",
      "        [1125.6410, 7539.1055, 1496.1613],\n",
      "        [ 986.4816, 7507.9043, 1316.3591],\n",
      "        [ 974.0759, 7449.2681, 1309.6721],\n",
      "        [ 764.0461, 7425.8481, 1029.5792],\n",
      "        [ 685.0311, 7383.8955,  926.6136],\n",
      "        [ 582.1685, 7334.4863,  792.5336],\n",
      "        [ 694.2891, 7269.7729,  954.6473],\n",
      "        [ 874.9428, 7237.8794, 1208.2701],\n",
      "        [1020.9114, 7131.8223, 1437.8290],\n",
      "        [ 667.7276, 7072.5601,  946.9351],\n",
      "        [ 937.0738, 6999.6699, 1340.3425],\n",
      "        [ 747.9575, 6909.1187, 1083.1268],\n",
      "        [ 748.7871, 6808.8413, 1101.9315],\n",
      "        [ 726.4311, 6685.0894, 1085.2969],\n",
      "        [ 481.0963, 6549.6240,  736.0893],\n",
      "        [ 577.9341, 6413.9414,  900.8208],\n",
      "        [ 547.2260, 6289.2285,  869.5088],\n",
      "        [ 615.8893, 6185.6123,  995.0984],\n",
      "        [ 696.5905, 6068.8979, 1147.6300],\n",
      "        [ 431.1541, 5964.8345,  720.5493],\n",
      "        [ 871.1683, 5865.1055, 1484.1957],\n",
      "        [ 943.0079, 5753.1724, 1640.8596],\n",
      "        [ 734.5274, 5677.8853, 1293.5233],\n",
      "        [ 724.2712, 5608.9604, 1288.2410],\n",
      "        [ 514.5552, 5524.3628,  930.1053],\n",
      "        [ 733.5892, 5429.3594, 1357.2393],\n",
      "        [ 530.3884, 5369.3652,  985.5975],\n",
      "        [ 661.6912, 5288.4482, 1253.9686],\n",
      "        [1435.9973, 5228.5659, 2743.9968],\n",
      "        [ 680.1306, 5165.9473, 1317.9467],\n",
      "        [ 584.8521, 5109.8911, 1140.1017],\n",
      "        [ 833.7244, 5046.2783, 1653.6703],\n",
      "        [1065.5496, 4977.3311, 2144.1558],\n",
      "        [ 656.4797, 4930.5078, 1330.2365],\n",
      "        [ 819.1633, 4883.7349, 1671.7526],\n",
      "        [ 832.1309, 4802.5776, 1736.9287],\n",
      "        [ 843.0313, 4775.5776, 1763.8710],\n",
      "        [ 692.4134, 4712.8745, 1471.2902],\n",
      "        [ 993.3712, 4687.6006, 2111.0471],\n",
      "        [1007.4892, 4640.8833, 2170.4104],\n",
      "        [ 982.4169, 4629.0308, 2097.5403],\n",
      "        [ 910.5218, 4513.2603, 2040.2037],\n",
      "        [1147.2563, 4500.7661, 2536.6184],\n",
      "        [ 975.9442, 4412.2188, 2219.0342],\n",
      "        [ 770.0633, 4423.0713, 1720.2224],\n",
      "        [ 941.2743, 4397.7637, 2117.7686],\n",
      "        [ 561.4031, 4339.7583, 1288.6232],\n",
      "        [ 886.5227, 4314.3330, 2054.0588],\n",
      "        [ 832.4415, 4264.4883, 1947.1993],\n",
      "        [ 777.9544, 4234.2764, 1816.9440],\n",
      "        [ 893.5073, 4181.1245, 2135.3225],\n",
      "        [ 718.8901, 4182.6890, 1693.7517],\n",
      "        [ 905.8572, 4128.3755, 2192.2454],\n",
      "        [ 728.4222, 4071.8335, 1800.9929],\n",
      "        [ 579.0107, 4035.6450, 1454.9392],\n",
      "        [ 899.3832, 4009.9963, 2253.9365],\n",
      "        [1163.7345, 3973.5793, 2943.4341],\n",
      "        [ 813.4238, 3948.8801, 2062.3733],\n",
      "        [ 631.1298, 3898.5828, 1631.7263],\n",
      "        [ 710.5768, 3896.4546, 1822.6024],\n",
      "        [ 370.1574, 3874.5652,  958.3269],\n",
      "        [1090.8809, 3898.2588, 2755.1650],\n",
      "        [ 716.0820, 3776.8247, 1919.8037],\n",
      "        [ 659.8889, 3804.7241, 1715.9954],\n",
      "        [ 655.2550, 3732.3250, 1776.8739],\n",
      "        [ 898.0132, 3677.1794, 2480.9539],\n",
      "        [1365.3679, 3628.8091, 3812.0713],\n",
      "        [ 830.6707, 3690.9475, 2227.7517],\n",
      "        [ 927.5990, 3659.7471, 2525.6597],\n",
      "        [1047.5021, 3608.6177, 2897.1741]])\n",
      "data:  tensor([[[0.0092, 0.0092, 0.0092,  ..., 0.6161, 0.2574, 0.0000]],\n",
      "\n",
      "        [[0.0097, 0.0096, 0.0088,  ..., 0.6203, 0.2574, 0.0435]],\n",
      "\n",
      "        [[0.0096, 0.0088, 0.0092,  ..., 0.6244, 0.2574, 0.0435]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0107, 0.0114, 0.0114,  ..., 0.6441, 0.2574, 0.5652]],\n",
      "\n",
      "        [[0.0109, 0.0098, 0.0085,  ..., 0.6423, 0.2574, 0.5652]],\n",
      "\n",
      "        [[0.0112, 0.0119, 0.0131,  ..., 0.6406, 0.2574, 0.5652]]])\n",
      "output:  tensor([[2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273],\n",
      "        [2.8518, 7.9240, 6.7273]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 591.3714, 3567.6648, 1665.4862],\n",
      "        [1207.6210, 3535.8889, 3424.7747],\n",
      "        [ 840.5634, 3546.8923, 2369.2817],\n",
      "        [1015.8586, 3497.1255, 2902.2383],\n",
      "        [ 697.0487, 3505.5430, 1993.1619],\n",
      "        [ 966.2244, 3441.3767, 2840.7153],\n",
      "        [1037.9863, 3449.5205, 3018.4648],\n",
      "        [ 822.5710, 3429.4966, 2420.4954],\n",
      "        [1291.9001, 3411.4050, 3817.3833],\n",
      "        [ 882.2296, 3416.2644, 2590.2847],\n",
      "        [1076.8694, 3375.5742, 3209.8293],\n",
      "        [ 704.2155, 3411.7959, 2035.1919],\n",
      "        [ 541.2419, 3371.0498, 1597.1218],\n",
      "        [ 890.6833, 3371.3850, 2616.1084],\n",
      "        [ 708.9946, 3296.7461, 2212.1775],\n",
      "        [1263.2244, 3264.4563, 3902.0508],\n",
      "        [1030.9872, 3346.5518, 3028.6069],\n",
      "        [ 902.9192, 3284.2515, 2760.2222],\n",
      "        [ 787.7118, 3268.0254, 2416.4214],\n",
      "        [ 716.5234, 3330.6836, 2087.2468],\n",
      "        [ 600.2271, 3083.0730, 2045.3739],\n",
      "        [1268.0540, 3248.6885, 3933.3435],\n",
      "        [1589.0938, 3204.7642, 5020.8125],\n",
      "        [ 556.7551, 3239.8801, 1697.4991],\n",
      "        [ 824.1142, 3223.5146, 2540.6248],\n",
      "        [1097.7721, 3190.5659, 3480.6340],\n",
      "        [ 922.6674, 3211.2434, 2880.7234],\n",
      "        [ 585.2139, 3220.4148, 1781.4847],\n",
      "        [ 588.0368, 3207.2163, 1781.4521],\n",
      "        [ 958.7640, 3166.1350, 3050.8569],\n",
      "        [ 863.1519, 3193.6099, 2651.1807],\n",
      "        [ 960.1777, 3131.3374, 3075.3879],\n",
      "        [ 955.7707, 3164.2080, 2994.1155],\n",
      "        [ 475.4704, 3097.0591, 1553.0317],\n",
      "        [ 860.8350, 3098.1875, 2770.7563],\n",
      "        [ 697.9216, 3064.7864, 2323.7673],\n",
      "        [ 714.8568, 3082.8833, 2293.7166],\n",
      "        [ 888.1513, 3058.7834, 2908.9512],\n",
      "        [ 568.1942, 3067.7856, 1849.6747],\n",
      "        [ 739.7803, 3038.0764, 2429.8423],\n",
      "        [ 537.3153, 3045.2258, 1776.2325],\n",
      "        [ 696.2045, 3064.2332, 2257.1589],\n",
      "        [ 791.1500, 2998.5032, 2676.8865],\n",
      "        [ 853.3296, 2949.5576, 2963.7000],\n",
      "        [1110.0762, 3023.1409, 3696.4629],\n",
      "        [ 697.6860, 3021.4092, 2331.3491],\n",
      "        [1003.7100, 3042.2097, 3291.6499],\n",
      "        [ 604.9941, 2903.9800, 2663.2195],\n",
      "        [ 751.8400, 3068.1487, 2426.1228],\n",
      "        [ 680.1411, 3034.9319, 2274.2339],\n",
      "        [ 735.2911, 3023.6770, 2414.5564],\n",
      "        [ 855.4484, 3098.4053, 2709.0859],\n",
      "        [ 566.9412, 3028.8828, 1893.1497],\n",
      "        [ 888.5242, 3059.2676, 2886.3906],\n",
      "        [1035.5131, 3047.6724, 3388.4614],\n",
      "        [ 582.2228, 3013.5225, 1953.2992],\n",
      "        [ 556.5419, 2992.5715, 1894.2339],\n",
      "        [ 736.7798, 2955.1038, 2552.5820],\n",
      "        [ 597.8727, 2963.4150, 2050.8276],\n",
      "        [ 532.0891, 2953.1155, 1816.5050],\n",
      "        [ 687.1375, 2958.3604, 2346.5945],\n",
      "        [ 607.9030, 2967.9392, 2046.9990],\n",
      "        [1030.5560, 2947.4238, 3500.9246],\n",
      "        [ 621.6226, 2971.4414, 2082.6279],\n",
      "        [ 571.7474, 2971.8662, 1886.0764],\n",
      "        [ 838.4935, 2986.7456, 2758.4177],\n",
      "        [ 565.7016, 2901.1536, 2007.5359],\n",
      "        [ 571.3713, 2986.4329, 1857.8536],\n",
      "        [ 740.4578, 2970.8662, 2456.4590],\n",
      "        [ 710.1793, 2996.2690, 2313.3918],\n",
      "        [ 985.1828, 2919.7993, 3418.6702],\n",
      "        [ 810.0140, 2910.6379, 2859.2419],\n",
      "        [ 490.6995, 2950.0435, 1680.6359],\n",
      "        [ 661.7111, 2971.7654, 2202.1326],\n",
      "        [ 784.6990, 2952.7412, 2680.1287],\n",
      "        [1094.8383, 2974.0835, 3658.9348],\n",
      "        [ 653.6713, 2928.5671, 2274.2659],\n",
      "        [ 974.1448, 2957.0645, 3290.7273],\n",
      "        [ 629.4066, 2924.0261, 2184.5430],\n",
      "        [ 834.9803, 2929.5615, 2843.9961],\n",
      "        [ 844.5876, 2915.7988, 2883.7708],\n",
      "        [ 886.7540, 2862.8708, 3138.2385],\n",
      "        [ 598.1870, 2920.4404, 2060.1306],\n",
      "        [ 707.5784, 2852.9116, 2535.4241],\n",
      "        [ 769.8352, 2856.2463, 2719.0598],\n",
      "        [ 932.3423, 2853.1575, 3195.3225],\n",
      "        [1320.2955, 2826.6096, 4595.1816],\n",
      "        [1213.3887, 2751.6079, 4516.9121],\n",
      "        [ 955.6646, 2837.4065, 3254.3289],\n",
      "        [ 920.5044, 2727.8638, 3373.8503],\n",
      "        [1297.9053, 2732.9856, 4720.1509],\n",
      "        [1050.2220, 2723.5911, 3873.5884],\n",
      "        [1059.9081, 2800.5701, 3674.1101],\n",
      "        [ 738.1156, 2767.9119, 2675.1038],\n",
      "        [ 619.8861, 2735.1753, 2303.5002],\n",
      "        [1016.4642, 2724.8308, 3755.6970],\n",
      "        [ 547.2605, 2753.5684, 1973.4796],\n",
      "        [ 264.5712, 2721.7534, 1014.2034],\n",
      "        [1032.5524, 2688.0671, 3876.4753],\n",
      "        [ 729.9562, 2746.3406, 2639.8262],\n",
      "        [ 852.7005, 2697.1492, 3206.0159],\n",
      "        [ 892.2726, 2697.1436, 3344.3682],\n",
      "        [ 932.5588, 2779.4851, 3310.8640],\n",
      "        [ 971.5214, 2734.1643, 3547.6548],\n",
      "        [ 856.1415, 2779.2939, 3059.0134],\n",
      "        [ 663.8058, 2715.5591, 2498.1365],\n",
      "        [ 745.3677, 2809.6353, 2604.9612],\n",
      "        [ 692.4807, 2788.6787, 2498.6194],\n",
      "        [ 737.7202, 2824.1018, 2590.1143],\n",
      "        [ 977.1287, 2829.0618, 3454.5864],\n",
      "        [ 761.5659, 2863.5298, 2650.9297],\n",
      "        [ 447.5872, 2861.0164, 1591.4568],\n",
      "        [ 857.3431, 2887.5642, 2980.0110],\n",
      "        [ 689.8777, 2861.1226, 2468.9119],\n",
      "        [ 616.5026, 2887.5195, 2189.6079],\n",
      "        [ 361.9618, 2930.4216, 1244.6638],\n",
      "        [ 630.7272, 2946.6755, 2152.0828],\n",
      "        [ 612.8369, 2951.9148, 2064.8853],\n",
      "        [ 708.9432, 3032.6082, 2307.1250],\n",
      "        [ 503.3546, 2970.6150, 1721.5624],\n",
      "        [ 781.0502, 3034.9375, 2571.6904],\n",
      "        [ 331.4117, 3051.3252, 1105.1851],\n",
      "        [ 638.1310, 3002.7593, 2172.8315],\n",
      "        [ 735.5637, 3103.8733, 2364.8779],\n",
      "        [ 575.8704, 3123.4673, 1824.2563],\n",
      "        [ 506.2217, 3200.1169, 1561.8705],\n",
      "        [ 548.7227, 3227.9495, 1713.7617],\n",
      "        [ 397.3317, 3281.7825, 1228.0283],\n",
      "        [ 230.3518, 3358.1978,  685.6056],\n",
      "        [ 475.2097, 3406.4172, 1390.7699],\n",
      "        [ 454.4503, 3443.8345, 1313.6592],\n",
      "        [ 363.7387, 3476.3530, 1030.1187],\n",
      "        [ 294.6581, 3511.1172,  833.1423],\n",
      "        [ 425.5206, 3523.7629, 1207.2841],\n",
      "        [ 431.4579, 3552.1426, 1219.9712],\n",
      "        [ 421.1011, 3574.8643, 1175.4604],\n",
      "        [ 457.2527, 3586.4431, 1276.5676],\n",
      "        [ 394.5169, 3632.2332, 1084.6650],\n",
      "        [ 422.3446, 3669.8401, 1158.2771],\n",
      "        [ 447.1587, 3717.1995, 1196.0688],\n",
      "        [ 680.4559, 3747.1768, 1769.1495],\n",
      "        [ 440.4769, 3774.6575, 1177.0172],\n",
      "        [ 338.9589, 3815.3088,  893.6567],\n",
      "        [ 386.0481, 3874.5427,  994.6265],\n",
      "        [ 525.0078, 3923.6616, 1326.4819],\n",
      "        [ 391.8119, 3965.5806,  989.3681],\n",
      "        [ 510.6858, 4013.5320, 1277.6208],\n",
      "        [ 524.1837, 4059.7856, 1293.0773],\n",
      "        [ 473.2973, 4099.6714, 1166.0994],\n",
      "        [ 597.0181, 4173.5620, 1413.3562]])\n",
      "data:  tensor([[[0.0068, 0.0073, 0.0075,  ..., 0.6370, 0.2574, 0.5652]],\n",
      "\n",
      "        [[0.0139, 0.0132, 0.0126,  ..., 0.6365, 0.2574, 0.5652]],\n",
      "\n",
      "        [[0.0112, 0.0119, 0.0114,  ..., 0.6335, 0.2574, 0.5652]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0059, 0.0059, 0.0055,  ..., 0.6673, 0.2616, 0.0435]],\n",
      "\n",
      "        [[0.0057, 0.0061, 0.0066,  ..., 0.6720, 0.2616, 0.0435]],\n",
      "\n",
      "        [[0.0067, 0.0068, 0.0066,  ..., 0.6775, 0.2616, 0.0435]]])\n",
      "output:  tensor([[2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790],\n",
      "        [2.9035, 7.9758, 6.7790]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 613.2878, 4173.8857, 1577.8431],\n",
      "        [ 516.1608, 4257.9253, 1215.5172],\n",
      "        [ 496.8799, 4298.4087, 1159.5372],\n",
      "        [ 555.8168, 4336.3345, 1288.6353],\n",
      "        [ 487.0228, 4261.2598, 1148.3925],\n",
      "        [ 516.1471, 4424.3672, 1168.1251],\n",
      "        [ 655.1718, 4474.6592, 1452.2083],\n",
      "        [ 475.1636, 4513.0596, 1050.9004],\n",
      "        [ 429.2864, 4542.2046,  940.3868],\n",
      "        [ 530.1525, 4575.7231, 1158.6914],\n",
      "        [ 521.5188, 4602.1704, 1134.1484],\n",
      "        [ 688.8114, 4615.4580, 1493.9625],\n",
      "        [ 359.3385, 4636.0015,  776.3610],\n",
      "        [ 407.5973, 4675.9102,  871.4901],\n",
      "        [ 403.6490, 4703.5918,  857.9648],\n",
      "        [ 472.2643, 4754.1064,  995.4823],\n",
      "        [ 522.4163, 4783.7319, 1096.6515],\n",
      "        [ 652.6816, 4820.8813, 1359.5201],\n",
      "        [ 406.2072, 4896.8608,  830.4504],\n",
      "        [ 528.6230, 4954.5415, 1067.0221],\n",
      "        [ 593.3148, 4980.4141, 1198.2101],\n",
      "        [ 540.5105, 5053.4834, 1071.5190],\n",
      "        [ 465.6870, 5112.8071,  913.2930],\n",
      "        [ 364.3282, 5178.7827,  703.9625],\n",
      "        [ 512.4901, 5237.0508,  979.2419],\n",
      "        [ 589.2368, 5294.4976, 1116.0162],\n",
      "        [ 451.0718, 5354.7534,  843.5568],\n",
      "        [ 647.2992, 5396.8906, 1203.6458],\n",
      "        [ 545.6113, 5468.1055,  998.6682],\n",
      "        [ 584.6805, 5523.3906, 1059.7639],\n",
      "        [ 520.1754, 5572.4761,  934.1721],\n",
      "        [ 812.0453, 5575.6821, 1462.5845],\n",
      "        [ 433.4156, 5616.0986,  773.1132],\n",
      "        [ 671.5328, 5618.7686, 1208.8636],\n",
      "        [ 570.2046, 5668.1162, 1006.3117],\n",
      "        [ 637.5338, 5683.3984, 1124.1754],\n",
      "        [ 618.1576, 5709.5273, 1083.8663],\n",
      "        [ 581.2642, 5719.7988, 1018.9314],\n",
      "        [ 484.0697, 5735.7007,  844.2650],\n",
      "        [ 485.3571, 5732.6680,  847.2628],\n",
      "        [ 615.3113, 5712.5659, 1081.4166],\n",
      "        [ 541.3443, 5701.0093,  951.6385],\n",
      "        [ 585.6720, 5691.4248, 1030.4125],\n",
      "        [ 645.2754, 5687.7886, 1133.9150],\n",
      "        [ 523.1426, 5641.0156,  930.1299],\n",
      "        [ 529.4551, 5603.4087,  946.4579],\n",
      "        [ 713.0353, 5571.4150, 1281.3474],\n",
      "        [ 504.4232, 5550.3521,  910.0482],\n",
      "        [ 485.3494, 5523.1899,  880.8619],\n",
      "        [ 517.6187, 5481.7847,  945.6317],\n",
      "        [ 587.0423, 5412.9321, 1086.4681],\n",
      "        [ 578.2124, 5352.7632, 1081.6997],\n",
      "        [ 433.0572, 5281.7905,  821.6763],\n",
      "        [ 569.0369, 5195.1650, 1096.9694],\n",
      "        [ 466.0798, 5099.7368,  911.6894],\n",
      "        [ 433.8657, 4942.8687,  878.1199],\n",
      "        [ 320.6695, 4769.3550,  673.1476],\n",
      "        [ 661.3633, 4600.6006, 1442.2559],\n",
      "        [ 523.9323, 4443.2183, 1159.2538],\n",
      "        [ 367.0700, 4250.8652,  856.7810],\n",
      "        [ 310.4930, 4045.4141,  769.6988],\n",
      "        [ 352.6816, 3853.5247,  914.0275],\n",
      "        [ 531.5637, 3664.6792, 1441.7076],\n",
      "        [ 187.0917, 3464.9084,  540.2205],\n",
      "        [ 247.7670, 3292.1379,  749.1057],\n",
      "        [ 407.1898, 3171.4412, 1277.4017],\n",
      "        [ 314.0290, 2956.8467, 1070.4659],\n",
      "        [ 254.4971, 2793.5659,  940.8830],\n",
      "        [ 368.7363, 2696.8198, 1321.3324],\n",
      "        [ 299.8133, 2550.5581, 1196.4187],\n",
      "        [ 391.9200, 2465.3066, 1524.0413],\n",
      "        [ 298.7489, 2403.1123, 1228.7247],\n",
      "        [ 260.5619, 2297.4348, 1131.1244],\n",
      "        [ 234.2250, 2250.7290, 1036.3346],\n",
      "        [ 211.0228, 2182.0144,  966.5756],\n",
      "        [ 203.4061, 2133.7129,  952.8096],\n",
      "        [ 246.0933, 2097.5916, 1166.7469],\n",
      "        [ 236.3395, 2062.3750, 1137.0531],\n",
      "        [ 152.9742, 2042.4740,  742.5757],\n",
      "        [ 245.7173, 2031.4425, 1207.6895],\n",
      "        [ 253.4625, 2046.0822, 1227.9186],\n",
      "        [ 237.9859, 2068.7593, 1142.2806],\n",
      "        [ 189.4692, 2077.0479,  931.3444],\n",
      "        [ 365.4964, 2143.6326, 1687.6377],\n",
      "        [ 309.4438, 2148.1345, 1439.5936],\n",
      "        [ 318.8101, 2179.1509, 1455.8785],\n",
      "        [ 192.3214, 2208.0557,  863.3557],\n",
      "        [ 329.6658, 2210.1892, 1483.0861],\n",
      "        [ 345.7989, 2255.0073, 1512.4062],\n",
      "        [ 371.4703, 2277.2710, 1608.7694],\n",
      "        [ 323.7607, 2205.8943, 1516.8428],\n",
      "        [ 306.8019, 2273.8696, 1340.4469],\n",
      "        [ 289.9463, 2281.1475, 1280.8815],\n",
      "        [ 337.1230, 2328.6521, 1428.7194],\n",
      "        [ 477.6459, 2311.3928, 2066.6704],\n",
      "        [ 334.1527, 2333.3550, 1421.6307],\n",
      "        [ 159.6757, 2329.0264,  686.9200],\n",
      "        [ 360.9914, 2350.7817, 1521.9731],\n",
      "        [ 375.6996, 2348.6326, 1601.8728],\n",
      "        [ 312.8524, 2361.8420, 1327.8031],\n",
      "        [ 555.9105, 2365.5554, 2352.1892],\n",
      "        [ 335.6767, 2368.5996, 1428.3282],\n",
      "        [ 545.3646, 2414.2217, 2221.1111],\n",
      "        [ 392.8456, 2413.3896, 1614.0056],\n",
      "        [ 409.2672, 2399.3867, 1733.5024],\n",
      "        [ 452.3004, 2429.2244, 1841.3506],\n",
      "        [ 494.3719, 2448.9468, 2004.2698],\n",
      "        [ 566.0020, 2460.6260, 2298.6057],\n",
      "        [ 304.5709, 2461.9890, 1229.3253],\n",
      "        [ 433.1949, 2445.6904, 1789.3890],\n",
      "        [ 524.3271, 2473.6738, 2112.6270],\n",
      "        [ 547.8884, 2468.1943, 2219.2834],\n",
      "        [ 346.4980, 2465.4685, 1395.8827],\n",
      "        [ 368.2678, 2452.2439, 1525.5248],\n",
      "        [ 386.8076, 2486.5203, 1555.6654],\n",
      "        [ 594.8221, 2494.3792, 2365.6094],\n",
      "        [ 461.2953, 2507.8457, 1830.7792],\n",
      "        [ 491.7217, 2502.6345, 1968.9634],\n",
      "        [ 421.0394, 2510.1135, 1670.7136],\n",
      "        [ 373.6742, 2487.8777, 1526.9491],\n",
      "        [ 590.0539, 2482.9905, 2389.4678],\n",
      "        [ 459.4254, 2531.8132, 1812.6190],\n",
      "        [ 508.9846, 2529.5454, 2012.8328],\n",
      "        [ 705.9680, 2546.0339, 2759.6938],\n",
      "        [ 411.4185, 2529.5344, 1621.0233],\n",
      "        [ 268.3401, 2547.5417, 1047.2703],\n",
      "        [ 512.7842, 2578.5078, 1956.2365],\n",
      "        [ 397.8040, 2552.8369, 1545.1130],\n",
      "        [ 472.8662, 2552.7979, 1845.3641],\n",
      "        [ 655.3757, 2548.6533, 2565.2288],\n",
      "        [ 518.2360, 2557.6685, 2024.4382],\n",
      "        [ 443.3763, 2533.1536, 1760.1731],\n",
      "        [ 536.5881, 2551.0105, 2120.6858],\n",
      "        [ 660.4818, 2510.1357, 2708.5293],\n",
      "        [ 488.5291, 2584.0933, 1854.7866],\n",
      "        [ 629.6800, 2602.7319, 2380.4971],\n",
      "        [ 535.7277, 2567.1023, 2092.7190],\n",
      "        [ 503.5446, 2534.9465, 2019.2227],\n",
      "        [ 327.1646, 2551.3232, 1297.5328],\n",
      "        [ 654.2659, 2573.7378, 2536.5889],\n",
      "        [ 459.4084, 2593.1641, 1722.9421],\n",
      "        [ 452.7503, 2581.3677, 1752.8918],\n",
      "        [ 490.9848, 2556.2441, 1935.0354],\n",
      "        [ 753.2324, 2540.7051, 3008.8335],\n",
      "        [ 380.6929, 2588.3831, 1449.2791],\n",
      "        [ 341.7232, 2573.8899, 1338.2488],\n",
      "        [ 443.7970, 2591.5835, 1688.3647],\n",
      "        [ 566.2872, 2599.2690, 2162.0122],\n",
      "        [ 459.3751, 2593.8513, 1764.9537],\n",
      "        [ 665.3041, 2569.3699, 2599.1753]])\n",
      "data:  tensor([[[0.0072, 0.0073, 0.0073,  ..., 0.6833, 0.2616, 0.0435]],\n",
      "\n",
      "        [[0.0062, 0.0060, 0.0062,  ..., 0.6889, 0.2616, 0.0870]],\n",
      "\n",
      "        [[0.0059, 0.0060, 0.0056,  ..., 0.6957, 0.2616, 0.0870]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0067, 0.0065, 0.0061,  ..., 0.6335, 0.2616, 0.6087]],\n",
      "\n",
      "        [[0.0052, 0.0052, 0.0057,  ..., 0.6331, 0.2616, 0.6087]],\n",
      "\n",
      "        [[0.0080, 0.0083, 0.0075,  ..., 0.6299, 0.2616, 0.6087]]])\n",
      "output:  tensor([[2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307],\n",
      "        [2.9552, 8.0275, 6.8307]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 490.6834, 2585.4729, 1912.9031],\n",
      "        [ 468.2639, 2573.1738, 1843.1281],\n",
      "        [ 585.2798, 2559.8020, 2319.0657],\n",
      "        [ 476.0292, 2573.4028, 1871.3600],\n",
      "        [ 499.5497, 2577.5696, 1950.8821],\n",
      "        [ 562.2672, 2617.1370, 2095.7542],\n",
      "        [ 526.8206, 2589.7625, 2029.3734],\n",
      "        [ 431.8296, 2565.1809, 1710.8762],\n",
      "        [ 607.9488, 2575.6035, 2360.7424],\n",
      "        [ 480.7388, 2566.7839, 1890.3447],\n",
      "        [ 752.5339, 2591.0583, 2917.3521],\n",
      "        [ 740.7225, 2610.4065, 2795.7385],\n",
      "        [ 613.3240, 2609.9932, 2338.1206],\n",
      "        [ 461.4619, 2595.6609, 1772.3337],\n",
      "        [ 495.5782, 2577.6755, 1935.4832],\n",
      "        [ 751.9509, 2616.1484, 2852.5806],\n",
      "        [ 629.1060, 2589.0366, 2446.7759],\n",
      "        [ 527.7074, 2579.6472, 2056.3220],\n",
      "        [ 523.3305, 2614.9031, 1963.2412],\n",
      "        [ 577.5867, 2558.4167, 2284.3210],\n",
      "        [ 741.5750, 2601.2576, 2839.1309],\n",
      "        [ 538.8370, 2461.9387, 2515.5408],\n",
      "        [ 622.2458, 2586.5732, 2393.5530],\n",
      "        [ 694.2296, 2593.4434, 2656.1555],\n",
      "        [ 523.9529, 2533.9802, 2117.1772],\n",
      "        [ 497.7344, 2613.7188, 1853.0293],\n",
      "        [ 490.5422, 2592.1755, 1854.5558],\n",
      "        [ 615.4377, 2508.4880, 2648.2258],\n",
      "        [ 560.3395, 2612.4863, 2104.7664],\n",
      "        [ 460.6051, 2550.4016, 1831.3232],\n",
      "        [ 519.3437, 2605.1506, 1969.9210],\n",
      "        [ 520.1039, 2566.1191, 2069.6235],\n",
      "        [ 656.3756, 2588.2378, 2564.5183],\n",
      "        [ 578.9557, 2625.2080, 2194.4043],\n",
      "        [ 570.5438, 2626.4036, 2181.4431],\n",
      "        [ 609.9238, 2619.3601, 2339.1729],\n",
      "        [ 601.3613, 2647.3938, 2268.2361],\n",
      "        [ 678.3550, 2641.4062, 2499.9043],\n",
      "        [ 424.3118, 2645.3774, 1571.9033],\n",
      "        [ 434.2354, 2594.0632, 1711.9207],\n",
      "        [ 450.3387, 2597.0962, 1756.8574],\n",
      "        [ 523.1322, 2615.9976, 2007.8898],\n",
      "        [ 530.7384, 2602.5757, 2079.4001],\n",
      "        [ 596.2661, 2599.8220, 2318.8115],\n",
      "        [ 728.5658, 2599.5149, 2805.1899],\n",
      "        [ 549.0305, 2610.9485, 2093.4456],\n",
      "        [ 548.6909, 2594.5046, 2137.7944],\n",
      "        [ 521.0854, 2593.3765, 1995.7234],\n",
      "        [ 832.7856, 2618.1257, 3183.7058],\n",
      "        [ 464.6527, 2602.2239, 1763.7716],\n",
      "        [ 511.0455, 2566.3762, 2000.5542],\n",
      "        [ 634.4059, 2552.6248, 2501.1902],\n",
      "        [ 583.3528, 2578.4465, 2244.7759],\n",
      "        [ 648.2936, 2554.3728, 2538.7485],\n",
      "        [ 491.2685, 2549.8374, 1942.7023],\n",
      "        [ 650.2393, 2527.3669, 2603.4070],\n",
      "        [ 283.0304, 2525.0994, 1129.8379],\n",
      "        [ 642.8666, 2557.4170, 2493.0493],\n",
      "        [ 374.7600, 2519.9551, 1498.1041],\n",
      "        [ 593.6912, 2549.2678, 2304.0061],\n",
      "        [ 560.2576, 2504.2432, 2251.0637],\n",
      "        [ 578.2899, 2504.2869, 2290.4614],\n",
      "        [ 453.6560, 2523.7756, 1771.1021],\n",
      "        [ 422.4186, 2489.1567, 1721.0914],\n",
      "        [ 576.5953, 2478.8069, 2356.7603],\n",
      "        [ 652.9351, 2478.9409, 2638.9268],\n",
      "        [ 439.2194, 2469.1719, 1795.0978],\n",
      "        [ 486.5083, 2475.2991, 1956.1702],\n",
      "        [ 534.2956, 2486.4365, 2113.2620],\n",
      "        [ 481.5904, 2467.2505, 1951.7751],\n",
      "        [ 632.7958, 2463.8096, 2566.1377],\n",
      "        [ 513.2374, 2474.6345, 2040.8784],\n",
      "        [ 632.0169, 2428.1575, 2610.6238],\n",
      "        [ 463.0990, 2398.8394, 1967.4507],\n",
      "        [ 592.5578, 2471.6294, 2342.0225],\n",
      "        [ 472.9514, 2415.3220, 1957.8391],\n",
      "        [ 530.2469, 2379.2288, 2256.7795],\n",
      "        [ 614.4252, 2428.6658, 2499.0200],\n",
      "        [ 601.7728, 2396.5774, 2526.0247],\n",
      "        [ 437.2882, 2415.1155, 1789.3611],\n",
      "        [ 517.9821, 2384.1272, 2160.0637],\n",
      "        [ 475.0457, 2387.0540, 1984.4095],\n",
      "        [ 395.8357, 2349.0837, 1714.7089],\n",
      "        [ 341.0219, 2359.1040, 1464.9840],\n",
      "        [ 558.9424, 2359.7744, 2374.9243],\n",
      "        [ 535.2399, 2360.7463, 2277.3928],\n",
      "        [ 415.3014, 2326.8591, 1832.6785],\n",
      "        [ 532.8121, 2389.5618, 2189.9949],\n",
      "        [ 469.2105, 2348.5198, 2005.5914],\n",
      "        [ 557.6047, 2325.6582, 2415.9944],\n",
      "        [ 294.4372, 2369.8953, 1220.5273],\n",
      "        [ 427.0068, 2368.0242, 1771.1285],\n",
      "        [ 447.1013, 2339.7949, 1899.5195],\n",
      "        [ 394.0237, 2358.4395, 1654.0353],\n",
      "        [ 500.0530, 2341.3818, 2141.6360],\n",
      "        [ 319.5479, 2329.7358, 1378.6053],\n",
      "        [ 460.8917, 2342.0125, 1953.4817],\n",
      "        [ 367.6997, 2324.8765, 1600.2565],\n",
      "        [ 401.7892, 2364.5166, 1677.6898],\n",
      "        [ 464.9491, 2351.7815, 1967.1719],\n",
      "        [ 435.6841, 2369.4595, 1823.7098],\n",
      "        [ 464.1458, 2328.8140, 2010.0684],\n",
      "        [ 350.4135, 2363.6116, 1469.7965],\n",
      "        [ 320.5016, 2377.3071, 1341.7136],\n",
      "        [ 310.5330, 2349.3909, 1331.8250],\n",
      "        [ 266.1637, 2371.2693, 1103.9391],\n",
      "        [ 264.8527, 2395.8901, 1092.7051],\n",
      "        [ 311.3349, 2366.3765, 1328.8910],\n",
      "        [ 258.7866, 2393.4548, 1073.6223],\n",
      "        [ 385.7236, 2382.6694, 1621.6605],\n",
      "        [ 417.0463, 2362.7402, 1773.4315],\n",
      "        [ 302.7173, 2381.0942, 1277.4884],\n",
      "        [ 182.2043, 2386.8586,  763.9940],\n",
      "        [ 288.8675, 2399.1187, 1192.1709],\n",
      "        [ 276.0511, 2400.5930, 1126.1702],\n",
      "        [ 234.8787, 2384.7192,  990.8610],\n",
      "        [ 204.2129, 2392.8013,  853.0106],\n",
      "        [ 352.0836, 2393.8181, 1429.6866],\n",
      "        [ 227.4219, 2374.6375,  962.0728],\n",
      "        [ 254.4075, 2375.9165, 1063.4698],\n",
      "        [ 258.1334, 2379.9714, 1074.6475],\n",
      "        [ 212.7347, 2366.6389,  891.5853],\n",
      "        [ 312.8264, 2336.6338, 1341.6874],\n",
      "        [ 264.8270, 2340.3481, 1126.5310],\n",
      "        [ 191.6661, 2315.7998,  831.6606],\n",
      "        [ 165.6162, 2300.7805,  718.2717],\n",
      "        [ 232.0652, 2247.7629, 1078.8920],\n",
      "        [ 288.7220, 2288.8005, 1251.1793],\n",
      "        [ 220.1996, 2266.7312,  960.3042],\n",
      "        [ 231.6254, 2243.3840, 1015.1577],\n",
      "        [ 240.6288, 2207.2180, 1098.0073],\n",
      "        [ 228.5122, 2201.2024, 1032.3660],\n",
      "        [ 157.5080, 2168.1753,  723.2880],\n",
      "        [ 243.8707, 2141.0020, 1088.6101],\n",
      "        [ 208.0935, 2106.5002,  989.2838],\n",
      "        [ 147.1467, 2073.5908,  707.7473],\n",
      "        [ 240.9883, 2027.0468, 1204.3229],\n",
      "        [ 181.6633, 2018.4395,  889.6639],\n",
      "        [ 238.2309, 1979.2797, 1205.8938],\n",
      "        [ 154.0701, 1952.2848,  783.7556],\n",
      "        [ 238.6765, 1936.8467, 1218.2888],\n",
      "        [ 213.5247, 1891.9001, 1129.3574],\n",
      "        [ 176.8956, 1871.8425,  940.0776],\n",
      "        [ 120.7229, 1840.0612,  657.8491],\n",
      "        [ 134.7302, 1808.5367,  741.5719],\n",
      "        [ 148.1308, 1799.0693,  827.7013],\n",
      "        [ 113.8926, 1787.7587,  633.2584],\n",
      "        [ 165.2078, 1769.2205,  929.1988],\n",
      "        [ 183.0907, 1743.1642, 1050.2767],\n",
      "        [ 122.5652, 1724.3969,  716.1424]])\n",
      "data:  tensor([[[0.0057, 0.0059, 0.0061,  ..., 0.6299, 0.2616, 0.6087]],\n",
      "\n",
      "        [[0.0055, 0.0054, 0.0060,  ..., 0.6289, 0.2616, 0.6087]],\n",
      "\n",
      "        [[0.0063, 0.0065, 0.0065,  ..., 0.6263, 0.2616, 0.6087]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0019, 0.0020, 0.0019,  ..., 0.7577, 0.2658, 0.0870]],\n",
      "\n",
      "        [[0.0022, 0.0022, 0.0020,  ..., 0.7627, 0.2658, 0.0870]],\n",
      "\n",
      "        [[0.0013, 0.0013, 0.0015,  ..., 0.7669, 0.2658, 0.0870]]])\n",
      "output:  tensor([[3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824],\n",
      "        [3.0069, 8.0791, 6.8824]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[  81.8800, 1702.3845,  483.5308],\n",
      "        [ 163.4145, 1700.6307,  960.9391],\n",
      "        [ 128.5724, 1713.3265,  750.0123],\n",
      "        [ 100.7675, 1713.3098,  584.5694],\n",
      "        [  89.2968, 1710.4779,  523.4695],\n",
      "        [ 111.9063, 1711.0365,  653.7112],\n",
      "        [ 138.4959, 1702.5128,  815.6912],\n",
      "        [  89.9843, 1681.0144,  538.6989],\n",
      "        [  98.0652, 1670.2231,  589.7850],\n",
      "        [ 136.1346, 1667.5447,  813.3057],\n",
      "        [ 160.7146, 1633.5210,  985.6072],\n",
      "        [  79.1953, 1627.2876,  483.3499],\n",
      "        [ 102.3638, 1593.8975,  640.7125],\n",
      "        [ 122.9461, 1563.8811,  789.1758],\n",
      "        [  85.6341, 1551.1572,  546.1897],\n",
      "        [ 103.9989, 1534.7472,  670.4113],\n",
      "        [ 143.2816, 1521.1073,  933.7609],\n",
      "        [  50.4890, 1490.7001,  339.1639],\n",
      "        [  98.9424, 1482.0145,  663.9554],\n",
      "        [ 121.3175, 1473.1001,  826.8902],\n",
      "        [ 134.8929, 1469.7209,  907.0321],\n",
      "        [  80.2117, 1454.9248,  549.6742],\n",
      "        [ 168.7606, 1439.6987, 1168.6255],\n",
      "        [ 101.0518, 1421.4064,  702.0783],\n",
      "        [ 164.2418, 1398.0310, 1170.8679],\n",
      "        [ 123.6409, 1369.7908,  895.9940],\n",
      "        [ 451.4822, 2836.6133, 1279.5757],\n",
      "        [2948.8269, 9134.1094, 3216.9314],\n",
      "        [1159.0146, 9352.0713, 1233.5627],\n",
      "        [1403.5723, 9049.6895, 1553.7231],\n",
      "        [1221.6611, 8811.7812, 1389.6913],\n",
      "        [ 927.0325, 8747.7832, 1061.1644],\n",
      "        [1165.1539, 8702.5801, 1340.8420],\n",
      "        [1490.7235, 8643.4346, 1728.0605],\n",
      "        [1326.0115, 8588.5576, 1546.4491],\n",
      "        [1058.3057, 8524.5977, 1241.9537],\n",
      "        [1514.4526, 8461.9570, 1792.1016],\n",
      "        [1348.1021, 8404.2139, 1606.1732],\n",
      "        [1019.6761, 8360.0049, 1221.3646],\n",
      "        [ 927.5326, 8313.4893, 1116.5570],\n",
      "        [ 936.8948, 8268.9053, 1135.4152],\n",
      "        [1089.2130, 8212.5801, 1328.3857],\n",
      "        [1302.2574, 8153.0903, 1600.5983],\n",
      "        [ 833.1320, 8114.7012, 1027.6394],\n",
      "        [ 982.9386, 8072.2793, 1219.4868],\n",
      "        [1138.0920, 8006.5884, 1424.6836],\n",
      "        [ 973.4648, 7938.7192, 1226.6494],\n",
      "        [1106.1556, 7883.2607, 1404.9221],\n",
      "        [1323.1974, 7813.9614, 1697.1366],\n",
      "        [1078.0033, 7756.0010, 1394.5730],\n",
      "        [1007.2354, 7707.9600, 1309.5660],\n",
      "        [ 792.3327, 7660.0591, 1036.7242],\n",
      "        [ 865.6242, 7619.0112, 1137.0952],\n",
      "        [ 630.9302, 7572.7466,  833.7613],\n",
      "        [ 824.7712, 7511.6187, 1098.6191],\n",
      "        [ 932.7731, 7455.2056, 1253.6587],\n",
      "        [ 917.1936, 7401.0596, 1241.9561],\n",
      "        [ 810.3786, 7360.8330, 1101.4744],\n",
      "        [ 673.2413, 7318.9473,  919.5888],\n",
      "        [1009.8713, 7262.2544, 1394.8822],\n",
      "        [ 778.7386, 7207.2041, 1082.9126],\n",
      "        [ 668.0898, 7172.7749,  934.2614],\n",
      "        [ 698.2971, 7124.5054,  982.1976],\n",
      "        [ 868.1589, 7074.2749, 1228.1185],\n",
      "        [ 782.8061, 7014.2925, 1119.4314],\n",
      "        [ 865.8528, 6960.3086, 1247.4648],\n",
      "        [ 737.3989, 6907.6763, 1067.9023],\n",
      "        [ 738.7279, 6833.5737, 1085.3822],\n",
      "        [1046.6323, 6805.4229, 1518.7762],\n",
      "        [ 806.1671, 6724.9136, 1195.2578],\n",
      "        [ 796.4102, 6628.7817, 1205.0933],\n",
      "        [ 998.3608, 6429.9717, 1791.4030],\n",
      "        [ 768.5604, 6489.0068, 1186.3303],\n",
      "        [ 642.4846, 6431.8540,  998.8091],\n",
      "        [1038.1285, 6337.9341, 1644.5314],\n",
      "        [ 816.3429, 6301.7847, 1296.2979],\n",
      "        [ 536.9026, 6216.8184,  868.6318],\n",
      "        [ 935.1288, 6154.0991, 1522.8718],\n",
      "        [ 505.4838, 6088.1787,  833.4251],\n",
      "        [1042.8125, 6041.2271, 1727.7251],\n",
      "        [ 624.7794, 5979.4795, 1044.4039],\n",
      "        [ 722.2133, 5907.8066, 1228.4076],\n",
      "        [ 728.6735, 5867.7866, 1243.1230],\n",
      "        [ 685.3676, 5803.4526, 1183.1091],\n",
      "        [1035.4962, 5738.3486, 1808.4111],\n",
      "        [ 741.5516, 5689.1792, 1305.6904],\n",
      "        [ 830.7603, 5626.8452, 1474.1556],\n",
      "        [ 808.0881, 5566.3936, 1456.9771],\n",
      "        [ 924.4081, 5519.5591, 1678.2715],\n",
      "        [1195.1356, 5474.2329, 2161.4473],\n",
      "        [ 865.3484, 5416.0659, 1595.1954],\n",
      "        [ 919.5867, 5338.8462, 1728.8604],\n",
      "        [1138.1987, 5325.3799, 2129.4751],\n",
      "        [ 916.6713, 5240.9492, 1749.4510],\n",
      "        [ 888.9523, 5215.6025, 1701.8542],\n",
      "        [ 824.8240, 5142.5605, 1604.1147],\n",
      "        [ 894.2677, 5125.3521, 1720.0200],\n",
      "        [1081.8918, 5040.1675, 2143.2490],\n",
      "        [ 968.8762, 5025.6567, 1914.9631],\n",
      "        [ 695.6122, 4941.1035, 1410.7616],\n",
      "        [1145.3982, 4917.8511, 2323.7595],\n",
      "        [1162.5667, 4820.8252, 2428.2314],\n",
      "        [ 842.1375, 4811.3413, 1758.3854],\n",
      "        [1119.1658, 4671.5938, 2650.0337],\n",
      "        [1050.2919, 4740.7407, 2213.3005],\n",
      "        [1128.2678, 4665.7891, 2462.8279],\n",
      "        [ 758.1436, 4669.1123, 1633.7257],\n",
      "        [ 796.7371, 4579.3813, 1767.3809],\n",
      "        [ 795.2504, 4605.3989, 1715.1702],\n",
      "        [ 740.0015, 4571.2881, 1618.9050],\n",
      "        [ 816.1548, 4517.6504, 1801.9719],\n",
      "        [1016.3253, 4488.1089, 2268.7092],\n",
      "        [1070.5787, 4476.7646, 2382.8853],\n",
      "        [1055.1534, 4439.2192, 2379.1189],\n",
      "        [ 802.9521, 4411.4199, 1790.2517],\n",
      "        [1034.4438, 4394.0825, 2342.1875],\n",
      "        [1060.0063, 4339.7974, 2440.8423],\n",
      "        [1062.3470, 4312.9146, 2434.4700],\n",
      "        [ 958.0673, 4320.4937, 2190.8513],\n",
      "        [1033.5869, 4236.9014, 2451.1628],\n",
      "        [ 730.1255, 4195.0049, 1762.1241],\n",
      "        [ 549.5392, 4186.9507, 1317.6575],\n",
      "        [1177.1876, 4162.1060, 2822.5854],\n",
      "        [ 653.5620, 4136.6924, 1581.2495],\n",
      "        [ 879.3502, 4110.6860, 2126.7083],\n",
      "        [ 701.4108, 4074.8386, 1721.0533],\n",
      "        [ 750.0403, 4042.8672, 1868.8969],\n",
      "        [ 894.3873, 4019.5867, 2235.2314],\n",
      "        [ 999.0131, 4024.0830, 2476.4131],\n",
      "        [ 629.2427, 4009.2368, 1573.7468],\n",
      "        [ 834.9546, 3989.9670, 2092.4641],\n",
      "        [ 623.4811, 3963.2739, 1572.1677],\n",
      "        [ 909.1270, 3933.2910, 2317.0581],\n",
      "        [ 807.0374, 3899.4819, 2079.8762],\n",
      "        [ 787.9028, 3873.2190, 2060.0105],\n",
      "        [ 667.4760, 3905.6719, 1681.2705],\n",
      "        [ 489.5893, 3843.5266, 1280.5159],\n",
      "        [ 770.9268, 3793.4526, 2054.8005],\n",
      "        [ 540.6088, 3691.3667, 1743.3727],\n",
      "        [ 824.4987, 3791.0286, 2173.2551],\n",
      "        [ 768.4277, 3769.5913, 2027.1420],\n",
      "        [1134.8483, 3760.4536, 3025.3040],\n",
      "        [ 767.1093, 3748.9753, 2041.2633],\n",
      "        [ 808.4949, 3734.9614, 2139.5334],\n",
      "        [ 715.7928, 3695.1648, 1940.3466],\n",
      "        [ 818.6934, 3710.1562, 2174.1785],\n",
      "        [ 684.1284, 3632.4956, 1915.9606],\n",
      "        [ 869.7328, 3671.1135, 2356.6018],\n",
      "        [ 751.4738, 3643.6831, 2051.3416],\n",
      "        [ 765.2661, 3628.5242, 2100.4336]])\n",
      "data:  tensor([[[0.0009, 0.0009, 0.0010,  ..., 0.7716, 0.2658, 0.0870]],\n",
      "\n",
      "        [[0.0020, 0.0018, 0.0017,  ..., 0.7769, 0.2658, 0.1304]],\n",
      "\n",
      "        [[0.0016, 0.0018, 0.0017,  ..., 0.7819, 0.2658, 0.1304]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0102, 0.0088, 0.0080,  ..., 0.6228, 0.2658, 0.6522]],\n",
      "\n",
      "        [[0.0098, 0.0100, 0.0109,  ..., 0.6228, 0.2658, 0.6522]],\n",
      "\n",
      "        [[0.0088, 0.0099, 0.0099,  ..., 0.6200, 0.2658, 0.6522]]])\n",
      "output:  tensor([[3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340],\n",
      "        [3.0586, 8.1308, 6.9340]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 736.4898, 3608.3774, 2049.7202],\n",
      "        [1065.9730, 3538.8157, 3039.7542],\n",
      "        [ 799.6763, 3604.0039, 2197.4548],\n",
      "        [ 702.2981, 3539.9885, 2009.0049],\n",
      "        [ 624.5183, 3541.4744, 1760.2296],\n",
      "        [ 665.7252, 3366.8831, 1933.0973],\n",
      "        [ 880.3245, 3520.6963, 2487.2498],\n",
      "        [ 834.0914, 3492.1545, 2396.0935],\n",
      "        [1184.1987, 3492.4170, 3358.6265],\n",
      "        [ 678.1956, 3395.3691, 1962.8564],\n",
      "        [ 954.9106, 3451.4307, 2763.7910],\n",
      "        [ 744.4189, 3312.7876, 2299.4922],\n",
      "        [ 682.6498, 3421.7939, 2016.6781],\n",
      "        [ 828.4297, 3375.1497, 2488.1787],\n",
      "        [ 533.8762, 3403.0491, 1577.3657],\n",
      "        [ 558.3653, 3366.5256, 1675.4784],\n",
      "        [ 780.7214, 3388.6665, 2291.0154],\n",
      "        [ 466.2711, 3347.9707, 1410.7417],\n",
      "        [ 959.4540, 3391.1526, 2789.9668],\n",
      "        [ 693.7947, 3377.0989, 2024.7831],\n",
      "        [ 896.5042, 3390.0293, 2585.3616],\n",
      "        [ 887.6526, 3316.8203, 2648.4258],\n",
      "        [ 317.2155, 3311.2292,  955.0473],\n",
      "        [ 608.3593, 3278.6548, 1880.2904],\n",
      "        [ 752.3716, 3281.6318, 2321.2637],\n",
      "        [ 383.4828, 3283.7654, 1160.6932],\n",
      "        [ 704.2363, 3307.3083, 2099.1558],\n",
      "        [1022.3789, 3315.1389, 3043.6099],\n",
      "        [ 610.7674, 3248.7053, 1864.4954],\n",
      "        [ 800.3530, 3206.8198, 2515.1121],\n",
      "        [ 829.2175, 3201.9602, 2607.6848],\n",
      "        [ 382.6858, 3219.0967, 1174.8535],\n",
      "        [ 765.0200, 3202.4238, 2391.1216],\n",
      "        [ 745.1965, 3177.7136, 2368.6467],\n",
      "        [ 579.7729, 3188.8123, 1804.4877],\n",
      "        [ 845.0370, 3207.7302, 2584.0884],\n",
      "        [ 748.3517, 3142.5083, 2405.4412],\n",
      "        [ 719.7123, 3143.9158, 2305.3755],\n",
      "        [ 980.3790, 3114.7708, 3192.0349],\n",
      "        [ 610.1592, 3046.3708, 2240.9512],\n",
      "        [ 597.1306, 3129.9187, 1915.5106],\n",
      "        [ 774.4818, 3100.1704, 2530.4309],\n",
      "        [ 881.6544, 3085.4077, 2881.1155],\n",
      "        [1030.4407, 3133.2085, 3210.7017],\n",
      "        [ 458.7134, 3096.4336, 1468.3937],\n",
      "        [ 655.3274, 3087.7649, 2131.4897],\n",
      "        [ 815.1603, 3102.5386, 2621.7170],\n",
      "        [ 787.7918, 3081.3696, 2543.7473],\n",
      "        [ 675.6171, 3074.3486, 2195.7483],\n",
      "        [ 721.6348, 3017.6614, 2439.2673],\n",
      "        [ 692.9374, 3053.4980, 2269.7896],\n",
      "        [ 570.0876, 3022.2468, 1919.2778],\n",
      "        [ 592.7391, 2944.4189, 2192.6980],\n",
      "        [ 558.4824, 3033.0496, 1826.7554],\n",
      "        [ 675.3599, 2994.7332, 2263.9199],\n",
      "        [ 678.6164, 3004.8037, 2264.8169],\n",
      "        [ 869.1191, 2980.1882, 2937.7900],\n",
      "        [ 307.9129, 2973.5417, 1049.8521],\n",
      "        [ 634.8539, 2990.3318, 2111.2319],\n",
      "        [ 735.3320, 2977.6973, 2492.2717],\n",
      "        [ 557.5688, 2994.6548, 1859.4988],\n",
      "        [ 710.6584, 3001.8770, 2382.1658],\n",
      "        [ 536.5648, 3025.9336, 1779.7911],\n",
      "        [ 513.1623, 3033.7029, 1689.9476],\n",
      "        [ 627.7031, 3046.3206, 2071.4966],\n",
      "        [ 619.7662, 3086.6814, 1999.5511],\n",
      "        [ 600.8246, 3109.5649, 1905.5470],\n",
      "        [ 540.9659, 3100.1423, 1738.3093],\n",
      "        [ 636.2480, 3115.5583, 2049.9331],\n",
      "        [1021.2876, 3054.9111, 3402.6504],\n",
      "        [ 622.9144, 3129.3042, 1995.2056],\n",
      "        [ 656.0443, 3139.2576, 2099.6721],\n",
      "        [ 700.3204, 3152.6685, 2235.0847],\n",
      "        [ 451.0835, 3192.2805, 1398.5514],\n",
      "        [ 435.1462, 3207.2498, 1337.6635],\n",
      "        [ 786.7166, 3217.4600, 2436.1216],\n",
      "        [ 782.0825, 3243.1143, 2389.2241],\n",
      "        [1122.9973, 3226.8828, 3487.2280],\n",
      "        [ 708.2613, 3241.2151, 2183.1709],\n",
      "        [ 922.1344, 3242.5222, 2848.8411],\n",
      "        [ 688.4104, 3259.1504, 2108.0042],\n",
      "        [ 821.5784, 3211.2378, 2578.0835],\n",
      "        [ 946.8647, 3262.4065, 2899.3862],\n",
      "        [ 616.3610, 3286.6753, 1802.9199],\n",
      "        [ 830.1661, 3228.4058, 2584.5989],\n",
      "        [ 675.9202, 3259.2786, 2068.5776],\n",
      "        [ 733.3087, 3209.4729, 2373.0132],\n",
      "        [ 636.0283, 3283.9778, 1930.3655],\n",
      "        [ 665.7765, 3321.1379, 1938.7684],\n",
      "        [ 578.9206, 3313.7595, 1728.6924],\n",
      "        [ 596.9723, 3297.7402, 1817.9246],\n",
      "        [ 774.1865, 3290.6746, 2370.6433],\n",
      "        [ 724.1837, 3334.1465, 2167.3521],\n",
      "        [ 738.7194, 3337.0623, 2223.7510],\n",
      "        [ 530.3254, 3356.0249, 1600.5439],\n",
      "        [ 700.8922, 3391.8279, 2065.9065],\n",
      "        [ 554.9357, 3401.8147, 1628.9554],\n",
      "        [ 576.9241, 3422.2856, 1683.1293],\n",
      "        [ 720.0747, 3397.4692, 2140.3174],\n",
      "        [ 472.8362, 3449.2244, 1377.4071],\n",
      "        [ 831.7439, 3473.8118, 2402.6331],\n",
      "        [ 567.6480, 3494.2993, 1631.9126],\n",
      "        [ 607.2181, 3503.3142, 1745.6671],\n",
      "        [ 597.7328, 3520.1155, 1703.8969],\n",
      "        [ 481.9151, 3535.9783, 1366.5249],\n",
      "        [ 580.2779, 3560.1577, 1630.0143],\n",
      "        [ 401.0581, 3570.6418, 1132.2216],\n",
      "        [ 610.5302, 3594.1064, 1709.8414],\n",
      "        [ 408.3713, 3628.8088, 1125.0715],\n",
      "        [ 508.1504, 3644.8506, 1394.3691],\n",
      "        [ 376.4469, 3663.5339, 1031.5979],\n",
      "        [ 386.3193, 3708.5698, 1039.4536],\n",
      "        [ 569.4219, 3720.8245, 1537.7323],\n",
      "        [ 564.3244, 3767.5193, 1500.5521],\n",
      "        [ 402.3695, 3798.2449, 1063.4291],\n",
      "        [ 607.7131, 3830.8474, 1590.2762],\n",
      "        [ 516.0378, 3868.0300, 1338.0818],\n",
      "        [ 518.4236, 3892.7871, 1339.1368],\n",
      "        [ 469.6445, 3927.3704, 1199.7811],\n",
      "        [ 394.0808, 3972.1548,  994.9548],\n",
      "        [ 535.8366, 4018.7100, 1338.3439],\n",
      "        [ 519.4357, 4054.6580, 1287.0458],\n",
      "        [ 494.6483, 4094.8679, 1213.9916],\n",
      "        [ 421.9792, 4135.5024, 1032.8390],\n",
      "        [ 441.9445, 4192.4077, 1057.3094],\n",
      "        [ 572.9675, 4223.7866, 1351.9441],\n",
      "        [ 631.7817, 4297.1187, 1425.4722],\n",
      "        [ 387.4121, 4319.7959,  899.1797],\n",
      "        [ 737.9672, 4343.3608, 1701.6823],\n",
      "        [ 406.2931, 4419.8545,  917.9135],\n",
      "        [ 675.5278, 4466.6216, 1481.8043],\n",
      "        [ 562.7885, 4491.8848, 1250.7844],\n",
      "        [ 444.0036, 4531.8320,  982.1067],\n",
      "        [ 533.9500, 4554.8560, 1175.7627],\n",
      "        [ 538.1885, 4593.9092, 1178.4026],\n",
      "        [ 437.2989, 4639.2578,  944.2297],\n",
      "        [ 549.1260, 4666.6045, 1179.1857],\n",
      "        [ 462.9138, 4701.6758,  987.2043],\n",
      "        [ 438.3042, 4741.7183,  929.5007],\n",
      "        [ 492.7162, 4788.4800, 1032.4535],\n",
      "        [ 593.4391, 4843.1450, 1225.3821],\n",
      "        [ 527.6426, 4878.0376, 1083.2568],\n",
      "        [ 483.9147, 4902.6865,  989.8445],\n",
      "        [ 525.5664, 4984.1230, 1037.1521],\n",
      "        [ 519.0217, 4983.8491, 1045.5406],\n",
      "        [ 531.4221, 5031.4375, 1059.9224],\n",
      "        [ 554.2027, 4841.6426, 1472.2231],\n",
      "        [ 646.2565, 5102.9600, 1270.7235],\n",
      "        [ 484.9345, 4970.2822, 1051.6486],\n",
      "        [ 442.2721, 5185.7856,  854.9719]])\n",
      "data:  tensor([[[0.0082, 0.0082, 0.0082,  ..., 0.6192, 0.2658, 0.6522]],\n",
      "\n",
      "        [[0.0120, 0.0119, 0.0120,  ..., 0.6176, 0.2658, 0.6522]],\n",
      "\n",
      "        [[0.0103, 0.0094, 0.0083,  ..., 0.6157, 0.2658, 0.6522]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0075, 0.0071, 0.0074,  ..., 0.7886, 0.2700, 0.1304]],\n",
      "\n",
      "        [[0.0056, 0.0053, 0.0055,  ..., 0.7938, 0.2700, 0.1304]],\n",
      "\n",
      "        [[0.0053, 0.0058, 0.0051,  ..., 0.7987, 0.2700, 0.1304]]])\n",
      "output:  tensor([[3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856],\n",
      "        [3.1102, 8.1824, 6.9856]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 561.3310, 5255.6279, 1058.2351],\n",
      "        [ 839.4788, 5253.3657, 1611.6318],\n",
      "        [ 620.6378, 5340.9575, 1152.0354],\n",
      "        [ 592.9671, 5372.0576, 1109.9076],\n",
      "        [ 590.7797, 5410.5806, 1095.3425],\n",
      "        [ 712.0502, 5443.0103, 1311.5171],\n",
      "        [ 771.8363, 5459.5488, 1416.7594],\n",
      "        [ 583.9324, 5521.5254, 1058.9290],\n",
      "        [ 785.5812, 5537.2764, 1423.9044],\n",
      "        [ 804.9826, 5551.9771, 1453.2136],\n",
      "        [ 753.1969, 5588.4507, 1352.4563],\n",
      "        [ 490.6181, 5630.9004,  871.6685],\n",
      "        [ 610.0868, 5640.0381, 1083.9617],\n",
      "        [ 545.2425, 5640.2227,  968.5462],\n",
      "        [ 660.2959, 5653.6055, 1171.8741],\n",
      "        [ 681.8163, 5676.4722, 1205.9568],\n",
      "        [ 725.7736, 5694.1504, 1279.2117],\n",
      "        [ 670.1318, 5702.8750, 1178.9905],\n",
      "        [ 608.4705, 5721.2402, 1065.1310],\n",
      "        [ 662.5137, 5715.4312, 1163.0601],\n",
      "        [ 518.2006, 5737.6836,  905.2628],\n",
      "        [ 791.2653, 5735.0527, 1384.2031],\n",
      "        [ 671.8844, 5748.0391, 1174.1127],\n",
      "        [ 700.7092, 5738.5049, 1223.8253],\n",
      "        [ 677.0492, 5742.4204, 1182.3420],\n",
      "        [ 540.3300, 5759.5957,  939.1216],\n",
      "        [ 781.1099, 5744.4980, 1364.6544],\n",
      "        [ 614.0029, 5756.2891, 1064.8193],\n",
      "        [ 716.1951, 5736.5220, 1252.1906],\n",
      "        [ 965.3439, 5771.5039, 1632.3005],\n",
      "        [ 656.6279, 5736.7510, 1146.5944],\n",
      "        [ 669.5792, 5688.1865, 1181.8705],\n",
      "        [ 679.8356, 5680.6279, 1200.8965],\n",
      "        [ 668.6245, 5689.9946, 1172.1378],\n",
      "        [ 673.5977, 5669.2559, 1191.4884],\n",
      "        [ 752.4752, 5673.9365, 1328.1102],\n",
      "        [ 611.3553, 5637.7090, 1086.5597],\n",
      "        [ 717.0544, 5617.4614, 1280.0378],\n",
      "        [ 600.3358, 5615.4341, 1068.4958],\n",
      "        [ 766.6258, 5587.2275, 1375.5803],\n",
      "        [ 741.0589, 5558.9482, 1336.1906],\n",
      "        [ 676.0322, 5514.0967, 1229.9429],\n",
      "        [ 607.3081, 5491.4863, 1110.8540],\n",
      "        [ 796.0048, 5472.9873, 1452.8241],\n",
      "        [ 697.8167, 5434.8555, 1288.5283],\n",
      "        [ 650.7118, 5412.2173, 1200.3269],\n",
      "        [ 725.7531, 5366.1089, 1356.9071],\n",
      "        [ 598.3187, 5327.8818, 1124.3879],\n",
      "        [ 625.6194, 5282.0420, 1188.4027],\n",
      "        [ 816.5948, 5224.3545, 1575.9048],\n",
      "        [ 566.3596, 5193.7407, 1092.9565],\n",
      "        [ 643.4748, 5115.5439, 1262.7054],\n",
      "        [ 453.0799, 5094.0063,  888.5455],\n",
      "        [ 663.3550, 5019.6299, 1326.4850],\n",
      "        [ 857.6542, 4953.9946, 1740.9446],\n",
      "        [ 706.9149, 4925.2627, 1425.7863],\n",
      "        [ 773.2599, 4873.1445, 1588.0281],\n",
      "        [ 546.2658, 4797.2827, 1143.2318],\n",
      "        [ 638.0146, 4728.5532, 1357.8988],\n",
      "        [ 521.7573, 4698.0732, 1109.7483],\n",
      "        [ 796.1870, 4656.8745, 1690.8153],\n",
      "        [ 670.4235, 4612.8945, 1441.3661],\n",
      "        [ 476.4937, 4536.2554, 1055.3440],\n",
      "        [ 774.1697, 4505.7368, 1712.9373],\n",
      "        [ 574.3713, 4460.6899, 1279.7694],\n",
      "        [ 566.4851, 4397.2666, 1295.6495],\n",
      "        [ 528.0046, 4377.6167, 1202.2261],\n",
      "        [ 667.4949, 4320.1475, 1549.4342],\n",
      "        [ 720.9059, 4299.1240, 1675.1891],\n",
      "        [ 619.3796, 4252.8203, 1457.8357],\n",
      "        [ 542.8666, 4201.3608, 1293.6237],\n",
      "        [ 573.2043, 4156.6270, 1383.4751],\n",
      "        [ 624.8619, 4118.3496, 1522.5117],\n",
      "        [ 429.4837, 4103.5312, 1038.9701],\n",
      "        [ 756.1903, 4078.7261, 1850.8577],\n",
      "        [ 644.2126, 4001.1155, 1620.3270],\n",
      "        [ 786.1663, 3983.9346, 1982.2513],\n",
      "        [ 949.6075, 3958.0403, 2397.1648],\n",
      "        [ 491.5830, 3926.5493, 1250.7152],\n",
      "        [ 739.9564, 3895.6558, 1901.4993],\n",
      "        [ 661.2307, 3850.8882, 1734.3971],\n",
      "        [ 984.0601, 3893.9189, 2495.9919],\n",
      "        [ 754.7264, 3810.9294, 1989.5343],\n",
      "        [ 350.3135, 3778.6621,  930.6304],\n",
      "        [ 547.5374, 3741.8706, 1470.8513],\n",
      "        [ 670.7646, 3709.2681, 1828.2587],\n",
      "        [ 637.9847, 3693.1316, 1735.4462],\n",
      "        [ 829.2840, 3701.9902, 2213.9514],\n",
      "        [ 920.7252, 3700.3145, 2454.9915],\n",
      "        [ 673.9459, 3624.1174, 1873.9246],\n",
      "        [ 614.9558, 3614.4487, 1670.3893],\n",
      "        [ 704.9246, 3586.4207, 1972.8293],\n",
      "        [ 677.8605, 3563.1404, 1909.3630],\n",
      "        [ 865.5208, 3564.2966, 2352.5701],\n",
      "        [ 611.8332, 3510.9028, 1765.4957],\n",
      "        [ 583.3674, 3524.8352, 1639.3060],\n",
      "        [ 412.7629, 3507.8274, 1166.9055],\n",
      "        [ 947.5168, 3447.9231, 2762.4744],\n",
      "        [ 809.0101, 3467.7681, 2334.1047],\n",
      "        [ 796.4575, 3470.6113, 2275.4841],\n",
      "        [ 684.8220, 3414.7954, 1997.8551],\n",
      "        [ 526.8527, 3398.3687, 1554.9517],\n",
      "        [ 674.7703, 3376.9761, 2000.8035],\n",
      "        [ 669.0413, 3377.3447, 1969.5060],\n",
      "        [ 490.1644, 3352.0256, 1459.7644],\n",
      "        [ 698.5473, 3348.7805, 2066.0371],\n",
      "        [ 572.7996, 3270.1982, 1787.5018],\n",
      "        [ 688.7025, 3277.3142, 2126.9968],\n",
      "        [ 510.2998, 3294.6292, 1535.4642],\n",
      "        [ 607.4391, 3251.0344, 1874.2448],\n",
      "        [ 710.7134, 3240.1091, 2212.3982],\n",
      "        [ 480.7679, 3215.5945, 1516.5497],\n",
      "        [ 679.8513, 3219.0520, 2119.6892],\n",
      "        [ 483.6065, 3207.8865, 1513.5024],\n",
      "        [ 609.5668, 3141.2571, 1981.4714],\n",
      "        [ 999.4672, 3186.9800, 3116.5190],\n",
      "        [ 492.2844, 3151.4229, 1569.6511],\n",
      "        [ 618.2650, 3134.9121, 1979.3970],\n",
      "        [ 652.6137, 3107.5376, 2115.9290],\n",
      "        [ 772.8867, 3085.2849, 2524.7458],\n",
      "        [ 697.7729, 3088.8818, 2244.4612],\n",
      "        [ 827.2239, 3047.7393, 2734.5352],\n",
      "        [ 614.9102, 3037.8308, 2043.2853],\n",
      "        [ 767.9041, 3062.0046, 2555.5759],\n",
      "        [ 877.1843, 2892.2615, 3348.3259],\n",
      "        [ 767.8892, 3033.0327, 2526.0500],\n",
      "        [ 682.1477, 2985.4646, 2298.3950],\n",
      "        [ 615.4013, 3003.1616, 2021.3297],\n",
      "        [ 676.8660, 2932.0918, 2358.8188],\n",
      "        [ 529.5652, 2967.2356, 1767.2911],\n",
      "        [ 704.5898, 2908.3423, 2454.8823],\n",
      "        [ 447.6250, 2964.9792, 1481.5485],\n",
      "        [ 514.8480, 2937.7332, 1750.6246],\n",
      "        [ 609.8019, 2888.3909, 2135.1575],\n",
      "        [ 606.0624, 2876.6780, 2137.6606],\n",
      "        [ 833.9478, 2900.3047, 2878.1624],\n",
      "        [ 472.8505, 2870.1877, 1635.2188],\n",
      "        [ 593.0235, 2860.4800, 2076.8181],\n",
      "        [ 798.4551, 2859.6814, 2822.8101],\n",
      "        [ 594.5274, 2902.1479, 2001.9283],\n",
      "        [ 727.4723, 2860.6587, 2533.5903],\n",
      "        [ 600.1885, 2869.0986, 2062.6450],\n",
      "        [ 506.5150, 2859.6201, 1764.1866],\n",
      "        [ 874.5008, 2937.1633, 2938.6902],\n",
      "        [ 614.6813, 2830.4917, 2191.0601],\n",
      "        [ 588.8439, 2839.9478, 2090.0344],\n",
      "        [ 547.6487, 2849.0188, 1942.3652],\n",
      "        [ 642.3789, 2803.0391, 2343.8923],\n",
      "        [ 490.5648, 2856.9109, 1694.1659],\n",
      "        [ 566.0720, 2861.4575, 1946.2426]])\n",
      "data:  tensor([[[0.0064, 0.0063, 0.0056,  ..., 0.8051, 0.2700, 0.1304]],\n",
      "\n",
      "        [[0.0098, 0.0097, 0.0107,  ..., 0.8111, 0.2700, 0.1739]],\n",
      "\n",
      "        [[0.0072, 0.0072, 0.0076,  ..., 0.8159, 0.2700, 0.1739]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0075, 0.0076, 0.0075,  ..., 0.6370, 0.2700, 0.6957]],\n",
      "\n",
      "        [[0.0062, 0.0069, 0.0070,  ..., 0.6362, 0.2700, 0.6957]],\n",
      "\n",
      "        [[0.0069, 0.0069, 0.0068,  ..., 0.6335, 0.2700, 0.6957]]])\n",
      "output:  tensor([[3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373],\n",
      "        [3.1618, 8.2340, 7.0373]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 657.0630, 2798.8442, 2383.3330],\n",
      "        [ 419.9030, 2818.2930, 1523.1116],\n",
      "        [ 735.3051, 2809.9482, 2637.4529],\n",
      "        [ 581.5562, 2840.8413, 2048.0354],\n",
      "        [ 660.9705, 2844.2263, 2324.3337],\n",
      "        [ 713.4430, 2852.8000, 2498.5852],\n",
      "        [ 446.4130, 2835.6807, 1584.7690],\n",
      "        [ 523.1293, 2821.5884, 1871.1520],\n",
      "        [ 689.3508, 2867.4990, 2370.1875],\n",
      "        [ 494.4209, 2786.1262, 1842.6658],\n",
      "        [ 761.6318, 2821.1750, 2711.3477],\n",
      "        [ 711.4271, 2842.9138, 2496.9712],\n",
      "        [ 743.7888, 2831.4858, 2603.5266],\n",
      "        [ 475.0759, 2777.5972, 1753.1937],\n",
      "        [ 771.2535, 2772.3579, 2801.3884],\n",
      "        [ 565.0978, 2747.6140, 2096.2942],\n",
      "        [ 811.2751, 2805.3457, 2853.4968],\n",
      "        [ 507.4056, 2758.5059, 1798.3274],\n",
      "        [ 796.4324, 2733.8069, 2911.5447],\n",
      "        [ 776.5787, 2781.0767, 2735.2261],\n",
      "        [ 507.7605, 2736.3650, 1831.2389],\n",
      "        [ 951.3159, 2637.3176, 3642.4563],\n",
      "        [ 761.8260, 2715.8381, 2804.7512],\n",
      "        [ 538.1781, 2692.6194, 1994.0074],\n",
      "        [ 605.4565, 2695.8032, 2233.3760],\n",
      "        [ 627.6354, 2663.9771, 2388.8215],\n",
      "        [ 613.1552, 2644.1597, 2380.1497],\n",
      "        [ 569.8363, 2702.8018, 2099.4985],\n",
      "        [ 603.1921, 2678.8120, 2282.1931],\n",
      "        [ 663.9329, 2693.9600, 2494.9214],\n",
      "        [ 602.4849, 2725.1772, 2215.6340],\n",
      "        [ 583.4797, 2707.7896, 2164.8076],\n",
      "        [ 358.0784, 2720.9099, 1321.6191],\n",
      "        [ 688.1749, 2757.2546, 2455.0361],\n",
      "        [ 639.5657, 2713.1182, 2387.5957],\n",
      "        [ 465.5198, 2702.7124, 1747.2611],\n",
      "        [ 606.4811, 2745.7151, 2196.4231],\n",
      "        [ 703.3557, 2730.7571, 2577.0693],\n",
      "        [ 509.1119, 2735.2087, 1851.6942],\n",
      "        [ 560.4690, 2727.8359, 2017.0660],\n",
      "        [ 919.1208, 2748.6196, 3323.1704],\n",
      "        [ 642.8485, 2698.2219, 2398.3994],\n",
      "        [ 713.4332, 2776.1897, 2431.6626],\n",
      "        [ 435.2134, 2702.3438, 1609.8552],\n",
      "        [ 538.8500, 2689.7317, 2020.9475],\n",
      "        [ 528.0947, 2744.8269, 1904.7814],\n",
      "        [ 540.8292, 2679.6165, 2042.2336],\n",
      "        [ 501.0796, 2724.4456, 1821.0920],\n",
      "        [ 505.4509, 2733.2258, 1818.5740],\n",
      "        [ 639.4726, 2701.2769, 2377.1528],\n",
      "        [ 605.5166, 2689.2065, 2257.8088],\n",
      "        [ 523.2219, 2717.6033, 1907.9849],\n",
      "        [ 415.5464, 2659.5867, 1596.2814],\n",
      "        [ 474.8055, 2689.0002, 1773.9916],\n",
      "        [ 671.2803, 2693.9878, 2471.1169],\n",
      "        [ 639.0435, 2644.1987, 2448.8804],\n",
      "        [ 680.5469, 2682.4146, 2554.6782],\n",
      "        [ 693.7951, 2684.8164, 2599.3704],\n",
      "        [ 444.5031, 2665.3621, 1688.9799],\n",
      "        [ 406.7444, 2678.3596, 1532.0553],\n",
      "        [ 235.9868, 2657.3918,  904.9818],\n",
      "        [ 736.8777, 2702.4890, 2705.6160],\n",
      "        [ 503.4673, 2641.9924, 1943.1921],\n",
      "        [ 468.4113, 2680.0408, 1747.5431],\n",
      "        [ 531.4755, 2665.8369, 2013.4175],\n",
      "        [ 540.5438, 2687.3523, 1956.1184],\n",
      "        [ 407.2849, 2671.9531, 1532.5222],\n",
      "        [ 703.9592, 2672.5171, 2623.6057],\n",
      "        [ 473.7263, 2702.4832, 1727.6670],\n",
      "        [ 403.6442, 2686.8496, 1494.4662],\n",
      "        [ 402.4667, 2682.3699, 1500.6233],\n",
      "        [ 421.3186, 2692.5691, 1551.3260],\n",
      "        [ 697.0545, 2698.0486, 2581.7417],\n",
      "        [ 646.4686, 2718.9214, 2343.1624],\n",
      "        [ 541.1304, 2693.7805, 2014.7740],\n",
      "        [ 427.8641, 2697.3560, 1590.1368],\n",
      "        [ 508.6056, 2731.0142, 1840.4696],\n",
      "        [ 424.3343, 2728.3721, 1545.3793],\n",
      "        [ 398.1048, 2709.3535, 1487.4301],\n",
      "        [ 431.2286, 2700.8301, 1699.9216],\n",
      "        [ 597.2635, 2676.7847, 2255.2690],\n",
      "        [ 376.3083, 2721.9824, 1392.9655],\n",
      "        [ 449.3285, 2750.8369, 1631.6847],\n",
      "        [ 411.2492, 2756.3835, 1479.9545],\n",
      "        [ 494.9385, 2777.6025, 1758.9646],\n",
      "        [ 579.3324, 2758.9751, 2247.2478],\n",
      "        [ 440.2696, 2790.5833, 1578.6167],\n",
      "        [ 396.2368, 2808.6973, 1417.3951],\n",
      "        [ 382.4310, 2823.5266, 1349.2411],\n",
      "        [ 339.6542, 2843.8076, 1185.4163],\n",
      "        [ 446.8434, 2864.9597, 1552.8387],\n",
      "        [ 408.4850, 2883.5203, 1415.4338],\n",
      "        [ 362.6512, 2896.2776, 1240.3915],\n",
      "        [ 373.5534, 2884.8608, 1306.1525],\n",
      "        [ 337.9431, 2910.2188, 1161.7264],\n",
      "        [ 426.7120, 2953.0371, 1413.4071],\n",
      "        [ 329.1057, 2938.3140, 1122.6825],\n",
      "        [ 463.8689, 2879.1414, 1857.2164],\n",
      "        [ 461.8997, 2963.9180, 1568.5223],\n",
      "        [ 487.1497, 3003.1282, 1612.4275],\n",
      "        [ 378.6790, 3003.6531, 1269.8010],\n",
      "        [ 397.1704, 3028.6538, 1309.2657],\n",
      "        [ 340.2582, 3043.9858, 1113.6813],\n",
      "        [ 319.1264, 3081.6155, 1026.5275],\n",
      "        [ 461.5285, 3111.3579, 1442.6630],\n",
      "        [ 414.2881, 3107.2192, 1338.4681],\n",
      "        [ 398.7588, 3112.3745, 1298.0165],\n",
      "        [ 302.6253, 3152.5063,  966.0602],\n",
      "        [ 299.8304, 3180.5007,  943.1407],\n",
      "        [ 389.3478, 3220.9285, 1208.5968],\n",
      "        [ 363.3442, 3243.4548, 1125.8298],\n",
      "        [ 269.2077, 3288.8313,  817.8270],\n",
      "        [ 321.0569, 3309.0061,  975.1465],\n",
      "        [ 339.6243, 3346.8813, 1015.2859],\n",
      "        [ 392.1027, 3372.7368, 1165.6260],\n",
      "        [ 408.3936, 3405.6353, 1202.0110],\n",
      "        [ 316.3187, 3440.5222,  920.1256],\n",
      "        [ 328.2087, 3459.0715,  952.7578],\n",
      "        [ 380.1124, 3468.5669, 1104.1709],\n",
      "        [ 523.8776, 3466.6511, 1605.6154],\n",
      "        [ 396.9148, 3518.0042, 1131.3610],\n",
      "        [ 347.6194, 3541.6309,  979.9184],\n",
      "        [ 500.1277, 3557.4099, 1393.2186],\n",
      "        [ 411.6269, 3525.9021, 1197.5737],\n",
      "        [ 418.4990, 3550.9419, 1174.8679],\n",
      "        [ 329.0593, 3556.6726,  921.8768],\n",
      "        [ 485.3356, 3544.8481, 1375.3566],\n",
      "        [ 407.7518, 3575.2219, 1135.3060],\n",
      "        [ 258.2738, 3570.0889,  723.1887],\n",
      "        [ 284.6093, 3570.3066,  799.4285],\n",
      "        [ 462.7404, 3481.1733, 1446.7434],\n",
      "        [ 419.6676, 3579.2769, 1175.7115],\n",
      "        [ 322.2932, 3585.1921,  901.7186],\n",
      "        [ 253.8468, 3583.5667,  708.9885],\n",
      "        [ 459.4059, 3571.6360, 1307.5490],\n",
      "        [ 481.4863, 3588.4036, 1344.6389],\n",
      "        [ 412.3556, 3598.3123, 1148.9022],\n",
      "        [ 357.4821, 3644.2585,  965.5526],\n",
      "        [ 401.2222, 3647.4424, 1104.3717],\n",
      "        [ 558.9370, 3628.7573, 1632.3219],\n",
      "        [ 337.5981, 3684.0999,  917.7222],\n",
      "        [ 601.7578, 3672.8564, 1672.5316],\n",
      "        [ 454.0249, 3681.7764, 1235.9581],\n",
      "        [ 395.9309, 3673.1858, 1084.2494],\n",
      "        [ 445.7951, 3697.3655, 1181.5485],\n",
      "        [ 303.6403, 3677.9673,  826.1351],\n",
      "        [ 448.7991, 3630.7139, 1269.5741],\n",
      "        [ 471.8700, 3625.4243, 1283.3359],\n",
      "        [ 306.8851, 3589.7273,  853.6710],\n",
      "        [ 362.5052, 3415.6724, 1473.0986]])\n",
      "data:  tensor([[[0.0074, 0.0074, 0.0069,  ..., 0.6335, 0.2700, 0.6957]],\n",
      "\n",
      "        [[0.0051, 0.0051, 0.0057,  ..., 0.6335, 0.2700, 0.6957]],\n",
      "\n",
      "        [[0.0085, 0.0076, 0.0075,  ..., 0.6335, 0.2700, 0.6957]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0055, 0.0064, 0.0066,  ..., 0.8533, 0.2743, 0.1739]],\n",
      "\n",
      "        [[0.0036, 0.0038, 0.0036,  ..., 0.8583, 0.2743, 0.1739]],\n",
      "\n",
      "        [[0.0041, 0.0039, 0.0042,  ..., 0.8625, 0.2743, 0.1739]]])\n",
      "output:  tensor([[3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889],\n",
      "        [3.2134, 8.2856, 7.0889]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 321.9108, 3506.6824,  918.1485],\n",
      "        [ 306.6804, 3462.0823,  887.4874],\n",
      "        [ 258.1676, 3259.9043, 1200.5587],\n",
      "        [ 333.3184, 3324.0254,  997.5303],\n",
      "        [ 340.9453, 3257.0947, 1047.9712],\n",
      "        [ 257.4241, 3174.5859,  810.9527],\n",
      "        [ 315.1405, 2944.0728, 1130.1907],\n",
      "        [ 346.7906, 3025.1628, 1144.7318],\n",
      "        [ 378.2027, 2952.9983, 1242.4644],\n",
      "        [ 432.0256, 2875.0862, 1492.8440],\n",
      "        [ 181.2005, 2827.0623,  640.8647],\n",
      "        [ 226.1816, 2743.6316,  826.6473],\n",
      "        [ 404.2095, 2685.5034, 1497.3571],\n",
      "        [ 378.8265, 2637.6304, 1361.8574],\n",
      "        [ 274.3735, 2564.9070, 1072.1708],\n",
      "        [ 283.5749, 2515.1794, 1120.6821],\n",
      "        [ 289.2402, 2424.2869, 1310.3347],\n",
      "        [ 212.1324, 2313.0574,  922.4584],\n",
      "        [ 316.3829, 2332.5898, 1366.9519],\n",
      "        [ 258.7487, 2285.1914, 1146.2797],\n",
      "        [ 200.1593, 2244.4172,  890.1473],\n",
      "        [ 223.4334, 2196.1921, 1008.8362],\n",
      "        [ 152.7690, 2129.1775,  724.2991],\n",
      "        [ 379.2367, 2090.2354, 1810.5947],\n",
      "        [ 207.1819, 2081.4048,  975.4930],\n",
      "        [ 218.9909, 2040.8485, 1073.8053],\n",
      "        [ 211.4891, 2008.8771, 1054.7842],\n",
      "        [ 242.8095, 1978.8328, 1218.7196],\n",
      "        [ 224.6200, 1940.1588, 1180.8428],\n",
      "        [ 266.0533, 1949.7212, 1362.8883],\n",
      "        [ 160.9597, 1931.8811,  835.2678],\n",
      "        [ 228.3572, 1923.4807, 1159.6552],\n",
      "        [ 204.8663, 1912.3766, 1071.4340],\n",
      "        [ 194.0591, 1940.5275,  984.8439],\n",
      "        [ 141.0744, 1933.5232,  724.1675],\n",
      "        [ 153.7400, 1952.3688,  784.2215],\n",
      "        [ 119.4826, 1950.4807,  613.5734],\n",
      "        [ 221.3395, 1967.4888, 1135.0424],\n",
      "        [ 239.1838, 1972.6440, 1204.5935],\n",
      "        [ 164.5024, 1985.9265,  817.4382],\n",
      "        [ 256.1006, 1988.1997, 1268.8993],\n",
      "        [ 149.9139, 1978.4360,  751.4780],\n",
      "        [ 181.8763, 1994.5671,  897.8748],\n",
      "        [ 356.9382, 1996.8347, 1765.2972],\n",
      "        [ 295.4344, 2004.6211, 1449.4503],\n",
      "        [ 279.4174, 1976.2020, 1418.0046],\n",
      "        [ 274.5200, 2017.7860, 1334.5105],\n",
      "        [ 145.2889, 1975.4200,  744.1857],\n",
      "        [ 177.2499, 1996.3209,  885.0792],\n",
      "        [ 245.8754, 2004.6433, 1214.8744],\n",
      "        [ 138.1970, 1993.7628,  697.1226],\n",
      "        [ 232.3086, 1996.6661, 1169.1416],\n",
      "        [ 250.4072, 1994.9246, 1255.4607],\n",
      "        [ 225.4972, 2003.5933, 1130.8942],\n",
      "        [ 243.0881, 2011.7705, 1203.5588],\n",
      "        [ 339.3974, 2025.5219, 1669.2159],\n",
      "        [ 159.8316, 2012.8818,  797.0689],\n",
      "        [ 270.7843, 2032.1296, 1327.4515],\n",
      "        [ 241.0627, 2017.8252, 1202.7390],\n",
      "        [ 285.9930, 2048.7742, 1384.4996],\n",
      "        [ 278.4537, 2044.5684, 1351.2360],\n",
      "        [ 290.9445, 2050.1147, 1405.6725],\n",
      "        [ 317.1477, 2034.9111, 1554.5039],\n",
      "        [ 322.9406, 2056.9180, 1540.4529],\n",
      "        [ 223.8707, 2053.7285, 1070.8914],\n",
      "        [ 187.2842, 2017.4620,  940.4138],\n",
      "        [ 463.0684, 2074.2329, 2210.4478],\n",
      "        [ 300.2063, 2041.9656, 1469.4882],\n",
      "        [ 365.3319, 2030.8617, 1812.0406],\n",
      "        [ 319.9335, 2034.4308, 1588.6844],\n",
      "        [ 370.7011, 2019.2661, 1871.8929],\n",
      "        [ 375.1733, 2041.2170, 1853.4380],\n",
      "        [ 455.1870, 2016.3672, 2281.8108],\n",
      "        [ 539.1375, 2072.7358, 2577.9375],\n",
      "        [ 445.8754, 2061.4197, 2170.9458],\n",
      "        [ 312.0249, 2067.3459, 1503.4194],\n",
      "        [ 374.8672, 2044.6074, 1843.9318],\n",
      "        [ 281.2431, 2016.6241, 1352.4977],\n",
      "        [ 425.5951, 2076.8972, 2038.1492],\n",
      "        [ 430.7401, 2066.4355, 2076.7178],\n",
      "        [ 495.1270, 2054.2817, 2433.4294],\n",
      "        [ 401.1444, 2087.8113, 1912.4419],\n",
      "        [ 360.8333, 2068.4109, 1736.7548],\n",
      "        [ 411.7032, 2080.4998, 1948.5624],\n",
      "        [ 487.3895, 2040.5078, 2407.2849],\n",
      "        [ 482.7793, 2081.4829, 2319.1301],\n",
      "        [ 228.4544, 2068.1782, 1097.7112],\n",
      "        [ 421.8688, 2063.1345, 1990.2633],\n",
      "        [ 423.3108, 2076.6792, 2032.4006],\n",
      "        [ 455.2212, 2052.0864, 2235.9060],\n",
      "        [ 419.8435, 2063.8384, 2035.3992],\n",
      "        [ 345.0461, 2065.9329, 1668.1859],\n",
      "        [ 467.6483, 2055.6443, 2295.9441],\n",
      "        [ 410.6263, 2069.0886, 1983.4344],\n",
      "        [ 403.8772, 2075.3892, 1933.9950],\n",
      "        [ 435.3276, 2065.6479, 2092.2354],\n",
      "        [ 446.3045, 2071.2725, 2116.4492],\n",
      "        [ 471.2399, 2061.7046, 2297.8647],\n",
      "        [ 336.9964, 2077.6960, 1609.1860],\n",
      "        [ 439.6818, 2075.4170, 2121.3550],\n",
      "        [ 389.3343, 2048.4502, 1925.2543],\n",
      "        [ 373.3577, 2078.2769, 1774.8719],\n",
      "        [ 433.3731, 2051.5837, 2145.4590],\n",
      "        [ 275.5288, 2071.8479, 1311.2262],\n",
      "        [ 427.1792, 2074.3894, 2069.4937],\n",
      "        [ 543.5773, 2054.4436, 2637.8162],\n",
      "        [ 300.5285, 2086.1077, 1414.5702],\n",
      "        [ 456.5362, 2043.2670, 2285.2930],\n",
      "        [ 573.2078, 2077.1484, 2781.8652],\n",
      "        [ 421.0966, 2069.3569, 2069.6897],\n",
      "        [ 424.0597, 2082.6895, 2047.6818],\n",
      "        [ 386.9910, 2116.1743, 1802.2539],\n",
      "        [ 451.4153, 2070.6416, 2223.0029],\n",
      "        [ 451.0107, 2095.2568, 2160.2397],\n",
      "        [ 329.7932, 2112.8062, 1542.8513],\n",
      "        [ 467.3408, 2111.6667, 2223.5264],\n",
      "        [ 384.6249, 2125.1335, 1805.8660],\n",
      "        [ 313.6031, 2097.8203, 1517.8402],\n",
      "        [ 472.1576, 2108.5837, 2256.8052],\n",
      "        [ 442.6700, 2098.0830, 2156.5193],\n",
      "        [ 328.2729, 2103.9980, 1579.6855],\n",
      "        [ 371.2116, 2122.9104, 1753.3530],\n",
      "        [ 431.1588, 2098.1611, 2093.7627],\n",
      "        [ 333.1317, 2135.1707, 1553.4700],\n",
      "        [ 289.3741, 2113.8398, 1364.6887],\n",
      "        [ 359.9847, 2129.2834, 1669.2590],\n",
      "        [ 451.1187, 2097.0496, 2171.2905],\n",
      "        [ 363.5354, 2109.7791, 1715.2516],\n",
      "        [ 541.9824, 2066.5193, 2667.1045],\n",
      "        [ 402.8787, 2113.2141, 1897.4659],\n",
      "        [ 548.7623, 2133.0762, 2560.3174],\n",
      "        [ 395.4654, 2098.7700, 1899.7172],\n",
      "        [ 374.1961, 2125.6697, 1747.5200],\n",
      "        [ 505.5274, 2109.5334, 2416.5281],\n",
      "        [ 524.0759, 2101.6689, 2519.7095],\n",
      "        [ 380.1400, 2110.3989, 1804.9498],\n",
      "        [ 596.1910, 2108.7402, 2833.3035],\n",
      "        [ 200.0397, 2118.5537,  936.1813],\n",
      "        [ 466.1798, 2130.2610, 2160.9448],\n",
      "        [ 419.6703, 2118.1069, 1962.9163],\n",
      "        [ 438.9394, 2145.0181, 2011.4768],\n",
      "        [ 378.9726, 2090.6487, 1832.8721],\n",
      "        [ 535.3839, 2119.1123, 2523.4077],\n",
      "        [ 552.7670, 2094.2905, 2667.9402],\n",
      "        [ 479.2162, 2109.9019, 2279.0200],\n",
      "        [ 355.5700, 2119.1460, 1662.8716],\n",
      "        [ 308.6638, 2125.6531, 1437.0491],\n",
      "        [ 383.9703, 2094.5085, 1834.8832],\n",
      "        [ 518.4429, 2066.8320, 2538.0632],\n",
      "        [ 488.4569, 2063.1401, 2414.5476]])\n",
      "data:  tensor([[[0.0037, 0.0040, 0.0041,  ..., 0.8670, 0.2743, 0.1739]],\n",
      "\n",
      "        [[0.0036, 0.0036, 0.0032,  ..., 0.8717, 0.2743, 0.2174]],\n",
      "\n",
      "        [[0.0029, 0.0028, 0.0032,  ..., 0.8761, 0.2743, 0.2174]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0039, 0.0033, 0.0034,  ..., 0.6584, 0.2743, 0.7391]],\n",
      "\n",
      "        [[0.0061, 0.0065, 0.0060,  ..., 0.6584, 0.2743, 0.7391]],\n",
      "\n",
      "        [[0.0056, 0.0056, 0.0055,  ..., 0.6584, 0.2743, 0.7391]]])\n",
      "output:  tensor([[ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.2650,  8.3372,  7.1404],\n",
      "        [ 3.9394, 10.2015,  7.5861],\n",
      "        [ 3.9394, 10.2015,  7.5861],\n",
      "        [ 3.9394, 10.2015,  7.5861],\n",
      "        [ 3.9394, 10.2015,  7.5861],\n",
      "        [ 3.9394, 10.2015,  7.5861],\n",
      "        [ 6.0271, 12.2845,  6.0446],\n",
      "        [ 6.0271, 12.2845,  6.0446],\n",
      "        [ 6.0271, 12.2845,  6.0446],\n",
      "        [ 6.0271, 12.2845,  6.0446],\n",
      "        [ 7.3895, 12.2108,  5.6123],\n",
      "        [ 7.3895, 12.2108,  5.6123],\n",
      "        [ 8.9755, 11.5052,  9.1403],\n",
      "        [ 9.1605, 11.9141, 12.0444],\n",
      "        [ 9.1605, 11.9141, 12.0444],\n",
      "        [ 9.1605, 11.9141, 12.0444],\n",
      "        [ 8.1659, 12.6794, 11.4140],\n",
      "        [ 8.1659, 12.6794, 11.4140],\n",
      "        [ 8.3763, 14.3675,  9.4865],\n",
      "        [ 8.3763, 14.3675,  9.4865],\n",
      "        [ 8.3763, 14.3675,  9.4865],\n",
      "        [11.2318, 14.7961, 10.3797],\n",
      "        [10.8738, 13.7271, 11.2666]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 428.7388, 2085.3369, 2068.8660],\n",
      "        [ 450.5965, 2105.0872, 2114.9055],\n",
      "        [ 271.3235, 2093.6648, 1280.6177],\n",
      "        [ 283.5124, 2095.4075, 1349.3851],\n",
      "        [ 369.4833, 2087.8225, 1767.7770],\n",
      "        [ 447.6676, 2058.3254, 2198.9675],\n",
      "        [ 479.4621, 2061.8108, 2311.1335],\n",
      "        [ 442.6840, 2040.9880, 2187.7141],\n",
      "        [ 402.3824, 2061.0510, 1960.0125],\n",
      "        [ 333.1626, 2076.4561, 1585.8684],\n",
      "        [ 320.7622, 2030.7275, 1619.5024],\n",
      "        [ 523.0745, 2016.0992, 2641.2473],\n",
      "        [ 488.5477, 1952.9551, 2584.8906],\n",
      "        [ 333.5276, 2029.4763, 1637.1360],\n",
      "        [ 477.1880, 2011.0387, 2391.3757],\n",
      "        [ 481.2689, 1985.2617, 2469.9731],\n",
      "        [ 418.9262, 2008.4303, 2081.5684],\n",
      "        [ 284.3881, 2012.8317, 1417.4509],\n",
      "        [ 281.3239, 1993.7461, 1427.9641],\n",
      "        [ 466.8971, 1998.7061, 2354.5542],\n",
      "        [ 473.6377, 2007.3634, 2351.9604],\n",
      "        [ 314.1523, 1992.5005, 1611.8301],\n",
      "        [ 311.3463, 2019.7577, 1520.4604],\n",
      "        [ 324.3326, 2016.3002, 1591.3365],\n",
      "        [ 396.9037, 2013.5912, 1967.1471],\n",
      "        [ 418.0499, 1990.6460, 2116.8923],\n",
      "        [ 343.7219, 1996.1311, 1731.3674],\n",
      "        [ 520.5499, 2034.1068, 2511.3777],\n",
      "        [ 392.2961, 1982.4746, 1991.7408],\n",
      "        [ 482.4539, 1997.0751, 2399.1375],\n",
      "        [ 359.6400, 1967.0284, 1868.4373],\n",
      "        [ 503.5927, 2005.0344, 2501.7439],\n",
      "        [ 372.0915, 2003.4369, 1834.3872],\n",
      "        [ 593.2802, 1976.5540, 3025.5085],\n",
      "        [ 598.4980, 1997.0862, 3004.9026],\n",
      "        [ 340.8047, 1932.8810, 1717.6531],\n",
      "        [ 516.9404, 2000.2531, 2573.2327],\n",
      "        [ 297.1510, 1988.7916, 1488.2228],\n",
      "        [ 523.6110, 2008.3634, 2587.5425],\n",
      "        [ 334.9805, 1999.6052, 1655.4310],\n",
      "        [ 542.7762, 1984.1725, 2725.4600],\n",
      "        [ 371.3345, 1958.2279, 1924.7134],\n",
      "        [ 468.6713, 1962.3890, 2419.6030],\n",
      "        [ 322.3563, 1951.6091, 1684.9208],\n",
      "        [ 290.2761, 1960.9761, 1512.4972],\n",
      "        [ 207.5866, 1954.0052, 1103.8474],\n",
      "        [ 356.1504, 1951.9330, 1841.2853],\n",
      "        [ 426.5728, 1980.7543, 2144.7834],\n",
      "        [ 504.4868, 1996.3434, 2502.4941],\n",
      "        [ 538.9794, 1985.0271, 2685.7292],\n",
      "        [ 407.3981, 1991.8636, 2013.6803],\n",
      "        [ 345.9683, 1964.4166, 1788.2601],\n",
      "        [ 290.6348, 1913.1027, 1668.9771],\n",
      "        [ 334.3867, 1965.6174, 1695.6580],\n",
      "        [ 381.7901, 1964.2489, 1939.8296],\n",
      "        [ 396.0161, 1981.4468, 1966.6324],\n",
      "        [ 429.9818, 1976.6713, 2179.5750],\n",
      "        [ 383.0252, 1942.3818, 1988.3250],\n",
      "        [ 475.9842, 1965.1316, 2373.3760],\n",
      "        [ 196.6633, 1956.2898, 1021.2512],\n",
      "        [ 376.1086, 1916.5380, 2042.0974],\n",
      "        [ 462.1245, 1984.6027, 2280.8525],\n",
      "        [ 276.6236, 1949.2296, 1463.9309],\n",
      "        [ 308.3983, 1950.4572, 1595.6591],\n",
      "        [ 387.4465, 1980.1062, 1941.7694],\n",
      "        [ 407.6244, 1947.9895, 2145.7769],\n",
      "        [ 396.7355, 1979.8828, 1995.0375],\n",
      "        [ 335.9292, 1954.5302, 1734.1804],\n",
      "        [ 411.5802, 1902.0994, 2205.7683],\n",
      "        [ 388.1790, 1939.4551, 2027.3318],\n",
      "        [ 309.0534, 1935.6179, 1619.1536],\n",
      "        [ 395.1947, 1963.6571, 2025.1266],\n",
      "        [ 218.1082, 1969.0023, 1091.2908],\n",
      "        [ 284.6123, 1943.8508, 1474.3237],\n",
      "        [ 173.1202, 1933.4283,  908.4894],\n",
      "        [ 216.7235, 1936.3940, 1131.0958],\n",
      "        [ 173.5151, 1942.2589,  884.4347],\n",
      "        [ 297.7469, 1899.8762, 1600.8381],\n",
      "        [ 201.8251, 1905.9254, 1066.5911],\n",
      "        [ 314.8306, 1892.1962, 1666.9608],\n",
      "        [ 199.2650, 1865.6036, 1073.8206],\n",
      "        [ 259.2746, 1847.8867, 1411.6005],\n",
      "        [ 164.5996, 1833.8501,  903.4791],\n",
      "        [ 199.8646, 1802.4374, 1122.8251],\n",
      "        [ 201.8761, 1782.6871, 1141.5046],\n",
      "        [ 162.4296, 1796.8297,  892.7079],\n",
      "        [ 160.9247, 1759.7196,  925.1279],\n",
      "        [ 179.8421, 1732.6355, 1043.4982],\n",
      "        [ 191.2979, 1710.7236, 1114.4547],\n",
      "        [ 203.7681, 1699.3405, 1213.4691],\n",
      "        [1001.8053, 4237.7842, 2309.8374],\n",
      "        [ 722.7789, 5975.7427, 1220.3489],\n",
      "        [ 577.9066, 6166.0742,  937.5371],\n",
      "        [ 564.3719, 6283.8833,  895.1830],\n",
      "        [ 641.3720, 6193.3091, 1036.9576],\n",
      "        [1108.0154, 6194.2568, 1760.0961],\n",
      "        [ 718.5373, 6181.4121, 1165.0254],\n",
      "        [1122.9607, 6507.6099, 1693.8633],\n",
      "        [ 957.4045, 6826.3687, 1405.1206],\n",
      "        [ 811.7737, 6938.0396, 1184.5695],\n",
      "        [1313.8115, 7011.4771, 1827.8308],\n",
      "        [1032.0411, 6965.4136, 1477.9883],\n",
      "        [1106.4926, 6891.3945, 1608.3303],\n",
      "        [1254.2366, 6806.5454, 1854.7310],\n",
      "        [1046.4238, 6770.5083, 1550.4697],\n",
      "        [ 873.8020, 6772.2007, 1291.9515],\n",
      "        [1082.1979, 6731.0020, 1612.7047],\n",
      "        [1020.3742, 6699.5498, 1526.1571],\n",
      "        [ 962.7119, 6676.5879, 1443.6522],\n",
      "        [ 889.6534, 6755.7007, 1322.2628],\n",
      "        [ 798.7449, 6431.0278, 1247.6785],\n",
      "        [ 968.2582, 6052.4541, 1605.5079],\n",
      "        [ 878.6612, 6027.9004, 1460.0336],\n",
      "        [ 855.2287, 6012.7524, 1424.9265],\n",
      "        [ 770.0157, 5997.4033, 1287.6729],\n",
      "        [ 756.2029, 5974.6147, 1269.8552],\n",
      "        [ 851.7725, 5949.8428, 1435.3976],\n",
      "        [ 640.3690, 5938.0967, 1080.7290],\n",
      "        [ 842.0616, 5900.2158, 1430.6805],\n",
      "        [ 709.3801, 5880.2588, 1209.4580],\n",
      "        [ 730.7691, 5864.6421, 1247.9276],\n",
      "        [ 713.4418, 5854.1802, 1221.5629],\n",
      "        [ 851.6933, 5842.0654, 1461.7919],\n",
      "        [ 530.4512, 5840.3452,  909.0699],\n",
      "        [ 684.8602, 5831.9053, 1177.5701],\n",
      "        [ 750.8344, 5828.7886, 1290.8132],\n",
      "        [ 610.9213, 5822.4658, 1050.3403],\n",
      "        [ 566.1024, 5818.2178,  974.4478],\n",
      "        [ 856.8114, 5800.6099, 1481.9380],\n",
      "        [ 832.2443, 5812.3730, 1435.1234],\n",
      "        [ 707.6618, 5828.0684, 1217.6370],\n",
      "        [ 681.4151, 5841.4956, 1168.8135],\n",
      "        [ 893.3202, 5844.0874, 1533.1110],\n",
      "        [ 722.5676, 5867.9375, 1235.3599],\n",
      "        [1050.5740, 5867.4907, 1794.0677],\n",
      "        [ 708.2724, 5897.4678, 1203.8704],\n",
      "        [ 802.6302, 5897.4790, 1364.1542],\n",
      "        [ 777.0960, 5921.0220, 1315.2628],\n",
      "        [ 726.5126, 5923.1167, 1248.2574],\n",
      "        [1132.4521, 5920.5303, 1963.4801],\n",
      "        [ 607.6025, 5965.8677, 1019.5879],\n",
      "        [ 777.1772, 5993.4604, 1295.1748],\n",
      "        [ 553.3852, 6015.6626,  920.3321],\n",
      "        [ 800.0398, 6021.3540, 1330.8986],\n",
      "        [ 652.2401, 6042.5117, 1081.1577],\n",
      "        [ 628.7126, 6059.0840, 1040.1538],\n",
      "        [ 615.5563, 6067.0547, 1016.6458],\n",
      "        [ 899.9122, 6067.0044, 1486.9919],\n",
      "        [ 884.8604, 6079.4155, 1457.7114],\n",
      "        [ 824.7515, 6094.2168, 1355.1075]])\n",
      "data:  tensor([[[0.0056, 0.0056, 0.0061,  ..., 0.6584, 0.2743, 0.7391]],\n",
      "\n",
      "        [[0.0052, 0.0054, 0.0054,  ..., 0.6572, 0.2743, 0.7391]],\n",
      "\n",
      "        [[0.0032, 0.0032, 0.0034,  ..., 0.6548, 0.2743, 0.7391]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0101, 0.0091, 0.0089,  ..., 0.8683, 0.2785, 0.2174]],\n",
      "\n",
      "        [[0.0102, 0.0107, 0.0100,  ..., 0.8711, 0.2785, 0.2174]],\n",
      "\n",
      "        [[0.0101, 0.0102, 0.0110,  ..., 0.8719, 0.2785, 0.2174]]])\n",
      "Epoch 0, Iteration 80 Train Loss: 964.50\n",
      "output:  tensor([[15.6693, 18.6683, 15.1263],\n",
      "        [16.0878, 21.8735, 17.8225],\n",
      "        [16.0878, 21.8735, 17.8225],\n",
      "        [16.0878, 21.8735, 17.8225],\n",
      "        [13.6520, 21.1590, 14.6904],\n",
      "        [13.6520, 21.1590, 14.6904],\n",
      "        [13.6520, 21.1590, 14.6904],\n",
      "        [13.6520, 21.1590, 14.6904],\n",
      "        [14.9080, 19.7677, 13.3567],\n",
      "        [14.9080, 19.7677, 13.3567],\n",
      "        [14.9080, 19.7677, 13.3567],\n",
      "        [16.1969, 21.4144, 13.7468],\n",
      "        [14.9080, 19.7677, 13.3567],\n",
      "        [13.6520, 21.1590, 14.6904],\n",
      "        [16.1969, 21.4144, 13.7468],\n",
      "        [16.1969, 21.4144, 13.7468],\n",
      "        [14.9080, 19.7677, 13.3567],\n",
      "        [12.8307, 15.9371, 11.8022],\n",
      "        [12.8307, 15.9371, 11.8022],\n",
      "        [12.8307, 15.9371, 11.8022],\n",
      "        [12.8307, 15.9371, 11.8022],\n",
      "        [12.8307, 15.9371, 11.8022],\n",
      "        [12.8307, 15.9371, 11.8022],\n",
      "        [12.8307, 15.9371, 11.8022],\n",
      "        [10.7700, 18.5410, 14.5834],\n",
      "        [10.7700, 18.5410, 14.5834],\n",
      "        [12.8307, 15.9371, 11.8022],\n",
      "        [10.7700, 18.5410, 14.5834],\n",
      "        [ 9.6883, 14.2783, 11.0796],\n",
      "        [ 9.6883, 14.2783, 11.0796],\n",
      "        [ 9.6883, 14.2783, 11.0796],\n",
      "        [ 9.6883, 14.2783, 11.0796],\n",
      "        [ 6.1307, 12.3881,  6.1482],\n",
      "        [ 6.1307, 12.3881,  6.1482],\n",
      "        [ 5.5902, 11.8985,  6.2992],\n",
      "        [ 5.5902, 11.8985,  6.2992],\n",
      "        [ 4.7219, 11.2471,  7.0335],\n",
      "        [ 4.7219, 11.2471,  7.0335],\n",
      "        [ 4.7219, 11.2471,  7.0335],\n",
      "        [ 4.7219, 11.2471,  7.0335],\n",
      "        [ 4.7219, 11.2471,  7.0335],\n",
      "        [ 4.7219, 11.2471,  7.0335],\n",
      "        [ 4.7219, 11.2471,  7.0335],\n",
      "        [ 4.7219, 11.2471,  7.0335],\n",
      "        [ 4.7219, 11.2471,  7.0335],\n",
      "        [ 5.0366,  9.7151,  5.8685],\n",
      "        [ 5.0366,  9.7151,  5.8685],\n",
      "        [ 5.0366,  9.7151,  5.8685],\n",
      "        [ 3.7216,  9.3153,  7.5883],\n",
      "        [ 5.0366,  9.7151,  5.8685],\n",
      "        [ 5.0366,  9.7151,  5.8685],\n",
      "        [ 5.0366,  9.7151,  5.8685],\n",
      "        [ 3.7216,  9.3153,  7.5883],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920],\n",
      "        [ 3.3165,  8.3887,  7.1920]], grad_fn=<SelectBackward0>)\n",
      "targets:  tensor([[ 870.8713, 6097.0039, 1431.9402],\n",
      "        [ 780.5919, 6100.8188, 1282.8175],\n",
      "        [ 773.2942, 6108.7837, 1268.9979],\n",
      "        [ 874.1660, 6092.8315, 1439.7211],\n",
      "        [ 800.4200, 6083.0068, 1318.4525],\n",
      "        [ 769.0826, 6070.8247, 1270.9530],\n",
      "        [ 932.2275, 6046.1592, 1544.4575],\n",
      "        [ 764.5869, 6062.1787, 1264.8450],\n",
      "        [ 918.7010, 6042.5225, 1524.8855],\n",
      "        [ 684.3623, 6041.1489, 1134.2559],\n",
      "        [ 763.9141, 6020.6841, 1271.5488],\n",
      "        [ 825.9315, 5998.6602, 1380.9059],\n",
      "        [ 821.4487, 5972.1738, 1381.3672],\n",
      "        [ 749.0876, 5955.4790, 1260.1689],\n",
      "        [ 704.9746, 5927.5962, 1191.4930],\n",
      "        [ 784.4026, 5895.3452, 1333.0326],\n",
      "        [ 967.7411, 5858.2129, 1657.4393],\n",
      "        [ 897.1102, 5854.9233, 1521.0522],\n",
      "        [ 832.8208, 5832.1514, 1420.2837],\n",
      "        [ 633.0226, 5781.6416, 1097.3605],\n",
      "        [ 786.1513, 5747.3022, 1371.8616],\n",
      "        [ 934.5889, 5700.8140, 1658.2939],\n",
      "        [ 859.9073, 5694.9043, 1505.5250],\n",
      "        [1052.5837, 5665.3350, 1839.6866],\n",
      "        [ 682.6298, 5613.2612, 1217.4873],\n",
      "        [ 604.9055, 5571.6382, 1089.3168],\n",
      "        [ 965.0319, 5519.8940, 1752.5516],\n",
      "        [ 899.6945, 5477.9863, 1647.8995],\n",
      "        [ 783.3862, 5444.5234, 1441.7778],\n",
      "        [ 541.3046, 5413.6133, 1001.8824],\n",
      "        [ 755.2105, 5375.8613, 1406.5529],\n",
      "        [ 785.3181, 5317.7104, 1481.9087],\n",
      "        [ 560.8230, 5273.1162, 1063.4006],\n",
      "        [ 766.9699, 5212.3516, 1476.6094],\n",
      "        [ 700.2582, 5163.3223, 1358.2201],\n",
      "        [ 727.8074, 5122.7769, 1422.1504],\n",
      "        [ 501.8775, 5053.3550,  996.8691],\n",
      "        [ 682.8370, 4991.3672, 1371.1206],\n",
      "        [ 729.0350, 4947.0576, 1476.7249],\n",
      "        [ 962.4626, 4894.8096, 1974.4524],\n",
      "        [ 565.9192, 4857.0083, 1165.8220],\n",
      "        [ 567.0723, 4812.7603, 1176.7870],\n",
      "        [ 899.1984, 4745.9126, 1903.4009],\n",
      "        [ 635.4655, 4713.4443, 1350.4392],\n",
      "        [ 675.0179, 4677.4180, 1443.9464],\n",
      "        [ 736.4968, 4630.8184, 1595.3158],\n",
      "        [ 853.8874, 4569.0259, 1878.1864],\n",
      "        [ 607.5873, 4548.2700, 1337.3519],\n",
      "        [ 805.2092, 4489.7676, 1804.0576],\n",
      "        [ 579.7179, 4492.7559, 1289.5568],\n",
      "        [1282.8505, 4418.4185, 2889.8992],\n",
      "        [1121.2262, 4412.9731, 2523.7141],\n",
      "        [ 981.8488, 4353.0347, 2246.5222],\n",
      "        [ 872.5104, 4300.8218, 2042.5991],\n",
      "        [ 539.3760, 4320.3540, 1241.2469],\n",
      "        [ 886.0698, 4263.2705, 2114.0498],\n",
      "        [ 920.6996, 4284.9424, 2126.4111],\n",
      "        [ 778.3910, 4198.7524, 1854.8162],\n",
      "        [ 746.5555, 4172.3330, 1795.5806],\n",
      "        [ 782.0415, 4111.6807, 1916.9740],\n",
      "        [ 898.5886, 4147.5674, 2151.8989],\n",
      "        [ 939.1000, 4077.2793, 2313.0383],\n",
      "        [ 949.6130, 4035.7681, 2372.5862],\n",
      "        [ 722.2377, 4015.4368, 1809.1616],\n",
      "        [ 596.4802, 4034.4609, 1461.0216],\n",
      "        [ 859.3877, 3946.2771, 2202.4719],\n",
      "        [1132.2997, 3916.5513, 2918.1772],\n",
      "        [ 749.3061, 3903.6152, 1941.6691],\n",
      "        [ 846.7139, 3886.1216, 2195.3889],\n",
      "        [ 778.9163, 3919.6790, 1974.1312],\n",
      "        [ 702.0659, 3840.3872, 1837.6340],\n",
      "        [ 913.8417, 3867.4604, 2341.8799],\n",
      "        [ 722.3984, 3776.9475, 1942.5079],\n",
      "        [ 848.3730, 3753.7622, 2275.2458],\n",
      "        [ 908.3196, 3764.7822, 2416.9958],\n",
      "        [ 897.3370, 3698.5049, 2467.6055],\n",
      "        [ 764.0706, 3724.6619, 2067.9707],\n",
      "        [ 758.3639, 3714.2615, 2040.2570],\n",
      "        [1175.3630, 3662.3445, 3351.7378],\n",
      "        [ 578.0557, 3652.3127, 1606.0577],\n",
      "        [ 798.0963, 3671.2644, 2174.7607],\n",
      "        [ 730.1963, 3639.4272, 2008.9755],\n",
      "        [ 834.0027, 3612.5217, 2307.7549],\n",
      "        [ 574.1017, 3617.2693, 1602.0416],\n",
      "        [ 812.1754, 3676.0176, 2192.8892],\n",
      "        [ 603.0981, 3650.1680, 1652.6061],\n",
      "        [ 839.8920, 3601.0156, 2369.0007],\n",
      "        [ 822.5696, 3625.2344, 2282.9573],\n",
      "        [ 776.7796, 3626.3628, 2117.7502],\n",
      "        [ 562.5154, 3612.0974, 1548.1213],\n",
      "        [ 803.1389, 3556.1140, 2282.0754],\n",
      "        [ 712.9221, 3555.8291, 2007.3949],\n",
      "        [ 635.1112, 3534.0010, 1808.3204],\n",
      "        [ 776.0565, 3511.7261, 2266.1018],\n",
      "        [ 876.9269, 3504.1465, 2506.5149],\n",
      "        [ 803.2538, 3476.0237, 2322.1631],\n",
      "        [ 822.5433, 3462.4565, 2397.7061],\n",
      "        [ 693.0711, 3466.7852, 1994.0011],\n",
      "        [ 817.9920, 3437.9082, 2372.2629],\n",
      "        [ 863.8041, 3415.6946, 2532.4829],\n",
      "        [ 823.0744, 3402.2000, 2420.0000],\n",
      "        [ 649.4565, 3380.3721, 1935.8280],\n",
      "        [ 593.1678, 3341.6667, 1796.0789],\n",
      "        [ 743.7585, 3368.9050, 2209.4250],\n",
      "        [ 651.4944, 3356.8625, 1941.0842],\n",
      "        [ 481.7163, 3347.9539, 1436.9789],\n",
      "        [ 604.6039, 3299.4160, 1865.3207],\n",
      "        [ 677.6812, 3303.2029, 2059.3022],\n",
      "        [ 842.4716, 3266.0203, 2610.1624],\n",
      "        [ 707.5690, 3255.4302, 2201.5305],\n",
      "        [ 747.6240, 3247.5212, 2332.2405],\n",
      "        [ 564.4540, 3264.4841, 1733.6674],\n",
      "        [ 917.6943, 3236.6575, 2850.6389],\n",
      "        [ 628.4056, 3197.7544, 1988.3896],\n",
      "        [ 698.2658, 3180.2998, 2221.5171],\n",
      "        [ 583.5513, 3190.9233, 1836.6145],\n",
      "        [ 612.9045, 3167.1292, 1946.7419],\n",
      "        [ 598.1229, 3169.0115, 1899.0480],\n",
      "        [ 760.1419, 3163.8784, 2389.2615],\n",
      "        [ 890.1100, 3098.6455, 2909.9321],\n",
      "        [1198.0126, 3033.9822, 4017.8965],\n",
      "        [ 557.4457, 3113.3242, 1796.4584],\n",
      "        [ 660.7188, 2938.1968, 2532.5203],\n",
      "        [ 663.7581, 3061.7422, 2198.6003],\n",
      "        [ 609.8066, 3067.3220, 2002.8119],\n",
      "        [ 830.1790, 3034.9597, 2757.7812],\n",
      "        [ 713.8196, 3026.6206, 2361.8396],\n",
      "        [1091.4290, 3018.1921, 3636.1318],\n",
      "        [ 749.9484, 3016.7900, 2522.5330],\n",
      "        [ 830.0791, 2996.2300, 2802.0918],\n",
      "        [ 658.7006, 3022.2695, 2149.3281],\n",
      "        [ 890.1310, 2996.0957, 2978.8000],\n",
      "        [ 523.4871, 2977.1890, 1778.7594],\n",
      "        [ 627.7170, 2968.7158, 2130.3435],\n",
      "        [ 766.1562, 2995.6519, 2543.2224],\n",
      "        [ 530.4543, 2961.5664, 1802.3678],\n",
      "        [ 727.3189, 2928.1985, 2492.5364],\n",
      "        [ 570.0266, 2927.2827, 1965.1691],\n",
      "        [ 650.9792, 2919.5022, 2239.2910],\n",
      "        [ 828.3806, 2879.0742, 2920.0781],\n",
      "        [ 886.0419, 2904.4771, 3085.6892],\n",
      "        [ 679.8497, 2911.6155, 2314.8833],\n",
      "        [ 834.6113, 2883.8330, 2918.9150],\n",
      "        [ 650.9092, 2811.5625, 2339.0845],\n",
      "        [ 630.2476, 2865.3787, 2181.8127],\n",
      "        [ 564.8372, 2898.8245, 1925.2360],\n",
      "        [ 697.5948, 2860.7593, 2459.7744],\n",
      "        [ 835.1469, 2829.4976, 2988.4402],\n",
      "        [ 604.0822, 2861.5303, 2116.5752],\n",
      "        [ 676.3470, 2870.9529, 2358.6914]])\n",
      "data:  tensor([[[0.0101, 0.0095, 0.0087,  ..., 0.8754, 0.2785, 0.2174]],\n",
      "\n",
      "        [[0.0091, 0.0090, 0.0097,  ..., 0.8754, 0.2785, 0.2609]],\n",
      "\n",
      "        [[0.0091, 0.0100, 0.0097,  ..., 0.8784, 0.2785, 0.2609]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0097, 0.0098, 0.0104,  ..., 0.6085, 0.2785, 0.7826]],\n",
      "\n",
      "        [[0.0078, 0.0078, 0.0077,  ..., 0.6085, 0.2785, 0.7826]],\n",
      "\n",
      "        [[0.0071, 0.0073, 0.0075,  ..., 0.6085, 0.2785, 0.7826]]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[196], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# run forward pass\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m _, _, _, mem \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput: \u001b[39m\u001b[38;5;124m\"\u001b[39m, mem[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[18], line 48\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m mem_rec \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_steps):\n\u001b[0;32m---> 48\u001b[0m     spk1, syn1, mem1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslstm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msyn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     spk2, mem2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(spk1), mem2)\n\u001b[1;32m     50\u001b[0m     spk3, mem3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(spk2), mem3)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/snntorch/_neurons/slstm.py:243\u001b[0m, in \u001b[0;36mSLSTM.forward\u001b[0;34m(self, input_, syn, mem)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(correct_shape, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem_reset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem)\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msyn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_quant:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msyn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_quant(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msyn)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/snntorch/_neurons/slstm.py:281\u001b[0m, in \u001b[0;36mSLSTM._base_int\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_base_int\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_):\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_state_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/snntorch/_neurons/slstm.py:259\u001b[0m, in \u001b[0;36mSLSTM._base_state_function\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_base_state_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_):\n\u001b[0;32m--> 259\u001b[0m     base_fn_mem, base_fn_syn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msyn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base_fn_syn, base_fn_mem\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1705\u001b[0m, in \u001b[0;36mLSTMCell.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1703\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched \u001b[38;5;28;01melse\u001b[39;00m hx\n\u001b[0;32m-> 1705\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_cell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   1715\u001b[0m     ret \u001b[38;5;241m=\u001b[39m (ret[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), ret[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "power_mape = []\n",
    "voltage_mape = []\n",
    "current_mape = []\n",
    "\n",
    "E_actual_list = []\n",
    "E_pred_list = []\n",
    "\n",
    "max_act_list = []\n",
    "pred_act_list = []\n",
    "succ_act_list = []\n",
    "\n",
    "pred_act_naive_list = []\n",
    "false_act_naive_list = []\n",
    "succ_act_naive_list = []\n",
    "\n",
    "#Set parameters\n",
    "batchsize_list = [300, 150, 50, 20, 8]\n",
    "time_frame_list = ['3min', '5min', '15min', '30min', '60min']\n",
    "time_frame_seconds_list = [180, 300, 900, 1800, 3600]\n",
    "n = 0\n",
    "\n",
    "for j in range(len(batchsize_list)):\n",
    "    n += 1\n",
    "    if n == 2: #Select which timescales to train for\n",
    "        # #Normalize Data\n",
    "        X_normalized = ((X - X.min()) / (X.max() - X.min()))\n",
    "        # Split into training and testing sets (70% training, 30% testing)\n",
    "        X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
    "        y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
    "\n",
    "        # Split the training set into teacher and student subsets (50/50)\n",
    "        X_train_teacher, X_train_student = train_test_split(X_train, test_size=0.5, shuffle=False)\n",
    "        y_train_teacher, y_train_student = train_test_split(y_train, test_size=0.5, shuffle=False)\n",
    "\n",
    "        # Split the testing set into validation and final test sets (50/50)\n",
    "        X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "        y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "        # #Split train and test sets\n",
    "        # X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
    "        # y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
    "\n",
    "        # X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "        # y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "        batchsize = batchsize_list[j]\n",
    "        time_frame = time_frame_list[j]\n",
    "        time_frame_seconds = time_frame_seconds_list[j]\n",
    "\n",
    "        print(time_frame)\n",
    "\n",
    "        #Resample data\n",
    "        X_train_teacher = X_train_teacher.resample(time_frame).mean().dropna()\n",
    "        X_valid = X_valid.resample(time_frame).mean().dropna()\n",
    "        X_test = X_test.resample(time_frame).mean().dropna()\n",
    "\n",
    "        y_train_teacher = y_train_teacher.resample(time_frame).mean().dropna()\n",
    "        y_valid = y_valid.resample(time_frame).mean().dropna()\n",
    "        y_test = y_test.resample(time_frame).mean().dropna()\n",
    "\n",
    "        #Reshape data\n",
    "        X_train_teacher = X_train_teacher.values.reshape((X_train_teacher.shape[0], 1, X_train_teacher.shape[1]))\n",
    "        X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
    "        X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "        # convert to tensor\n",
    "        X_train_teacher = torch.tensor(X_train_teacher)\n",
    "        y_train_teacher = torch.tensor(y_train_teacher.values)\n",
    "        X_test = torch.tensor(X_test)\n",
    "        y_test = torch.tensor(y_test.values)\n",
    "\n",
    "        # make datasets\n",
    "        train_teacher_dataset = TensorDataset(X_train_teacher, y_train_teacher)\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "        # Create DataLoaders\n",
    "        train_loader = DataLoader(train_teacher_dataset, batch_size=batchsize, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
    "\n",
    "        # Define the number of time steps for the spiking\n",
    "        num_steps = 50\n",
    "        num_inputs = X_train_teacher.shape[2]\n",
    "\n",
    "        # create new inctance of the SNN Class\n",
    "        model = Net(num_inputs, num_steps).to(device)\n",
    "\n",
    "        # define optimizer\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "\n",
    "        # define loss function\n",
    "        def quantile_loss(y_true, y_pred, quantile=0.5):\n",
    "            error = y_true - y_pred\n",
    "            loss = torch.mean(torch.max(quantile * error, (quantile - 1) * error))\n",
    "            return loss\n",
    "        loss_fn = quantile_loss\n",
    "        #loss_fn = torch.nn.MSELoss()\n",
    "        #loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "        # initialize histories\n",
    "        loss_hist = []\n",
    "        avg_loss_hist = []\n",
    "        acc_hist = []\n",
    "        mape_hist = []\n",
    "        num_epochs = 10\n",
    "\n",
    "        # put model into train mode\n",
    "        print(y_train_teacher)\n",
    "        model.train()\n",
    "\n",
    "        # Train Loop\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (data, targets) in enumerate(iter(train_loader)):\n",
    "                # move to device\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # change to floats\n",
    "                data = data.float()\n",
    "                targets = targets.float()\n",
    "\n",
    "                # run forward pass\n",
    "                _, _, _, mem = model(data)\n",
    "                # calculate loss\n",
    "                print(\"output: \", mem[-1])\n",
    "                print(\"targets: \", targets)\n",
    "                print(\"data: \", data)\n",
    "                loss_val = loss_fn(mem[-1], targets)\n",
    "\n",
    "\n",
    "\n",
    "                # calculate and store MAPE Loss\n",
    "                mem_numpy = mem.cpu().detach().numpy()\n",
    "                #mem_numpy = mem.detach().numpy()\n",
    "                targets_numpy = targets.cpu().detach().numpy()\n",
    "                #targets_numpy = targets.detach().numpy()\n",
    "                mape_hist.append(MAPE(mem_numpy[-1], targets_numpy))\n",
    "                power_mape.append(MAPE(mem_numpy[-1][:,0], targets_numpy[:,0]))\n",
    "                voltage_mape.append(MAPE(mem_numpy[-1][:,1], targets_numpy[:,1]))\n",
    "                current_mape.append(MAPE(mem_numpy[-1][:,2], targets_numpy[:,2]))\n",
    "\n",
    "                # Gradient calculation + weight update\n",
    "                optimizer.zero_grad()\n",
    "                loss_val.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Store loss history for future plotting\n",
    "                loss_hist.append(loss_val.item())\n",
    "\n",
    "                if i%10 == 0:\n",
    "                    print(f\"Epoch {epoch}, Iteration {i} Train Loss: {loss_val.item():.2f}\")\n",
    "                if len(loss_hist) > 100:\n",
    "                    avg_loss_hist.append(sum(loss_hist[-100:])/len(loss_hist[-100:]))\n",
    "                else:\n",
    "                    avg_loss_hist.append(0)\n",
    "\n",
    "            if len(loss_hist) > 100:\n",
    "                print(f'New Epoch! Avg loss for the last 100 iterations: {avg_loss_hist[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "tdiFDL0f6avz"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Save model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m :optimizer\u001b[38;5;241m.\u001b[39mstate_dict()}\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(checkpoint, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnn_5min_quant50.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnn_5min_quant50.pth \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/trained_models_folder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#Save model\n",
    "checkpoint = {'state_dict': model.state_dict(),'optimizer' :optimizer.state_dict()}\n",
    "torch.save(checkpoint, 'snn_5min_quant50.pth')\n",
    "%mv snn_5min_quant50.pth '/content/trained_models_folder'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cObaSZAexrX-"
   },
   "source": [
    "###  2.2 TSRV on Type 1 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v4vqZtr8uEaI",
    "outputId": "4c11e54e-7f03-472e-bf9f-8f2f48290f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sensorID', 'raw_VWC', 'temp', 'EC', 'I1L_valid', 'I2L_valid',\n",
      "       'I1H [nA]', 'Current (uA)', 'Voltage (mV)', 'V2 [10nV]', 'I2H [nA]',\n",
      "       'I2L [10pA]', 'tsd', 'hour', 'Power (uW)', 'power - 1h', 'power - 2h',\n",
      "       'power - 3h', 'EC - 1h', 'EC - 2h', 'EC - 3h', 'temp - 1h', 'temp - 2h',\n",
      "       'temp - 3h', 'raw_VWC - 1h', 'raw_VWC - 2h', 'raw_VWC - 3h', 'V1 - 1h',\n",
      "       'V1 - 2h', 'V1 - 3h', 'I1L - 1h', 'I1L - 2h', 'I1L - 3h', 'I1H - 1h',\n",
      "       'I1H - 2h', 'I1H - 3h'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "yGI1Hcg3ti48",
    "outputId": "2259451e-b5b0-43a0-d8d3-ff18b9726e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0 Train Loss: 22023.83\n",
      "Epoch 0, Iteration 10 Train Loss: 2211.05\n",
      "Epoch 0, Iteration 20 Train Loss: 1743.63\n",
      "Epoch 0, Iteration 30 Train Loss: 1186.39\n",
      "Epoch 0, Iteration 40 Train Loss: 1397.68\n",
      "Epoch 0, Iteration 50 Train Loss: 1696.61\n",
      "Epoch 0, Iteration 60 Train Loss: 1203.95\n",
      "Epoch 0, Iteration 70 Train Loss: 2920.93\n",
      "Epoch 0, Iteration 80 Train Loss: 1765.85\n",
      "Epoch 0, Iteration 90 Train Loss: 2137.57\n",
      "Epoch 1, Iteration 0 Train Loss: 21849.47\n",
      "Epoch 1, Iteration 10 Train Loss: 1908.69\n",
      "Epoch 1, Iteration 20 Train Loss: 1313.47\n",
      "Epoch 1, Iteration 30 Train Loss: 790.27\n",
      "Epoch 1, Iteration 40 Train Loss: 948.51\n",
      "Epoch 1, Iteration 50 Train Loss: 1170.54\n",
      "Epoch 1, Iteration 60 Train Loss: 748.62\n",
      "Epoch 1, Iteration 70 Train Loss: 2351.21\n",
      "Epoch 1, Iteration 80 Train Loss: 1193.84\n",
      "Epoch 1, Iteration 90 Train Loss: 1607.48\n",
      "New Epoch! Avg loss for the last 100 iterations: 2057.9776861572263\n",
      "Epoch 2, Iteration 0 Train Loss: 21298.33\n",
      "Epoch 2, Iteration 10 Train Loss: 1501.35\n",
      "Epoch 2, Iteration 20 Train Loss: 998.89\n",
      "Epoch 2, Iteration 30 Train Loss: 526.90\n",
      "Epoch 2, Iteration 40 Train Loss: 694.05\n",
      "Epoch 2, Iteration 50 Train Loss: 906.56\n",
      "Epoch 2, Iteration 60 Train Loss: 565.22\n",
      "Epoch 2, Iteration 70 Train Loss: 2069.83\n",
      "Epoch 2, Iteration 80 Train Loss: 941.65\n",
      "Epoch 2, Iteration 90 Train Loss: 1412.79\n",
      "New Epoch! Avg loss for the last 100 iterations: 1753.4434071350097\n",
      "Epoch 3, Iteration 0 Train Loss: 21021.34\n",
      "Epoch 3, Iteration 10 Train Loss: 1254.03\n",
      "Epoch 3, Iteration 20 Train Loss: 776.43\n",
      "Epoch 3, Iteration 30 Train Loss: 341.61\n",
      "Epoch 3, Iteration 40 Train Loss: 504.38\n",
      "Epoch 3, Iteration 50 Train Loss: 694.78\n",
      "Epoch 3, Iteration 60 Train Loss: 433.95\n",
      "Epoch 3, Iteration 70 Train Loss: 1866.21\n",
      "Epoch 3, Iteration 80 Train Loss: 742.02\n",
      "Epoch 3, Iteration 90 Train Loss: 1294.74\n",
      "New Epoch! Avg loss for the last 100 iterations: 1553.691488494873\n",
      "Epoch 4, Iteration 0 Train Loss: 20792.76\n",
      "Epoch 4, Iteration 10 Train Loss: 1045.38\n",
      "Epoch 4, Iteration 20 Train Loss: 587.03\n",
      "Epoch 4, Iteration 30 Train Loss: 211.01\n",
      "Epoch 4, Iteration 40 Train Loss: 374.11\n",
      "Epoch 4, Iteration 50 Train Loss: 512.57\n",
      "Epoch 4, Iteration 60 Train Loss: 345.38\n",
      "Epoch 4, Iteration 70 Train Loss: 1709.49\n",
      "Epoch 4, Iteration 80 Train Loss: 566.82\n",
      "Epoch 4, Iteration 90 Train Loss: 1202.57\n",
      "New Epoch! Avg loss for the last 100 iterations: 1398.1909908294679\n",
      "Epoch 5, Iteration 0 Train Loss: 20594.45\n",
      "Epoch 5, Iteration 10 Train Loss: 869.26\n",
      "Epoch 5, Iteration 20 Train Loss: 418.14\n",
      "Epoch 5, Iteration 30 Train Loss: 94.61\n",
      "Epoch 5, Iteration 40 Train Loss: 262.53\n",
      "Epoch 5, Iteration 50 Train Loss: 350.10\n",
      "Epoch 5, Iteration 60 Train Loss: 285.69\n",
      "Epoch 5, Iteration 70 Train Loss: 1589.61\n",
      "Epoch 5, Iteration 80 Train Loss: 411.78\n",
      "Epoch 5, Iteration 90 Train Loss: 1119.70\n",
      "New Epoch! Avg loss for the last 100 iterations: 1271.6241143798827\n",
      "Epoch 6, Iteration 0 Train Loss: 20420.51\n",
      "Epoch 6, Iteration 10 Train Loss: 736.21\n",
      "Epoch 6, Iteration 20 Train Loss: 262.49\n",
      "Epoch 6, Iteration 30 Train Loss: 46.19\n",
      "Epoch 6, Iteration 40 Train Loss: 180.54\n",
      "Epoch 6, Iteration 50 Train Loss: 204.07\n",
      "Epoch 6, Iteration 60 Train Loss: 242.43\n",
      "Epoch 6, Iteration 70 Train Loss: 1492.71\n",
      "Epoch 6, Iteration 80 Train Loss: 292.46\n",
      "Epoch 6, Iteration 90 Train Loss: 1046.36\n",
      "New Epoch! Avg loss for the last 100 iterations: 1174.1021392440796\n",
      "Epoch 7, Iteration 0 Train Loss: 20268.17\n",
      "Epoch 7, Iteration 10 Train Loss: 649.82\n",
      "Epoch 7, Iteration 20 Train Loss: 133.17\n",
      "Epoch 7, Iteration 30 Train Loss: 33.32\n",
      "Epoch 7, Iteration 40 Train Loss: 111.85\n",
      "Epoch 7, Iteration 50 Train Loss: 108.75\n",
      "Epoch 7, Iteration 60 Train Loss: 203.07\n",
      "Epoch 7, Iteration 70 Train Loss: 1413.04\n",
      "Epoch 7, Iteration 80 Train Loss: 212.59\n",
      "Epoch 7, Iteration 90 Train Loss: 981.11\n",
      "New Epoch! Avg loss for the last 100 iterations: 1106.282715549469\n",
      "Epoch 8, Iteration 0 Train Loss: 20145.85\n",
      "Epoch 8, Iteration 10 Train Loss: 583.18\n",
      "Epoch 8, Iteration 20 Train Loss: 59.40\n",
      "Epoch 8, Iteration 30 Train Loss: 38.59\n",
      "Epoch 8, Iteration 40 Train Loss: 59.53\n",
      "Epoch 8, Iteration 50 Train Loss: 61.00\n",
      "Epoch 8, Iteration 60 Train Loss: 166.90\n",
      "Epoch 8, Iteration 70 Train Loss: 1344.81\n",
      "Epoch 8, Iteration 80 Train Loss: 146.49\n",
      "Epoch 8, Iteration 90 Train Loss: 917.49\n",
      "New Epoch! Avg loss for the last 100 iterations: 1057.8584557723998\n",
      "Epoch 9, Iteration 0 Train Loss: 20040.42\n",
      "Epoch 9, Iteration 10 Train Loss: 526.99\n",
      "Epoch 9, Iteration 20 Train Loss: 29.11\n",
      "Epoch 9, Iteration 30 Train Loss: 44.20\n",
      "Epoch 9, Iteration 40 Train Loss: 34.05\n",
      "Epoch 9, Iteration 50 Train Loss: 41.97\n",
      "Epoch 9, Iteration 60 Train Loss: 139.37\n",
      "Epoch 9, Iteration 70 Train Loss: 1282.16\n",
      "Epoch 9, Iteration 80 Train Loss: 90.22\n",
      "Epoch 9, Iteration 90 Train Loss: 861.07\n",
      "New Epoch! Avg loss for the last 100 iterations: 1020.9884093093872\n",
      "Epoch 10, Iteration 0 Train Loss: 19943.04\n",
      "Epoch 10, Iteration 10 Train Loss: 482.48\n",
      "Epoch 10, Iteration 20 Train Loss: 20.62\n",
      "Epoch 10, Iteration 30 Train Loss: 49.11\n",
      "Epoch 10, Iteration 40 Train Loss: 37.84\n",
      "Epoch 10, Iteration 50 Train Loss: 36.49\n",
      "Epoch 10, Iteration 60 Train Loss: 114.90\n",
      "Epoch 10, Iteration 70 Train Loss: 1227.74\n",
      "Epoch 10, Iteration 80 Train Loss: 55.55\n",
      "Epoch 10, Iteration 90 Train Loss: 817.92\n",
      "New Epoch! Avg loss for the last 100 iterations: 992.6474810028076\n",
      "Epoch 11, Iteration 0 Train Loss: 19858.87\n",
      "Epoch 11, Iteration 10 Train Loss: 444.78\n",
      "Epoch 11, Iteration 20 Train Loss: 23.74\n",
      "Epoch 11, Iteration 30 Train Loss: 53.31\n",
      "Epoch 11, Iteration 40 Train Loss: 41.95\n",
      "Epoch 11, Iteration 50 Train Loss: 35.47\n",
      "Epoch 11, Iteration 60 Train Loss: 99.76\n",
      "Epoch 11, Iteration 70 Train Loss: 1181.99\n",
      "Epoch 11, Iteration 80 Train Loss: 32.24\n",
      "Epoch 11, Iteration 90 Train Loss: 785.69\n",
      "New Epoch! Avg loss for the last 100 iterations: 971.3683575820922\n",
      "Epoch 12, Iteration 0 Train Loss: 19785.56\n",
      "Epoch 12, Iteration 10 Train Loss: 414.83\n",
      "Epoch 12, Iteration 20 Train Loss: 27.55\n",
      "Epoch 12, Iteration 30 Train Loss: 57.08\n",
      "Epoch 12, Iteration 40 Train Loss: 45.67\n",
      "Epoch 12, Iteration 50 Train Loss: 37.30\n",
      "Epoch 12, Iteration 60 Train Loss: 86.14\n",
      "Epoch 12, Iteration 70 Train Loss: 1139.70\n",
      "Epoch 12, Iteration 80 Train Loss: 23.08\n",
      "Epoch 12, Iteration 90 Train Loss: 756.79\n",
      "New Epoch! Avg loss for the last 100 iterations: 954.7050536537171\n",
      "Epoch 13, Iteration 0 Train Loss: 19719.60\n",
      "Epoch 13, Iteration 10 Train Loss: 388.56\n",
      "Epoch 13, Iteration 20 Train Loss: 30.97\n",
      "Epoch 13, Iteration 30 Train Loss: 60.42\n",
      "Epoch 13, Iteration 40 Train Loss: 48.96\n",
      "Epoch 13, Iteration 50 Train Loss: 39.12\n",
      "Epoch 13, Iteration 60 Train Loss: 74.81\n",
      "Epoch 13, Iteration 70 Train Loss: 1102.47\n",
      "Epoch 13, Iteration 80 Train Loss: 26.22\n",
      "Epoch 13, Iteration 90 Train Loss: 730.67\n",
      "New Epoch! Avg loss for the last 100 iterations: 941.2894047164917\n",
      "Epoch 14, Iteration 0 Train Loss: 19659.57\n",
      "Epoch 14, Iteration 10 Train Loss: 364.61\n",
      "Epoch 14, Iteration 20 Train Loss: 34.12\n",
      "Epoch 14, Iteration 30 Train Loss: 63.52\n",
      "Epoch 14, Iteration 40 Train Loss: 52.04\n",
      "Epoch 14, Iteration 50 Train Loss: 40.95\n",
      "Epoch 14, Iteration 60 Train Loss: 72.69\n",
      "Epoch 14, Iteration 70 Train Loss: 1066.47\n",
      "Epoch 14, Iteration 80 Train Loss: 29.26\n",
      "Epoch 14, Iteration 90 Train Loss: 709.35\n",
      "New Epoch! Avg loss for the last 100 iterations: 929.584207725525\n",
      "Epoch 15, Iteration 0 Train Loss: 19601.89\n",
      "Epoch 15, Iteration 10 Train Loss: 341.91\n",
      "Epoch 15, Iteration 20 Train Loss: 37.16\n",
      "Epoch 15, Iteration 30 Train Loss: 66.54\n",
      "Epoch 15, Iteration 40 Train Loss: 55.05\n",
      "Epoch 15, Iteration 50 Train Loss: 42.78\n",
      "Epoch 15, Iteration 60 Train Loss: 70.86\n",
      "Epoch 15, Iteration 70 Train Loss: 1031.28\n",
      "Epoch 15, Iteration 80 Train Loss: 32.17\n",
      "Epoch 15, Iteration 90 Train Loss: 688.77\n",
      "New Epoch! Avg loss for the last 100 iterations: 918.8475792121887\n",
      "Epoch 16, Iteration 0 Train Loss: 19547.32\n",
      "Epoch 16, Iteration 10 Train Loss: 321.04\n",
      "Epoch 16, Iteration 20 Train Loss: 40.04\n",
      "Epoch 16, Iteration 30 Train Loss: 69.36\n",
      "Epoch 16, Iteration 40 Train Loss: 57.84\n",
      "Epoch 16, Iteration 50 Train Loss: 44.55\n",
      "Epoch 16, Iteration 60 Train Loss: 69.09\n",
      "Epoch 16, Iteration 70 Train Loss: 997.74\n",
      "Epoch 16, Iteration 80 Train Loss: 34.93\n",
      "Epoch 16, Iteration 90 Train Loss: 668.91\n",
      "New Epoch! Avg loss for the last 100 iterations: 909.3845602989197\n",
      "Epoch 17, Iteration 0 Train Loss: 19495.35\n",
      "Epoch 17, Iteration 10 Train Loss: 301.22\n",
      "Epoch 17, Iteration 20 Train Loss: 42.77\n",
      "Epoch 17, Iteration 30 Train Loss: 72.07\n",
      "Epoch 17, Iteration 40 Train Loss: 60.54\n",
      "Epoch 17, Iteration 50 Train Loss: 46.28\n",
      "Epoch 17, Iteration 60 Train Loss: 69.49\n",
      "Epoch 17, Iteration 70 Train Loss: 966.01\n",
      "Epoch 17, Iteration 80 Train Loss: 37.57\n",
      "Epoch 17, Iteration 90 Train Loss: 650.39\n",
      "New Epoch! Avg loss for the last 100 iterations: 901.3115556144714\n",
      "Epoch 18, Iteration 0 Train Loss: 19445.61\n",
      "Epoch 18, Iteration 10 Train Loss: 282.50\n",
      "Epoch 18, Iteration 20 Train Loss: 45.38\n",
      "Epoch 18, Iteration 30 Train Loss: 74.69\n",
      "Epoch 18, Iteration 40 Train Loss: 63.15\n",
      "Epoch 18, Iteration 50 Train Loss: 47.90\n",
      "Epoch 18, Iteration 60 Train Loss: 72.09\n",
      "Epoch 18, Iteration 70 Train Loss: 935.87\n",
      "Epoch 18, Iteration 80 Train Loss: 40.17\n",
      "Epoch 18, Iteration 90 Train Loss: 632.42\n",
      "New Epoch! Avg loss for the last 100 iterations: 894.2223109436035\n",
      "Epoch 19, Iteration 0 Train Loss: 19396.19\n",
      "Epoch 19, Iteration 10 Train Loss: 264.24\n",
      "Epoch 19, Iteration 20 Train Loss: 47.99\n",
      "Epoch 19, Iteration 30 Train Loss: 77.27\n",
      "Epoch 19, Iteration 40 Train Loss: 65.72\n",
      "Epoch 19, Iteration 50 Train Loss: 49.57\n",
      "Epoch 19, Iteration 60 Train Loss: 74.66\n",
      "Epoch 19, Iteration 70 Train Loss: 905.64\n",
      "Epoch 19, Iteration 80 Train Loss: 42.70\n",
      "Epoch 19, Iteration 90 Train Loss: 614.60\n",
      "New Epoch! Avg loss for the last 100 iterations: 887.5212768745422\n",
      "Epoch 20, Iteration 0 Train Loss: 19348.38\n",
      "Epoch 20, Iteration 10 Train Loss: 250.07\n",
      "Epoch 20, Iteration 20 Train Loss: 50.51\n",
      "Epoch 20, Iteration 30 Train Loss: 79.79\n",
      "Epoch 20, Iteration 40 Train Loss: 68.23\n",
      "Epoch 20, Iteration 50 Train Loss: 51.88\n",
      "Epoch 20, Iteration 60 Train Loss: 77.15\n",
      "Epoch 20, Iteration 70 Train Loss: 875.70\n",
      "Epoch 20, Iteration 80 Train Loss: 45.19\n",
      "Epoch 20, Iteration 90 Train Loss: 596.79\n",
      "New Epoch! Avg loss for the last 100 iterations: 881.185779838562\n",
      "Epoch 21, Iteration 0 Train Loss: 19301.15\n",
      "Epoch 21, Iteration 10 Train Loss: 235.95\n",
      "Epoch 21, Iteration 20 Train Loss: 53.04\n",
      "Epoch 21, Iteration 30 Train Loss: 82.30\n",
      "Epoch 21, Iteration 40 Train Loss: 70.74\n",
      "Epoch 21, Iteration 50 Train Loss: 54.38\n",
      "Epoch 21, Iteration 60 Train Loss: 79.65\n",
      "Epoch 21, Iteration 70 Train Loss: 845.63\n",
      "Epoch 21, Iteration 80 Train Loss: 47.67\n",
      "Epoch 21, Iteration 90 Train Loss: 579.13\n",
      "New Epoch! Avg loss for the last 100 iterations: 875.0239909553528\n",
      "Epoch 22, Iteration 0 Train Loss: 19254.38\n",
      "Epoch 22, Iteration 10 Train Loss: 222.04\n",
      "Epoch 22, Iteration 20 Train Loss: 55.51\n",
      "Epoch 22, Iteration 30 Train Loss: 84.76\n",
      "Epoch 22, Iteration 40 Train Loss: 73.19\n",
      "Epoch 22, Iteration 50 Train Loss: 56.83\n",
      "Epoch 22, Iteration 60 Train Loss: 82.09\n",
      "Epoch 22, Iteration 70 Train Loss: 816.08\n",
      "Epoch 22, Iteration 80 Train Loss: 50.11\n",
      "Epoch 22, Iteration 90 Train Loss: 561.47\n",
      "New Epoch! Avg loss for the last 100 iterations: 869.1401818084717\n",
      "Epoch 23, Iteration 0 Train Loss: 19208.21\n",
      "Epoch 23, Iteration 10 Train Loss: 208.52\n",
      "Epoch 23, Iteration 20 Train Loss: 57.93\n",
      "Epoch 23, Iteration 30 Train Loss: 87.16\n",
      "Epoch 23, Iteration 40 Train Loss: 75.58\n",
      "Epoch 23, Iteration 50 Train Loss: 59.21\n",
      "Epoch 23, Iteration 60 Train Loss: 84.46\n",
      "Epoch 23, Iteration 70 Train Loss: 788.82\n",
      "Epoch 23, Iteration 80 Train Loss: 52.46\n",
      "Epoch 23, Iteration 90 Train Loss: 543.98\n",
      "New Epoch! Avg loss for the last 100 iterations: 863.4273038864136\n",
      "Epoch 24, Iteration 0 Train Loss: 19163.70\n",
      "Epoch 24, Iteration 10 Train Loss: 196.16\n",
      "Epoch 24, Iteration 20 Train Loss: 60.30\n",
      "Epoch 24, Iteration 30 Train Loss: 89.51\n",
      "Epoch 24, Iteration 40 Train Loss: 77.93\n",
      "Epoch 24, Iteration 50 Train Loss: 61.55\n",
      "Epoch 24, Iteration 60 Train Loss: 86.78\n",
      "Epoch 24, Iteration 70 Train Loss: 763.69\n",
      "Epoch 24, Iteration 80 Train Loss: 54.76\n",
      "Epoch 24, Iteration 90 Train Loss: 526.65\n",
      "New Epoch! Avg loss for the last 100 iterations: 858.1060397720337\n",
      "Discontinuity\n",
      "2021-09-24 04:17:21-07:00 2021-08-17 06:37:13-07:00\n",
      "0.04926536 0.0012214300000000001\n",
      "Oracle activations: 34073\n",
      "It's over 9000! 2021-08-17 06:00:00-07:00 2021-09-24 04:00:00-07:00\n",
      "Runtime total_E activations: 90546.45301288954\n",
      "Runtime total_E_pred activations: 217609.9672111894\n",
      "It's over 9000! 2021-08-17 06:00:00-07:00 2021-09-24 04:00:00-07:00\n",
      "Naive total_E activations: 33131.984647044796\n",
      "Naive total_E_pred activations: 39451.51160935354\n",
      "Dataset, train set, and test set size: 1476 738 369\n",
      "Timeframe: 60min\n",
      "Minimal Application\n",
      "Naive vs. DL succesful activations: 0.6196012210170813\n",
      "Maximum possible activations: 34073\n",
      "Predicted activations: 66326\n",
      "Successful activations: 9540, 14.383%\n",
      "Failed activations: 55448, 83.599%\n",
      "Missed activations: 24533, 72.001%\n",
      "Naive predicted activations (usual actual energy average): 46478\n",
      "Naive successful activations (usual actual energy average): 15397, 33.128%\n",
      "Naive failed activations (usual actual energy average): 29769, 64.050%\n",
      "Naive missed activations (usual actual energy average): 18676, 54.812%\n",
      "Voltage overestimation rate: 100.000%\n",
      "Test MAPE power: 2.027503\n",
      "Test MAPE voltage: 0.628806\n",
      "Test MAPE current: 0.450791\n",
      "Epoch 0, Iteration 0 Train Loss: 22023.80\n",
      "Epoch 0, Iteration 10 Train Loss: 2212.84\n",
      "Epoch 0, Iteration 20 Train Loss: 1747.41\n",
      "Epoch 0, Iteration 30 Train Loss: 1192.65\n",
      "Epoch 0, Iteration 40 Train Loss: 1407.05\n",
      "Epoch 0, Iteration 50 Train Loss: 1710.03\n",
      "Epoch 0, Iteration 60 Train Loss: 1223.64\n",
      "Epoch 0, Iteration 70 Train Loss: 2950.82\n",
      "Epoch 0, Iteration 80 Train Loss: 1814.60\n",
      "Epoch 0, Iteration 90 Train Loss: 2223.56\n",
      "Epoch 0, Iteration 100 Train Loss: 1405.54\n",
      "Epoch 0, Iteration 110 Train Loss: 1731.01\n",
      "Epoch 0, Iteration 120 Train Loss: 1808.59\n",
      "Epoch 0, Iteration 130 Train Loss: 1544.61\n",
      "Epoch 0, Iteration 140 Train Loss: 1297.70\n",
      "Epoch 0, Iteration 150 Train Loss: 1610.35\n",
      "Epoch 0, Iteration 160 Train Loss: 1687.54\n",
      "Epoch 0, Iteration 170 Train Loss: 1113.21\n",
      "Epoch 0, Iteration 180 Train Loss: 1200.09\n",
      "New Epoch! Avg loss for the last 100 iterations: 1503.9342529296875\n",
      "Epoch 1, Iteration 0 Train Loss: 21267.39\n",
      "Epoch 1, Iteration 10 Train Loss: 1474.68\n",
      "Epoch 1, Iteration 20 Train Loss: 969.37\n",
      "Epoch 1, Iteration 30 Train Loss: 488.85\n",
      "Epoch 1, Iteration 40 Train Loss: 649.53\n",
      "Epoch 1, Iteration 50 Train Loss: 856.47\n",
      "Epoch 1, Iteration 60 Train Loss: 519.55\n",
      "Epoch 1, Iteration 70 Train Loss: 2004.89\n",
      "Epoch 1, Iteration 80 Train Loss: 883.62\n",
      "Epoch 1, Iteration 90 Train Loss: 1362.54\n",
      "Epoch 1, Iteration 100 Train Loss: 553.02\n",
      "Epoch 1, Iteration 110 Train Loss: 806.29\n",
      "Epoch 1, Iteration 120 Train Loss: 1000.45\n",
      "Epoch 1, Iteration 130 Train Loss: 861.22\n",
      "Epoch 1, Iteration 140 Train Loss: 719.23\n",
      "Epoch 1, Iteration 150 Train Loss: 1022.17\n",
      "Epoch 1, Iteration 160 Train Loss: 1083.70\n",
      "Epoch 1, Iteration 170 Train Loss: 586.83\n",
      "Epoch 1, Iteration 180 Train Loss: 685.11\n",
      "New Epoch! Avg loss for the last 100 iterations: 814.9352656555176\n",
      "Epoch 2, Iteration 0 Train Loss: 20693.27\n",
      "Epoch 2, Iteration 10 Train Loss: 945.34\n",
      "Epoch 2, Iteration 20 Train Loss: 481.77\n",
      "Epoch 2, Iteration 30 Train Loss: 129.99\n",
      "Epoch 2, Iteration 40 Train Loss: 292.35\n",
      "Epoch 2, Iteration 50 Train Loss: 403.39\n",
      "Epoch 2, Iteration 60 Train Loss: 296.28\n",
      "Epoch 2, Iteration 70 Train Loss: 1621.90\n",
      "Epoch 2, Iteration 80 Train Loss: 453.70\n",
      "Epoch 2, Iteration 90 Train Loss: 1131.22\n",
      "Epoch 2, Iteration 100 Train Loss: 242.35\n",
      "Epoch 2, Iteration 110 Train Loss: 392.27\n",
      "Epoch 2, Iteration 120 Train Loss: 734.44\n",
      "Epoch 2, Iteration 130 Train Loss: 538.85\n",
      "Epoch 2, Iteration 140 Train Loss: 324.08\n",
      "Epoch 2, Iteration 150 Train Loss: 707.84\n",
      "Epoch 2, Iteration 160 Train Loss: 697.41\n",
      "Epoch 2, Iteration 170 Train Loss: 236.71\n",
      "Epoch 2, Iteration 180 Train Loss: 467.67\n",
      "New Epoch! Avg loss for the last 100 iterations: 493.77716423034667\n",
      "Epoch 3, Iteration 0 Train Loss: 20300.80\n",
      "Epoch 3, Iteration 10 Train Loss: 664.70\n",
      "Epoch 3, Iteration 20 Train Loss: 127.35\n",
      "Epoch 3, Iteration 30 Train Loss: 33.34\n",
      "Epoch 3, Iteration 40 Train Loss: 96.76\n",
      "Epoch 3, Iteration 50 Train Loss: 102.83\n",
      "Epoch 3, Iteration 60 Train Loss: 192.05\n",
      "Epoch 3, Iteration 70 Train Loss: 1400.72\n",
      "Epoch 3, Iteration 80 Train Loss: 196.19\n",
      "Epoch 3, Iteration 90 Train Loss: 962.86\n",
      "Epoch 3, Iteration 100 Train Loss: 105.27\n",
      "Epoch 3, Iteration 110 Train Loss: 113.40\n",
      "Epoch 3, Iteration 120 Train Loss: 557.62\n",
      "Epoch 3, Iteration 130 Train Loss: 371.60\n",
      "Epoch 3, Iteration 140 Train Loss: 84.24\n",
      "Epoch 3, Iteration 150 Train Loss: 538.65\n",
      "Epoch 3, Iteration 160 Train Loss: 478.49\n",
      "Epoch 3, Iteration 170 Train Loss: 134.90\n",
      "Epoch 3, Iteration 180 Train Loss: 369.03\n",
      "New Epoch! Avg loss for the last 100 iterations: 327.701100358963\n",
      "Epoch 4, Iteration 0 Train Loss: 20046.31\n",
      "Epoch 4, Iteration 10 Train Loss: 523.71\n",
      "Epoch 4, Iteration 20 Train Loss: 21.41\n",
      "Epoch 4, Iteration 30 Train Loss: 43.94\n",
      "Epoch 4, Iteration 40 Train Loss: 32.73\n",
      "Epoch 4, Iteration 50 Train Loss: 37.14\n",
      "Epoch 4, Iteration 60 Train Loss: 125.16\n",
      "Epoch 4, Iteration 70 Train Loss: 1256.49\n",
      "Epoch 4, Iteration 80 Train Loss: 70.16\n",
      "Epoch 4, Iteration 90 Train Loss: 835.07\n",
      "Epoch 4, Iteration 100 Train Loss: 48.15\n",
      "Epoch 4, Iteration 110 Train Loss: 36.01\n",
      "Epoch 4, Iteration 120 Train Loss: 432.98\n",
      "Epoch 4, Iteration 130 Train Loss: 261.48\n",
      "Epoch 4, Iteration 140 Train Loss: 20.98\n",
      "Epoch 4, Iteration 150 Train Loss: 414.95\n",
      "Epoch 4, Iteration 160 Train Loss: 342.69\n",
      "Epoch 4, Iteration 170 Train Loss: 97.18\n",
      "Epoch 4, Iteration 180 Train Loss: 295.72\n",
      "New Epoch! Avg loss for the last 100 iterations: 250.16662691116332\n",
      "Epoch 5, Iteration 0 Train Loss: 19881.63\n",
      "Epoch 5, Iteration 10 Train Loss: 446.84\n",
      "Epoch 5, Iteration 20 Train Loss: 22.76\n",
      "Epoch 5, Iteration 30 Train Loss: 52.42\n",
      "Epoch 5, Iteration 40 Train Loss: 41.05\n",
      "Epoch 5, Iteration 50 Train Loss: 33.67\n",
      "Epoch 5, Iteration 60 Train Loss: 85.75\n",
      "Epoch 5, Iteration 70 Train Loss: 1147.56\n",
      "Epoch 5, Iteration 80 Train Loss: 19.93\n",
      "Epoch 5, Iteration 90 Train Loss: 759.69\n",
      "Epoch 5, Iteration 100 Train Loss: 38.38\n",
      "Epoch 5, Iteration 110 Train Loss: 29.97\n",
      "Epoch 5, Iteration 120 Train Loss: 342.86\n",
      "Epoch 5, Iteration 130 Train Loss: 198.99\n",
      "Epoch 5, Iteration 140 Train Loss: 20.56\n",
      "Epoch 5, Iteration 150 Train Loss: 322.49\n",
      "Epoch 5, Iteration 160 Train Loss: 250.78\n",
      "Epoch 5, Iteration 170 Train Loss: 78.35\n",
      "Epoch 5, Iteration 180 Train Loss: 239.62\n",
      "New Epoch! Avg loss for the last 100 iterations: 209.29172178268433\n",
      "Epoch 6, Iteration 0 Train Loss: 19762.63\n",
      "Epoch 6, Iteration 10 Train Loss: 393.56\n",
      "Epoch 6, Iteration 20 Train Loss: 29.10\n",
      "Epoch 6, Iteration 30 Train Loss: 58.63\n",
      "Epoch 6, Iteration 40 Train Loss: 47.18\n",
      "Epoch 6, Iteration 50 Train Loss: 37.14\n",
      "Epoch 6, Iteration 60 Train Loss: 67.99\n",
      "Epoch 6, Iteration 70 Train Loss: 1062.45\n",
      "Epoch 6, Iteration 80 Train Loss: 24.53\n",
      "Epoch 6, Iteration 90 Train Loss: 704.64\n",
      "Epoch 6, Iteration 100 Train Loss: 44.19\n",
      "Epoch 6, Iteration 110 Train Loss: 30.81\n",
      "Epoch 6, Iteration 120 Train Loss: 284.89\n",
      "Epoch 6, Iteration 130 Train Loss: 157.03\n",
      "Epoch 6, Iteration 140 Train Loss: 26.15\n",
      "Epoch 6, Iteration 150 Train Loss: 250.86\n",
      "Epoch 6, Iteration 160 Train Loss: 180.15\n",
      "Epoch 6, Iteration 170 Train Loss: 70.44\n",
      "Epoch 6, Iteration 180 Train Loss: 199.37\n",
      "New Epoch! Avg loss for the last 100 iterations: 184.1804153060913\n",
      "Epoch 7, Iteration 0 Train Loss: 19660.81\n",
      "Epoch 7, Iteration 10 Train Loss: 350.15\n",
      "Epoch 7, Iteration 20 Train Loss: 34.27\n",
      "Epoch 7, Iteration 30 Train Loss: 63.75\n",
      "Epoch 7, Iteration 40 Train Loss: 52.27\n",
      "Epoch 7, Iteration 50 Train Loss: 40.33\n",
      "Epoch 7, Iteration 60 Train Loss: 63.46\n",
      "Epoch 7, Iteration 70 Train Loss: 991.04\n",
      "Epoch 7, Iteration 80 Train Loss: 29.50\n",
      "Epoch 7, Iteration 90 Train Loss: 662.36\n",
      "Epoch 7, Iteration 100 Train Loss: 49.06\n",
      "Epoch 7, Iteration 110 Train Loss: 32.83\n",
      "Epoch 7, Iteration 120 Train Loss: 243.49\n",
      "Epoch 7, Iteration 130 Train Loss: 125.56\n",
      "Epoch 7, Iteration 140 Train Loss: 30.92\n",
      "Epoch 7, Iteration 150 Train Loss: 207.80\n",
      "Epoch 7, Iteration 160 Train Loss: 131.63\n",
      "Epoch 7, Iteration 170 Train Loss: 67.69\n",
      "Epoch 7, Iteration 180 Train Loss: 167.99\n",
      "New Epoch! Avg loss for the last 100 iterations: 167.39197404861451\n",
      "Epoch 8, Iteration 0 Train Loss: 19574.72\n",
      "Epoch 8, Iteration 10 Train Loss: 312.95\n",
      "Epoch 8, Iteration 20 Train Loss: 38.84\n",
      "Epoch 8, Iteration 30 Train Loss: 68.29\n",
      "Epoch 8, Iteration 40 Train Loss: 56.78\n",
      "Epoch 8, Iteration 50 Train Loss: 43.52\n",
      "Epoch 8, Iteration 60 Train Loss: 65.78\n",
      "Epoch 8, Iteration 70 Train Loss: 925.64\n",
      "Epoch 8, Iteration 80 Train Loss: 33.88\n",
      "Epoch 8, Iteration 90 Train Loss: 623.40\n",
      "Epoch 8, Iteration 100 Train Loss: 53.39\n",
      "Epoch 8, Iteration 110 Train Loss: 36.02\n",
      "Epoch 8, Iteration 120 Train Loss: 206.62\n",
      "Epoch 8, Iteration 130 Train Loss: 104.10\n",
      "Epoch 8, Iteration 140 Train Loss: 35.18\n",
      "Epoch 8, Iteration 150 Train Loss: 172.30\n",
      "Epoch 8, Iteration 160 Train Loss: 96.08\n",
      "Epoch 8, Iteration 170 Train Loss: 67.46\n",
      "Epoch 8, Iteration 180 Train Loss: 138.93\n",
      "New Epoch! Avg loss for the last 100 iterations: 154.40458992004395\n",
      "Epoch 9, Iteration 0 Train Loss: 19496.45\n",
      "Epoch 9, Iteration 10 Train Loss: 283.98\n",
      "Epoch 9, Iteration 20 Train Loss: 43.04\n",
      "Epoch 9, Iteration 30 Train Loss: 72.46\n",
      "Epoch 9, Iteration 40 Train Loss: 60.94\n",
      "Epoch 9, Iteration 50 Train Loss: 46.46\n",
      "Epoch 9, Iteration 60 Train Loss: 69.90\n",
      "Epoch 9, Iteration 70 Train Loss: 863.53\n",
      "Epoch 9, Iteration 80 Train Loss: 38.00\n",
      "Epoch 9, Iteration 90 Train Loss: 586.06\n",
      "Epoch 9, Iteration 100 Train Loss: 57.48\n",
      "Epoch 9, Iteration 110 Train Loss: 38.97\n",
      "Epoch 9, Iteration 120 Train Loss: 177.78\n",
      "Epoch 9, Iteration 130 Train Loss: 83.44\n",
      "Epoch 9, Iteration 140 Train Loss: 39.09\n",
      "Epoch 9, Iteration 150 Train Loss: 143.25\n",
      "Epoch 9, Iteration 160 Train Loss: 69.49\n",
      "Epoch 9, Iteration 170 Train Loss: 68.58\n",
      "Epoch 9, Iteration 180 Train Loss: 111.81\n",
      "New Epoch! Avg loss for the last 100 iterations: 143.49215589523317\n",
      "Epoch 10, Iteration 0 Train Loss: 19427.19\n",
      "Epoch 10, Iteration 10 Train Loss: 260.24\n",
      "Epoch 10, Iteration 20 Train Loss: 46.91\n",
      "Epoch 10, Iteration 30 Train Loss: 76.37\n",
      "Epoch 10, Iteration 40 Train Loss: 64.84\n",
      "Epoch 10, Iteration 50 Train Loss: 49.34\n",
      "Epoch 10, Iteration 60 Train Loss: 73.77\n",
      "Epoch 10, Iteration 70 Train Loss: 801.12\n",
      "Epoch 10, Iteration 80 Train Loss: 41.87\n",
      "Epoch 10, Iteration 90 Train Loss: 548.23\n",
      "Epoch 10, Iteration 100 Train Loss: 61.32\n",
      "Epoch 10, Iteration 110 Train Loss: 41.94\n",
      "Epoch 10, Iteration 120 Train Loss: 148.27\n",
      "Epoch 10, Iteration 130 Train Loss: 67.07\n",
      "Epoch 10, Iteration 140 Train Loss: 42.91\n",
      "Epoch 10, Iteration 150 Train Loss: 114.45\n",
      "Epoch 10, Iteration 160 Train Loss: 49.25\n",
      "Epoch 10, Iteration 170 Train Loss: 69.31\n",
      "Epoch 10, Iteration 180 Train Loss: 91.47\n",
      "New Epoch! Avg loss for the last 100 iterations: 133.8959818649292\n",
      "Epoch 11, Iteration 0 Train Loss: 19356.89\n",
      "Epoch 11, Iteration 10 Train Loss: 236.92\n",
      "Epoch 11, Iteration 20 Train Loss: 50.60\n",
      "Epoch 11, Iteration 30 Train Loss: 80.02\n",
      "Epoch 11, Iteration 40 Train Loss: 68.47\n",
      "Epoch 11, Iteration 50 Train Loss: 52.35\n",
      "Epoch 11, Iteration 60 Train Loss: 77.38\n",
      "Epoch 11, Iteration 70 Train Loss: 748.58\n",
      "Epoch 11, Iteration 80 Train Loss: 45.46\n",
      "Epoch 11, Iteration 90 Train Loss: 512.63\n",
      "Epoch 11, Iteration 100 Train Loss: 64.87\n",
      "Epoch 11, Iteration 110 Train Loss: 45.43\n",
      "Epoch 11, Iteration 120 Train Loss: 126.38\n",
      "Epoch 11, Iteration 130 Train Loss: 55.08\n",
      "Epoch 11, Iteration 140 Train Loss: 46.37\n",
      "Epoch 11, Iteration 150 Train Loss: 92.97\n",
      "Epoch 11, Iteration 160 Train Loss: 37.31\n",
      "Epoch 11, Iteration 170 Train Loss: 71.56\n",
      "Epoch 11, Iteration 180 Train Loss: 77.80\n",
      "New Epoch! Avg loss for the last 100 iterations: 126.0976382637024\n",
      "Epoch 12, Iteration 0 Train Loss: 19292.86\n",
      "Epoch 12, Iteration 10 Train Loss: 215.21\n",
      "Epoch 12, Iteration 20 Train Loss: 54.03\n",
      "Epoch 12, Iteration 30 Train Loss: 83.46\n",
      "Epoch 12, Iteration 40 Train Loss: 71.91\n",
      "Epoch 12, Iteration 50 Train Loss: 55.50\n",
      "Epoch 12, Iteration 60 Train Loss: 80.78\n",
      "Epoch 12, Iteration 70 Train Loss: 698.94\n",
      "Epoch 12, Iteration 80 Train Loss: 48.86\n",
      "Epoch 12, Iteration 90 Train Loss: 477.91\n",
      "Epoch 12, Iteration 100 Train Loss: 68.25\n",
      "Epoch 12, Iteration 110 Train Loss: 48.78\n",
      "Epoch 12, Iteration 120 Train Loss: 107.10\n",
      "Epoch 12, Iteration 130 Train Loss: 47.29\n",
      "Epoch 12, Iteration 140 Train Loss: 49.61\n",
      "Epoch 12, Iteration 150 Train Loss: 74.08\n",
      "Epoch 12, Iteration 160 Train Loss: 30.01\n",
      "Epoch 12, Iteration 170 Train Loss: 74.19\n",
      "Epoch 12, Iteration 180 Train Loss: 66.30\n",
      "New Epoch! Avg loss for the last 100 iterations: 119.54981201171876\n",
      "Epoch 13, Iteration 0 Train Loss: 19234.42\n",
      "Epoch 13, Iteration 10 Train Loss: 194.93\n",
      "Epoch 13, Iteration 20 Train Loss: 57.21\n",
      "Epoch 13, Iteration 30 Train Loss: 86.64\n",
      "Epoch 13, Iteration 40 Train Loss: 75.08\n",
      "Epoch 13, Iteration 50 Train Loss: 58.66\n",
      "Epoch 13, Iteration 60 Train Loss: 83.94\n",
      "Epoch 13, Iteration 70 Train Loss: 658.61\n",
      "Epoch 13, Iteration 80 Train Loss: 52.00\n",
      "Epoch 13, Iteration 90 Train Loss: 445.27\n",
      "Epoch 13, Iteration 100 Train Loss: 71.36\n",
      "Epoch 13, Iteration 110 Train Loss: 51.87\n",
      "Epoch 13, Iteration 120 Train Loss: 90.88\n",
      "Epoch 13, Iteration 130 Train Loss: 47.36\n",
      "Epoch 13, Iteration 140 Train Loss: 52.56\n",
      "Epoch 13, Iteration 150 Train Loss: 58.45\n",
      "Epoch 13, Iteration 160 Train Loss: 26.46\n",
      "Epoch 13, Iteration 170 Train Loss: 76.41\n",
      "Epoch 13, Iteration 180 Train Loss: 59.69\n",
      "New Epoch! Avg loss for the last 100 iterations: 114.46207761764526\n",
      "Epoch 14, Iteration 0 Train Loss: 19181.74\n",
      "Epoch 14, Iteration 10 Train Loss: 177.02\n",
      "Epoch 14, Iteration 20 Train Loss: 60.00\n",
      "Epoch 14, Iteration 30 Train Loss: 89.41\n",
      "Epoch 14, Iteration 40 Train Loss: 77.84\n",
      "Epoch 14, Iteration 50 Train Loss: 61.41\n",
      "Epoch 14, Iteration 60 Train Loss: 86.68\n",
      "Epoch 14, Iteration 70 Train Loss: 622.42\n",
      "Epoch 14, Iteration 80 Train Loss: 54.73\n",
      "Epoch 14, Iteration 90 Train Loss: 415.65\n",
      "Epoch 14, Iteration 100 Train Loss: 74.08\n",
      "Epoch 14, Iteration 110 Train Loss: 54.59\n",
      "Epoch 14, Iteration 120 Train Loss: 80.59\n",
      "Epoch 14, Iteration 130 Train Loss: 50.07\n",
      "Epoch 14, Iteration 140 Train Loss: 55.26\n",
      "Epoch 14, Iteration 150 Train Loss: 48.02\n",
      "Epoch 14, Iteration 160 Train Loss: 29.06\n",
      "Epoch 14, Iteration 170 Train Loss: 79.25\n",
      "Epoch 14, Iteration 180 Train Loss: 55.81\n",
      "New Epoch! Avg loss for the last 100 iterations: 110.61079139709473\n",
      "Epoch 15, Iteration 0 Train Loss: 19132.69\n",
      "Epoch 15, Iteration 10 Train Loss: 160.44\n",
      "Epoch 15, Iteration 20 Train Loss: 62.80\n",
      "Epoch 15, Iteration 30 Train Loss: 92.24\n",
      "Epoch 15, Iteration 40 Train Loss: 80.66\n",
      "Epoch 15, Iteration 50 Train Loss: 64.21\n",
      "Epoch 15, Iteration 60 Train Loss: 89.46\n",
      "Epoch 15, Iteration 70 Train Loss: 587.29\n",
      "Epoch 15, Iteration 80 Train Loss: 57.51\n",
      "Epoch 15, Iteration 90 Train Loss: 385.13\n",
      "Epoch 15, Iteration 100 Train Loss: 76.86\n",
      "Epoch 15, Iteration 110 Train Loss: 57.35\n",
      "Epoch 15, Iteration 120 Train Loss: 69.93\n",
      "Epoch 15, Iteration 130 Train Loss: 52.82\n",
      "Epoch 15, Iteration 140 Train Loss: 57.97\n",
      "Epoch 15, Iteration 150 Train Loss: 41.34\n",
      "Epoch 15, Iteration 160 Train Loss: 31.79\n",
      "Epoch 15, Iteration 170 Train Loss: 81.44\n",
      "Epoch 15, Iteration 180 Train Loss: 55.48\n",
      "New Epoch! Avg loss for the last 100 iterations: 107.08996452331543\n",
      "Epoch 16, Iteration 0 Train Loss: 19082.96\n",
      "Epoch 16, Iteration 10 Train Loss: 149.77\n",
      "Epoch 16, Iteration 20 Train Loss: 65.43\n",
      "Epoch 16, Iteration 30 Train Loss: 94.86\n",
      "Epoch 16, Iteration 40 Train Loss: 83.27\n",
      "Epoch 16, Iteration 50 Train Loss: 66.81\n",
      "Epoch 16, Iteration 60 Train Loss: 92.05\n",
      "Epoch 16, Iteration 70 Train Loss: 558.41\n",
      "Epoch 16, Iteration 80 Train Loss: 60.09\n",
      "Epoch 16, Iteration 90 Train Loss: 356.15\n",
      "Epoch 16, Iteration 100 Train Loss: 79.43\n",
      "Epoch 16, Iteration 110 Train Loss: 59.89\n",
      "Epoch 16, Iteration 120 Train Loss: 60.44\n",
      "Epoch 16, Iteration 130 Train Loss: 55.21\n",
      "Epoch 16, Iteration 140 Train Loss: 60.22\n",
      "Epoch 16, Iteration 150 Train Loss: 40.84\n",
      "Epoch 16, Iteration 160 Train Loss: 33.92\n",
      "Epoch 16, Iteration 170 Train Loss: 84.73\n",
      "Epoch 16, Iteration 180 Train Loss: 57.65\n",
      "New Epoch! Avg loss for the last 100 iterations: 104.26410133361816\n",
      "Epoch 17, Iteration 0 Train Loss: 19040.68\n",
      "Epoch 17, Iteration 10 Train Loss: 140.56\n",
      "Epoch 17, Iteration 20 Train Loss: 67.87\n",
      "Epoch 17, Iteration 30 Train Loss: 97.31\n",
      "Epoch 17, Iteration 40 Train Loss: 85.71\n",
      "Epoch 17, Iteration 50 Train Loss: 69.24\n",
      "Epoch 17, Iteration 60 Train Loss: 94.47\n",
      "Epoch 17, Iteration 70 Train Loss: 530.38\n",
      "Epoch 17, Iteration 80 Train Loss: 62.54\n",
      "Epoch 17, Iteration 90 Train Loss: 327.87\n",
      "Epoch 17, Iteration 100 Train Loss: 81.86\n",
      "Epoch 17, Iteration 110 Train Loss: 62.33\n",
      "Epoch 17, Iteration 120 Train Loss: 56.65\n",
      "Epoch 17, Iteration 130 Train Loss: 57.74\n",
      "Epoch 17, Iteration 140 Train Loss: 62.85\n",
      "Epoch 17, Iteration 150 Train Loss: 43.52\n",
      "Epoch 17, Iteration 160 Train Loss: 36.59\n",
      "Epoch 17, Iteration 170 Train Loss: 86.34\n",
      "Epoch 17, Iteration 180 Train Loss: 60.19\n",
      "New Epoch! Avg loss for the last 100 iterations: 102.00724571228028\n",
      "Epoch 18, Iteration 0 Train Loss: 18992.50\n",
      "Epoch 18, Iteration 10 Train Loss: 130.36\n",
      "Epoch 18, Iteration 20 Train Loss: 70.38\n",
      "Epoch 18, Iteration 30 Train Loss: 99.80\n",
      "Epoch 18, Iteration 40 Train Loss: 88.19\n",
      "Epoch 18, Iteration 50 Train Loss: 71.71\n",
      "Epoch 18, Iteration 60 Train Loss: 96.95\n",
      "Epoch 18, Iteration 70 Train Loss: 502.88\n",
      "Epoch 18, Iteration 80 Train Loss: 65.00\n",
      "Epoch 18, Iteration 90 Train Loss: 300.70\n",
      "Epoch 18, Iteration 100 Train Loss: 84.29\n",
      "Epoch 18, Iteration 110 Train Loss: 64.75\n",
      "Epoch 18, Iteration 120 Train Loss: 56.16\n",
      "Epoch 18, Iteration 130 Train Loss: 60.10\n",
      "Epoch 18, Iteration 140 Train Loss: 65.20\n",
      "Epoch 18, Iteration 150 Train Loss: 45.86\n",
      "Epoch 18, Iteration 160 Train Loss: 38.92\n",
      "Epoch 18, Iteration 170 Train Loss: 88.57\n",
      "Epoch 18, Iteration 180 Train Loss: 62.51\n",
      "New Epoch! Avg loss for the last 100 iterations: 100.30204025268554\n",
      "Epoch 19, Iteration 0 Train Loss: 18946.03\n",
      "Epoch 19, Iteration 10 Train Loss: 122.81\n",
      "Epoch 19, Iteration 20 Train Loss: 72.74\n",
      "Epoch 19, Iteration 30 Train Loss: 102.01\n",
      "Epoch 19, Iteration 40 Train Loss: 90.38\n",
      "Epoch 19, Iteration 50 Train Loss: 73.89\n",
      "Epoch 19, Iteration 60 Train Loss: 99.12\n",
      "Epoch 19, Iteration 70 Train Loss: 477.84\n",
      "Epoch 19, Iteration 80 Train Loss: 67.15\n",
      "Epoch 19, Iteration 90 Train Loss: 275.52\n",
      "Epoch 19, Iteration 100 Train Loss: 86.46\n",
      "Epoch 19, Iteration 110 Train Loss: 66.91\n",
      "Epoch 19, Iteration 120 Train Loss: 58.31\n",
      "Epoch 19, Iteration 130 Train Loss: 62.23\n",
      "Epoch 19, Iteration 140 Train Loss: 67.29\n",
      "Epoch 19, Iteration 150 Train Loss: 47.94\n",
      "Epoch 19, Iteration 160 Train Loss: 40.99\n",
      "Epoch 19, Iteration 170 Train Loss: 90.83\n",
      "Epoch 19, Iteration 180 Train Loss: 64.57\n",
      "New Epoch! Avg loss for the last 100 iterations: 98.92023929595948\n",
      "Epoch 20, Iteration 0 Train Loss: 18905.93\n",
      "Epoch 20, Iteration 10 Train Loss: 119.83\n",
      "Epoch 20, Iteration 20 Train Loss: 74.76\n",
      "Epoch 20, Iteration 30 Train Loss: 104.17\n",
      "Epoch 20, Iteration 40 Train Loss: 92.54\n",
      "Epoch 20, Iteration 50 Train Loss: 76.05\n",
      "Epoch 20, Iteration 60 Train Loss: 101.28\n",
      "Epoch 20, Iteration 70 Train Loss: 457.16\n",
      "Epoch 20, Iteration 80 Train Loss: 69.31\n",
      "Epoch 20, Iteration 90 Train Loss: 250.90\n",
      "Epoch 20, Iteration 100 Train Loss: 88.60\n",
      "Epoch 20, Iteration 110 Train Loss: 69.05\n",
      "Epoch 20, Iteration 120 Train Loss: 60.45\n",
      "Epoch 20, Iteration 130 Train Loss: 64.36\n",
      "Epoch 20, Iteration 140 Train Loss: 69.40\n",
      "Epoch 20, Iteration 150 Train Loss: 50.03\n",
      "Epoch 20, Iteration 160 Train Loss: 43.07\n",
      "Epoch 20, Iteration 170 Train Loss: 93.10\n",
      "Epoch 20, Iteration 180 Train Loss: 66.61\n",
      "New Epoch! Avg loss for the last 100 iterations: 97.8128419494629\n",
      "Epoch 21, Iteration 0 Train Loss: 18866.57\n",
      "Epoch 21, Iteration 10 Train Loss: 117.36\n",
      "Epoch 21, Iteration 20 Train Loss: 76.91\n",
      "Epoch 21, Iteration 30 Train Loss: 106.24\n",
      "Epoch 21, Iteration 40 Train Loss: 94.56\n",
      "Epoch 21, Iteration 50 Train Loss: 78.05\n",
      "Epoch 21, Iteration 60 Train Loss: 103.28\n",
      "Epoch 21, Iteration 70 Train Loss: 439.28\n",
      "Epoch 21, Iteration 80 Train Loss: 71.32\n",
      "Epoch 21, Iteration 90 Train Loss: 232.58\n",
      "Epoch 21, Iteration 100 Train Loss: 90.58\n",
      "Epoch 21, Iteration 110 Train Loss: 71.01\n",
      "Epoch 21, Iteration 120 Train Loss: 62.49\n",
      "Epoch 21, Iteration 130 Train Loss: 66.36\n",
      "Epoch 21, Iteration 140 Train Loss: 71.34\n",
      "Epoch 21, Iteration 150 Train Loss: 51.92\n",
      "Epoch 21, Iteration 160 Train Loss: 44.93\n",
      "Epoch 21, Iteration 170 Train Loss: 95.12\n",
      "Epoch 21, Iteration 180 Train Loss: 68.46\n",
      "New Epoch! Avg loss for the last 100 iterations: 97.10407962799073\n",
      "Epoch 22, Iteration 0 Train Loss: 18825.83\n",
      "Epoch 22, Iteration 10 Train Loss: 119.51\n",
      "Epoch 22, Iteration 20 Train Loss: 78.87\n",
      "Epoch 22, Iteration 30 Train Loss: 108.14\n",
      "Epoch 22, Iteration 40 Train Loss: 96.50\n",
      "Epoch 22, Iteration 50 Train Loss: 79.99\n",
      "Epoch 22, Iteration 60 Train Loss: 105.22\n",
      "Epoch 22, Iteration 70 Train Loss: 422.28\n",
      "Epoch 22, Iteration 80 Train Loss: 73.27\n",
      "Epoch 22, Iteration 90 Train Loss: 214.50\n",
      "Epoch 22, Iteration 100 Train Loss: 92.60\n",
      "Epoch 22, Iteration 110 Train Loss: 72.97\n",
      "Epoch 22, Iteration 120 Train Loss: 64.49\n",
      "Epoch 22, Iteration 130 Train Loss: 68.26\n",
      "Epoch 22, Iteration 140 Train Loss: 73.27\n",
      "Epoch 22, Iteration 150 Train Loss: 53.88\n",
      "Epoch 22, Iteration 160 Train Loss: 46.95\n",
      "Epoch 22, Iteration 170 Train Loss: 96.93\n",
      "Epoch 22, Iteration 180 Train Loss: 70.46\n",
      "New Epoch! Avg loss for the last 100 iterations: 96.47717441558838\n",
      "Epoch 23, Iteration 0 Train Loss: 18784.35\n",
      "Epoch 23, Iteration 10 Train Loss: 120.86\n",
      "Epoch 23, Iteration 20 Train Loss: 80.86\n",
      "Epoch 23, Iteration 30 Train Loss: 110.11\n",
      "Epoch 23, Iteration 40 Train Loss: 98.46\n",
      "Epoch 23, Iteration 50 Train Loss: 81.95\n",
      "Epoch 23, Iteration 60 Train Loss: 107.44\n",
      "Epoch 23, Iteration 70 Train Loss: 403.58\n",
      "Epoch 23, Iteration 80 Train Loss: 75.19\n",
      "Epoch 23, Iteration 90 Train Loss: 196.45\n",
      "Epoch 23, Iteration 100 Train Loss: 94.44\n",
      "Epoch 23, Iteration 110 Train Loss: 74.87\n",
      "Epoch 23, Iteration 120 Train Loss: 66.34\n",
      "Epoch 23, Iteration 130 Train Loss: 70.08\n",
      "Epoch 23, Iteration 140 Train Loss: 75.09\n",
      "Epoch 23, Iteration 150 Train Loss: 55.83\n",
      "Epoch 23, Iteration 160 Train Loss: 49.26\n",
      "Epoch 23, Iteration 170 Train Loss: 98.77\n",
      "Epoch 23, Iteration 180 Train Loss: 72.23\n",
      "New Epoch! Avg loss for the last 100 iterations: 95.81461601257324\n",
      "Epoch 24, Iteration 0 Train Loss: 18746.00\n",
      "Epoch 24, Iteration 10 Train Loss: 122.87\n",
      "Epoch 24, Iteration 20 Train Loss: 82.55\n",
      "Epoch 24, Iteration 30 Train Loss: 111.97\n",
      "Epoch 24, Iteration 40 Train Loss: 100.30\n",
      "Epoch 24, Iteration 50 Train Loss: 83.78\n",
      "Epoch 24, Iteration 60 Train Loss: 109.03\n",
      "Epoch 24, Iteration 70 Train Loss: 386.12\n",
      "Epoch 24, Iteration 80 Train Loss: 77.04\n",
      "Epoch 24, Iteration 90 Train Loss: 179.08\n",
      "Epoch 24, Iteration 100 Train Loss: 96.28\n",
      "Epoch 24, Iteration 110 Train Loss: 76.70\n",
      "Epoch 24, Iteration 120 Train Loss: 68.14\n",
      "Epoch 24, Iteration 130 Train Loss: 71.71\n",
      "Epoch 24, Iteration 140 Train Loss: 76.51\n",
      "Epoch 24, Iteration 150 Train Loss: 57.00\n",
      "Epoch 24, Iteration 160 Train Loss: 49.99\n",
      "Epoch 24, Iteration 170 Train Loss: 101.36\n",
      "Epoch 24, Iteration 180 Train Loss: 73.57\n",
      "New Epoch! Avg loss for the last 100 iterations: 95.28480339050293\n",
      "Discontinuity\n",
      "2021-11-22 01:00:02-08:00 2021-11-10 23:59:53-08:00\n",
      "0.0012214300000000001 0.02535399\n",
      "Oracle activations: 10996\n",
      "It's over 9000! 2021-11-10 23:00:00-08:00 2021-11-22 01:00:00-08:00\n",
      "Runtime total_E activations: 23962.27847877203\n",
      "Runtime total_E_pred activations: 211656.0136320862\n",
      "It's over 9000! 2021-11-10 23:00:00-08:00 2021-11-22 01:00:00-08:00\n",
      "Naive total_E activations: 10347.00107440601\n",
      "Naive total_E_pred activations: 14082.006131178232\n",
      "Dataset, train set, and test set size: 2211 1474 367\n",
      "Timeframe: 60min\n",
      "Minimal Application\n",
      "Naive vs. DL succesful activations: 0.000171939477303989\n",
      "Maximum possible activations: 10996\n",
      "Predicted activations: 123057\n",
      "Successful activations: 1, 0.001%\n",
      "Failed activations: 95812, 77.860%\n",
      "Missed activations: 10995, 99.991%\n",
      "Naive predicted activations (usual actual energy average): 15217\n",
      "Naive successful activations (usual actual energy average): 5816, 38.220%\n",
      "Naive failed activations (usual actual energy average): 9401, 61.780%\n",
      "Naive missed activations (usual actual energy average): 5180, 47.108%\n",
      "Voltage overestimation rate: 100.000%\n",
      "Test MAPE power: 1.869074\n",
      "Test MAPE voltage: 3.086367\n",
      "Test MAPE current: 0.118307\n",
      "Epoch 0, Iteration 0 Train Loss: 22023.71\n",
      "Epoch 0, Iteration 10 Train Loss: 2214.24\n",
      "Epoch 0, Iteration 20 Train Loss: 1750.80\n",
      "Epoch 0, Iteration 30 Train Loss: 1199.06\n",
      "Epoch 0, Iteration 40 Train Loss: 1416.59\n",
      "Epoch 0, Iteration 50 Train Loss: 1723.04\n",
      "Epoch 0, Iteration 60 Train Loss: 1241.31\n",
      "Epoch 0, Iteration 70 Train Loss: 2975.13\n",
      "Epoch 0, Iteration 80 Train Loss: 1848.36\n",
      "Epoch 0, Iteration 90 Train Loss: 2271.48\n",
      "Epoch 0, Iteration 100 Train Loss: 1476.44\n",
      "Epoch 0, Iteration 110 Train Loss: 1843.13\n",
      "Epoch 0, Iteration 120 Train Loss: 2002.87\n",
      "Epoch 0, Iteration 130 Train Loss: 1921.66\n",
      "Epoch 0, Iteration 140 Train Loss: 1815.84\n",
      "Epoch 0, Iteration 150 Train Loss: 2173.53\n",
      "Epoch 0, Iteration 160 Train Loss: 2296.93\n",
      "Epoch 0, Iteration 170 Train Loss: 1730.51\n",
      "Epoch 0, Iteration 180 Train Loss: 1828.53\n",
      "Epoch 0, Iteration 190 Train Loss: 1582.28\n",
      "Epoch 0, Iteration 200 Train Loss: 1401.03\n",
      "Epoch 0, Iteration 210 Train Loss: 1435.18\n",
      "Epoch 0, Iteration 220 Train Loss: 1894.03\n",
      "Epoch 0, Iteration 230 Train Loss: 1513.42\n",
      "Epoch 0, Iteration 240 Train Loss: 1333.09\n",
      "Epoch 0, Iteration 250 Train Loss: 813.98\n",
      "Epoch 0, Iteration 260 Train Loss: 857.90\n",
      "Epoch 0, Iteration 270 Train Loss: 317.48\n",
      "New Epoch! Avg loss for the last 100 iterations: 1236.7516720581054\n",
      "Epoch 1, Iteration 0 Train Loss: 20899.91\n",
      "Epoch 1, Iteration 10 Train Loss: 1154.24\n",
      "Epoch 1, Iteration 20 Train Loss: 701.67\n",
      "Epoch 1, Iteration 30 Train Loss: 280.06\n",
      "Epoch 1, Iteration 40 Train Loss: 430.01\n",
      "Epoch 1, Iteration 50 Train Loss: 577.99\n",
      "Epoch 1, Iteration 60 Train Loss: 364.46\n",
      "Epoch 1, Iteration 70 Train Loss: 1729.59\n",
      "Epoch 1, Iteration 80 Train Loss: 595.07\n",
      "Epoch 1, Iteration 90 Train Loss: 1202.94\n",
      "Epoch 1, Iteration 100 Train Loss: 319.41\n",
      "Epoch 1, Iteration 110 Train Loss: 501.15\n",
      "Epoch 1, Iteration 120 Train Loss: 799.21\n",
      "Epoch 1, Iteration 130 Train Loss: 601.02\n",
      "Epoch 1, Iteration 140 Train Loss: 402.07\n",
      "Epoch 1, Iteration 150 Train Loss: 748.66\n",
      "Epoch 1, Iteration 160 Train Loss: 749.40\n",
      "Epoch 1, Iteration 170 Train Loss: 272.45\n",
      "Epoch 1, Iteration 180 Train Loss: 484.19\n",
      "Epoch 1, Iteration 190 Train Loss: 157.79\n",
      "Epoch 1, Iteration 200 Train Loss: 272.81\n",
      "Epoch 1, Iteration 210 Train Loss: 104.33\n",
      "Epoch 1, Iteration 220 Train Loss: 369.08\n",
      "Epoch 1, Iteration 230 Train Loss: 153.76\n",
      "Epoch 1, Iteration 240 Train Loss: 116.92\n",
      "Epoch 1, Iteration 250 Train Loss: 164.20\n",
      "Epoch 1, Iteration 260 Train Loss: 179.53\n",
      "Epoch 1, Iteration 270 Train Loss: 38.34\n",
      "New Epoch! Avg loss for the last 100 iterations: 176.75044609069823\n",
      "Epoch 2, Iteration 0 Train Loss: 20185.08\n",
      "Epoch 2, Iteration 10 Train Loss: 585.78\n",
      "Epoch 2, Iteration 20 Train Loss: 69.31\n",
      "Epoch 2, Iteration 30 Train Loss: 37.71\n",
      "Epoch 2, Iteration 40 Train Loss: 83.38\n",
      "Epoch 2, Iteration 50 Train Loss: 55.86\n",
      "Epoch 2, Iteration 60 Train Loss: 182.57\n",
      "Epoch 2, Iteration 70 Train Loss: 1369.68\n",
      "Epoch 2, Iteration 80 Train Loss: 167.29\n",
      "Epoch 2, Iteration 90 Train Loss: 936.08\n",
      "Epoch 2, Iteration 100 Train Loss: 89.75\n",
      "Epoch 2, Iteration 110 Train Loss: 41.44\n",
      "Epoch 2, Iteration 120 Train Loss: 523.14\n",
      "Epoch 2, Iteration 130 Train Loss: 335.39\n",
      "Epoch 2, Iteration 140 Train Loss: 31.38\n",
      "Epoch 2, Iteration 150 Train Loss: 494.75\n",
      "Epoch 2, Iteration 160 Train Loss: 418.94\n",
      "Epoch 2, Iteration 170 Train Loss: 72.57\n",
      "Epoch 2, Iteration 180 Train Loss: 336.17\n",
      "Epoch 2, Iteration 190 Train Loss: 24.45\n",
      "Epoch 2, Iteration 200 Train Loss: 131.14\n",
      "Epoch 2, Iteration 210 Train Loss: 34.82\n",
      "Epoch 2, Iteration 220 Train Loss: 87.62\n",
      "Epoch 2, Iteration 230 Train Loss: 46.13\n",
      "Epoch 2, Iteration 240 Train Loss: 34.05\n",
      "Epoch 2, Iteration 250 Train Loss: 79.82\n",
      "Epoch 2, Iteration 260 Train Loss: 72.14\n",
      "Epoch 2, Iteration 270 Train Loss: 54.31\n",
      "New Epoch! Avg loss for the last 100 iterations: 73.94092569351196\n",
      "Epoch 3, Iteration 0 Train Loss: 19884.17\n",
      "Epoch 3, Iteration 10 Train Loss: 468.43\n",
      "Epoch 3, Iteration 20 Train Loss: 22.98\n",
      "Epoch 3, Iteration 30 Train Loss: 52.75\n",
      "Epoch 3, Iteration 40 Train Loss: 41.41\n",
      "Epoch 3, Iteration 50 Train Loss: 25.97\n",
      "Epoch 3, Iteration 60 Train Loss: 111.51\n",
      "Epoch 3, Iteration 70 Train Loss: 1217.82\n",
      "Epoch 3, Iteration 80 Train Loss: 46.74\n",
      "Epoch 3, Iteration 90 Train Loss: 805.12\n",
      "Epoch 3, Iteration 100 Train Loss: 41.92\n",
      "Epoch 3, Iteration 110 Train Loss: 20.32\n",
      "Epoch 3, Iteration 120 Train Loss: 389.53\n",
      "Epoch 3, Iteration 130 Train Loss: 231.04\n",
      "Epoch 3, Iteration 140 Train Loss: 21.90\n",
      "Epoch 3, Iteration 150 Train Loss: 369.43\n",
      "Epoch 3, Iteration 160 Train Loss: 296.12\n",
      "Epoch 3, Iteration 170 Train Loss: 33.04\n",
      "Epoch 3, Iteration 180 Train Loss: 263.83\n",
      "Epoch 3, Iteration 190 Train Loss: 33.29\n",
      "Epoch 3, Iteration 200 Train Loss: 66.87\n",
      "Epoch 3, Iteration 210 Train Loss: 38.44\n",
      "Epoch 3, Iteration 220 Train Loss: 34.88\n",
      "Epoch 3, Iteration 230 Train Loss: 29.79\n",
      "Epoch 3, Iteration 240 Train Loss: 27.90\n",
      "Epoch 3, Iteration 250 Train Loss: 53.03\n",
      "Epoch 3, Iteration 260 Train Loss: 43.63\n",
      "Epoch 3, Iteration 270 Train Loss: 62.81\n",
      "New Epoch! Avg loss for the last 100 iterations: 54.36867282867431\n",
      "Epoch 4, Iteration 0 Train Loss: 19725.05\n",
      "Epoch 4, Iteration 10 Train Loss: 407.96\n",
      "Epoch 4, Iteration 20 Train Loss: 31.38\n",
      "Epoch 4, Iteration 30 Train Loss: 61.12\n",
      "Epoch 4, Iteration 40 Train Loss: 49.72\n",
      "Epoch 4, Iteration 50 Train Loss: 33.42\n",
      "Epoch 4, Iteration 60 Train Loss: 76.96\n",
      "Epoch 4, Iteration 70 Train Loss: 1112.38\n",
      "Epoch 4, Iteration 80 Train Loss: 27.45\n",
      "Epoch 4, Iteration 90 Train Loss: 731.82\n",
      "Epoch 4, Iteration 100 Train Loss: 47.32\n",
      "Epoch 4, Iteration 110 Train Loss: 28.11\n",
      "Epoch 4, Iteration 120 Train Loss: 310.70\n",
      "Epoch 4, Iteration 130 Train Loss: 174.69\n",
      "Epoch 4, Iteration 140 Train Loss: 29.54\n",
      "Epoch 4, Iteration 150 Train Loss: 274.54\n",
      "Epoch 4, Iteration 160 Train Loss: 204.43\n",
      "Epoch 4, Iteration 170 Train Loss: 33.97\n",
      "Epoch 4, Iteration 180 Train Loss: 213.26\n",
      "Epoch 4, Iteration 190 Train Loss: 40.53\n",
      "Epoch 4, Iteration 200 Train Loss: 58.85\n",
      "Epoch 4, Iteration 210 Train Loss: 45.58\n",
      "Epoch 4, Iteration 220 Train Loss: 26.35\n",
      "Epoch 4, Iteration 230 Train Loss: 34.81\n",
      "Epoch 4, Iteration 240 Train Loss: 34.75\n",
      "Epoch 4, Iteration 250 Train Loss: 51.01\n",
      "Epoch 4, Iteration 260 Train Loss: 41.53\n",
      "Epoch 4, Iteration 270 Train Loss: 69.29\n",
      "New Epoch! Avg loss for the last 100 iterations: 52.85868638992309\n",
      "Epoch 5, Iteration 0 Train Loss: 19603.31\n",
      "Epoch 5, Iteration 10 Train Loss: 360.18\n",
      "Epoch 5, Iteration 20 Train Loss: 37.73\n",
      "Epoch 5, Iteration 30 Train Loss: 67.36\n",
      "Epoch 5, Iteration 40 Train Loss: 55.90\n",
      "Epoch 5, Iteration 50 Train Loss: 39.57\n",
      "Epoch 5, Iteration 60 Train Loss: 70.26\n",
      "Epoch 5, Iteration 70 Train Loss: 1023.57\n",
      "Epoch 5, Iteration 80 Train Loss: 33.46\n",
      "Epoch 5, Iteration 90 Train Loss: 679.24\n",
      "Epoch 5, Iteration 100 Train Loss: 53.22\n",
      "Epoch 5, Iteration 110 Train Loss: 33.93\n",
      "Epoch 5, Iteration 120 Train Loss: 256.88\n",
      "Epoch 5, Iteration 130 Train Loss: 135.70\n",
      "Epoch 5, Iteration 140 Train Loss: 35.22\n",
      "Epoch 5, Iteration 150 Train Loss: 218.52\n",
      "Epoch 5, Iteration 160 Train Loss: 141.74\n",
      "Epoch 5, Iteration 170 Train Loss: 39.43\n",
      "Epoch 5, Iteration 180 Train Loss: 175.47\n",
      "Epoch 5, Iteration 190 Train Loss: 45.70\n",
      "Epoch 5, Iteration 200 Train Loss: 60.75\n",
      "Epoch 5, Iteration 210 Train Loss: 50.50\n",
      "Epoch 5, Iteration 220 Train Loss: 27.63\n",
      "Epoch 5, Iteration 230 Train Loss: 39.56\n",
      "Epoch 5, Iteration 240 Train Loss: 39.41\n",
      "Epoch 5, Iteration 250 Train Loss: 52.21\n",
      "Epoch 5, Iteration 260 Train Loss: 43.07\n",
      "Epoch 5, Iteration 270 Train Loss: 73.70\n",
      "New Epoch! Avg loss for the last 100 iterations: 55.04367702484131\n",
      "Epoch 6, Iteration 0 Train Loss: 19520.56\n",
      "Epoch 6, Iteration 10 Train Loss: 326.19\n",
      "Epoch 6, Iteration 20 Train Loss: 42.29\n",
      "Epoch 6, Iteration 30 Train Loss: 71.94\n",
      "Epoch 6, Iteration 40 Train Loss: 60.47\n",
      "Epoch 6, Iteration 50 Train Loss: 44.12\n",
      "Epoch 6, Iteration 60 Train Loss: 69.54\n",
      "Epoch 6, Iteration 70 Train Loss: 947.73\n",
      "Epoch 6, Iteration 80 Train Loss: 38.01\n",
      "Epoch 6, Iteration 90 Train Loss: 633.34\n",
      "Epoch 6, Iteration 100 Train Loss: 57.74\n",
      "Epoch 6, Iteration 110 Train Loss: 38.43\n",
      "Epoch 6, Iteration 120 Train Loss: 212.53\n",
      "Epoch 6, Iteration 130 Train Loss: 109.04\n",
      "Epoch 6, Iteration 140 Train Loss: 39.64\n",
      "Epoch 6, Iteration 150 Train Loss: 176.21\n",
      "Epoch 6, Iteration 160 Train Loss: 99.64\n",
      "Epoch 6, Iteration 170 Train Loss: 43.75\n",
      "Epoch 6, Iteration 180 Train Loss: 141.59\n",
      "Epoch 6, Iteration 190 Train Loss: 50.16\n",
      "Epoch 6, Iteration 200 Train Loss: 63.70\n",
      "Epoch 6, Iteration 210 Train Loss: 55.05\n",
      "Epoch 6, Iteration 220 Train Loss: 30.74\n",
      "Epoch 6, Iteration 230 Train Loss: 44.13\n",
      "Epoch 6, Iteration 240 Train Loss: 43.96\n",
      "Epoch 6, Iteration 250 Train Loss: 55.03\n",
      "Epoch 6, Iteration 260 Train Loss: 46.64\n",
      "Epoch 6, Iteration 270 Train Loss: 78.16\n",
      "New Epoch! Avg loss for the last 100 iterations: 58.16828601837158\n",
      "Epoch 7, Iteration 0 Train Loss: 19436.10\n",
      "Epoch 7, Iteration 10 Train Loss: 293.74\n",
      "Epoch 7, Iteration 20 Train Loss: 46.74\n",
      "Epoch 7, Iteration 30 Train Loss: 76.32\n",
      "Epoch 7, Iteration 40 Train Loss: 64.82\n",
      "Epoch 7, Iteration 50 Train Loss: 48.44\n",
      "Epoch 7, Iteration 60 Train Loss: 73.84\n",
      "Epoch 7, Iteration 70 Train Loss: 875.08\n",
      "Epoch 7, Iteration 80 Train Loss: 42.24\n",
      "Epoch 7, Iteration 90 Train Loss: 589.91\n",
      "Epoch 7, Iteration 100 Train Loss: 61.93\n",
      "Epoch 7, Iteration 110 Train Loss: 42.59\n",
      "Epoch 7, Iteration 120 Train Loss: 179.03\n",
      "Epoch 7, Iteration 130 Train Loss: 84.93\n",
      "Epoch 7, Iteration 140 Train Loss: 43.74\n",
      "Epoch 7, Iteration 150 Train Loss: 143.19\n",
      "Epoch 7, Iteration 160 Train Loss: 70.64\n",
      "Epoch 7, Iteration 170 Train Loss: 47.71\n",
      "Epoch 7, Iteration 180 Train Loss: 112.48\n",
      "Epoch 7, Iteration 190 Train Loss: 54.05\n",
      "Epoch 7, Iteration 200 Train Loss: 67.06\n",
      "Epoch 7, Iteration 210 Train Loss: 58.90\n",
      "Epoch 7, Iteration 220 Train Loss: 34.12\n",
      "Epoch 7, Iteration 230 Train Loss: 47.96\n",
      "Epoch 7, Iteration 240 Train Loss: 47.79\n",
      "Epoch 7, Iteration 250 Train Loss: 58.83\n",
      "Epoch 7, Iteration 260 Train Loss: 50.07\n",
      "Epoch 7, Iteration 270 Train Loss: 81.94\n",
      "New Epoch! Avg loss for the last 100 iterations: 61.38369632720947\n",
      "Epoch 8, Iteration 0 Train Loss: 19364.17\n",
      "Epoch 8, Iteration 10 Train Loss: 270.02\n",
      "Epoch 8, Iteration 20 Train Loss: 50.63\n",
      "Epoch 8, Iteration 30 Train Loss: 80.21\n",
      "Epoch 8, Iteration 40 Train Loss: 68.70\n",
      "Epoch 8, Iteration 50 Train Loss: 52.30\n",
      "Epoch 8, Iteration 60 Train Loss: 77.68\n",
      "Epoch 8, Iteration 70 Train Loss: 807.61\n",
      "Epoch 8, Iteration 80 Train Loss: 46.07\n",
      "Epoch 8, Iteration 90 Train Loss: 549.37\n",
      "Epoch 8, Iteration 100 Train Loss: 65.71\n",
      "Epoch 8, Iteration 110 Train Loss: 46.34\n",
      "Epoch 8, Iteration 120 Train Loss: 147.82\n",
      "Epoch 8, Iteration 130 Train Loss: 68.58\n",
      "Epoch 8, Iteration 140 Train Loss: 47.23\n",
      "Epoch 8, Iteration 150 Train Loss: 111.74\n",
      "Epoch 8, Iteration 160 Train Loss: 49.97\n",
      "Epoch 8, Iteration 170 Train Loss: 51.01\n",
      "Epoch 8, Iteration 180 Train Loss: 89.86\n",
      "Epoch 8, Iteration 190 Train Loss: 57.23\n",
      "Epoch 8, Iteration 200 Train Loss: 70.74\n",
      "Epoch 8, Iteration 210 Train Loss: 61.98\n",
      "Epoch 8, Iteration 220 Train Loss: 37.77\n",
      "Epoch 8, Iteration 230 Train Loss: 51.04\n",
      "Epoch 8, Iteration 240 Train Loss: 50.84\n",
      "Epoch 8, Iteration 250 Train Loss: 61.92\n",
      "Epoch 8, Iteration 260 Train Loss: 53.38\n",
      "Epoch 8, Iteration 270 Train Loss: 85.16\n",
      "New Epoch! Avg loss for the last 100 iterations: 64.51632789611817\n",
      "Epoch 9, Iteration 0 Train Loss: 19302.87\n",
      "Epoch 9, Iteration 10 Train Loss: 249.31\n",
      "Epoch 9, Iteration 20 Train Loss: 54.07\n",
      "Epoch 9, Iteration 30 Train Loss: 83.69\n",
      "Epoch 9, Iteration 40 Train Loss: 72.18\n",
      "Epoch 9, Iteration 50 Train Loss: 55.76\n",
      "Epoch 9, Iteration 60 Train Loss: 81.14\n",
      "Epoch 9, Iteration 70 Train Loss: 751.24\n",
      "Epoch 9, Iteration 80 Train Loss: 49.54\n",
      "Epoch 9, Iteration 90 Train Loss: 510.27\n",
      "Epoch 9, Iteration 100 Train Loss: 69.15\n",
      "Epoch 9, Iteration 110 Train Loss: 49.77\n",
      "Epoch 9, Iteration 120 Train Loss: 124.76\n",
      "Epoch 9, Iteration 130 Train Loss: 55.11\n",
      "Epoch 9, Iteration 140 Train Loss: 50.79\n",
      "Epoch 9, Iteration 150 Train Loss: 90.27\n",
      "Epoch 9, Iteration 160 Train Loss: 37.01\n",
      "Epoch 9, Iteration 170 Train Loss: 54.61\n",
      "Epoch 9, Iteration 180 Train Loss: 77.12\n",
      "Epoch 9, Iteration 190 Train Loss: 60.84\n",
      "Epoch 9, Iteration 200 Train Loss: 73.66\n",
      "Epoch 9, Iteration 210 Train Loss: 65.62\n",
      "Epoch 9, Iteration 220 Train Loss: 40.71\n",
      "Epoch 9, Iteration 230 Train Loss: 54.66\n",
      "Epoch 9, Iteration 240 Train Loss: 54.46\n",
      "Epoch 9, Iteration 250 Train Loss: 65.51\n",
      "Epoch 9, Iteration 260 Train Loss: 56.48\n",
      "Epoch 9, Iteration 270 Train Loss: 88.68\n",
      "New Epoch! Avg loss for the last 100 iterations: 67.55155437469483\n",
      "Epoch 10, Iteration 0 Train Loss: 19236.10\n",
      "Epoch 10, Iteration 10 Train Loss: 227.23\n",
      "Epoch 10, Iteration 20 Train Loss: 57.63\n",
      "Epoch 10, Iteration 30 Train Loss: 87.23\n",
      "Epoch 10, Iteration 40 Train Loss: 75.71\n",
      "Epoch 10, Iteration 50 Train Loss: 59.27\n",
      "Epoch 10, Iteration 60 Train Loss: 84.63\n",
      "Epoch 10, Iteration 70 Train Loss: 698.91\n",
      "Epoch 10, Iteration 80 Train Loss: 53.00\n",
      "Epoch 10, Iteration 90 Train Loss: 473.10\n",
      "Epoch 10, Iteration 100 Train Loss: 72.60\n",
      "Epoch 10, Iteration 110 Train Loss: 53.19\n",
      "Epoch 10, Iteration 120 Train Loss: 104.14\n",
      "Epoch 10, Iteration 130 Train Loss: 49.14\n",
      "Epoch 10, Iteration 140 Train Loss: 54.11\n",
      "Epoch 10, Iteration 150 Train Loss: 70.31\n",
      "Epoch 10, Iteration 160 Train Loss: 31.93\n",
      "Epoch 10, Iteration 170 Train Loss: 57.83\n",
      "Epoch 10, Iteration 180 Train Loss: 65.16\n",
      "Epoch 10, Iteration 190 Train Loss: 64.01\n",
      "Epoch 10, Iteration 200 Train Loss: 76.65\n",
      "Epoch 10, Iteration 210 Train Loss: 68.77\n",
      "Epoch 10, Iteration 220 Train Loss: 43.70\n",
      "Epoch 10, Iteration 230 Train Loss: 57.79\n",
      "Epoch 10, Iteration 240 Train Loss: 57.59\n",
      "Epoch 10, Iteration 250 Train Loss: 68.64\n",
      "Epoch 10, Iteration 260 Train Loss: 59.41\n",
      "Epoch 10, Iteration 270 Train Loss: 91.85\n",
      "New Epoch! Avg loss for the last 100 iterations: 70.42177341461182\n",
      "Epoch 11, Iteration 0 Train Loss: 19174.53\n",
      "Epoch 11, Iteration 10 Train Loss: 207.35\n",
      "Epoch 11, Iteration 20 Train Loss: 61.03\n",
      "Epoch 11, Iteration 30 Train Loss: 90.61\n",
      "Epoch 11, Iteration 40 Train Loss: 79.07\n",
      "Epoch 11, Iteration 50 Train Loss: 62.62\n",
      "Epoch 11, Iteration 60 Train Loss: 87.99\n",
      "Epoch 11, Iteration 70 Train Loss: 661.13\n",
      "Epoch 11, Iteration 80 Train Loss: 56.29\n",
      "Epoch 11, Iteration 90 Train Loss: 442.94\n",
      "Epoch 11, Iteration 100 Train Loss: 75.82\n",
      "Epoch 11, Iteration 110 Train Loss: 56.39\n",
      "Epoch 11, Iteration 120 Train Loss: 91.61\n",
      "Epoch 11, Iteration 130 Train Loss: 51.94\n",
      "Epoch 11, Iteration 140 Train Loss: 57.13\n",
      "Epoch 11, Iteration 150 Train Loss: 58.81\n",
      "Epoch 11, Iteration 160 Train Loss: 31.25\n",
      "Epoch 11, Iteration 170 Train Loss: 60.67\n",
      "Epoch 11, Iteration 180 Train Loss: 62.06\n",
      "Epoch 11, Iteration 190 Train Loss: 66.79\n",
      "Epoch 11, Iteration 200 Train Loss: 78.54\n",
      "Epoch 11, Iteration 210 Train Loss: 71.53\n",
      "Epoch 11, Iteration 220 Train Loss: 45.60\n",
      "Epoch 11, Iteration 230 Train Loss: 60.39\n",
      "Epoch 11, Iteration 240 Train Loss: 60.09\n",
      "Epoch 11, Iteration 250 Train Loss: 71.08\n",
      "Epoch 11, Iteration 260 Train Loss: 61.08\n",
      "Epoch 11, Iteration 270 Train Loss: 94.11\n",
      "New Epoch! Avg loss for the last 100 iterations: 72.50084308624268\n",
      "Epoch 12, Iteration 0 Train Loss: 19134.09\n",
      "Epoch 12, Iteration 10 Train Loss: 194.42\n",
      "Epoch 12, Iteration 20 Train Loss: 63.24\n",
      "Epoch 12, Iteration 30 Train Loss: 92.85\n",
      "Epoch 12, Iteration 40 Train Loss: 81.30\n",
      "Epoch 12, Iteration 50 Train Loss: 64.83\n",
      "Epoch 12, Iteration 60 Train Loss: 90.19\n",
      "Epoch 12, Iteration 70 Train Loss: 627.92\n",
      "Epoch 12, Iteration 80 Train Loss: 58.55\n",
      "Epoch 12, Iteration 90 Train Loss: 415.01\n",
      "Epoch 12, Iteration 100 Train Loss: 78.12\n",
      "Epoch 12, Iteration 110 Train Loss: 58.69\n",
      "Epoch 12, Iteration 120 Train Loss: 81.45\n",
      "Epoch 12, Iteration 130 Train Loss: 54.26\n",
      "Epoch 12, Iteration 140 Train Loss: 59.43\n",
      "Epoch 12, Iteration 150 Train Loss: 48.62\n",
      "Epoch 12, Iteration 160 Train Loss: 33.53\n",
      "Epoch 12, Iteration 170 Train Loss: 62.96\n",
      "Epoch 12, Iteration 180 Train Loss: 58.22\n",
      "Epoch 12, Iteration 190 Train Loss: 69.03\n",
      "Epoch 12, Iteration 200 Train Loss: 81.08\n",
      "Epoch 12, Iteration 210 Train Loss: 73.72\n",
      "Epoch 12, Iteration 220 Train Loss: 48.11\n",
      "Epoch 12, Iteration 230 Train Loss: 62.70\n",
      "Epoch 12, Iteration 240 Train Loss: 62.47\n",
      "Epoch 12, Iteration 250 Train Loss: 73.51\n",
      "Epoch 12, Iteration 260 Train Loss: 63.84\n",
      "Epoch 12, Iteration 270 Train Loss: 96.67\n",
      "New Epoch! Avg loss for the last 100 iterations: 74.93843959808349\n",
      "Epoch 13, Iteration 0 Train Loss: 19084.44\n",
      "Epoch 13, Iteration 10 Train Loss: 177.45\n",
      "Epoch 13, Iteration 20 Train Loss: 65.87\n",
      "Epoch 13, Iteration 30 Train Loss: 95.46\n",
      "Epoch 13, Iteration 40 Train Loss: 83.90\n",
      "Epoch 13, Iteration 50 Train Loss: 67.43\n",
      "Epoch 13, Iteration 60 Train Loss: 92.76\n",
      "Epoch 13, Iteration 70 Train Loss: 594.32\n",
      "Epoch 13, Iteration 80 Train Loss: 61.09\n",
      "Epoch 13, Iteration 90 Train Loss: 386.20\n",
      "Epoch 13, Iteration 100 Train Loss: 80.64\n",
      "Epoch 13, Iteration 110 Train Loss: 61.18\n",
      "Epoch 13, Iteration 120 Train Loss: 71.67\n",
      "Epoch 13, Iteration 130 Train Loss: 56.72\n",
      "Epoch 13, Iteration 140 Train Loss: 61.87\n",
      "Epoch 13, Iteration 150 Train Loss: 44.00\n",
      "Epoch 13, Iteration 160 Train Loss: 35.94\n",
      "Epoch 13, Iteration 170 Train Loss: 65.35\n",
      "Epoch 13, Iteration 180 Train Loss: 59.45\n",
      "Epoch 13, Iteration 190 Train Loss: 71.38\n",
      "Epoch 13, Iteration 200 Train Loss: 83.64\n",
      "Epoch 13, Iteration 210 Train Loss: 76.05\n",
      "Epoch 13, Iteration 220 Train Loss: 50.67\n",
      "Epoch 13, Iteration 230 Train Loss: 65.02\n",
      "Epoch 13, Iteration 240 Train Loss: 64.79\n",
      "Epoch 13, Iteration 250 Train Loss: 75.84\n",
      "Epoch 13, Iteration 260 Train Loss: 66.29\n",
      "Epoch 13, Iteration 270 Train Loss: 99.07\n",
      "New Epoch! Avg loss for the last 100 iterations: 77.36823314666748\n",
      "Epoch 14, Iteration 0 Train Loss: 19038.71\n",
      "Epoch 14, Iteration 10 Train Loss: 167.41\n",
      "Epoch 14, Iteration 20 Train Loss: 68.34\n",
      "Epoch 14, Iteration 30 Train Loss: 97.96\n",
      "Epoch 14, Iteration 40 Train Loss: 86.36\n",
      "Epoch 14, Iteration 50 Train Loss: 69.88\n",
      "Epoch 14, Iteration 60 Train Loss: 95.21\n",
      "Epoch 14, Iteration 70 Train Loss: 566.98\n",
      "Epoch 14, Iteration 80 Train Loss: 63.61\n",
      "Epoch 14, Iteration 90 Train Loss: 359.30\n",
      "Epoch 14, Iteration 100 Train Loss: 83.12\n",
      "Epoch 14, Iteration 110 Train Loss: 63.63\n",
      "Epoch 14, Iteration 120 Train Loss: 63.57\n",
      "Epoch 14, Iteration 130 Train Loss: 59.10\n",
      "Epoch 14, Iteration 140 Train Loss: 64.22\n",
      "Epoch 14, Iteration 150 Train Loss: 44.97\n",
      "Epoch 14, Iteration 160 Train Loss: 38.22\n",
      "Epoch 14, Iteration 170 Train Loss: 67.54\n",
      "Epoch 14, Iteration 180 Train Loss: 61.62\n",
      "Epoch 14, Iteration 190 Train Loss: 73.54\n",
      "Epoch 14, Iteration 200 Train Loss: 85.85\n",
      "Epoch 14, Iteration 210 Train Loss: 78.21\n",
      "Epoch 14, Iteration 220 Train Loss: 52.89\n",
      "Epoch 14, Iteration 230 Train Loss: 67.18\n",
      "Epoch 14, Iteration 240 Train Loss: 66.95\n",
      "Epoch 14, Iteration 250 Train Loss: 78.00\n",
      "Epoch 14, Iteration 260 Train Loss: 68.50\n",
      "Epoch 14, Iteration 270 Train Loss: 101.24\n",
      "New Epoch! Avg loss for the last 100 iterations: 79.55953289031983\n",
      "Epoch 15, Iteration 0 Train Loss: 18997.54\n",
      "Epoch 15, Iteration 10 Train Loss: 158.41\n",
      "Epoch 15, Iteration 20 Train Loss: 70.49\n",
      "Epoch 15, Iteration 30 Train Loss: 100.06\n",
      "Epoch 15, Iteration 40 Train Loss: 88.49\n",
      "Epoch 15, Iteration 50 Train Loss: 72.01\n",
      "Epoch 15, Iteration 60 Train Loss: 97.32\n",
      "Epoch 15, Iteration 70 Train Loss: 543.40\n",
      "Epoch 15, Iteration 80 Train Loss: 65.63\n",
      "Epoch 15, Iteration 90 Train Loss: 335.54\n",
      "Epoch 15, Iteration 100 Train Loss: 85.13\n",
      "Epoch 15, Iteration 110 Train Loss: 65.65\n",
      "Epoch 15, Iteration 120 Train Loss: 60.42\n",
      "Epoch 15, Iteration 130 Train Loss: 61.10\n",
      "Epoch 15, Iteration 140 Train Loss: 66.22\n",
      "Epoch 15, Iteration 150 Train Loss: 46.96\n",
      "Epoch 15, Iteration 160 Train Loss: 40.19\n",
      "Epoch 15, Iteration 170 Train Loss: 69.52\n",
      "Epoch 15, Iteration 180 Train Loss: 63.57\n",
      "Epoch 15, Iteration 190 Train Loss: 75.47\n",
      "Epoch 15, Iteration 200 Train Loss: 87.80\n",
      "Epoch 15, Iteration 210 Train Loss: 80.12\n",
      "Epoch 15, Iteration 220 Train Loss: 54.83\n",
      "Epoch 15, Iteration 230 Train Loss: 69.09\n",
      "Epoch 15, Iteration 240 Train Loss: 68.84\n",
      "Epoch 15, Iteration 250 Train Loss: 79.89\n",
      "Epoch 15, Iteration 260 Train Loss: 70.40\n",
      "Epoch 15, Iteration 270 Train Loss: 103.14\n",
      "New Epoch! Avg loss for the last 100 iterations: 81.48407653808594\n",
      "Epoch 16, Iteration 0 Train Loss: 18961.31\n",
      "Epoch 16, Iteration 10 Train Loss: 150.98\n",
      "Epoch 16, Iteration 20 Train Loss: 72.46\n",
      "Epoch 16, Iteration 30 Train Loss: 102.02\n",
      "Epoch 16, Iteration 40 Train Loss: 90.44\n",
      "Epoch 16, Iteration 50 Train Loss: 73.95\n",
      "Epoch 16, Iteration 60 Train Loss: 99.26\n",
      "Epoch 16, Iteration 70 Train Loss: 521.10\n",
      "Epoch 16, Iteration 80 Train Loss: 67.61\n",
      "Epoch 16, Iteration 90 Train Loss: 312.92\n",
      "Epoch 16, Iteration 100 Train Loss: 87.14\n",
      "Epoch 16, Iteration 110 Train Loss: 67.66\n",
      "Epoch 16, Iteration 120 Train Loss: 59.07\n",
      "Epoch 16, Iteration 130 Train Loss: 63.05\n",
      "Epoch 16, Iteration 140 Train Loss: 68.14\n",
      "Epoch 16, Iteration 150 Train Loss: 48.85\n",
      "Epoch 16, Iteration 160 Train Loss: 42.07\n",
      "Epoch 16, Iteration 170 Train Loss: 71.37\n",
      "Epoch 16, Iteration 180 Train Loss: 65.41\n",
      "Epoch 16, Iteration 190 Train Loss: 77.30\n",
      "Epoch 16, Iteration 200 Train Loss: 89.63\n",
      "Epoch 16, Iteration 210 Train Loss: 81.94\n",
      "Epoch 16, Iteration 220 Train Loss: 56.66\n",
      "Epoch 16, Iteration 230 Train Loss: 70.90\n",
      "Epoch 16, Iteration 240 Train Loss: 70.65\n",
      "Epoch 16, Iteration 250 Train Loss: 81.70\n",
      "Epoch 16, Iteration 260 Train Loss: 72.20\n",
      "Epoch 16, Iteration 270 Train Loss: 104.96\n",
      "New Epoch! Avg loss for the last 100 iterations: 83.30359783172608\n",
      "Epoch 17, Iteration 0 Train Loss: 18926.92\n",
      "Epoch 17, Iteration 10 Train Loss: 143.73\n",
      "Epoch 17, Iteration 20 Train Loss: 74.31\n",
      "Epoch 17, Iteration 30 Train Loss: 103.89\n",
      "Epoch 17, Iteration 40 Train Loss: 92.30\n",
      "Epoch 17, Iteration 50 Train Loss: 75.80\n",
      "Epoch 17, Iteration 60 Train Loss: 101.11\n",
      "Epoch 17, Iteration 70 Train Loss: 500.30\n",
      "Epoch 17, Iteration 80 Train Loss: 69.43\n",
      "Epoch 17, Iteration 90 Train Loss: 292.53\n",
      "Epoch 17, Iteration 100 Train Loss: 88.93\n",
      "Epoch 17, Iteration 110 Train Loss: 69.43\n",
      "Epoch 17, Iteration 120 Train Loss: 60.83\n",
      "Epoch 17, Iteration 130 Train Loss: 64.80\n",
      "Epoch 17, Iteration 140 Train Loss: 69.87\n",
      "Epoch 17, Iteration 150 Train Loss: 50.58\n",
      "Epoch 17, Iteration 160 Train Loss: 43.79\n",
      "Epoch 17, Iteration 170 Train Loss: 73.09\n",
      "Epoch 17, Iteration 180 Train Loss: 67.12\n",
      "Epoch 17, Iteration 190 Train Loss: 79.01\n",
      "Epoch 17, Iteration 200 Train Loss: 91.34\n",
      "Epoch 17, Iteration 210 Train Loss: 83.63\n",
      "Epoch 17, Iteration 220 Train Loss: 58.36\n",
      "Epoch 17, Iteration 230 Train Loss: 72.59\n",
      "Epoch 17, Iteration 240 Train Loss: 72.33\n",
      "Epoch 17, Iteration 250 Train Loss: 83.38\n",
      "Epoch 17, Iteration 260 Train Loss: 73.87\n",
      "Epoch 17, Iteration 270 Train Loss: 106.64\n",
      "New Epoch! Avg loss for the last 100 iterations: 84.99554859161377\n",
      "Epoch 18, Iteration 0 Train Loss: 18894.90\n",
      "Epoch 18, Iteration 10 Train Loss: 139.14\n",
      "Epoch 18, Iteration 20 Train Loss: 76.02\n",
      "Epoch 18, Iteration 30 Train Loss: 105.57\n",
      "Epoch 18, Iteration 40 Train Loss: 93.98\n",
      "Epoch 18, Iteration 50 Train Loss: 77.47\n",
      "Epoch 18, Iteration 60 Train Loss: 102.78\n",
      "Epoch 18, Iteration 70 Train Loss: 481.44\n",
      "Epoch 18, Iteration 80 Train Loss: 71.11\n",
      "Epoch 18, Iteration 90 Train Loss: 273.46\n",
      "Epoch 18, Iteration 100 Train Loss: 90.42\n",
      "Epoch 18, Iteration 110 Train Loss: 70.87\n",
      "Epoch 18, Iteration 120 Train Loss: 62.23\n",
      "Epoch 18, Iteration 130 Train Loss: 66.12\n",
      "Epoch 18, Iteration 140 Train Loss: 71.08\n",
      "Epoch 18, Iteration 150 Train Loss: 51.71\n",
      "Epoch 18, Iteration 160 Train Loss: 44.92\n",
      "Epoch 18, Iteration 170 Train Loss: 74.18\n",
      "Epoch 18, Iteration 180 Train Loss: 68.16\n",
      "Epoch 18, Iteration 190 Train Loss: 79.99\n",
      "Epoch 18, Iteration 200 Train Loss: 93.26\n",
      "Epoch 18, Iteration 210 Train Loss: 84.53\n",
      "Epoch 18, Iteration 220 Train Loss: 60.22\n",
      "Epoch 18, Iteration 230 Train Loss: 73.51\n",
      "Epoch 18, Iteration 240 Train Loss: 73.25\n",
      "Epoch 18, Iteration 250 Train Loss: 84.34\n",
      "Epoch 18, Iteration 260 Train Loss: 75.31\n",
      "Epoch 18, Iteration 270 Train Loss: 107.72\n",
      "New Epoch! Avg loss for the last 100 iterations: 86.41824733734131\n",
      "Epoch 19, Iteration 0 Train Loss: 18859.54\n",
      "Epoch 19, Iteration 10 Train Loss: 139.52\n",
      "Epoch 19, Iteration 20 Train Loss: 77.13\n",
      "Epoch 19, Iteration 30 Train Loss: 106.69\n",
      "Epoch 19, Iteration 40 Train Loss: 95.08\n",
      "Epoch 19, Iteration 50 Train Loss: 78.55\n",
      "Epoch 19, Iteration 60 Train Loss: 103.87\n",
      "Epoch 19, Iteration 70 Train Loss: 467.86\n",
      "Epoch 19, Iteration 80 Train Loss: 72.21\n",
      "Epoch 19, Iteration 90 Train Loss: 255.75\n",
      "Epoch 19, Iteration 100 Train Loss: 91.72\n",
      "Epoch 19, Iteration 110 Train Loss: 72.24\n",
      "Epoch 19, Iteration 120 Train Loss: 63.66\n",
      "Epoch 19, Iteration 130 Train Loss: 67.60\n",
      "Epoch 19, Iteration 140 Train Loss: 72.61\n",
      "Epoch 19, Iteration 150 Train Loss: 53.28\n",
      "Epoch 19, Iteration 160 Train Loss: 46.50\n",
      "Epoch 19, Iteration 170 Train Loss: 75.76\n",
      "Epoch 19, Iteration 180 Train Loss: 69.75\n",
      "Epoch 19, Iteration 190 Train Loss: 81.61\n",
      "Epoch 19, Iteration 200 Train Loss: 94.41\n",
      "Epoch 19, Iteration 210 Train Loss: 86.18\n",
      "Epoch 19, Iteration 220 Train Loss: 61.40\n",
      "Epoch 19, Iteration 230 Train Loss: 75.13\n",
      "Epoch 19, Iteration 240 Train Loss: 74.87\n",
      "Epoch 19, Iteration 250 Train Loss: 85.91\n",
      "Epoch 19, Iteration 260 Train Loss: 76.71\n",
      "Epoch 19, Iteration 270 Train Loss: 109.22\n",
      "New Epoch! Avg loss for the last 100 iterations: 87.81723182678223\n",
      "Epoch 20, Iteration 0 Train Loss: 18828.78\n",
      "Epoch 20, Iteration 10 Train Loss: 137.14\n",
      "Epoch 20, Iteration 20 Train Loss: 78.78\n",
      "Epoch 20, Iteration 30 Train Loss: 108.18\n",
      "Epoch 20, Iteration 40 Train Loss: 96.57\n",
      "Epoch 20, Iteration 50 Train Loss: 80.05\n",
      "Epoch 20, Iteration 60 Train Loss: 105.36\n",
      "Epoch 20, Iteration 70 Train Loss: 454.83\n",
      "Epoch 20, Iteration 80 Train Loss: 73.71\n",
      "Epoch 20, Iteration 90 Train Loss: 242.06\n",
      "Epoch 20, Iteration 100 Train Loss: 93.18\n",
      "Epoch 20, Iteration 110 Train Loss: 73.67\n",
      "Epoch 20, Iteration 120 Train Loss: 65.08\n",
      "Epoch 20, Iteration 130 Train Loss: 68.98\n",
      "Epoch 20, Iteration 140 Train Loss: 73.98\n",
      "Epoch 20, Iteration 150 Train Loss: 54.63\n",
      "Epoch 20, Iteration 160 Train Loss: 47.83\n",
      "Epoch 20, Iteration 170 Train Loss: 77.08\n",
      "Epoch 20, Iteration 180 Train Loss: 71.07\n",
      "Epoch 20, Iteration 190 Train Loss: 82.93\n",
      "Epoch 20, Iteration 200 Train Loss: 95.64\n",
      "Epoch 20, Iteration 210 Train Loss: 87.50\n",
      "Epoch 20, Iteration 220 Train Loss: 62.63\n",
      "Epoch 20, Iteration 230 Train Loss: 76.44\n",
      "Epoch 20, Iteration 240 Train Loss: 76.18\n",
      "Epoch 20, Iteration 250 Train Loss: 87.11\n",
      "Epoch 20, Iteration 260 Train Loss: 77.64\n",
      "Epoch 20, Iteration 270 Train Loss: 110.42\n",
      "New Epoch! Avg loss for the last 100 iterations: 88.97119983673096\n",
      "Epoch 21, Iteration 0 Train Loss: 18803.87\n",
      "Epoch 21, Iteration 10 Train Loss: 135.66\n",
      "Epoch 21, Iteration 20 Train Loss: 80.98\n",
      "Epoch 21, Iteration 30 Train Loss: 110.52\n",
      "Epoch 21, Iteration 40 Train Loss: 98.90\n",
      "Epoch 21, Iteration 50 Train Loss: 82.37\n",
      "Epoch 21, Iteration 60 Train Loss: 107.67\n",
      "Epoch 21, Iteration 70 Train Loss: 443.54\n",
      "Epoch 21, Iteration 80 Train Loss: 75.95\n",
      "Epoch 21, Iteration 90 Train Loss: 231.57\n",
      "Epoch 21, Iteration 100 Train Loss: 95.41\n",
      "Epoch 21, Iteration 110 Train Loss: 75.89\n",
      "Epoch 21, Iteration 120 Train Loss: 67.28\n",
      "Epoch 21, Iteration 130 Train Loss: 71.19\n",
      "Epoch 21, Iteration 140 Train Loss: 76.18\n",
      "Epoch 21, Iteration 150 Train Loss: 56.83\n",
      "Epoch 21, Iteration 160 Train Loss: 50.00\n",
      "Epoch 21, Iteration 170 Train Loss: 79.22\n",
      "Epoch 21, Iteration 180 Train Loss: 73.18\n",
      "Epoch 21, Iteration 190 Train Loss: 84.58\n",
      "Epoch 21, Iteration 200 Train Loss: 96.35\n",
      "Epoch 21, Iteration 210 Train Loss: 88.57\n",
      "Epoch 21, Iteration 220 Train Loss: 63.32\n",
      "Epoch 21, Iteration 230 Train Loss: 77.50\n",
      "Epoch 21, Iteration 240 Train Loss: 77.22\n",
      "Epoch 21, Iteration 250 Train Loss: 88.26\n",
      "Epoch 21, Iteration 260 Train Loss: 78.58\n",
      "Epoch 21, Iteration 270 Train Loss: 111.51\n",
      "New Epoch! Avg loss for the last 100 iterations: 90.02913715362548\n",
      "Epoch 22, Iteration 0 Train Loss: 18782.75\n",
      "Epoch 22, Iteration 10 Train Loss: 139.82\n",
      "Epoch 22, Iteration 20 Train Loss: 81.98\n",
      "Epoch 22, Iteration 30 Train Loss: 111.61\n",
      "Epoch 22, Iteration 40 Train Loss: 99.99\n",
      "Epoch 22, Iteration 50 Train Loss: 83.46\n",
      "Epoch 22, Iteration 60 Train Loss: 108.78\n",
      "Epoch 22, Iteration 70 Train Loss: 432.48\n",
      "Epoch 22, Iteration 80 Train Loss: 77.11\n",
      "Epoch 22, Iteration 90 Train Loss: 220.20\n",
      "Epoch 22, Iteration 100 Train Loss: 96.59\n",
      "Epoch 22, Iteration 110 Train Loss: 77.09\n",
      "Epoch 22, Iteration 120 Train Loss: 68.51\n",
      "Epoch 22, Iteration 130 Train Loss: 72.42\n",
      "Epoch 22, Iteration 140 Train Loss: 77.41\n",
      "Epoch 22, Iteration 150 Train Loss: 58.06\n",
      "Epoch 22, Iteration 160 Train Loss: 51.23\n",
      "Epoch 22, Iteration 170 Train Loss: 80.48\n",
      "Epoch 22, Iteration 180 Train Loss: 74.46\n",
      "Epoch 22, Iteration 190 Train Loss: 86.31\n",
      "Epoch 22, Iteration 200 Train Loss: 97.43\n",
      "Epoch 22, Iteration 210 Train Loss: 90.76\n",
      "Epoch 22, Iteration 220 Train Loss: 64.42\n",
      "Epoch 22, Iteration 230 Train Loss: 78.71\n",
      "Epoch 22, Iteration 240 Train Loss: 78.43\n",
      "Epoch 22, Iteration 250 Train Loss: 89.46\n",
      "Epoch 22, Iteration 260 Train Loss: 79.77\n",
      "Epoch 22, Iteration 270 Train Loss: 112.70\n",
      "New Epoch! Avg loss for the last 100 iterations: 91.42187995910645\n",
      "Epoch 23, Iteration 0 Train Loss: 18759.47\n",
      "Epoch 23, Iteration 10 Train Loss: 139.10\n",
      "Epoch 23, Iteration 20 Train Loss: 82.53\n",
      "Epoch 23, Iteration 30 Train Loss: 111.73\n",
      "Epoch 23, Iteration 40 Train Loss: 100.10\n",
      "Epoch 23, Iteration 50 Train Loss: 83.56\n",
      "Epoch 23, Iteration 60 Train Loss: 109.42\n",
      "Epoch 23, Iteration 70 Train Loss: 421.11\n",
      "Epoch 23, Iteration 80 Train Loss: 77.03\n",
      "Epoch 23, Iteration 90 Train Loss: 205.91\n",
      "Epoch 23, Iteration 100 Train Loss: 96.49\n",
      "Epoch 23, Iteration 110 Train Loss: 76.96\n",
      "Epoch 23, Iteration 120 Train Loss: 68.34\n",
      "Epoch 23, Iteration 130 Train Loss: 72.20\n",
      "Epoch 23, Iteration 140 Train Loss: 77.15\n",
      "Epoch 23, Iteration 150 Train Loss: 57.77\n",
      "Epoch 23, Iteration 160 Train Loss: 51.69\n",
      "Epoch 23, Iteration 170 Train Loss: 80.37\n",
      "Epoch 23, Iteration 180 Train Loss: 74.34\n",
      "Epoch 23, Iteration 190 Train Loss: 86.15\n",
      "Epoch 23, Iteration 200 Train Loss: 99.62\n",
      "Epoch 23, Iteration 210 Train Loss: 90.40\n",
      "Epoch 23, Iteration 220 Train Loss: 66.34\n",
      "Epoch 23, Iteration 230 Train Loss: 79.19\n",
      "Epoch 23, Iteration 240 Train Loss: 78.86\n",
      "Epoch 23, Iteration 250 Train Loss: 89.90\n",
      "Epoch 23, Iteration 260 Train Loss: 80.81\n",
      "Epoch 23, Iteration 270 Train Loss: 113.26\n",
      "New Epoch! Avg loss for the last 100 iterations: 92.34595840454102\n",
      "Epoch 24, Iteration 0 Train Loss: 18736.72\n",
      "Epoch 24, Iteration 10 Train Loss: 144.77\n",
      "Epoch 24, Iteration 20 Train Loss: 83.35\n",
      "Epoch 24, Iteration 30 Train Loss: 112.96\n",
      "Epoch 24, Iteration 40 Train Loss: 101.36\n",
      "Epoch 24, Iteration 50 Train Loss: 84.81\n",
      "Epoch 24, Iteration 60 Train Loss: 110.13\n",
      "Epoch 24, Iteration 70 Train Loss: 412.96\n",
      "Epoch 24, Iteration 80 Train Loss: 79.53\n",
      "Epoch 24, Iteration 90 Train Loss: 201.03\n",
      "Epoch 24, Iteration 100 Train Loss: 97.90\n",
      "Epoch 24, Iteration 110 Train Loss: 78.37\n",
      "Epoch 24, Iteration 120 Train Loss: 69.70\n",
      "Epoch 24, Iteration 130 Train Loss: 73.57\n",
      "Epoch 24, Iteration 140 Train Loss: 78.52\n",
      "Epoch 24, Iteration 150 Train Loss: 59.16\n",
      "Epoch 24, Iteration 160 Train Loss: 53.06\n",
      "Epoch 24, Iteration 170 Train Loss: 81.62\n",
      "Epoch 24, Iteration 180 Train Loss: 75.68\n",
      "Epoch 24, Iteration 190 Train Loss: 87.42\n",
      "Epoch 24, Iteration 200 Train Loss: 99.11\n",
      "Epoch 24, Iteration 210 Train Loss: 91.93\n",
      "Epoch 24, Iteration 220 Train Loss: 66.07\n",
      "Epoch 24, Iteration 230 Train Loss: 80.80\n",
      "Epoch 24, Iteration 240 Train Loss: 80.51\n",
      "Epoch 24, Iteration 250 Train Loss: 91.52\n",
      "Epoch 24, Iteration 260 Train Loss: 81.42\n",
      "Epoch 24, Iteration 270 Train Loss: 114.67\n",
      "New Epoch! Avg loss for the last 100 iterations: 92.9377163696289\n",
      "Oracle activations: 13933\n",
      "Runtime total_E activations: 13178.756812865347\n",
      "Runtime total_E_pred activations: 116487.56229749684\n",
      "Naive total_E activations: 13178.756812865347\n",
      "Naive total_E_pred activations: 5153.631019665272\n",
      "Dataset, train set, and test set size: 2945 2209 368\n",
      "Timeframe: 60min\n",
      "Minimal Application\n",
      "Naive vs. DL succesful activations: 0.0\n",
      "Maximum possible activations: 13933\n",
      "Predicted activations: 117165\n",
      "Successful activations: 0, 0.000%\n",
      "Failed activations: 96311, 82.201%\n",
      "Missed activations: 13933, 100.000%\n",
      "Naive predicted activations (usual actual energy average): 13729\n",
      "Naive successful activations (usual actual energy average): 12324, 89.766%\n",
      "Naive failed activations (usual actual energy average): 1405, 10.234%\n",
      "Naive missed activations (usual actual energy average): 1609, 11.548%\n",
      "Voltage overestimation rate: 100.000%\n",
      "Test MAPE power: 1.523185\n",
      "Test MAPE voltage: 3.003681\n",
      "Test MAPE current: 0.344037\n",
      "Epoch 0, Iteration 0 Train Loss: 22023.83\n",
      "Epoch 0, Iteration 10 Train Loss: 2213.21\n",
      "Epoch 0, Iteration 20 Train Loss: 1748.05\n",
      "Epoch 0, Iteration 30 Train Loss: 1194.54\n",
      "Epoch 0, Iteration 40 Train Loss: 1410.26\n",
      "Epoch 0, Iteration 50 Train Loss: 1714.53\n",
      "Epoch 0, Iteration 60 Train Loss: 1230.16\n",
      "Epoch 0, Iteration 70 Train Loss: 2960.65\n",
      "Epoch 0, Iteration 80 Train Loss: 1829.65\n",
      "Epoch 0, Iteration 90 Train Loss: 2247.15\n",
      "Epoch 0, Iteration 100 Train Loss: 1444.37\n",
      "Epoch 0, Iteration 110 Train Loss: 1799.76\n",
      "Epoch 0, Iteration 120 Train Loss: 1941.94\n",
      "Epoch 0, Iteration 130 Train Loss: 1831.85\n",
      "Epoch 0, Iteration 140 Train Loss: 1673.40\n",
      "Epoch 0, Iteration 150 Train Loss: 1922.59\n",
      "Epoch 0, Iteration 160 Train Loss: 1789.55\n",
      "Epoch 0, Iteration 170 Train Loss: 1081.50\n",
      "Epoch 0, Iteration 180 Train Loss: 1149.89\n",
      "Epoch 0, Iteration 190 Train Loss: 891.02\n",
      "Epoch 0, Iteration 200 Train Loss: 730.94\n",
      "Epoch 0, Iteration 210 Train Loss: 729.80\n",
      "Epoch 0, Iteration 220 Train Loss: 1096.27\n",
      "Epoch 0, Iteration 230 Train Loss: 816.83\n",
      "Epoch 0, Iteration 240 Train Loss: 780.73\n",
      "Epoch 0, Iteration 250 Train Loss: 584.31\n",
      "Epoch 0, Iteration 260 Train Loss: 720.96\n",
      "Epoch 0, Iteration 270 Train Loss: 246.01\n",
      "Epoch 0, Iteration 280 Train Loss: 467.69\n",
      "Epoch 0, Iteration 290 Train Loss: 352.01\n",
      "Epoch 0, Iteration 300 Train Loss: 244.67\n",
      "Epoch 0, Iteration 310 Train Loss: 350.94\n",
      "Epoch 0, Iteration 320 Train Loss: 412.26\n",
      "Epoch 0, Iteration 330 Train Loss: 141.12\n",
      "Epoch 0, Iteration 340 Train Loss: 363.19\n",
      "Epoch 0, Iteration 350 Train Loss: 585.64\n",
      "Epoch 0, Iteration 360 Train Loss: 460.53\n",
      "New Epoch! Avg loss for the last 100 iterations: 375.8723070526123\n",
      "Epoch 1, Iteration 0 Train Loss: 20707.68\n",
      "Epoch 1, Iteration 10 Train Loss: 926.20\n",
      "Epoch 1, Iteration 20 Train Loss: 438.06\n",
      "Epoch 1, Iteration 30 Train Loss: 149.61\n",
      "Epoch 1, Iteration 40 Train Loss: 322.64\n",
      "Epoch 1, Iteration 50 Train Loss: 351.71\n",
      "Epoch 1, Iteration 60 Train Loss: 322.11\n",
      "Epoch 1, Iteration 70 Train Loss: 1645.11\n",
      "Epoch 1, Iteration 80 Train Loss: 415.71\n",
      "Epoch 1, Iteration 90 Train Loss: 1157.82\n",
      "Epoch 1, Iteration 100 Train Loss: 249.03\n",
      "Epoch 1, Iteration 110 Train Loss: 341.90\n",
      "Epoch 1, Iteration 120 Train Loss: 746.14\n",
      "Epoch 1, Iteration 130 Train Loss: 551.96\n",
      "Epoch 1, Iteration 140 Train Loss: 278.60\n",
      "Epoch 1, Iteration 150 Train Loss: 718.02\n",
      "Epoch 1, Iteration 160 Train Loss: 676.80\n",
      "Epoch 1, Iteration 170 Train Loss: 188.40\n",
      "Epoch 1, Iteration 180 Train Loss: 468.57\n",
      "Epoch 1, Iteration 190 Train Loss: 100.83\n",
      "Epoch 1, Iteration 200 Train Loss: 210.36\n",
      "Epoch 1, Iteration 210 Train Loss: 91.60\n",
      "Epoch 1, Iteration 220 Train Loss: 314.28\n",
      "Epoch 1, Iteration 230 Train Loss: 104.76\n",
      "Epoch 1, Iteration 240 Train Loss: 76.42\n",
      "Epoch 1, Iteration 250 Train Loss: 131.71\n",
      "Epoch 1, Iteration 260 Train Loss: 142.47\n",
      "Epoch 1, Iteration 270 Train Loss: 40.01\n",
      "Epoch 1, Iteration 280 Train Loss: 139.79\n",
      "Epoch 1, Iteration 290 Train Loss: 84.48\n",
      "Epoch 1, Iteration 300 Train Loss: 37.08\n",
      "Epoch 1, Iteration 310 Train Loss: 25.88\n",
      "Epoch 1, Iteration 320 Train Loss: 55.12\n",
      "Epoch 1, Iteration 330 Train Loss: 35.56\n",
      "Epoch 1, Iteration 340 Train Loss: 88.89\n",
      "Epoch 1, Iteration 350 Train Loss: 100.95\n",
      "Epoch 1, Iteration 360 Train Loss: 77.28\n",
      "New Epoch! Avg loss for the last 100 iterations: 89.15602685928344\n",
      "Epoch 2, Iteration 0 Train Loss: 20102.08\n",
      "Epoch 2, Iteration 10 Train Loss: 564.00\n",
      "Epoch 2, Iteration 20 Train Loss: 62.82\n",
      "Epoch 2, Iteration 30 Train Loss: 40.90\n",
      "Epoch 2, Iteration 40 Train Loss: 77.57\n",
      "Epoch 2, Iteration 50 Train Loss: 46.51\n",
      "Epoch 2, Iteration 60 Train Loss: 179.31\n",
      "Epoch 2, Iteration 70 Train Loss: 1367.80\n",
      "Epoch 2, Iteration 80 Train Loss: 162.36\n",
      "Epoch 2, Iteration 90 Train Loss: 932.10\n",
      "Epoch 2, Iteration 100 Train Loss: 89.64\n",
      "Epoch 2, Iteration 110 Train Loss: 27.99\n",
      "Epoch 2, Iteration 120 Train Loss: 521.06\n",
      "Epoch 2, Iteration 130 Train Loss: 333.74\n",
      "Epoch 2, Iteration 140 Train Loss: 32.73\n",
      "Epoch 2, Iteration 150 Train Loss: 493.85\n",
      "Epoch 2, Iteration 160 Train Loss: 419.38\n",
      "Epoch 2, Iteration 170 Train Loss: 39.00\n",
      "Epoch 2, Iteration 180 Train Loss: 334.17\n",
      "Epoch 2, Iteration 190 Train Loss: 24.81\n",
      "Epoch 2, Iteration 200 Train Loss: 84.70\n",
      "Epoch 2, Iteration 210 Train Loss: 31.18\n",
      "Epoch 2, Iteration 220 Train Loss: 68.17\n",
      "Epoch 2, Iteration 230 Train Loss: 30.52\n",
      "Epoch 2, Iteration 240 Train Loss: 23.98\n",
      "Epoch 2, Iteration 250 Train Loss: 61.79\n",
      "Epoch 2, Iteration 260 Train Loss: 51.89\n",
      "Epoch 2, Iteration 270 Train Loss: 55.54\n",
      "Epoch 2, Iteration 280 Train Loss: 56.91\n",
      "Epoch 2, Iteration 290 Train Loss: 68.27\n",
      "Epoch 2, Iteration 300 Train Loss: 49.18\n",
      "Epoch 2, Iteration 310 Train Loss: 38.90\n",
      "Epoch 2, Iteration 320 Train Loss: 34.08\n",
      "Epoch 2, Iteration 330 Train Loss: 49.63\n",
      "Epoch 2, Iteration 340 Train Loss: 58.22\n",
      "Epoch 2, Iteration 350 Train Loss: 18.34\n",
      "Epoch 2, Iteration 360 Train Loss: 25.26\n",
      "New Epoch! Avg loss for the last 100 iterations: 65.99074178695679\n",
      "Epoch 3, Iteration 0 Train Loss: 19854.02\n",
      "Epoch 3, Iteration 10 Train Loss: 465.28\n",
      "Epoch 3, Iteration 20 Train Loss: 24.05\n",
      "Epoch 3, Iteration 30 Train Loss: 53.81\n",
      "Epoch 3, Iteration 40 Train Loss: 42.47\n",
      "Epoch 3, Iteration 50 Train Loss: 26.59\n",
      "Epoch 3, Iteration 60 Train Loss: 111.19\n",
      "Epoch 3, Iteration 70 Train Loss: 1217.18\n",
      "Epoch 3, Iteration 80 Train Loss: 45.57\n",
      "Epoch 3, Iteration 90 Train Loss: 803.26\n",
      "Epoch 3, Iteration 100 Train Loss: 42.59\n",
      "Epoch 3, Iteration 110 Train Loss: 21.60\n",
      "Epoch 3, Iteration 120 Train Loss: 386.45\n",
      "Epoch 3, Iteration 130 Train Loss: 229.53\n",
      "Epoch 3, Iteration 140 Train Loss: 23.26\n",
      "Epoch 3, Iteration 150 Train Loss: 365.95\n",
      "Epoch 3, Iteration 160 Train Loss: 292.80\n",
      "Epoch 3, Iteration 170 Train Loss: 28.43\n",
      "Epoch 3, Iteration 180 Train Loss: 260.05\n",
      "Epoch 3, Iteration 190 Train Loss: 34.50\n",
      "Epoch 3, Iteration 200 Train Loss: 58.85\n",
      "Epoch 3, Iteration 210 Train Loss: 39.52\n",
      "Epoch 3, Iteration 220 Train Loss: 28.00\n",
      "Epoch 3, Iteration 230 Train Loss: 28.84\n",
      "Epoch 3, Iteration 240 Train Loss: 28.82\n",
      "Epoch 3, Iteration 250 Train Loss: 49.30\n",
      "Epoch 3, Iteration 260 Train Loss: 39.54\n",
      "Epoch 3, Iteration 270 Train Loss: 63.49\n",
      "Epoch 3, Iteration 280 Train Loss: 50.44\n",
      "Epoch 3, Iteration 290 Train Loss: 65.94\n",
      "Epoch 3, Iteration 300 Train Loss: 56.60\n",
      "Epoch 3, Iteration 310 Train Loss: 46.18\n",
      "Epoch 3, Iteration 320 Train Loss: 41.21\n",
      "Epoch 3, Iteration 330 Train Loss: 56.79\n",
      "Epoch 3, Iteration 340 Train Loss: 61.31\n",
      "Epoch 3, Iteration 350 Train Loss: 25.44\n",
      "Epoch 3, Iteration 360 Train Loss: 32.29\n",
      "New Epoch! Avg loss for the last 100 iterations: 64.89345724105834\n",
      "Epoch 4, Iteration 0 Train Loss: 19721.64\n",
      "Epoch 4, Iteration 10 Train Loss: 412.13\n",
      "Epoch 4, Iteration 20 Train Loss: 31.36\n",
      "Epoch 4, Iteration 30 Train Loss: 61.17\n",
      "Epoch 4, Iteration 40 Train Loss: 49.81\n",
      "Epoch 4, Iteration 50 Train Loss: 33.53\n",
      "Epoch 4, Iteration 60 Train Loss: 75.06\n",
      "Epoch 4, Iteration 70 Train Loss: 1106.81\n",
      "Epoch 4, Iteration 80 Train Loss: 27.75\n",
      "Epoch 4, Iteration 90 Train Loss: 725.75\n",
      "Epoch 4, Iteration 100 Train Loss: 47.77\n",
      "Epoch 4, Iteration 110 Train Loss: 28.60\n",
      "Epoch 4, Iteration 120 Train Loss: 302.18\n",
      "Epoch 4, Iteration 130 Train Loss: 169.07\n",
      "Epoch 4, Iteration 140 Train Loss: 30.11\n",
      "Epoch 4, Iteration 150 Train Loss: 263.93\n",
      "Epoch 4, Iteration 160 Train Loss: 194.23\n",
      "Epoch 4, Iteration 170 Train Loss: 34.65\n",
      "Epoch 4, Iteration 180 Train Loss: 207.24\n",
      "Epoch 4, Iteration 190 Train Loss: 41.23\n",
      "Epoch 4, Iteration 200 Train Loss: 57.62\n",
      "Epoch 4, Iteration 210 Train Loss: 46.26\n",
      "Epoch 4, Iteration 220 Train Loss: 25.34\n",
      "Epoch 4, Iteration 230 Train Loss: 35.47\n",
      "Epoch 4, Iteration 240 Train Loss: 35.40\n",
      "Epoch 4, Iteration 250 Train Loss: 50.20\n",
      "Epoch 4, Iteration 260 Train Loss: 40.75\n",
      "Epoch 4, Iteration 270 Train Loss: 69.91\n",
      "Epoch 4, Iteration 280 Train Loss: 56.82\n",
      "Epoch 4, Iteration 290 Train Loss: 69.60\n",
      "Epoch 4, Iteration 300 Train Loss: 62.98\n",
      "Epoch 4, Iteration 310 Train Loss: 52.56\n",
      "Epoch 4, Iteration 320 Train Loss: 47.63\n",
      "Epoch 4, Iteration 330 Train Loss: 63.14\n",
      "Epoch 4, Iteration 340 Train Loss: 65.77\n",
      "Epoch 4, Iteration 350 Train Loss: 31.67\n",
      "Epoch 4, Iteration 360 Train Loss: 38.47\n",
      "New Epoch! Avg loss for the last 100 iterations: 66.2963768196106\n",
      "Epoch 5, Iteration 0 Train Loss: 19604.55\n",
      "Epoch 5, Iteration 10 Train Loss: 361.54\n",
      "Epoch 5, Iteration 20 Train Loss: 37.58\n",
      "Epoch 5, Iteration 30 Train Loss: 67.27\n",
      "Epoch 5, Iteration 40 Train Loss: 55.84\n",
      "Epoch 5, Iteration 50 Train Loss: 39.51\n",
      "Epoch 5, Iteration 60 Train Loss: 68.81\n",
      "Epoch 5, Iteration 70 Train Loss: 1013.91\n",
      "Epoch 5, Iteration 80 Train Loss: 33.58\n",
      "Epoch 5, Iteration 90 Train Loss: 671.24\n",
      "Epoch 5, Iteration 100 Train Loss: 53.45\n",
      "Epoch 5, Iteration 110 Train Loss: 34.20\n",
      "Epoch 5, Iteration 120 Train Loss: 247.65\n",
      "Epoch 5, Iteration 130 Train Loss: 129.09\n",
      "Epoch 5, Iteration 140 Train Loss: 35.60\n",
      "Epoch 5, Iteration 150 Train Loss: 208.04\n",
      "Epoch 5, Iteration 160 Train Loss: 131.19\n",
      "Epoch 5, Iteration 170 Train Loss: 39.76\n",
      "Epoch 5, Iteration 180 Train Loss: 167.03\n",
      "Epoch 5, Iteration 190 Train Loss: 46.19\n",
      "Epoch 5, Iteration 200 Train Loss: 61.88\n",
      "Epoch 5, Iteration 210 Train Loss: 51.09\n",
      "Epoch 5, Iteration 220 Train Loss: 29.00\n",
      "Epoch 5, Iteration 230 Train Loss: 40.25\n",
      "Epoch 5, Iteration 240 Train Loss: 40.15\n",
      "Epoch 5, Iteration 250 Train Loss: 53.59\n",
      "Epoch 5, Iteration 260 Train Loss: 44.09\n",
      "Epoch 5, Iteration 270 Train Loss: 74.73\n",
      "Epoch 5, Iteration 280 Train Loss: 61.62\n",
      "Epoch 5, Iteration 290 Train Loss: 73.43\n",
      "Epoch 5, Iteration 300 Train Loss: 67.70\n",
      "Epoch 5, Iteration 310 Train Loss: 57.24\n",
      "Epoch 5, Iteration 320 Train Loss: 52.26\n",
      "Epoch 5, Iteration 330 Train Loss: 67.80\n",
      "Epoch 5, Iteration 340 Train Loss: 69.75\n",
      "Epoch 5, Iteration 350 Train Loss: 36.27\n",
      "Epoch 5, Iteration 360 Train Loss: 43.03\n",
      "New Epoch! Avg loss for the last 100 iterations: 67.85045776367187\n",
      "Epoch 6, Iteration 0 Train Loss: 19518.38\n",
      "Epoch 6, Iteration 10 Train Loss: 322.10\n",
      "Epoch 6, Iteration 20 Train Loss: 42.37\n",
      "Epoch 6, Iteration 30 Train Loss: 72.06\n",
      "Epoch 6, Iteration 40 Train Loss: 60.60\n",
      "Epoch 6, Iteration 50 Train Loss: 44.23\n",
      "Epoch 6, Iteration 60 Train Loss: 69.67\n",
      "Epoch 6, Iteration 70 Train Loss: 932.71\n",
      "Epoch 6, Iteration 80 Train Loss: 38.21\n",
      "Epoch 6, Iteration 90 Train Loss: 622.71\n",
      "Epoch 6, Iteration 100 Train Loss: 58.04\n",
      "Epoch 6, Iteration 110 Train Loss: 38.76\n",
      "Epoch 6, Iteration 120 Train Loss: 203.06\n",
      "Epoch 6, Iteration 130 Train Loss: 101.78\n",
      "Epoch 6, Iteration 140 Train Loss: 40.07\n",
      "Epoch 6, Iteration 150 Train Loss: 166.09\n",
      "Epoch 6, Iteration 160 Train Loss: 89.34\n",
      "Epoch 6, Iteration 170 Train Loss: 44.19\n",
      "Epoch 6, Iteration 180 Train Loss: 131.07\n",
      "Epoch 6, Iteration 190 Train Loss: 50.60\n",
      "Epoch 6, Iteration 200 Train Loss: 65.51\n",
      "Epoch 6, Iteration 210 Train Loss: 55.51\n",
      "Epoch 6, Iteration 220 Train Loss: 32.48\n",
      "Epoch 6, Iteration 230 Train Loss: 44.65\n",
      "Epoch 6, Iteration 240 Train Loss: 44.51\n",
      "Epoch 6, Iteration 250 Train Loss: 56.99\n",
      "Epoch 6, Iteration 260 Train Loss: 47.96\n",
      "Epoch 6, Iteration 270 Train Loss: 78.99\n",
      "Epoch 6, Iteration 280 Train Loss: 65.87\n",
      "Epoch 6, Iteration 290 Train Loss: 77.33\n",
      "Epoch 6, Iteration 300 Train Loss: 71.93\n",
      "Epoch 6, Iteration 310 Train Loss: 61.47\n",
      "Epoch 6, Iteration 320 Train Loss: 56.47\n",
      "Epoch 6, Iteration 330 Train Loss: 71.97\n",
      "Epoch 6, Iteration 340 Train Loss: 73.68\n",
      "Epoch 6, Iteration 350 Train Loss: 40.39\n",
      "Epoch 6, Iteration 360 Train Loss: 47.13\n",
      "New Epoch! Avg loss for the last 100 iterations: 69.33222686767579\n",
      "Epoch 7, Iteration 0 Train Loss: 19440.80\n",
      "Epoch 7, Iteration 10 Train Loss: 292.18\n",
      "Epoch 7, Iteration 20 Train Loss: 46.56\n",
      "Epoch 7, Iteration 30 Train Loss: 76.26\n",
      "Epoch 7, Iteration 40 Train Loss: 64.79\n",
      "Epoch 7, Iteration 50 Train Loss: 48.41\n",
      "Epoch 7, Iteration 60 Train Loss: 73.85\n",
      "Epoch 7, Iteration 70 Train Loss: 857.35\n",
      "Epoch 7, Iteration 80 Train Loss: 42.34\n",
      "Epoch 7, Iteration 90 Train Loss: 577.61\n",
      "Epoch 7, Iteration 100 Train Loss: 62.12\n",
      "Epoch 7, Iteration 110 Train Loss: 42.81\n",
      "Epoch 7, Iteration 120 Train Loss: 168.16\n",
      "Epoch 7, Iteration 130 Train Loss: 77.01\n",
      "Epoch 7, Iteration 140 Train Loss: 43.99\n",
      "Epoch 7, Iteration 150 Train Loss: 131.55\n",
      "Epoch 7, Iteration 160 Train Loss: 61.67\n",
      "Epoch 7, Iteration 170 Train Loss: 47.82\n",
      "Epoch 7, Iteration 180 Train Loss: 103.05\n",
      "Epoch 7, Iteration 190 Train Loss: 54.08\n",
      "Epoch 7, Iteration 200 Train Loss: 69.78\n",
      "Epoch 7, Iteration 210 Train Loss: 58.88\n",
      "Epoch 7, Iteration 220 Train Loss: 36.95\n",
      "Epoch 7, Iteration 230 Train Loss: 48.03\n",
      "Epoch 7, Iteration 240 Train Loss: 47.92\n",
      "Epoch 7, Iteration 250 Train Loss: 61.01\n",
      "Epoch 7, Iteration 260 Train Loss: 51.50\n",
      "Epoch 7, Iteration 270 Train Loss: 82.70\n",
      "Epoch 7, Iteration 280 Train Loss: 69.58\n",
      "Epoch 7, Iteration 290 Train Loss: 80.64\n",
      "Epoch 7, Iteration 300 Train Loss: 75.59\n",
      "Epoch 7, Iteration 310 Train Loss: 65.09\n",
      "Epoch 7, Iteration 320 Train Loss: 60.05\n",
      "Epoch 7, Iteration 330 Train Loss: 75.50\n",
      "Epoch 7, Iteration 340 Train Loss: 76.99\n",
      "Epoch 7, Iteration 350 Train Loss: 43.85\n",
      "Epoch 7, Iteration 360 Train Loss: 50.55\n",
      "New Epoch! Avg loss for the last 100 iterations: 71.06084827423096\n",
      "Epoch 8, Iteration 0 Train Loss: 19376.29\n",
      "Epoch 8, Iteration 10 Train Loss: 270.07\n",
      "Epoch 8, Iteration 20 Train Loss: 50.21\n",
      "Epoch 8, Iteration 30 Train Loss: 79.93\n",
      "Epoch 8, Iteration 40 Train Loss: 68.45\n",
      "Epoch 8, Iteration 50 Train Loss: 52.04\n",
      "Epoch 8, Iteration 60 Train Loss: 77.48\n",
      "Epoch 8, Iteration 70 Train Loss: 792.20\n",
      "Epoch 8, Iteration 80 Train Loss: 46.02\n",
      "Epoch 8, Iteration 90 Train Loss: 536.78\n",
      "Epoch 8, Iteration 100 Train Loss: 65.79\n",
      "Epoch 8, Iteration 110 Train Loss: 46.46\n",
      "Epoch 8, Iteration 120 Train Loss: 138.19\n",
      "Epoch 8, Iteration 130 Train Loss: 62.82\n",
      "Epoch 8, Iteration 140 Train Loss: 47.61\n",
      "Epoch 8, Iteration 150 Train Loss: 102.90\n",
      "Epoch 8, Iteration 160 Train Loss: 44.25\n",
      "Epoch 8, Iteration 170 Train Loss: 51.53\n",
      "Epoch 8, Iteration 180 Train Loss: 84.18\n",
      "Epoch 8, Iteration 190 Train Loss: 57.69\n",
      "Epoch 8, Iteration 200 Train Loss: 72.93\n",
      "Epoch 8, Iteration 210 Train Loss: 62.25\n",
      "Epoch 8, Iteration 220 Train Loss: 39.65\n",
      "Epoch 8, Iteration 230 Train Loss: 51.24\n",
      "Epoch 8, Iteration 240 Train Loss: 51.06\n",
      "Epoch 8, Iteration 250 Train Loss: 63.50\n",
      "Epoch 8, Iteration 260 Train Loss: 54.21\n",
      "Epoch 8, Iteration 270 Train Loss: 85.72\n",
      "Epoch 8, Iteration 280 Train Loss: 72.56\n",
      "Epoch 8, Iteration 290 Train Loss: 83.14\n",
      "Epoch 8, Iteration 300 Train Loss: 78.49\n",
      "Epoch 8, Iteration 310 Train Loss: 67.93\n",
      "Epoch 8, Iteration 320 Train Loss: 62.82\n",
      "Epoch 8, Iteration 330 Train Loss: 78.17\n",
      "Epoch 8, Iteration 340 Train Loss: 79.32\n",
      "Epoch 8, Iteration 350 Train Loss: 46.38\n",
      "Epoch 8, Iteration 360 Train Loss: 53.04\n",
      "New Epoch! Avg loss for the last 100 iterations: 73.08559143066407\n",
      "Epoch 9, Iteration 0 Train Loss: 19329.64\n",
      "Epoch 9, Iteration 10 Train Loss: 254.90\n",
      "Epoch 9, Iteration 20 Train Loss: 52.85\n",
      "Epoch 9, Iteration 30 Train Loss: 82.58\n",
      "Epoch 9, Iteration 40 Train Loss: 71.09\n",
      "Epoch 9, Iteration 50 Train Loss: 54.65\n",
      "Epoch 9, Iteration 60 Train Loss: 80.08\n",
      "Epoch 9, Iteration 70 Train Loss: 748.01\n",
      "Epoch 9, Iteration 80 Train Loss: 48.66\n",
      "Epoch 9, Iteration 90 Train Loss: 504.58\n",
      "Epoch 9, Iteration 100 Train Loss: 68.43\n",
      "Epoch 9, Iteration 110 Train Loss: 49.10\n",
      "Epoch 9, Iteration 120 Train Loss: 119.66\n",
      "Epoch 9, Iteration 130 Train Loss: 51.51\n",
      "Epoch 9, Iteration 140 Train Loss: 50.21\n",
      "Epoch 9, Iteration 150 Train Loss: 84.48\n",
      "Epoch 9, Iteration 160 Train Loss: 34.35\n",
      "Epoch 9, Iteration 170 Train Loss: 54.11\n",
      "Epoch 9, Iteration 180 Train Loss: 72.94\n",
      "Epoch 9, Iteration 190 Train Loss: 60.31\n",
      "Epoch 9, Iteration 200 Train Loss: 75.60\n",
      "Epoch 9, Iteration 210 Train Loss: 65.09\n",
      "Epoch 9, Iteration 220 Train Loss: 42.50\n",
      "Epoch 9, Iteration 230 Train Loss: 54.20\n",
      "Epoch 9, Iteration 240 Train Loss: 54.07\n",
      "Epoch 9, Iteration 250 Train Loss: 66.41\n",
      "Epoch 9, Iteration 260 Train Loss: 57.24\n",
      "Epoch 9, Iteration 270 Train Loss: 88.89\n",
      "Epoch 9, Iteration 280 Train Loss: 75.76\n",
      "Epoch 9, Iteration 290 Train Loss: 86.29\n",
      "Epoch 9, Iteration 300 Train Loss: 81.74\n",
      "Epoch 9, Iteration 310 Train Loss: 71.21\n",
      "Epoch 9, Iteration 320 Train Loss: 66.13\n",
      "Epoch 9, Iteration 330 Train Loss: 81.35\n",
      "Epoch 9, Iteration 340 Train Loss: 82.42\n",
      "Epoch 9, Iteration 350 Train Loss: 49.47\n",
      "Epoch 9, Iteration 360 Train Loss: 56.12\n",
      "New Epoch! Avg loss for the last 100 iterations: 75.68848857879638\n",
      "Epoch 10, Iteration 0 Train Loss: 19271.11\n",
      "Epoch 10, Iteration 10 Train Loss: 234.19\n",
      "Epoch 10, Iteration 20 Train Loss: 56.00\n",
      "Epoch 10, Iteration 30 Train Loss: 85.74\n",
      "Epoch 10, Iteration 40 Train Loss: 74.25\n",
      "Epoch 10, Iteration 50 Train Loss: 57.79\n",
      "Epoch 10, Iteration 60 Train Loss: 83.21\n",
      "Epoch 10, Iteration 70 Train Loss: 700.49\n",
      "Epoch 10, Iteration 80 Train Loss: 52.17\n",
      "Epoch 10, Iteration 90 Train Loss: 471.38\n",
      "Epoch 10, Iteration 100 Train Loss: 71.52\n",
      "Epoch 10, Iteration 110 Train Loss: 52.17\n",
      "Epoch 10, Iteration 120 Train Loss: 101.19\n",
      "Epoch 10, Iteration 130 Train Loss: 47.88\n",
      "Epoch 10, Iteration 140 Train Loss: 53.14\n",
      "Epoch 10, Iteration 150 Train Loss: 66.85\n",
      "Epoch 10, Iteration 160 Train Loss: 30.09\n",
      "Epoch 10, Iteration 170 Train Loss: 56.88\n",
      "Epoch 10, Iteration 180 Train Loss: 63.18\n",
      "Epoch 10, Iteration 190 Train Loss: 62.99\n",
      "Epoch 10, Iteration 200 Train Loss: 78.45\n",
      "Epoch 10, Iteration 210 Train Loss: 67.74\n",
      "Epoch 10, Iteration 220 Train Loss: 45.35\n",
      "Epoch 10, Iteration 230 Train Loss: 56.87\n",
      "Epoch 10, Iteration 240 Train Loss: 56.74\n",
      "Epoch 10, Iteration 250 Train Loss: 69.14\n",
      "Epoch 10, Iteration 260 Train Loss: 59.89\n",
      "Epoch 10, Iteration 270 Train Loss: 91.62\n",
      "Epoch 10, Iteration 280 Train Loss: 78.50\n",
      "Epoch 10, Iteration 290 Train Loss: 88.86\n",
      "Epoch 10, Iteration 300 Train Loss: 84.46\n",
      "Epoch 10, Iteration 310 Train Loss: 73.92\n",
      "Epoch 10, Iteration 320 Train Loss: 68.83\n",
      "Epoch 10, Iteration 330 Train Loss: 84.01\n",
      "Epoch 10, Iteration 340 Train Loss: 84.94\n",
      "Epoch 10, Iteration 350 Train Loss: 52.08\n",
      "Epoch 10, Iteration 360 Train Loss: 58.72\n",
      "New Epoch! Avg loss for the last 100 iterations: 78.20951030731202\n",
      "Epoch 11, Iteration 0 Train Loss: 19218.42\n",
      "Epoch 11, Iteration 10 Train Loss: 217.51\n",
      "Epoch 11, Iteration 20 Train Loss: 59.43\n",
      "Epoch 11, Iteration 30 Train Loss: 89.05\n",
      "Epoch 11, Iteration 40 Train Loss: 77.51\n",
      "Epoch 11, Iteration 50 Train Loss: 61.03\n",
      "Epoch 11, Iteration 60 Train Loss: 86.40\n",
      "Epoch 11, Iteration 70 Train Loss: 665.34\n",
      "Epoch 11, Iteration 80 Train Loss: 54.84\n",
      "Epoch 11, Iteration 90 Train Loss: 443.96\n",
      "Epoch 11, Iteration 100 Train Loss: 74.50\n",
      "Epoch 11, Iteration 110 Train Loss: 55.10\n",
      "Epoch 11, Iteration 120 Train Loss: 90.34\n",
      "Epoch 11, Iteration 130 Train Loss: 50.73\n",
      "Epoch 11, Iteration 140 Train Loss: 55.92\n",
      "Epoch 11, Iteration 150 Train Loss: 57.11\n",
      "Epoch 11, Iteration 160 Train Loss: 30.07\n",
      "Epoch 11, Iteration 170 Train Loss: 59.51\n",
      "Epoch 11, Iteration 180 Train Loss: 60.58\n",
      "Epoch 11, Iteration 190 Train Loss: 65.58\n",
      "Epoch 11, Iteration 200 Train Loss: 80.29\n",
      "Epoch 11, Iteration 210 Train Loss: 70.32\n",
      "Epoch 11, Iteration 220 Train Loss: 47.17\n",
      "Epoch 11, Iteration 230 Train Loss: 59.43\n",
      "Epoch 11, Iteration 240 Train Loss: 59.24\n",
      "Epoch 11, Iteration 250 Train Loss: 70.97\n",
      "Epoch 11, Iteration 260 Train Loss: 62.06\n",
      "Epoch 11, Iteration 270 Train Loss: 93.97\n",
      "Epoch 11, Iteration 280 Train Loss: 80.85\n",
      "Epoch 11, Iteration 290 Train Loss: 91.06\n",
      "Epoch 11, Iteration 300 Train Loss: 86.82\n",
      "Epoch 11, Iteration 310 Train Loss: 76.28\n",
      "Epoch 11, Iteration 320 Train Loss: 71.18\n",
      "Epoch 11, Iteration 330 Train Loss: 86.31\n",
      "Epoch 11, Iteration 340 Train Loss: 87.07\n",
      "Epoch 11, Iteration 350 Train Loss: 54.33\n",
      "Epoch 11, Iteration 360 Train Loss: 60.97\n",
      "New Epoch! Avg loss for the last 100 iterations: 80.32018230438233\n",
      "Epoch 12, Iteration 0 Train Loss: 19179.35\n",
      "Epoch 12, Iteration 10 Train Loss: 202.58\n",
      "Epoch 12, Iteration 20 Train Loss: 60.91\n",
      "Epoch 12, Iteration 30 Train Loss: 90.59\n",
      "Epoch 12, Iteration 40 Train Loss: 79.07\n",
      "Epoch 12, Iteration 50 Train Loss: 62.59\n",
      "Epoch 12, Iteration 60 Train Loss: 87.98\n",
      "Epoch 12, Iteration 70 Train Loss: 636.93\n",
      "Epoch 12, Iteration 80 Train Loss: 56.45\n",
      "Epoch 12, Iteration 90 Train Loss: 420.06\n",
      "Epoch 12, Iteration 100 Train Loss: 76.15\n",
      "Epoch 12, Iteration 110 Train Loss: 56.77\n",
      "Epoch 12, Iteration 120 Train Loss: 81.54\n",
      "Epoch 12, Iteration 130 Train Loss: 52.42\n",
      "Epoch 12, Iteration 140 Train Loss: 57.61\n",
      "Epoch 12, Iteration 150 Train Loss: 48.28\n",
      "Epoch 12, Iteration 160 Train Loss: 31.74\n",
      "Epoch 12, Iteration 170 Train Loss: 61.17\n",
      "Epoch 12, Iteration 180 Train Loss: 57.09\n",
      "Epoch 12, Iteration 190 Train Loss: 67.20\n",
      "Epoch 12, Iteration 200 Train Loss: 82.56\n",
      "Epoch 12, Iteration 210 Train Loss: 71.92\n",
      "Epoch 12, Iteration 220 Train Loss: 49.43\n",
      "Epoch 12, Iteration 230 Train Loss: 61.02\n",
      "Epoch 12, Iteration 240 Train Loss: 60.88\n",
      "Epoch 12, Iteration 250 Train Loss: 73.21\n",
      "Epoch 12, Iteration 260 Train Loss: 63.95\n",
      "Epoch 12, Iteration 270 Train Loss: 95.79\n",
      "Epoch 12, Iteration 280 Train Loss: 82.67\n",
      "Epoch 12, Iteration 290 Train Loss: 92.86\n",
      "Epoch 12, Iteration 300 Train Loss: 88.62\n",
      "Epoch 12, Iteration 310 Train Loss: 78.07\n",
      "Epoch 12, Iteration 320 Train Loss: 72.96\n",
      "Epoch 12, Iteration 330 Train Loss: 88.09\n",
      "Epoch 12, Iteration 340 Train Loss: 88.85\n",
      "Epoch 12, Iteration 350 Train Loss: 56.09\n",
      "Epoch 12, Iteration 360 Train Loss: 62.71\n",
      "New Epoch! Avg loss for the last 100 iterations: 82.21838298797607\n",
      "Epoch 13, Iteration 0 Train Loss: 19146.52\n",
      "Epoch 13, Iteration 10 Train Loss: 191.43\n",
      "Epoch 13, Iteration 20 Train Loss: 62.73\n",
      "Epoch 13, Iteration 30 Train Loss: 92.44\n",
      "Epoch 13, Iteration 40 Train Loss: 80.92\n",
      "Epoch 13, Iteration 50 Train Loss: 64.43\n",
      "Epoch 13, Iteration 60 Train Loss: 89.81\n",
      "Epoch 13, Iteration 70 Train Loss: 611.08\n",
      "Epoch 13, Iteration 80 Train Loss: 58.30\n",
      "Epoch 13, Iteration 90 Train Loss: 398.35\n",
      "Epoch 13, Iteration 100 Train Loss: 78.02\n",
      "Epoch 13, Iteration 110 Train Loss: 58.64\n",
      "Epoch 13, Iteration 120 Train Loss: 73.94\n",
      "Epoch 13, Iteration 130 Train Loss: 54.29\n",
      "Epoch 13, Iteration 140 Train Loss: 59.47\n",
      "Epoch 13, Iteration 150 Train Loss: 43.69\n",
      "Epoch 13, Iteration 160 Train Loss: 33.56\n",
      "Epoch 13, Iteration 170 Train Loss: 62.96\n",
      "Epoch 13, Iteration 180 Train Loss: 57.04\n",
      "Epoch 13, Iteration 190 Train Loss: 68.82\n",
      "Epoch 13, Iteration 200 Train Loss: 84.44\n",
      "Epoch 13, Iteration 210 Train Loss: 73.25\n",
      "Epoch 13, Iteration 220 Train Loss: 51.47\n",
      "Epoch 13, Iteration 230 Train Loss: 62.26\n",
      "Epoch 13, Iteration 240 Train Loss: 62.08\n",
      "Epoch 13, Iteration 250 Train Loss: 74.52\n",
      "Epoch 13, Iteration 260 Train Loss: 65.02\n",
      "Epoch 13, Iteration 270 Train Loss: 96.95\n",
      "Epoch 13, Iteration 280 Train Loss: 83.79\n",
      "Epoch 13, Iteration 290 Train Loss: 93.67\n",
      "Epoch 13, Iteration 300 Train Loss: 89.64\n",
      "Epoch 13, Iteration 310 Train Loss: 79.03\n",
      "Epoch 13, Iteration 320 Train Loss: 73.87\n",
      "Epoch 13, Iteration 330 Train Loss: 89.01\n",
      "Epoch 13, Iteration 340 Train Loss: 89.57\n",
      "Epoch 13, Iteration 350 Train Loss: 56.98\n",
      "Epoch 13, Iteration 360 Train Loss: 63.56\n",
      "New Epoch! Avg loss for the last 100 iterations: 83.46507740020752\n",
      "Epoch 14, Iteration 0 Train Loss: 19130.96\n",
      "Epoch 14, Iteration 10 Train Loss: 186.82\n",
      "Epoch 14, Iteration 20 Train Loss: 63.69\n",
      "Epoch 14, Iteration 30 Train Loss: 93.44\n",
      "Epoch 14, Iteration 40 Train Loss: 81.91\n",
      "Epoch 14, Iteration 50 Train Loss: 65.42\n",
      "Epoch 14, Iteration 60 Train Loss: 90.80\n",
      "Epoch 14, Iteration 70 Train Loss: 596.14\n",
      "Epoch 14, Iteration 80 Train Loss: 59.38\n",
      "Epoch 14, Iteration 90 Train Loss: 383.33\n",
      "Epoch 14, Iteration 100 Train Loss: 79.14\n",
      "Epoch 14, Iteration 110 Train Loss: 59.74\n",
      "Epoch 14, Iteration 120 Train Loss: 67.23\n",
      "Epoch 14, Iteration 130 Train Loss: 56.20\n",
      "Epoch 14, Iteration 140 Train Loss: 61.33\n",
      "Epoch 14, Iteration 150 Train Loss: 42.09\n",
      "Epoch 14, Iteration 160 Train Loss: 35.35\n",
      "Epoch 14, Iteration 170 Train Loss: 64.72\n",
      "Epoch 14, Iteration 180 Train Loss: 58.78\n",
      "Epoch 14, Iteration 190 Train Loss: 70.67\n",
      "Epoch 14, Iteration 200 Train Loss: 86.05\n",
      "Epoch 14, Iteration 210 Train Loss: 75.35\n",
      "Epoch 14, Iteration 220 Train Loss: 52.89\n",
      "Epoch 14, Iteration 230 Train Loss: 64.43\n",
      "Epoch 14, Iteration 240 Train Loss: 64.28\n",
      "Epoch 14, Iteration 250 Train Loss: 76.58\n",
      "Epoch 14, Iteration 260 Train Loss: 67.28\n",
      "Epoch 14, Iteration 270 Train Loss: 99.21\n",
      "Epoch 14, Iteration 280 Train Loss: 86.09\n",
      "Epoch 14, Iteration 290 Train Loss: 96.11\n",
      "Epoch 14, Iteration 300 Train Loss: 92.02\n",
      "Epoch 14, Iteration 310 Train Loss: 81.45\n",
      "Epoch 14, Iteration 320 Train Loss: 76.34\n",
      "Epoch 14, Iteration 330 Train Loss: 91.44\n",
      "Epoch 14, Iteration 340 Train Loss: 92.06\n",
      "Epoch 14, Iteration 350 Train Loss: 59.40\n",
      "Epoch 14, Iteration 360 Train Loss: 66.00\n",
      "New Epoch! Avg loss for the last 100 iterations: 85.49743061065674\n",
      "Epoch 15, Iteration 0 Train Loss: 19084.22\n",
      "Epoch 15, Iteration 10 Train Loss: 174.65\n",
      "Epoch 15, Iteration 20 Train Loss: 66.06\n",
      "Epoch 15, Iteration 30 Train Loss: 95.77\n",
      "Epoch 15, Iteration 40 Train Loss: 84.23\n",
      "Epoch 15, Iteration 50 Train Loss: 67.73\n",
      "Epoch 15, Iteration 60 Train Loss: 93.09\n",
      "Epoch 15, Iteration 70 Train Loss: 572.35\n",
      "Epoch 15, Iteration 80 Train Loss: 61.57\n",
      "Epoch 15, Iteration 90 Train Loss: 361.12\n",
      "Epoch 15, Iteration 100 Train Loss: 81.28\n",
      "Epoch 15, Iteration 110 Train Loss: 61.86\n",
      "Epoch 15, Iteration 120 Train Loss: 62.02\n",
      "Epoch 15, Iteration 130 Train Loss: 57.45\n",
      "Epoch 15, Iteration 140 Train Loss: 62.57\n",
      "Epoch 15, Iteration 150 Train Loss: 43.31\n",
      "Epoch 15, Iteration 160 Train Loss: 36.60\n",
      "Epoch 15, Iteration 170 Train Loss: 65.95\n",
      "Epoch 15, Iteration 180 Train Loss: 60.00\n",
      "Epoch 15, Iteration 190 Train Loss: 71.89\n",
      "Epoch 15, Iteration 200 Train Loss: 87.48\n",
      "Epoch 15, Iteration 210 Train Loss: 76.55\n",
      "Epoch 15, Iteration 220 Train Loss: 54.53\n",
      "Epoch 15, Iteration 230 Train Loss: 65.65\n",
      "Epoch 15, Iteration 240 Train Loss: 65.50\n",
      "Epoch 15, Iteration 250 Train Loss: 77.98\n",
      "Epoch 15, Iteration 260 Train Loss: 68.54\n",
      "Epoch 15, Iteration 270 Train Loss: 100.49\n",
      "Epoch 15, Iteration 280 Train Loss: 87.36\n",
      "Epoch 15, Iteration 290 Train Loss: 97.33\n",
      "Epoch 15, Iteration 300 Train Loss: 93.29\n",
      "Epoch 15, Iteration 310 Train Loss: 82.71\n",
      "Epoch 15, Iteration 320 Train Loss: 77.58\n",
      "Epoch 15, Iteration 330 Train Loss: 92.68\n",
      "Epoch 15, Iteration 340 Train Loss: 93.26\n",
      "Epoch 15, Iteration 350 Train Loss: 60.63\n",
      "Epoch 15, Iteration 360 Train Loss: 67.23\n",
      "New Epoch! Avg loss for the last 100 iterations: 86.80113403320313\n",
      "Epoch 16, Iteration 0 Train Loss: 19061.08\n",
      "Epoch 16, Iteration 10 Train Loss: 170.20\n",
      "Epoch 16, Iteration 20 Train Loss: 67.33\n",
      "Epoch 16, Iteration 30 Train Loss: 97.04\n",
      "Epoch 16, Iteration 40 Train Loss: 85.50\n",
      "Epoch 16, Iteration 50 Train Loss: 69.00\n",
      "Epoch 16, Iteration 60 Train Loss: 94.36\n",
      "Epoch 16, Iteration 70 Train Loss: 557.41\n",
      "Epoch 16, Iteration 80 Train Loss: 62.88\n",
      "Epoch 16, Iteration 90 Train Loss: 346.02\n",
      "Epoch 16, Iteration 100 Train Loss: 82.60\n",
      "Epoch 16, Iteration 110 Train Loss: 63.19\n",
      "Epoch 16, Iteration 120 Train Loss: 60.04\n",
      "Epoch 16, Iteration 130 Train Loss: 58.76\n",
      "Epoch 16, Iteration 140 Train Loss: 63.88\n",
      "Epoch 16, Iteration 150 Train Loss: 44.62\n",
      "Epoch 16, Iteration 160 Train Loss: 37.91\n",
      "Epoch 16, Iteration 170 Train Loss: 67.24\n",
      "Epoch 16, Iteration 180 Train Loss: 61.26\n",
      "Epoch 16, Iteration 190 Train Loss: 73.12\n",
      "Epoch 16, Iteration 200 Train Loss: 88.71\n",
      "Epoch 16, Iteration 210 Train Loss: 77.77\n",
      "Epoch 16, Iteration 220 Train Loss: 55.75\n",
      "Epoch 16, Iteration 230 Train Loss: 66.86\n",
      "Epoch 16, Iteration 240 Train Loss: 66.71\n",
      "Epoch 16, Iteration 250 Train Loss: 79.17\n",
      "Epoch 16, Iteration 260 Train Loss: 69.73\n",
      "Epoch 16, Iteration 270 Train Loss: 101.69\n",
      "Epoch 16, Iteration 280 Train Loss: 88.57\n",
      "Epoch 16, Iteration 290 Train Loss: 98.49\n",
      "Epoch 16, Iteration 300 Train Loss: 94.48\n",
      "Epoch 16, Iteration 310 Train Loss: 83.90\n",
      "Epoch 16, Iteration 320 Train Loss: 78.77\n",
      "Epoch 16, Iteration 330 Train Loss: 93.87\n",
      "Epoch 16, Iteration 340 Train Loss: 94.40\n",
      "Epoch 16, Iteration 350 Train Loss: 61.81\n",
      "Epoch 16, Iteration 360 Train Loss: 68.40\n",
      "New Epoch! Avg loss for the last 100 iterations: 87.99209510803223\n",
      "Epoch 17, Iteration 0 Train Loss: 19038.99\n",
      "Epoch 17, Iteration 10 Train Loss: 165.60\n",
      "Epoch 17, Iteration 20 Train Loss: 68.51\n",
      "Epoch 17, Iteration 30 Train Loss: 98.22\n",
      "Epoch 17, Iteration 40 Train Loss: 86.67\n",
      "Epoch 17, Iteration 50 Train Loss: 70.16\n",
      "Epoch 17, Iteration 60 Train Loss: 95.52\n",
      "Epoch 17, Iteration 70 Train Loss: 544.05\n",
      "Epoch 17, Iteration 80 Train Loss: 64.06\n",
      "Epoch 17, Iteration 90 Train Loss: 332.54\n",
      "Epoch 17, Iteration 100 Train Loss: 83.78\n",
      "Epoch 17, Iteration 110 Train Loss: 64.36\n",
      "Epoch 17, Iteration 120 Train Loss: 58.27\n",
      "Epoch 17, Iteration 130 Train Loss: 59.94\n",
      "Epoch 17, Iteration 140 Train Loss: 65.06\n",
      "Epoch 17, Iteration 150 Train Loss: 45.77\n",
      "Epoch 17, Iteration 160 Train Loss: 39.03\n",
      "Epoch 17, Iteration 170 Train Loss: 68.36\n",
      "Epoch 17, Iteration 180 Train Loss: 62.37\n",
      "Epoch 17, Iteration 190 Train Loss: 74.23\n",
      "Epoch 17, Iteration 200 Train Loss: 89.81\n",
      "Epoch 17, Iteration 210 Train Loss: 78.86\n",
      "Epoch 17, Iteration 220 Train Loss: 56.83\n",
      "Epoch 17, Iteration 230 Train Loss: 67.95\n",
      "Epoch 17, Iteration 240 Train Loss: 67.80\n",
      "Epoch 17, Iteration 250 Train Loss: 80.22\n",
      "Epoch 17, Iteration 260 Train Loss: 70.79\n",
      "Epoch 17, Iteration 270 Train Loss: 102.78\n",
      "Epoch 17, Iteration 280 Train Loss: 89.65\n",
      "Epoch 17, Iteration 290 Train Loss: 99.53\n",
      "Epoch 17, Iteration 300 Train Loss: 95.56\n",
      "Epoch 17, Iteration 310 Train Loss: 84.97\n",
      "Epoch 17, Iteration 320 Train Loss: 79.84\n",
      "Epoch 17, Iteration 330 Train Loss: 94.94\n",
      "Epoch 17, Iteration 340 Train Loss: 95.46\n",
      "Epoch 17, Iteration 350 Train Loss: 62.90\n",
      "Epoch 17, Iteration 360 Train Loss: 69.48\n",
      "New Epoch! Avg loss for the last 100 iterations: 89.07453510284424\n",
      "Epoch 18, Iteration 0 Train Loss: 19018.39\n",
      "Epoch 18, Iteration 10 Train Loss: 161.12\n",
      "Epoch 18, Iteration 20 Train Loss: 69.63\n",
      "Epoch 18, Iteration 30 Train Loss: 99.32\n",
      "Epoch 18, Iteration 40 Train Loss: 87.76\n",
      "Epoch 18, Iteration 50 Train Loss: 71.24\n",
      "Epoch 18, Iteration 60 Train Loss: 96.60\n",
      "Epoch 18, Iteration 70 Train Loss: 531.96\n",
      "Epoch 18, Iteration 80 Train Loss: 65.15\n",
      "Epoch 18, Iteration 90 Train Loss: 320.39\n",
      "Epoch 18, Iteration 100 Train Loss: 84.88\n",
      "Epoch 18, Iteration 110 Train Loss: 65.48\n",
      "Epoch 18, Iteration 120 Train Loss: 56.96\n",
      "Epoch 18, Iteration 130 Train Loss: 60.96\n",
      "Epoch 18, Iteration 140 Train Loss: 66.05\n",
      "Epoch 18, Iteration 150 Train Loss: 46.76\n",
      "Epoch 18, Iteration 160 Train Loss: 40.01\n",
      "Epoch 18, Iteration 170 Train Loss: 69.31\n",
      "Epoch 18, Iteration 180 Train Loss: 63.31\n",
      "Epoch 18, Iteration 190 Train Loss: 75.16\n",
      "Epoch 18, Iteration 200 Train Loss: 90.72\n",
      "Epoch 18, Iteration 210 Train Loss: 79.79\n",
      "Epoch 18, Iteration 220 Train Loss: 57.73\n",
      "Epoch 18, Iteration 230 Train Loss: 68.87\n",
      "Epoch 18, Iteration 240 Train Loss: 68.72\n",
      "Epoch 18, Iteration 250 Train Loss: 81.11\n",
      "Epoch 18, Iteration 260 Train Loss: 71.68\n",
      "Epoch 18, Iteration 270 Train Loss: 103.70\n",
      "Epoch 18, Iteration 280 Train Loss: 90.57\n",
      "Epoch 18, Iteration 290 Train Loss: 100.41\n",
      "Epoch 18, Iteration 300 Train Loss: 96.47\n",
      "Epoch 18, Iteration 310 Train Loss: 85.88\n",
      "Epoch 18, Iteration 320 Train Loss: 80.74\n",
      "Epoch 18, Iteration 330 Train Loss: 95.83\n",
      "Epoch 18, Iteration 340 Train Loss: 96.30\n",
      "Epoch 18, Iteration 350 Train Loss: 63.75\n",
      "Epoch 18, Iteration 360 Train Loss: 70.33\n",
      "New Epoch! Avg loss for the last 100 iterations: 89.94097873687744\n",
      "Epoch 19, Iteration 0 Train Loss: 19002.32\n",
      "Epoch 19, Iteration 10 Train Loss: 157.79\n",
      "Epoch 19, Iteration 20 Train Loss: 70.49\n",
      "Epoch 19, Iteration 30 Train Loss: 100.19\n",
      "Epoch 19, Iteration 40 Train Loss: 88.63\n",
      "Epoch 19, Iteration 50 Train Loss: 72.11\n",
      "Epoch 19, Iteration 60 Train Loss: 97.48\n",
      "Epoch 19, Iteration 70 Train Loss: 521.95\n",
      "Epoch 19, Iteration 80 Train Loss: 66.04\n",
      "Epoch 19, Iteration 90 Train Loss: 310.17\n",
      "Epoch 19, Iteration 100 Train Loss: 85.76\n",
      "Epoch 19, Iteration 110 Train Loss: 66.35\n",
      "Epoch 19, Iteration 120 Train Loss: 57.83\n",
      "Epoch 19, Iteration 130 Train Loss: 61.83\n",
      "Epoch 19, Iteration 140 Train Loss: 66.92\n",
      "Epoch 19, Iteration 150 Train Loss: 47.63\n",
      "Epoch 19, Iteration 160 Train Loss: 40.88\n",
      "Epoch 19, Iteration 170 Train Loss: 70.18\n",
      "Epoch 19, Iteration 180 Train Loss: 64.18\n",
      "Epoch 19, Iteration 190 Train Loss: 76.03\n",
      "Epoch 19, Iteration 200 Train Loss: 91.57\n",
      "Epoch 19, Iteration 210 Train Loss: 80.65\n",
      "Epoch 19, Iteration 220 Train Loss: 58.56\n",
      "Epoch 19, Iteration 230 Train Loss: 69.73\n",
      "Epoch 19, Iteration 240 Train Loss: 69.57\n",
      "Epoch 19, Iteration 250 Train Loss: 81.96\n",
      "Epoch 19, Iteration 260 Train Loss: 72.56\n",
      "Epoch 19, Iteration 270 Train Loss: 104.51\n",
      "Epoch 19, Iteration 280 Train Loss: 91.38\n",
      "Epoch 19, Iteration 290 Train Loss: 101.29\n",
      "Epoch 19, Iteration 300 Train Loss: 97.27\n",
      "Epoch 19, Iteration 310 Train Loss: 86.68\n",
      "Epoch 19, Iteration 320 Train Loss: 81.54\n",
      "Epoch 19, Iteration 330 Train Loss: 96.62\n",
      "Epoch 19, Iteration 340 Train Loss: 97.17\n",
      "Epoch 19, Iteration 350 Train Loss: 64.54\n",
      "Epoch 19, Iteration 360 Train Loss: 71.12\n",
      "New Epoch! Avg loss for the last 100 iterations: 90.75917461395264\n",
      "Epoch 20, Iteration 0 Train Loss: 18987.44\n",
      "Epoch 20, Iteration 10 Train Loss: 154.52\n",
      "Epoch 20, Iteration 20 Train Loss: 71.30\n",
      "Epoch 20, Iteration 30 Train Loss: 101.00\n",
      "Epoch 20, Iteration 40 Train Loss: 89.44\n",
      "Epoch 20, Iteration 50 Train Loss: 72.92\n",
      "Epoch 20, Iteration 60 Train Loss: 98.28\n",
      "Epoch 20, Iteration 70 Train Loss: 512.20\n",
      "Epoch 20, Iteration 80 Train Loss: 66.82\n",
      "Epoch 20, Iteration 90 Train Loss: 300.96\n",
      "Epoch 20, Iteration 100 Train Loss: 86.51\n",
      "Epoch 20, Iteration 110 Train Loss: 67.09\n",
      "Epoch 20, Iteration 120 Train Loss: 58.55\n",
      "Epoch 20, Iteration 130 Train Loss: 62.54\n",
      "Epoch 20, Iteration 140 Train Loss: 67.61\n",
      "Epoch 20, Iteration 150 Train Loss: 48.31\n",
      "Epoch 20, Iteration 160 Train Loss: 41.56\n",
      "Epoch 20, Iteration 170 Train Loss: 70.86\n",
      "Epoch 20, Iteration 180 Train Loss: 64.85\n",
      "Epoch 20, Iteration 190 Train Loss: 76.70\n",
      "Epoch 20, Iteration 200 Train Loss: 92.34\n",
      "Epoch 20, Iteration 210 Train Loss: 81.32\n",
      "Epoch 20, Iteration 220 Train Loss: 59.43\n",
      "Epoch 20, Iteration 230 Train Loss: 70.39\n",
      "Epoch 20, Iteration 240 Train Loss: 70.25\n",
      "Epoch 20, Iteration 250 Train Loss: 82.63\n",
      "Epoch 20, Iteration 260 Train Loss: 73.20\n",
      "Epoch 20, Iteration 270 Train Loss: 105.25\n",
      "Epoch 20, Iteration 280 Train Loss: 92.12\n",
      "Epoch 20, Iteration 290 Train Loss: 101.90\n",
      "Epoch 20, Iteration 300 Train Loss: 98.01\n",
      "Epoch 20, Iteration 310 Train Loss: 87.42\n",
      "Epoch 20, Iteration 320 Train Loss: 82.27\n",
      "Epoch 20, Iteration 330 Train Loss: 97.37\n",
      "Epoch 20, Iteration 340 Train Loss: 97.80\n",
      "Epoch 20, Iteration 350 Train Loss: 65.31\n",
      "Epoch 20, Iteration 360 Train Loss: 71.88\n",
      "New Epoch! Avg loss for the last 100 iterations: 91.49620178222656\n",
      "Epoch 21, Iteration 0 Train Loss: 18972.88\n",
      "Epoch 21, Iteration 10 Train Loss: 151.56\n",
      "Epoch 21, Iteration 20 Train Loss: 72.08\n",
      "Epoch 21, Iteration 30 Train Loss: 101.78\n",
      "Epoch 21, Iteration 40 Train Loss: 89.89\n",
      "Epoch 21, Iteration 50 Train Loss: 73.37\n",
      "Epoch 21, Iteration 60 Train Loss: 98.71\n",
      "Epoch 21, Iteration 70 Train Loss: 511.44\n",
      "Epoch 21, Iteration 80 Train Loss: 67.21\n",
      "Epoch 21, Iteration 90 Train Loss: 300.58\n",
      "Epoch 21, Iteration 100 Train Loss: 86.86\n",
      "Epoch 21, Iteration 110 Train Loss: 67.43\n",
      "Epoch 21, Iteration 120 Train Loss: 58.90\n",
      "Epoch 21, Iteration 130 Train Loss: 62.89\n",
      "Epoch 21, Iteration 140 Train Loss: 67.97\n",
      "Epoch 21, Iteration 150 Train Loss: 48.67\n",
      "Epoch 21, Iteration 160 Train Loss: 41.92\n",
      "Epoch 21, Iteration 170 Train Loss: 71.21\n",
      "Epoch 21, Iteration 180 Train Loss: 65.21\n",
      "Epoch 21, Iteration 190 Train Loss: 76.92\n",
      "Epoch 21, Iteration 200 Train Loss: 91.89\n",
      "Epoch 21, Iteration 210 Train Loss: 81.28\n",
      "Epoch 21, Iteration 220 Train Loss: 58.49\n",
      "Epoch 21, Iteration 230 Train Loss: 70.19\n",
      "Epoch 21, Iteration 240 Train Loss: 69.95\n",
      "Epoch 21, Iteration 250 Train Loss: 81.88\n",
      "Epoch 21, Iteration 260 Train Loss: 72.60\n",
      "Epoch 21, Iteration 270 Train Loss: 104.65\n",
      "Epoch 21, Iteration 280 Train Loss: 91.47\n",
      "Epoch 21, Iteration 290 Train Loss: 101.17\n",
      "Epoch 21, Iteration 300 Train Loss: 97.27\n",
      "Epoch 21, Iteration 310 Train Loss: 86.63\n",
      "Epoch 21, Iteration 320 Train Loss: 81.44\n",
      "Epoch 21, Iteration 330 Train Loss: 96.54\n",
      "Epoch 21, Iteration 340 Train Loss: 96.97\n",
      "Epoch 21, Iteration 350 Train Loss: 64.45\n",
      "Epoch 21, Iteration 360 Train Loss: 71.00\n",
      "New Epoch! Avg loss for the last 100 iterations: 91.00118942260742\n",
      "Epoch 22, Iteration 0 Train Loss: 18969.87\n",
      "Epoch 22, Iteration 10 Train Loss: 154.67\n",
      "Epoch 22, Iteration 20 Train Loss: 71.11\n",
      "Epoch 22, Iteration 30 Train Loss: 101.87\n",
      "Epoch 22, Iteration 40 Train Loss: 89.19\n",
      "Epoch 22, Iteration 50 Train Loss: 72.65\n",
      "Epoch 22, Iteration 60 Train Loss: 98.91\n",
      "Epoch 22, Iteration 70 Train Loss: 505.31\n",
      "Epoch 22, Iteration 80 Train Loss: 66.52\n",
      "Epoch 22, Iteration 90 Train Loss: 291.81\n",
      "Epoch 22, Iteration 100 Train Loss: 86.52\n",
      "Epoch 22, Iteration 110 Train Loss: 67.08\n",
      "Epoch 22, Iteration 120 Train Loss: 58.68\n",
      "Epoch 22, Iteration 130 Train Loss: 62.46\n",
      "Epoch 22, Iteration 140 Train Loss: 67.47\n",
      "Epoch 22, Iteration 150 Train Loss: 48.14\n",
      "Epoch 22, Iteration 160 Train Loss: 42.40\n",
      "Epoch 22, Iteration 170 Train Loss: 70.85\n",
      "Epoch 22, Iteration 180 Train Loss: 64.85\n",
      "Epoch 22, Iteration 190 Train Loss: 76.68\n",
      "Epoch 22, Iteration 200 Train Loss: 92.75\n",
      "Epoch 22, Iteration 210 Train Loss: 81.28\n",
      "Epoch 22, Iteration 220 Train Loss: 60.12\n",
      "Epoch 22, Iteration 230 Train Loss: 70.36\n",
      "Epoch 22, Iteration 240 Train Loss: 70.22\n",
      "Epoch 22, Iteration 250 Train Loss: 82.81\n",
      "Epoch 22, Iteration 260 Train Loss: 73.21\n",
      "Epoch 22, Iteration 270 Train Loss: 105.25\n",
      "Epoch 22, Iteration 280 Train Loss: 92.10\n",
      "Epoch 22, Iteration 290 Train Loss: 101.79\n",
      "Epoch 22, Iteration 300 Train Loss: 97.95\n",
      "Epoch 22, Iteration 310 Train Loss: 87.33\n",
      "Epoch 22, Iteration 320 Train Loss: 82.16\n",
      "Epoch 22, Iteration 330 Train Loss: 97.27\n",
      "Epoch 22, Iteration 340 Train Loss: 97.66\n",
      "Epoch 22, Iteration 350 Train Loss: 65.18\n",
      "Epoch 22, Iteration 360 Train Loss: 71.74\n",
      "New Epoch! Avg loss for the last 100 iterations: 91.70239856719971\n",
      "Epoch 23, Iteration 0 Train Loss: 18944.65\n",
      "Epoch 23, Iteration 10 Train Loss: 153.38\n",
      "Epoch 23, Iteration 20 Train Loss: 71.90\n",
      "Epoch 23, Iteration 30 Train Loss: 101.52\n",
      "Epoch 23, Iteration 40 Train Loss: 89.92\n",
      "Epoch 23, Iteration 50 Train Loss: 73.36\n",
      "Epoch 23, Iteration 60 Train Loss: 99.11\n",
      "Epoch 23, Iteration 70 Train Loss: 489.78\n",
      "Epoch 23, Iteration 80 Train Loss: 67.13\n",
      "Epoch 23, Iteration 90 Train Loss: 277.94\n",
      "Epoch 23, Iteration 100 Train Loss: 86.78\n",
      "Epoch 23, Iteration 110 Train Loss: 67.34\n",
      "Epoch 23, Iteration 120 Train Loss: 59.94\n",
      "Epoch 23, Iteration 130 Train Loss: 63.68\n",
      "Epoch 23, Iteration 140 Train Loss: 67.77\n",
      "Epoch 23, Iteration 150 Train Loss: 49.86\n",
      "Epoch 23, Iteration 160 Train Loss: 42.91\n",
      "Epoch 23, Iteration 170 Train Loss: 71.02\n",
      "Epoch 23, Iteration 180 Train Loss: 65.96\n",
      "Epoch 23, Iteration 190 Train Loss: 76.82\n",
      "Epoch 23, Iteration 200 Train Loss: 92.48\n",
      "Epoch 23, Iteration 210 Train Loss: 81.39\n",
      "Epoch 23, Iteration 220 Train Loss: 59.94\n",
      "Epoch 23, Iteration 230 Train Loss: 70.43\n",
      "Epoch 23, Iteration 240 Train Loss: 70.27\n",
      "Epoch 23, Iteration 250 Train Loss: 82.64\n",
      "Epoch 23, Iteration 260 Train Loss: 73.15\n",
      "Epoch 23, Iteration 270 Train Loss: 105.26\n",
      "Epoch 23, Iteration 280 Train Loss: 92.00\n",
      "Epoch 23, Iteration 290 Train Loss: 101.62\n",
      "Epoch 23, Iteration 300 Train Loss: 97.61\n",
      "Epoch 23, Iteration 310 Train Loss: 86.87\n",
      "Epoch 23, Iteration 320 Train Loss: 81.59\n",
      "Epoch 23, Iteration 330 Train Loss: 97.64\n",
      "Epoch 23, Iteration 340 Train Loss: 97.33\n",
      "Epoch 23, Iteration 350 Train Loss: 64.55\n",
      "Epoch 23, Iteration 360 Train Loss: 71.05\n",
      "New Epoch! Avg loss for the last 100 iterations: 91.36141368865967\n",
      "Epoch 24, Iteration 0 Train Loss: 18932.65\n",
      "Epoch 24, Iteration 10 Train Loss: 153.28\n",
      "Epoch 24, Iteration 20 Train Loss: 71.21\n",
      "Epoch 24, Iteration 30 Train Loss: 98.23\n",
      "Epoch 24, Iteration 40 Train Loss: 89.74\n",
      "Epoch 24, Iteration 50 Train Loss: 73.22\n",
      "Epoch 24, Iteration 60 Train Loss: 97.80\n",
      "Epoch 24, Iteration 70 Train Loss: 499.94\n",
      "Epoch 24, Iteration 80 Train Loss: 67.13\n",
      "Epoch 24, Iteration 90 Train Loss: 288.25\n",
      "Epoch 24, Iteration 100 Train Loss: 85.72\n",
      "Epoch 24, Iteration 110 Train Loss: 66.14\n",
      "Epoch 24, Iteration 120 Train Loss: 56.73\n",
      "Epoch 24, Iteration 130 Train Loss: 60.96\n",
      "Epoch 24, Iteration 140 Train Loss: 65.24\n",
      "Epoch 24, Iteration 150 Train Loss: 46.65\n",
      "Epoch 24, Iteration 160 Train Loss: 41.40\n",
      "Epoch 24, Iteration 170 Train Loss: 68.23\n",
      "Epoch 24, Iteration 180 Train Loss: 62.86\n",
      "Epoch 24, Iteration 190 Train Loss: 73.71\n",
      "Epoch 24, Iteration 200 Train Loss: 95.33\n",
      "Epoch 24, Iteration 210 Train Loss: 76.36\n",
      "Epoch 24, Iteration 220 Train Loss: 62.86\n",
      "Epoch 24, Iteration 230 Train Loss: 65.91\n",
      "Epoch 24, Iteration 240 Train Loss: 65.94\n",
      "Epoch 24, Iteration 250 Train Loss: 81.23\n",
      "Epoch 24, Iteration 260 Train Loss: 70.19\n",
      "Epoch 24, Iteration 270 Train Loss: 101.44\n",
      "Epoch 24, Iteration 280 Train Loss: 88.31\n",
      "Epoch 24, Iteration 290 Train Loss: 98.68\n",
      "Epoch 24, Iteration 300 Train Loss: 94.17\n",
      "Epoch 24, Iteration 310 Train Loss: 83.55\n",
      "Epoch 24, Iteration 320 Train Loss: 78.38\n",
      "Epoch 24, Iteration 330 Train Loss: 93.49\n",
      "Epoch 24, Iteration 340 Train Loss: 94.47\n",
      "Epoch 24, Iteration 350 Train Loss: 61.40\n",
      "Epoch 24, Iteration 360 Train Loss: 65.37\n",
      "New Epoch! Avg loss for the last 100 iterations: 87.8874264907837\n",
      "Oracle activations: 3948\n",
      "Runtime total_E activations: 3453.388158721049\n",
      "Runtime total_E_pred activations: 85525.46181033726\n",
      "Naive total_E activations: 3453.388158721049\n",
      "Naive total_E_pred activations: 4740.710587099603\n",
      "Dataset, train set, and test set size: 3680 2943 368\n",
      "Timeframe: 60min\n",
      "Minimal Application\n",
      "Naive vs. DL succesful activations: 0.0\n",
      "Maximum possible activations: 3948\n",
      "Predicted activations: 85775\n",
      "Successful activations: 0, 0.000%\n",
      "Failed activations: 85754, 99.976%\n",
      "Missed activations: 3948, 100.000%\n",
      "Naive predicted activations (usual actual energy average): 4881\n",
      "Naive successful activations (usual actual energy average): 1049, 21.491%\n",
      "Naive failed activations (usual actual energy average): 3832, 78.509%\n",
      "Naive missed activations (usual actual energy average): 2899, 73.430%\n",
      "Voltage overestimation rate: 100.000%\n",
      "Test MAPE power: 2.560718\n",
      "Test MAPE voltage: 4.244638\n",
      "Test MAPE current: 0.347500\n"
     ]
    }
   ],
   "source": [
    "power_mape = []\n",
    "voltage_mape = []\n",
    "current_mape = []\n",
    "\n",
    "E_actual_list = []\n",
    "E_pred_list = []\n",
    "\n",
    "max_act_list = []\n",
    "pred_act_list = []\n",
    "succ_act_list = []\n",
    "\n",
    "pred_act_naive_list = []\n",
    "false_act_naive_list = []\n",
    "succ_act_naive_list = []\n",
    "\n",
    "#Set parameters\n",
    "batchsize = 8\n",
    "time_frame = '60min'\n",
    "time_frame_seconds = 3600\n",
    "n = 0\n",
    "splits = TimeSeriesSplit(n_splits=4)\n",
    "for train_index, test_index in splits.split(X):\n",
    "    n += 1\n",
    "    if n >= 1:\n",
    "        #Split train and test sets\n",
    "        X_train = X.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "\n",
    "        X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "        y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "        #Set dataset bounds\n",
    "        train_bound_lower = y_train.index[0]\n",
    "        train_bound_upper = y_train.index[-1]\n",
    "        valid_bound_lower = y_valid.index[0]\n",
    "        valid_bound_upper = y_valid.index[-1]\n",
    "        test_bound_lower = y_test.index[0]\n",
    "        test_bound_upper = y_test.index[-1]\n",
    "\n",
    "        #Resample data\n",
    "        X_train = X_train.resample(time_frame).mean().dropna()\n",
    "        X_valid = X_valid.resample(time_frame).mean().dropna()\n",
    "        X_test = X_test.resample(time_frame).mean().dropna()\n",
    "\n",
    "        y_train = y_train.resample(time_frame).mean().dropna()\n",
    "        y_valid = y_valid.resample(time_frame).mean().dropna()\n",
    "        y_test = y_test.resample(time_frame).mean().dropna()\n",
    "\n",
    "        #Reshape data\n",
    "        X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
    "        X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "        # convert to tensor\n",
    "        X_train = torch.tensor(X_train)\n",
    "        y_train_index_labels = y_train.index\n",
    "        y_train_column_labels = y_train.columns\n",
    "        y_train = torch.tensor(y_train.values)\n",
    "\n",
    "        X_test = torch.tensor(X_test)\n",
    "        y_test_index_labels = y_test.index\n",
    "        y_test_column_labels = y_test.columns\n",
    "        y_test = torch.tensor(y_test.values)\n",
    "\n",
    "        # make datasets\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "        # Create DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
    "\n",
    "        # Define the number of time steps for the spiking\n",
    "        num_steps = 50\n",
    "        num_inputs = X_train.shape[2]\n",
    "\n",
    "        # create new inctance of the SNN Class\n",
    "        model = Net(num_inputs, num_steps).to(device)\n",
    "\n",
    "        # define optimizer\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "\n",
    "        # define loss function\n",
    "        def quantile_loss(y_true, y_pred, quantile=0.05):\n",
    "            error = y_true - y_pred\n",
    "            loss = torch.mean(torch.max(quantile * error, (quantile - 1) * error))\n",
    "            return loss\n",
    "        loss_fn = quantile_loss\n",
    "\n",
    "        # initialize histories\n",
    "        loss_hist = []\n",
    "        avg_loss_hist = []\n",
    "        acc_hist = []\n",
    "        num_epochs = 25\n",
    "\n",
    "        # put model into train mode\n",
    "        model.train()\n",
    "\n",
    "        # Train Loop\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (data, targets) in enumerate(iter(train_loader)):\n",
    "                # move to device\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # change to floats\n",
    "                data = data.float()\n",
    "                targets = targets.float()\n",
    "\n",
    "                _, _, _, mem = model(data)\n",
    "\n",
    "                loss_val = loss_fn(mem[-1], targets)\n",
    "\n",
    "                # Gradient calculation + weight update\n",
    "                optimizer.zero_grad()\n",
    "                loss_val.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Store loss history for future plotting\n",
    "                loss_hist.append(loss_val.item())\n",
    "\n",
    "                # Update loss plot\n",
    "                # update_loss_plot(loss_hist)\n",
    "\n",
    "                if i%10 == 0:\n",
    "                    print(f\"Epoch {epoch}, Iteration {i} Train Loss: {loss_val.item():.2f}\")\n",
    "                if len(loss_hist) > 100:\n",
    "                    avg_loss_hist.append(sum(loss_hist[-100:])/len(loss_hist[-100:]))\n",
    "                else:\n",
    "                    avg_loss_hist.append(0)\n",
    "\n",
    "            if len(loss_hist) > 100:\n",
    "                print(f'New Epoch! Avg loss for the last 100 iterations: {avg_loss_hist[-1]}')\n",
    "\n",
    "        # predictions\n",
    "        model.eval()\n",
    "\n",
    "        train_pred = []\n",
    "        test_pred = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in train_dataset:\n",
    "                data = data.to(device)\n",
    "                data = data.float()\n",
    "                _, _, _, train_mem = model(data)\n",
    "                train_pred.append(train_mem[-1])\n",
    "\n",
    "            for data, target in test_dataset:\n",
    "                data = data.to(device)\n",
    "                data = data.float()\n",
    "                _, _, _, test_mem = model(data)\n",
    "                test_pred.append(test_mem[-1])\n",
    "\n",
    "        # Convert to numpy\n",
    "        train_pred = torch.cat(train_pred, dim=0)\n",
    "        train_pred = train_pred.numpy()\n",
    "        test_pred = torch.cat(test_pred, dim=0)\n",
    "        test_pred = test_pred.numpy()\n",
    "\n",
    "        # convert tensors back to dataframe\n",
    "        y_test = pd.DataFrame(y_test, index=y_test_index_labels, columns=y_test_column_labels)\n",
    "        y_train = pd.DataFrame(y_test, index=y_train_index_labels, columns=y_test_column_labels)\n",
    "\n",
    "        #Prepare data for runtime simulation\n",
    "        y_test['power pred'] = test_pred[:, 0]\n",
    "        y_test['V1 [mV] pred'] = test_pred[:, 1]\n",
    "        y_test['I1L [Î¼A] pred'] = test_pred[:, 2]\n",
    "\n",
    "        days  = getMFC_data(y_test, test_pred)\n",
    "\n",
    "        v_test = df.loc[(((df.index >= test_bound_lower)) & (df.index <= test_bound_upper))]['Voltage (mV)']\n",
    "        v_test = v_test.drop(v_test[(v_test.index > '2021-11-11') & (v_test.index < '2021-11-22 01:00:00')].index)\n",
    "        v_test = pd.DataFrame(v_test)/1E5\n",
    "        v_avg_true = v_test['Voltage (mV)'].resample(time_frame).mean().dropna()\n",
    "        v_avg_pred = y_test['V1 [mV] pred']/1E5\n",
    "        C0 = [0.007000000000000006, 0.007000000000000006, 0.007000000000000006]\n",
    "\n",
    "        #Remove first and last entries of averaged data to prevent overestimation of available energy\n",
    "        v_avg_true = v_avg_true[1:][:-1]\n",
    "        v_avg_pred = v_avg_pred[1:][:-1]\n",
    "\n",
    "        #Run oracle model\n",
    "        max_act = oracle_simulate(v_test, C0)\n",
    "\n",
    "        #Call simulate function\n",
    "        pred_act, false_act, succ_act, total_E, total_E_pred = simulate(days, v_avg_true, v_avg_pred, v_test, C0)\n",
    "\n",
    "        #Run naive model\n",
    "        v_valid = df.loc[(((df.index >= valid_bound_lower)) & (df.index < valid_bound_upper))]['Voltage (mV)']/1E5\n",
    "        pred_act_naive, false_act_naive, succ_act_naive, total_E = naive_simulate(days, v_avg_true, v_valid, v_test, C0)\n",
    "        print(\"Dataset, train set, and test set size:\", len(y_train) + len(y_valid) + len(y_test), len(y_train), len(y_test))\n",
    "        print('Timeframe:', time_frame)\n",
    "\n",
    "        print('Minimal Application')\n",
    "        print(\"Naive vs. DL succesful activations:\", succ_act/succ_act_naive)\n",
    "        #print('Predicted vs. Actual percent difference: %.3f%%' % ((total_E * 100 / total_E_pred) - 100))\n",
    "        print('Maximum possible activations:', max_act)\n",
    "        print('Predicted activations:', pred_act)\n",
    "        print('Successful activations: %d, %.3f%%' % (succ_act, succ_act * 100/pred_act))\n",
    "        print('Failed activations: %d, %.3f%%' % (false_act, false_act * 100/pred_act))\n",
    "        print('Missed activations: %d, %.3f%%' % (max_act - succ_act, (max_act - succ_act) * 100/max_act))\n",
    "\n",
    "        #Naive model\n",
    "        print('Naive predicted activations (usual actual energy average):', pred_act_naive)\n",
    "        print('Naive successful activations (usual actual energy average): %d, %.3f%%' % (succ_act_naive, succ_act_naive * 100/pred_act_naive))\n",
    "        print('Naive failed activations (usual actual energy average): %d, %.3f%%' % (false_act_naive, false_act_naive * 100/pred_act_naive))\n",
    "        print('Naive missed activations (usual actual energy average): %d, %.3f%%' % (max_act - succ_act_naive, (max_act - succ_act_naive) * 100/max_act))\n",
    "\n",
    "\n",
    "        print('Voltage overestimation rate: %.3f%%' % ((y_test['Voltage (mV)'].values <= y_test['Voltage (mV)']).mean() * 100))\n",
    "        print(\"Test MAPE power: %3f\" %  MAPE(y_test['Power (uW)'].values.ravel(), y_test['power pred']))\n",
    "        print(\"Test MAPE voltage: %3f\" % MAPE(y_test['Voltage (mV)'], y_test['V1 [mV] pred']))\n",
    "        print(\"Test MAPE current: %3f\" % MAPE(y_test['Current (uA)'], y_test['I1L [Î¼A] pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G7rCcFXwuEaJ",
    "outputId": "8d82859e-8389-447d-f904-51de89db6319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Minute-Ahead Predictions:\n",
      "Power Predictions (uW):\n"
     ]
    },
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/_libs/index.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), 0)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5-Minute-Ahead Predictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPower Predictions (uW):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43my_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mVoltage Predictions (mV):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_test[:, \u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/core/frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3760\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3762\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3659\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3659\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:5736\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   5733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5734\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5735\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5736\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 0)"
     ]
    }
   ],
   "source": [
    "# Print out predictions for power, voltage, and current\n",
    "print(\"5-Minute-Ahead Predictions:\")\n",
    "print(\"Power Predictions (uW):\")\n",
    "print(y_test[:, 0])\n",
    "\n",
    "print(\"\\nVoltage Predictions (mV):\")\n",
    "print(y_test[:, 1])\n",
    "\n",
    "print(\"\\nCurrent Predictions (Î¼A):\")\n",
    "print(y_test[:, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPNnFqb6xANP"
   },
   "source": [
    "##  3. Graph Selected Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJ3n4i54x7gr"
   },
   "source": [
    "###  3.1 Load and graph pretrained SNN models\n",
    "\n",
    "###  In order to use pretrained models it is neccesary to download the ```trained_models``` directory from [Hugging Face](https://huggingface.co/datasets/adunlop621/Soil_MFC/tree/main) and store it in the same directory as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lqz6FbuV2biX",
    "outputId": "fa2dee73-5cbd-4c61-c923-a9993f93d1d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-53e4bd9d71dc>:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/content/trained_models_folder/snn_5min_quant50.pth\", map_location=torch.device('cpu')), strict=False)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "time_frame = '60min'\n",
    "batchsize = 8\n",
    "\n",
    "X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"],df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"], df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"], df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1)\n",
    "#X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1)\n",
    "y = pd.concat([df[\"Power (uW)\"], df['Voltage (mV)'], df['Current (uA)']], axis = 1)\n",
    "\n",
    "#Normalize Data\n",
    "X_normalized = ((X - X.min()) / (X.max() - X.min()))\n",
    "\n",
    "#Split train and test sets\n",
    "# X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
    "# y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "# y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "# Split into training and testing sets (70% training, 30% testing)\n",
    "X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
    "y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Split the training set into teacher and student subsets (50/50)\n",
    "X_train_teacher, X_train_student = train_test_split(X_train, test_size=0.5, shuffle=False)\n",
    "y_train_teacher, y_train_student = train_test_split(y_train, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Split the testing set into validation and final test sets (50/50)\n",
    "X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "#Resample data\n",
    "X_train_student = X_train_student.resample(time_frame).mean().dropna()\n",
    "y_train_student = y_train_student.resample(time_frame).mean().dropna()\n",
    "X_valid = X_valid.resample(time_frame).mean().dropna()\n",
    "y_valid = y_valid.resample(time_frame).mean().dropna()\n",
    "\n",
    "X_test = X_test.resample(time_frame).mean().dropna()\n",
    "y_test = y_test.resample(time_frame).mean().dropna()\n",
    "\n",
    "#Define mv1\n",
    "mv1 = y_train_student\n",
    "\n",
    "#Reshape data\n",
    "X_train_teacher = X_train_teacher.values.reshape((X_train_teacher.shape[0], 1, X_train_teacher.shape[1]))\n",
    "X_train_student = X_train_student.values.reshape((X_train_student.shape[0], 1, X_train_student.shape[1]))\n",
    "X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
    "X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# convert to tensor\n",
    "X_train_teacher = torch.tensor(X_train_teacher)\n",
    "y_train_teacher = torch.tensor(y_train_teacher.values)\n",
    "X_train_student = torch.tensor(X_train_student)\n",
    "y_train_student = torch.tensor(y_train_student.values)\n",
    "X_valid = torch.tensor(X_valid)\n",
    "y_valid = torch.tensor(y_valid.values)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test.values)\n",
    "\n",
    "# make datasets\n",
    "train_teacher_dataset = TensorDataset(X_train_teacher, y_train_teacher)\n",
    "train_student_dataset = TensorDataset(X_train_student, y_train_student)\n",
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_teacher_loader = DataLoader(train_teacher_dataset, batch_size=batchsize, shuffle=False)\n",
    "train_student_loader = DataLoader(train_student_dataset, batch_size=batchsize, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batchsize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
    "\n",
    "num_steps = 50\n",
    "num_inputs = X_train_student.shape[2]\n",
    "\n",
    "# create new inctance of the SNN Class\n",
    "model = Net(num_inputs, num_steps).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"trained_models/snn_5min_quant50.pth\", map_location=torch.device('cpu')), strict=False)\n",
    "model.eval()\n",
    "actuals = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in train_student_loader:\n",
    "\n",
    "        # prepare data\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        data = data.float()\n",
    "        targets = targets.float()\n",
    "\n",
    "        _, _, _, output = model(data)\n",
    "\n",
    "        output = output.cpu()\n",
    "        output = output.squeeze(1).detach()\n",
    "\n",
    "        prediction = output[-1]\n",
    "\n",
    "        actuals.append(targets)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "# Convert lists to tensors\n",
    "actuals = torch.cat(actuals, dim=0)\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "\n",
    "mv1[\"power_pred_med\"] = predictions[:, 0]\n",
    "mv1[\"voltage_pred_med\"] = predictions[:, 1]\n",
    "mv1[\"current_pred_med\"] = predictions[:, 2]\n",
    "#mv1 = mv1.loc[(mv1.index > '2022-01-04') & (mv1.index < '2022-01-06')]\n",
    "mv1 = mv1.loc[(mv1.index > '2021-12-12') & (mv1.index < '2021-12-14')]\n",
    "mv2 = mv1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A30xOdiNWLp4",
    "outputId": "07760aa8-8abb-47dd-b6d7-96527552a328"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mv1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmv1\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mv1' is not defined"
     ]
    }
   ],
   "source": [
    "print(mv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cbZHNDVyAzb"
   },
   "source": [
    "###  3.2 SNN Peformance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831
    },
    "collapsed": true,
    "id": "1o7no5x6w7vP",
    "outputId": "b6644230-369f-437b-cdd7-d4c6e0096cfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_models/snn_5min_quant50.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 122\u001b[0m\n\u001b[1;32m    119\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    120\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m--> 122\u001b[0m _, _, _, output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    125\u001b[0m actuals\u001b[38;5;241m.\u001b[39mappend(targets)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[18], line 48\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m mem_rec \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_steps):\n\u001b[0;32m---> 48\u001b[0m     spk1, syn1, mem1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslstm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msyn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     spk2, mem2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(spk1), mem2)\n\u001b[1;32m     50\u001b[0m     spk3, mem3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(spk2), mem3)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/snntorch/_neurons/slstm.py:243\u001b[0m, in \u001b[0;36mSLSTM.forward\u001b[0;34m(self, input_, syn, mem)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(correct_shape, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem_reset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem)\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msyn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_quant:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msyn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_quant(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msyn)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/snntorch/_neurons/slstm.py:281\u001b[0m, in \u001b[0;36mSLSTM._base_int\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_base_int\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_):\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_state_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/snntorch/_neurons/slstm.py:259\u001b[0m, in \u001b[0;36mSLSTM._base_state_function\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_base_state_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_):\n\u001b[0;32m--> 259\u001b[0m     base_fn_mem, base_fn_syn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msyn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base_fn_syn, base_fn_mem\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1705\u001b[0m, in \u001b[0;36mLSTMCell.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1703\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched \u001b[38;5;28;01melse\u001b[39;00m hx\n\u001b[0;32m-> 1705\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_cell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   1715\u001b[0m     ret \u001b[38;5;241m=\u001b[39m (ret[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), ret[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Set parameters\n",
    "batchsize_list = [300, 150, 50, 20, 8]\n",
    "time_frame_list = ['3min', '5min', '15min', '30min', '60min']\n",
    "#time_frame_list = ['3min', '5min']\n",
    "time_frame_seconds_list = [180, 300, 900, 1800, 3600]\n",
    "#time_frame_seconds_list = [180, 300]\n",
    "n = 0\n",
    "\n",
    "snn_power_mape_list = []\n",
    "snn_volt_mape_list = []\n",
    "snn_curr_mape_list = []\n",
    "\n",
    "# Dictionary to store mv variables\n",
    "mv_dict = {}\n",
    "\n",
    "for j in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[j]\n",
    "    time_frame = time_frame_list[1]\n",
    "    time_frame_seconds = time_frame_seconds_list[1]\n",
    "    #time_frame = '5min'\n",
    "    #time_frame_seconds = 300\n",
    "\n",
    "    n += 1\n",
    "\n",
    "    X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"],\n",
    "                   df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"],\n",
    "                   df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"],\n",
    "                   df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"],\n",
    "                   df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"],\n",
    "                   df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"],\n",
    "                   df[\"tsd\"], df[\"hour\"]], axis=1)\n",
    "    y = pd.concat([df[\"Power (uW)\"], df['Voltage (mV)'], df['Current (uA)']], axis=1)\n",
    "\n",
    "    # Normalize Data\n",
    "    X_normalized = ((X - X.min()) / (X.max() - X.min()))\n",
    "\n",
    "    # Split train and test sets\n",
    "    X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
    "    y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
    "\n",
    "\n",
    "    # Split the training set into teacher and student subsets (50/50)\n",
    "    X_train_teacher, X_train_student = train_test_split(X_train, test_size=0.5, shuffle=False)\n",
    "    y_train_teacher, y_train_student = train_test_split(y_train, test_size=0.5, shuffle=False)\n",
    "\n",
    "    # Split the testing set into validation and final test sets (50/50)\n",
    "    X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "    y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "    #Resample data\n",
    "    # X_train_student = X_train_student.resample(time_frame).mean().dropna()\n",
    "    # y_train_student = y_train_student.resample(time_frame).mean().dropna()\n",
    "    X_valid = X_valid.resample(time_frame).mean().dropna()\n",
    "    y_valid = y_valid.resample(time_frame).mean().dropna()\n",
    "\n",
    "    X_test = X_test.resample(time_frame).mean().dropna()\n",
    "    y_test = y_test.resample(time_frame).mean().dropna()\n",
    "\n",
    "    # Calculate actual energy generated in test set\n",
    "    E_actual = 0\n",
    "    for i in range(len(y_train_student) - 1):\n",
    "        t = (y_train_student.index[i+1] - y_train_student.index[i]).total_seconds()\n",
    "        if t < 180:\n",
    "            E_actual += y_train_student['Power (uW)'][i] * t\n",
    "\n",
    "\n",
    "    # Define mv variable for the current time frame\n",
    "    mv_dict[time_frame] = y_train_student\n",
    "\n",
    "    #Reshape data\n",
    "    X_train_teacher = X_train_teacher.values.reshape((X_train_teacher.shape[0], 1, X_train_teacher.shape[1]))\n",
    "    X_train_student = X_train_student.values.reshape((X_train_student.shape[0], 1, X_train_student.shape[1]))\n",
    "    X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
    "    X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "    # convert to tensor\n",
    "    X_train_teacher = torch.tensor(X_train_teacher)\n",
    "    y_train_teacher = torch.tensor(y_train_teacher.values)\n",
    "    X_train_student = torch.tensor(X_train_student)\n",
    "    y_train_student = torch.tensor(y_train_student.values)\n",
    "    X_valid = torch.tensor(X_valid)\n",
    "    y_valid = torch.tensor(y_valid.values)\n",
    "    X_test = torch.tensor(X_test)\n",
    "    y_test = torch.tensor(y_test.values)\n",
    "\n",
    "    # make datasets\n",
    "    train_teacher_dataset = TensorDataset(X_train_teacher, y_train_teacher)\n",
    "    train_student_dataset = TensorDataset(X_train_student, y_train_student)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_teacher_loader = DataLoader(train_teacher_dataset, batch_size=batchsize, shuffle=False)\n",
    "    train_student_loader = DataLoader(train_student_dataset, batch_size=batchsize, shuffle=False)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batchsize, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
    "\n",
    "    num_steps = 50\n",
    "    num_inputs = X_train_student.shape[2]\n",
    "\n",
    "    # Create new instance of the SNN Class\n",
    "    model = Net(num_inputs, num_steps).to(device)\n",
    "\n",
    "    file = 'trained_models/snn_' + time_frame + '_quant50.pth'\n",
    "    print(file)\n",
    "\n",
    "    checkpoint = torch.load(file, map_location=torch.device('cpu'), weights_only=True)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data, targets in train_student_loader:\n",
    "            #print(i)\n",
    "            # Prepare data\n",
    "            data = data.to(device).float()\n",
    "            targets = targets.to(device).float()\n",
    "\n",
    "            _, _, _, output = model(data)\n",
    "\n",
    "            output = output.cpu().squeeze(1).detach()\n",
    "            actuals.append(targets)\n",
    "            predictions.append(output[-1])\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    actuals = torch.cat(actuals, dim=0)\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "\n",
    "    mv = mv_dict[time_frame]\n",
    "    mv[\"power_pred_med_\" + time_frame] = predictions[:, 0].numpy()\n",
    "    mv[\"voltage_pred_med_\" + time_frame] = predictions[:, 1].numpy()\n",
    "    mv[\"current_pred_med_\" + time_frame] = predictions[:, 2].numpy()\n",
    "\n",
    "    print(f'Voltage overestimation rate for {time_frame}: %.3f%%' % (\n",
    "        (mv['Voltage (mV)'].values <= mv[\"voltage_pred_med_\" + time_frame]).mean() * 100))\n",
    "    print(f\"Test MAPE power ({time_frame}): %3f\" % MAPE(mv['Power (uW)'].values.ravel(), mv[\"power_pred_med_\" + time_frame]))\n",
    "    print(f\"Test MAPE voltage ({time_frame}): %3f\" % MAPE(mv['Voltage (mV)'], mv[\"voltage_pred_med_\" + time_frame]))\n",
    "    print(f\"Test MAPE current ({time_frame}): %3f\" % MAPE(mv['Current (uA)'], mv[\"current_pred_med_\" + time_frame]))\n",
    "\n",
    "    E_pred = 0\n",
    "    for i in range(len(mv) - 1):\n",
    "        t = (mv.index[i+1] - mv.index[i]).total_seconds()\n",
    "        if t <= time_frame_seconds + 50:\n",
    "            E_pred += mv[\"power_pred_med_\" + time_frame][i] * t\n",
    "\n",
    "    print(f'Predicted vs. Actual Total Energy Percent Difference ({time_frame}): %.3f%%' % (\n",
    "        (E_pred - E_actual) * 100 / E_actual))\n",
    "\n",
    "    V_actual = mv['Voltage (mV)'].mean()\n",
    "    V_pred = mv[\"voltage_pred_med_\" + time_frame].mean()\n",
    "    print(f'Predicted vs. Actual Total Voltage Percent Difference ({time_frame}): %.3f%%' % (\n",
    "        (V_pred - V_actual) * 100 / V_actual))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Npx5qknU0O-J",
    "outputId": "a55b8709-b96e-402a-dce0-b655cce94a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_models/snn_5min_quant50.pth\n",
      "predictions:  tensor([[ 716.3546, 3280.2092, 1918.9449],\n",
      "        [ 716.3546, 3280.2092, 1918.9449],\n",
      "        [ 716.3546, 3280.2092, 1918.9449],\n",
      "        ...,\n",
      "        [ 518.3690, 2080.7988, 2589.0771],\n",
      "        [ 518.3690, 2080.7988, 2589.0771],\n",
      "        [ 523.3364, 2113.8623, 2593.1101]])\n",
      "actuals:  tensor([[28544.3105, 30979.2246,  9214.1084],\n",
      "        [28838.2109, 30978.0762,  9309.3037],\n",
      "        [29093.4941, 30980.6113,  9391.0059],\n",
      "        ...,\n",
      "        [ 1025.7499,  3787.1465,  2725.7690],\n",
      "        [  966.0275,  3962.2629,  2456.9177],\n",
      "        [  872.7040,  4119.1685,  2137.6677]])\n",
      "Voltage overestimation rate for 5min: 61.404%\n",
      "Test MAPE power (5min): 49.838094\n",
      "Test MAPE voltage (5min): 27.417626\n",
      "Test MAPE current (5min): 30.729853\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Set parameters\n",
    "batchsize_list = [300, 150, 50, 20, 8]\n",
    "time_frame = '5min'\n",
    "time_frame_seconds = 300\n",
    "\n",
    "# Initialize results storage\n",
    "snn_power_mape_list = []\n",
    "snn_volt_mape_list = []\n",
    "snn_curr_mape_list = []\n",
    "\n",
    "# Dictionary to store mv variables\n",
    "mv_dict = {}\n",
    "\n",
    "# Process only the first batch size\n",
    "batchsize = batchsize_list[1]  # Pick the first batch size\n",
    "\n",
    "X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"],\n",
    "               df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"],\n",
    "               df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"],\n",
    "               df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"],\n",
    "               df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"],\n",
    "               df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"],\n",
    "               df[\"tsd\"], df[\"hour\"]], axis=1)\n",
    "y = pd.concat([df[\"Power (uW)\"], df['Voltage (mV)'], df['Current (uA)']], axis=1)\n",
    "\n",
    "# Normalize Data\n",
    "X_normalized = ((X - X.min()) / (X.max() - X.min()))\n",
    "\n",
    "# Split train and test sets\n",
    "X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
    "y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Split the testing set into validation and final test sets (50/50)\n",
    "X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Resample data\n",
    "X_train = X_train.resample(time_frame).mean().dropna()\n",
    "y_train = y_train.resample(time_frame).mean().dropna()\n",
    "X_valid = X_valid.resample(time_frame).mean().dropna()\n",
    "y_valid = y_valid.resample(time_frame).mean().dropna()\n",
    "\n",
    "X_test = X_test.resample(time_frame).mean().dropna()\n",
    "y_test = y_test.resample(time_frame).mean().dropna()\n",
    "\n",
    "# Define mv variable for the current time frame\n",
    "mv_dict[time_frame] = y_train\n",
    "\n",
    "# Reshape data\n",
    "X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
    "X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Convert to tensor\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train.values)\n",
    "X_valid = torch.tensor(X_valid)\n",
    "y_valid = torch.tensor(y_valid.values)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test.values)\n",
    "\n",
    "# Make datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batchsize, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
    "\n",
    "num_steps = 50\n",
    "num_inputs = X_train.shape[2]\n",
    "\n",
    "# Create new instance of the SNN Class\n",
    "model = Net(num_inputs, num_steps).to(device)\n",
    "\n",
    "file = 'trained_models/snn_' + time_frame + '_quant50.pth'\n",
    "print(file)\n",
    "\n",
    "checkpoint = torch.load(file, map_location=torch.device('cpu'), weights_only=True)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model.eval()\n",
    "actuals = []\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for data, targets in train_loader:\n",
    "        # Prepare data\n",
    "        data = data.to(device).float()\n",
    "        targets = targets.to(device).float()\n",
    "\n",
    "        _, _, _, output = model(data)\n",
    "\n",
    "        output = output.cpu().squeeze(1).detach()\n",
    "        actuals.append(targets)\n",
    "        predictions.append(output[-1])\n",
    "\n",
    "# Convert lists to tensors\n",
    "actuals = torch.cat(actuals, dim=0)\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "print(\"predictions: \", predictions)\n",
    "print(\"actuals: \", actuals)\n",
    "\n",
    "mv = mv_dict[time_frame]\n",
    "mv[\"power_pred_med_\" + time_frame] = predictions[:, 0].numpy()\n",
    "mv[\"voltage_pred_med_\" + time_frame] = predictions[:, 1].numpy()\n",
    "mv[\"current_pred_med_\" + time_frame] = predictions[:, 2].numpy()\n",
    "\n",
    "print(f'Voltage overestimation rate for {time_frame}: %.3f%%' % (\n",
    "    (mv['Voltage (mV)'].values <= mv[\"voltage_pred_med_\" + time_frame]).mean() * 100))\n",
    "print(f\"Test MAPE power ({time_frame}): %3f\" % MAPE(mv['Power (uW)'].values.ravel(), mv[\"power_pred_med_\" + time_frame]))\n",
    "print(f\"Test MAPE voltage ({time_frame}): %3f\" % MAPE(mv['Voltage (mV)'], mv[\"voltage_pred_med_\" + time_frame]))\n",
    "print(f\"Test MAPE current ({time_frame}): %3f\" % MAPE(mv['Current (uA)'], mv[\"current_pred_med_\" + time_frame]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rbLhQ6qyuEaQ",
    "outputId": "1f369586-627c-4487-9935-64c30e69fd9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Power (uW)', 'Voltage (mV)', 'Current (uA)', 'power_pred_med_5min',\n",
      "       'voltage_pred_med_5min', 'current_pred_med_5min'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(mv_dict['5min'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMZgk8NYuEaQ"
   },
   "source": [
    "# Student Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDBlmqCZuEaR"
   },
   "source": [
    "## Train Student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Power (uW)  Voltage (mV)  Current (uA)\n",
      "timestamp                                                          \n",
      "2021-06-04 00:00:50-07:00  29283.382409     30977.020      9453.260\n",
      "2021-06-04 00:01:04-07:00  31207.828873     30968.787     10077.188\n",
      "2021-06-04 00:01:17-07:00  28851.587848     30978.863      9313.314\n",
      "2021-06-04 00:01:31-07:00  28715.818842     30978.003      9269.745\n",
      "2021-06-04 00:01:45-07:00  27285.687612     30983.655      8806.478\n",
      "...                                 ...           ...           ...\n",
      "2022-01-26 23:58:52-08:00   9185.733148      2438.569     37668.539\n",
      "2022-01-26 23:59:06-08:00     41.767202       855.004       488.503\n",
      "2022-01-26 23:59:19-08:00     19.878576       855.250       232.430\n",
      "2022-01-26 23:59:33-08:00    105.926588       820.843      1290.461\n",
      "2022-01-26 23:59:47-08:00    284.662956       713.814      3987.915\n",
      "\n",
      "[969652 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JDO5w2qMuEaR",
    "outputId": "60478509-388c-4cd5-a6ee-b899f94de651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10285, 20)\n",
      "(10285, 3)\n",
      "NaN Entries in teacher_5min_preds_df.index:\n",
      "DatetimeIndex([], dtype='datetime64[ns, US/Pacific]', name='timestamp', freq=None)\n",
      "Updated X Shape: Rows 10285 Columns 23\n",
      "(10285, 23)\n",
      "(10285, 3)\n",
      "Training labels shape (X_with_teacher): (10285, 23)\n",
      "Training set shape (X_train_final): (7199, 23)\n",
      "Training labels shape (y_train_final): (7199, 3)\n",
      "QUANTILE LOSS:  tensor(11591.4219, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.7238, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 0, Loss: 11591.4219\n",
      "QUANTILE LOSS:  tensor(10171.4805, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8215.1436, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.7411, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6147.6899, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.7397, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6030.6597, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.7282, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1375.8539, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.4816, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(720.2476, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.3549, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(770.2957, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.2309, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.4085, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(838.0635, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.3712, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1147.8749, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6325, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 10, Loss: 1147.8749\n",
      "QUANTILE LOSS:  tensor(734.9440, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.3259, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(503.7939, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.2399, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(568.2648, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6214, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(742.5877, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.3259, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(464.4542, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.2066, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1508.3823, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6592, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1342.1171, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.7160, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(923.8755, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(939.3871, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.7079, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(895.1331, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6892, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 20, Loss: 895.1331\n",
      "QUANTILE LOSS:  tensor(1503.3959, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6913, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1270.2887, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.7121, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1120.8384, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6931, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(844.5560, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.9382, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6814, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.0898, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6232, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(684.1411, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5714, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(803.9602, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6317, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(799.6597, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5762, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(616.2614, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5617, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 30, Loss: 616.2614\n",
      "QUANTILE LOSS:  tensor(741.2571, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6572, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.7943, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5833, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(590.1140, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5826, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(717.7635, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6566, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(732.3862, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5819, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(800.7762, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6067, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.9638, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6498, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(787.7515, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5651, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(616.3831, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5759, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.2378, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6494, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 40, Loss: 769.2378\n",
      "QUANTILE LOSS:  tensor(769.7674, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5774, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(714.1102, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5901, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(915.0472, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6572, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.8342, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5669, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(589.7495, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5772, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(754.9385, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6467, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(783.3145, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5661, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(777.9546, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5470, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1322.7782, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6560, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(852.2112, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.4788, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 50, Loss: 852.2112\n",
      "QUANTILE LOSS:  tensor(1394.6118, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6633, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(904.1045, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6129, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(765.7785, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.4658, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(401.6605, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6306, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(760.3004, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6403, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.3228, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6028, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2530.8113, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6613, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1324.1029, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(940.4993, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6270, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(543.2443, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6374, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 60, Loss: 543.2443\n",
      "QUANTILE LOSS:  tensor(811.3996, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6338, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(883.5464, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6225, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(395.3756, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6136, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(783.9614, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6284, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(993.6768, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6111, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1392.1519, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6058, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(969.6685, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6279, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(947.1154, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5925, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1831.3309, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.4731, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1326.7223, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5591, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 70, Loss: 1326.7223\n",
      "QUANTILE LOSS:  tensor(1032.5543, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5373, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(911.2642, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.3171, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1013.4139, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.4514, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(944.8359, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.4040, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1551.6180, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5819, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1174.2142, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.5319, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(998.6760, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(0.6921, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(750.9958, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1.0175, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(993.7852, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1.1902, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(850.1397, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1.5424, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 80, Loss: 850.1397\n",
      "QUANTILE LOSS:  tensor(523.2082, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1.8431, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(914.5377, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2.2252, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(887.6950, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2.4342, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1634.7382, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2.7135, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1248.0797, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3.0336, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1039.3206, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3.3456, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1143.2522, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3.6713, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(919.8005, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4.1270, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(907.3283, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4.5271, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1504.3612, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4.8892, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 90, Loss: 1504.3612\n",
      "QUANTILE LOSS:  tensor(1179.8777, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(996.8466, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5.8688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(935.1730, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6.3376, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(926.5814, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6.9378, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(907.5942, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7.4406, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1655.1328, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7.9356, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1223.4698, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8.5440, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1007.1075, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9.1680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(872.4406, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9.7404, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(934.3989, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10.4582, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 100, Loss: 934.3989\n",
      "QUANTILE LOSS:  tensor(862.1008, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11.0946, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(471.7903, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11.8166, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(802.5849, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12.4598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(937.7525, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13.1667, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1577.7852, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13.7083, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1166.2456, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14.4756, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(947.5178, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15.3738, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(830.1326, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15.9499, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(925.1331, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16.7844, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(943.8373, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17.6483, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 110, Loss: 943.8373\n",
      "QUANTILE LOSS:  tensor(1445.6527, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18.1902, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1163.0731, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19.0621, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1015.1851, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20.0254, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(964.8096, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20.6410, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(826.3480, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21.6472, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(777.8679, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22.5793, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1073.0472, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23.2640, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1190.8209, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24.0028, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(988.5999, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25.0080, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1157.9940, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25.7075, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 120, Loss: 1157.9940\n",
      "QUANTILE LOSS:  tensor(991.1265, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26.6804, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(871.6401, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27.7735, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(711.5178, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28.4910, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(721.2796, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29.6626, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(716.4761, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30.6679, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1352.2449, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31.1125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1089.8505, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32.1591, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(919.0266, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33.2879, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1057.5886, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34.0034, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(930.6491, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35.1253, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 130, Loss: 930.6491\n",
      "QUANTILE LOSS:  tensor(916.0009, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36.2381, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1197.6748, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36.8782, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1067.8870, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37.9671, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(896.0566, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39.2461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(980.1275, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39.9347, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(925.2666, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41.1585, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(781.6650, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42.4180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1050.1935, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43.0841, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1086.3716, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44.0873, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(900.9352, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45.5923, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 140, Loss: 900.9352\n",
      "QUANTILE LOSS:  tensor(964.4645, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46.1567, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(933.3205, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47.4520, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(791.4152, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48.7988, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(711.8240, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49.5628, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(668.9686, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51.1322, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(655.4328, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53.0902, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1700.6274, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52.5557, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1331.8713, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53.7277, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1130.9640, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54.9427, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1202.2688, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56.1735, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 150, Loss: 1202.2688\n",
      "QUANTILE LOSS:  tensor(1135.7167, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57.3739, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(954.9951, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58.8365, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(957.2057, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59.5878, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(984.3074, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61.0459, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(887.9470, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62.2236, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(906.7604, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63.1762, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(960.0949, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64.6895, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(935.9067, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66.3883, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1446.4327, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66.4496, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1138.3278, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67.8415, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 160, Loss: 1138.3278\n",
      "QUANTILE LOSS:  tensor(929.7713, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69.9135, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.5561, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70.2746, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(970.7075, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72.3397, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(922.1143, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73.5955, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1215.0179, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73.8055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1073.1635, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75.8348, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(894.5103, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77.3559, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(895.5491, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77.7590, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(979.8684, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79.8341, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(866.2366, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81.1967, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 170, Loss: 866.2366\n",
      "QUANTILE LOSS:  tensor(1129.4395, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1011.0113, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83.5575, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(835.6518, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85.2043, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(878.8260, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85.5265, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(933.8575, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87.5908, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(793.2687, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89.1624, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(834.8064, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89.5275, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(883.3307, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91.9701, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(813.7805, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(93.2207, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1107.7740, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(93.2246, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 180, Loss: 1107.7740\n",
      "QUANTILE LOSS:  tensor(957.6398, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(95.7699, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.8758, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(97.2427, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(818.2067, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(98.0954, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(866.5690, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(100.4504, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(815.7356, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(101.2826, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(791.4096, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(102.8334, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(838.4866, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(104.8828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(842.4045, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(105.5151, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(873.9699, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(106.8805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(845.4409, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(108.8434, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 190, Loss: 845.4409\n",
      "QUANTILE LOSS:  tensor(771.4177, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(110.0417, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.7814, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(112.4780, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(813.9961, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(113.2143, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.7300, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(114.4181, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(748.4033, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(116.9595, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(807.7260, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(117.6609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.7620, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(117.8101, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(713.4968, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(121.6223, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(783.7110, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(121.7978, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(748.1934, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(122.0328, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 200, Loss: 748.1934\n",
      "QUANTILE LOSS:  tensor(911.0046, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(125.0323, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(877.8660, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(125.4100, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.4164, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(126.4834, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(771.2682, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(130.2506, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(840.5242, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(129.8045, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(738.4875, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(137.9123, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(789.4736, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(134.5762, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(824.4100, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(134.0784, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(774.6428, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(139.7531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(832.4398, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(138.7131, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 210, Loss: 832.4398\n",
      "QUANTILE LOSS:  tensor(854.9509, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(138.6528, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(786.1771, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(144.5737, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(731.5999, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(142.7395, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(922.2592, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(143.0728, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(979.2927, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(144.9780, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(974.8128, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(147.7466, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(935.0754, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(147.9000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(953.1506, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(149.8013, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1055.0331, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(152.1082, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1068.5604, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(153.5347, grad_fn=<MeanBackward0>)\n",
      "Epoch 1/25, Batch 220, Loss: 1068.5604\n",
      "QUANTILE LOSS:  tensor(1058.8007, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(154.6552, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1087.5278, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(155.9428, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(762.4813, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(157.6513, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.7370, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(159.2465, grad_fn=<MeanBackward0>)\n",
      "Epoch 1, Avg Loss: 1108.1106\n",
      "QUANTILE LOSS:  tensor(11584.7373, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(160.7781, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 0, Loss: 11584.7373\n",
      "QUANTILE LOSS:  tensor(10164.7637, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(162.4492, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8208.3945, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(163.7734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6140.9087, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(165.4396, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6023.8472, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(167.2057, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1369.0087, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(170.3206, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(713.3706, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(174.8486, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.3868, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(173.0039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(759.2901, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(176.6478, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.0911, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(181.3876, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1140.8706, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(178.5668, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 10, Loss: 1140.8706\n",
      "QUANTILE LOSS:  tensor(727.9080, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(183.2407, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(496.7262, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(188.4964, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(561.1655, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(184.1022, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(735.4567, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(188.3352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(457.2917, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(195.1833, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1501.1884, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(188.7392, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1334.8915, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(189.5126, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(916.6183, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(191.7782, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(932.0984, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(193.1229, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(887.8132, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(195.1896, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 20, Loss: 887.8132\n",
      "QUANTILE LOSS:  tensor(1496.0444, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(196.9470, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1262.9059, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(198.3605, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1113.4242, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(200.4178, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.1105, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(202.9681, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(885.4614, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(204.2457, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(872.5817, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(206.8029, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(676.6018, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(209.5553, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(796.3897, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(210.3825, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(792.0580, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(212.8200, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(608.6285, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(215.3436, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 30, Loss: 608.6285\n",
      "QUANTILE LOSS:  tensor(733.5930, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(215.5826, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(777.0991, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(218.2830, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(582.3878, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(220.4587, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(710.0063, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(221.1295, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(724.5980, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(223.8695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(792.9568, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(225.6234, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.1135, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(226.8216, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.8702, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(229.7341, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(608.4709, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(231.8451, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(761.2947, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(232.4808, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 40, Loss: 761.2947\n",
      "QUANTILE LOSS:  tensor(761.7935, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(235.2710, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(706.1054, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(237.2293, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(907.0115, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(237.9222, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(776.7678, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(241.1677, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(581.6522, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(243.2535, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(746.8104, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(243.9497, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(775.1556, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(246.9990, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.7651, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(249.6010, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1314.5579, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(249.4561, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.9604, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(253.1719, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 50, Loss: 843.9604\n",
      "QUANTILE LOSS:  tensor(1386.3304, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(253.2796, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(895.7924, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(255.9299, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.4358, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(259.9929, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(393.2870, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(259.8529, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.8964, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(261.6727, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(770.8882, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(263.8187, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2522.3462, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(265.1902, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1315.6073, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(267.2342, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(931.9733, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(269.8081, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(534.6878, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(271.6958, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 60, Loss: 534.6878\n",
      "QUANTILE LOSS:  tensor(802.8126, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(273.8086, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.9290, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(275.9137, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(386.7278, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(278.2858, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(775.2831, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(280.0208, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(984.9682, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(282.1606, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1383.4225, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(284.0433, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(960.8990, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(286.1537, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(938.3157, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(288.2950, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1822.5751, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(289.7900, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1317.8936, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(292.0493, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 70, Loss: 1317.8936\n",
      "QUANTILE LOSS:  tensor(1023.6937, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(294.4404, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(902.5229, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(296.2537, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1004.5350, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(298.7085, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(935.9355, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(301.1040, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1543.1549, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(302.4793, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1165.7125, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(304.8735, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(990.2067, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(307.1082, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(742.6159, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(309.1045, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(985.4217, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(311.5257, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(841.8217, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(313.7967, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 80, Loss: 841.8217\n",
      "QUANTILE LOSS:  tensor(514.9414, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(316.0963, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(906.3066, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(318.1888, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(879.4778, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(320.1927, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1626.5433, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(321.9401, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1239.9020, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(324.3296, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1031.1633, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(326.7173, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1135.1146, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(328.6135, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(911.6831, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(331.4658, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(899.2318, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(333.6277, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1496.2855, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(335.1979, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 90, Loss: 1496.2855\n",
      "QUANTILE LOSS:  tensor(1171.8234, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(337.6813, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(988.8135, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(340.0435, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(927.1614, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(342.0431, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(918.5911, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(344.8595, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(899.6248, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(346.8652, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1647.1842, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(348.6820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1215.5422, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(351.1660, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(999.2004, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(353.6279, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(864.5535, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(355.6608, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(926.5317, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(358.4698, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 100, Loss: 926.5317\n",
      "QUANTILE LOSS:  tensor(854.2532, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(360.7308, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(463.9621, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(363.3589, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(794.7757, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(365.4780, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(929.9619, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(367.9455, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1570.0127, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(369.3321, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1158.4911, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(371.9279, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(939.7808, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(375.1431, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(822.4128, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(376.6049, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(917.4304, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(379.3289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(936.1512, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(382.1769, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 110, Loss: 936.1512\n",
      "QUANTILE LOSS:  tensor(1437.9828, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(383.3807, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1155.4193, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(386.1243, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1007.5468, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(389.2707, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(957.1866, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(390.7079, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(818.7402, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(393.8912, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(770.2747, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(396.7490, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1065.4685, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(398.3864, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1183.2565, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(400.3086, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(981.0491, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(403.3130, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1150.4570, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(404.9944, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 120, Loss: 1150.4570\n",
      "QUANTILE LOSS:  tensor(983.6028, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(407.7697, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(864.1296, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(410.9928, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(704.0200, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(412.6770, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(713.7946, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(416.1192, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(709.0035, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(418.8872, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1344.7845, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(419.4854, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1082.4020, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(422.3411, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(911.5898, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(425.4744, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1050.1632, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(427.0570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(923.2352, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(430.0946, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 130, Loss: 923.2352\n",
      "QUANTILE LOSS:  tensor(908.5980, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(433.1031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1190.2828, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(434.3518, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1060.5057, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(437.1869, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(888.6858, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(440.6599, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(972.7673, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(442.0637, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(917.9163, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(445.2778, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(774.3249, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(448.6377, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1042.8632, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(449.8811, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1079.0509, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(452.2998, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(893.6241, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(456.3428, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 140, Loss: 893.6241\n",
      "QUANTILE LOSS:  tensor(957.1628, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(457.2734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(926.0280, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(460.5598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.1318, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(463.9958, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(704.5495, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(465.5371, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(661.7029, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(469.6117, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(648.1757, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(474.8718, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1693.3790, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(472.3818, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1324.6310, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(475.1599, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1123.7319, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(478.0444, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1195.0450, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(480.9590, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 150, Loss: 1195.0450\n",
      "QUANTILE LOSS:  tensor(1128.5011, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(483.7799, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(947.7873, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(487.3847, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(950.0057, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(488.7689, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(977.1149, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(492.2829, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.7621, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(495.0360, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(899.5832, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(496.9433, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(952.9250, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(500.5563, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(928.7441, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(504.7213, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1439.2773, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(504.0987, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1131.1793, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(507.3086, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 160, Loss: 1131.1793\n",
      "QUANTILE LOSS:  tensor(922.6299, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(512.5016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(879.4216, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(512.6840, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(963.5797, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(517.6882, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(914.9932, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(520.6705, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1207.9036, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(520.3033, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1066.0555, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(525.1735, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(887.4089, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(528.8250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(888.4541, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(528.9805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(972.7796, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(533.8561, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(859.1541, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(537.0347, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 170, Loss: 859.1541\n",
      "QUANTILE LOSS:  tensor(1122.3630, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(536.6009, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1003.9410, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(541.6999, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(828.5874, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(545.5633, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(871.7675, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(545.5421, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(926.8049, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(550.2354, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(786.2219, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(553.8697, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(827.7653, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(553.9630, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(876.2952, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(559.5514, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(806.7505, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(562.3080, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1100.7494, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(561.5038, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 180, Loss: 1100.7494\n",
      "QUANTILE LOSS:  tensor(950.6208, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(567.2939, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(818.8621, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(570.5677, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(811.1984, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(571.8906, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(859.5659, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(577.1537, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(808.7378, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(578.7939, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.4168, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(581.7983, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.4990, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(586.2971, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(835.4219, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(587.3823, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(866.9922, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(589.9061, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(838.4682, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(594.2128, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 190, Loss: 838.4682\n",
      "QUANTILE LOSS:  tensor(764.4499, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(596.5788, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(762.8184, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(601.5898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(807.0378, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(602.9681, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(756.7766, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(605.3040, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(741.4545, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(610.4867, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(800.7818, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(611.7793, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(719.8224, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(611.6873, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(706.5617, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(619.7343, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(776.7805, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(619.8217, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(741.2672, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(619.9183, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 200, Loss: 741.2672\n",
      "QUANTILE LOSS:  tensor(904.0828, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(626.1273, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(870.9487, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(626.6334, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.5034, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(628.5933, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(764.3594, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(636.4885, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(833.6196, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(635.1133, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(731.5870, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(652.4577, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(782.5773, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(644.7436, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(817.5178, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(643.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(767.7546, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(655.1758, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.5557, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(652.5416, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 210, Loss: 825.5557\n",
      "QUANTILE LOSS:  tensor(848.0707, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(652.0104, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.3010, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(664.3401, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(724.7277, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(660.0219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(915.3908, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(660.3484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(972.4282, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(664.0397, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(967.9521, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(669.4907, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(928.2183, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(669.4954, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(946.2972, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(673.1459, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1048.1835, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(677.5113, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1061.7145, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(680.2624, grad_fn=<MeanBackward0>)\n",
      "Epoch 2/25, Batch 220, Loss: 1061.7145\n",
      "QUANTILE LOSS:  tensor(1051.9584, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(682.2429, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1080.6891, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(684.5270, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(755.6461, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(687.7679, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(738.9054, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(690.7240, grad_fn=<MeanBackward0>)\n",
      "Epoch 2, Avg Loss: 1100.5906\n",
      "QUANTILE LOSS:  tensor(11577.9092, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(693.3013, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 0, Loss: 11577.9092\n",
      "QUANTILE LOSS:  tensor(10157.9385, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(696.4096, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8201.5732, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(698.7759, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6134.0903, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(701.8535, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6017.0317, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(705.1331, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1362.1976, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(711.3447, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(706.5630, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(720.2128, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(756.5823, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(715.9363, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(752.4890, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(723.1599, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(824.2933, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(732.2859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1134.0760, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(726.1778, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 10, Loss: 1134.0760\n",
      "QUANTILE LOSS:  tensor(721.1166, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(735.4183, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(489.9380, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(745.5273, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(554.3803, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(736.2592, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(728.6747, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(744.5593, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(450.5686, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(757.7324, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1494.4125, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(744.4846, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1328.1187, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(745.6752, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(909.8488, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(749.8268, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(925.3318, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(752.1559, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(881.0495, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(755.9011, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 20, Loss: 881.0495\n",
      "QUANTILE LOSS:  tensor(1489.2837, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(759.0293, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1256.1482, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(761.4772, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1106.6693, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(765.1826, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(830.3586, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(769.8423, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(878.7123, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(772.0046, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(865.8356, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(776.6736, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(669.8585, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(781.6939, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(789.6491, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(782.9625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(785.3203, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(787.3753, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(601.8935, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(791.8997, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 30, Loss: 601.8935\n",
      "QUANTILE LOSS:  tensor(726.8608, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(792.0256, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(770.3694, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(796.9235, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(575.6608, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(800.7593, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(703.2820, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(801.7149, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(717.8765, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(806.6592, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(786.2379, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(809.6699, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(765.3972, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(811.6286, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(773.1565, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(816.8726, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(601.7597, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(820.5374, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(754.5860, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(821.4222, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 40, Loss: 754.5860\n",
      "QUANTILE LOSS:  tensor(755.0873, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(826.4016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(699.4019, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(829.7664, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(900.3104, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(830.7629, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(770.0691, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(836.5630, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(574.9560, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(840.1440, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(740.1167, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(841.1393, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(768.4644, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(846.5436, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.0762, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(851.0705, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1307.8713, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(850.5054, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.2762, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(857.1667, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 50, Loss: 837.2762\n",
      "QUANTILE LOSS:  tensor(1379.6484, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(857.0003, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(889.1129, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(861.6114, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(750.7586, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(868.8365, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(386.6122, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(868.2397, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.2239, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(871.2920, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(764.2180, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(874.9612, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2515.6780, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(877.1689, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1308.9415, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(880.6185, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(925.3096, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(885.0259, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(528.0264, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(888.1752, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 60, Loss: 528.0264\n",
      "QUANTILE LOSS:  tensor(796.1535, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(891.7304, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(868.2720, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(895.2728, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(380.0730, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(899.2783, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(768.6306, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(902.1355, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(978.3176, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(905.7282, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1376.7740, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(908.8367, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(954.2529, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(912.3516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(931.6716, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(915.9440, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1815.9330, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(918.3326, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1311.2535, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(922.1016, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 70, Loss: 1311.2535\n",
      "QUANTILE LOSS:  tensor(1017.0557, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(926.1121, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(895.8870, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(929.0637, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(997.9011, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(933.1601, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(929.3036, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(937.1655, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1536.5249, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(939.3249, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1159.0846, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(943.2981, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(983.5808, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(946.9836, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(735.9919, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(950.2451, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(978.7997, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(954.2461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(835.2017, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(957.9811, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 80, Loss: 835.2017\n",
      "QUANTILE LOSS:  tensor(508.3233, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(961.7553, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(899.6902, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(965.1652, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(872.8633, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(968.4193, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1619.9308, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(971.2200, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1233.2914, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(975.1340, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1024.5546, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(979.0385, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1128.5079, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(982.0861, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(905.0781, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(986.7800, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.6285, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(990.2825, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1489.6842, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(992.7546, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 90, Loss: 1489.6842\n",
      "QUANTILE LOSS:  tensor(1165.2238, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(996.7981, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(982.2159, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1000.6270, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(920.5654, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1003.8286, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(911.9969, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1008.4226, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(893.0325, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1011.6291, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1640.5938, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1014.5059, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1208.9535, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1018.5181, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(992.6132, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1022.4866, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(857.9682, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1025.7207, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(919.9480, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1030.2662, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 100, Loss: 919.9480\n",
      "QUANTILE LOSS:  tensor(847.6714, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1033.8815, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(457.3818, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1038.1080, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(788.1971, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1041.4713, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(923.3850, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1045.4291, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1563.4374, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1047.5386, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1151.9176, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1051.6937, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(933.2089, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1056.8890, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(815.8426, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1059.1211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(910.8616, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1063.4740, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(929.5842, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1068.0376, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 110, Loss: 929.5842\n",
      "QUANTILE LOSS:  tensor(1431.4174, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1069.8311, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1148.8555, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1074.2025, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1000.9846, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1079.2491, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(950.6260, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1081.4209, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(812.1810, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1086.5092, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.7171, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1091.0599, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1058.9125, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1093.5522, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1176.7020, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1096.5319, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(974.4963, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1101.3027, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1143.9056, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1103.8711, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 120, Loss: 1143.9056\n",
      "QUANTILE LOSS:  tensor(977.0530, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1108.2487, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(857.5812, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1113.3627, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(697.4732, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1115.9264, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(707.2493, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1121.3892, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(702.4597, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1125.7369, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1338.2421, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1126.5071, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1075.8612, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1130.9869, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(905.0505, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1135.9191, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1043.6252, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1138.3021, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(916.6987, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1143.0646, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 130, Loss: 916.6987\n",
      "QUANTILE LOSS:  tensor(902.0629, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1147.7841, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1183.7491, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1149.6080, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1053.9735, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1154.0254, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(882.1550, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1159.4791, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(966.2379, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1161.5552, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(911.3882, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1166.5735, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(767.7983, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1171.8383, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1036.3380, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1173.6329, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1072.5271, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1177.3502, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(887.1017, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1183.6940, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 140, Loss: 887.1017\n",
      "QUANTILE LOSS:  tensor(950.6417, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1184.9896, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(919.5083, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1190.0923, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(777.6133, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1195.4349, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(698.0325, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1197.7097, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(655.1873, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1204.0674, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(641.6614, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1212.3280, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1686.8661, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1208.1204, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1318.1196, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1212.3782, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1117.2217, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1216.8007, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1188.5361, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1221.2682, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 150, Loss: 1188.5361\n",
      "QUANTILE LOSS:  tensor(1121.9933, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1225.5858, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(941.2808, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1231.1595, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(943.5005, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1233.1511, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(970.6111, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1238.5592, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.2595, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1242.7734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(893.0818, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1245.5802, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(946.4250, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1251.1309, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(922.2453, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1257.5685, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1432.7798, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1256.3600, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1124.6829, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1261.2554, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 160, Loss: 1124.6829\n",
      "QUANTILE LOSS:  tensor(916.1348, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1269.3209, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(872.9277, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1269.3745, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(957.0871, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1277.0892, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(908.5019, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1281.6796, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1201.4135, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1280.8197, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1059.5665, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1288.3156, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.9211, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1293.9465, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(881.9673, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1293.9088, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(966.2942, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1301.3783, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(852.6697, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1306.2520, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 170, Loss: 852.6697\n",
      "QUANTILE LOSS:  tensor(1115.8800, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1305.2904, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(997.4590, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1313.0946, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(822.1067, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1319.0221, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(865.2879, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1318.7181, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(920.3264, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1325.8575, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.7446, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1331.4187, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(821.2890, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1331.2937, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(869.8201, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1339.8075, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(800.2766, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1343.9825, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1094.2767, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1342.4642, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 180, Loss: 1094.2767\n",
      "QUANTILE LOSS:  tensor(944.1492, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1351.2755, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(812.3916, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1356.2407, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(804.7290, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1358.0281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(853.0977, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1366.0072, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(802.2706, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1368.4243, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(777.9507, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1372.8022, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.0339, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1379.5962, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(828.9580, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1381.1302, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(860.5294, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1384.7565, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(832.0063, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1391.2637, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 190, Loss: 832.0063\n",
      "QUANTILE LOSS:  tensor(757.9892, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1394.7408, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(756.3588, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1402.1704, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(800.5793, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1404.1730, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(750.3190, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1407.5879, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(734.9980, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1415.2534, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(794.3264, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1417.1241, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(713.3680, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1416.8361, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(700.1082, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1428.8507, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(770.3281, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1428.8837, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(734.8159, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1428.8788, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 200, Loss: 734.8159\n",
      "QUANTILE LOSS:  tensor(897.6325, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1438.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(864.4993, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1438.7577, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.0550, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1441.5704, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.9121, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1453.3436, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(827.1733, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1451.1281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(725.1418, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1477.1256, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(776.1331, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1465.3501, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(811.0745, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1462.9884, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(761.3123, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1480.7480, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(819.1143, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1476.6486, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 210, Loss: 819.1143\n",
      "QUANTILE LOSS:  tensor(841.6304, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1475.7025, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.8615, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1494.0585, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(718.2893, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1487.4390, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(908.9533, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1487.7845, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(965.9917, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1493.1744, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(961.5166, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1501.1652, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(921.7838, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1501.0546, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(939.8636, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1506.3705, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1041.7506, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1512.6935, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1055.2827, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1516.7128, grad_fn=<MeanBackward0>)\n",
      "Epoch 3/25, Batch 220, Loss: 1055.2827\n",
      "QUANTILE LOSS:  tensor(1045.5276, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1519.5248, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1074.2593, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1522.7694, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(749.2172, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1527.4738, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(732.4774, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1531.7330, grad_fn=<MeanBackward0>)\n",
      "Epoch 3, Avg Loss: 1094.0065\n",
      "QUANTILE LOSS:  tensor(11571.4814, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1535.3179, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 0, Loss: 11571.4814\n",
      "QUANTILE LOSS:  tensor(10151.5117, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1539.8022, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8195.1475, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1543.1719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6127.6665, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1547.6016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6010.6079, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1552.3300, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1355.7750, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1561.4828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(700.1413, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1574.4668, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(750.1617, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1567.9191, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(746.0691, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1578.5431, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(817.8742, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1591.8334, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1127.6578, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1582.6432, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 10, Loss: 1127.6578\n",
      "QUANTILE LOSS:  tensor(714.6994, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1596.2194, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(483.5216, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1610.9386, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(547.9648, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1597.0848, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(722.2601, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1609.2533, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(444.6289, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1628.4316, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1487.9999, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1608.7264, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1321.7072, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1610.2936, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(903.4383, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1616.2168, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(918.9225, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1619.4669, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.6412, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1624.7954, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 20, Loss: 874.6412\n",
      "QUANTILE LOSS:  tensor(1482.8765, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1629.2167, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1249.7418, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1632.6426, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1100.2640, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1637.9102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(823.9543, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1644.5717, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(872.3089, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1647.5767, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(859.4330, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1654.2550, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(663.4568, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1661.4337, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(783.2484, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1663.1302, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.9204, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1669.4269, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(595.4945, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1675.8619, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 30, Loss: 595.4945\n",
      "QUANTILE LOSS:  tensor(720.4626, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1675.8920, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.9722, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1682.8901, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(569.2645, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1688.3164, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(696.8865, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1689.5566, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(711.4816, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1696.6105, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.8441, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1700.8307, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(759.0041, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1703.5282, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.7642, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1711.0065, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(595.3682, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1716.1665, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(748.1954, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1717.3046, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 40, Loss: 748.1954\n",
      "QUANTILE LOSS:  tensor(748.6974, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1724.3846, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(693.0127, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1729.1053, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(893.9221, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1730.4078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.6816, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1738.6587, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(568.5693, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1743.6827, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(733.7307, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1744.9799, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(762.0791, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1752.6473, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(756.7365, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1759.0287, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1301.4878, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1758.0695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(830.8934, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1767.5502, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 50, Loss: 830.8934\n",
      "QUANTILE LOSS:  tensor(1373.2666, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1767.1322, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(882.7317, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1773.6259, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(744.3781, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1783.8822, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(380.2325, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1782.8607, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(738.8449, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1787.1031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.8398, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1792.2402, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2509.3010, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1795.2604, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1302.5648, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1800.0684, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(918.9339, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1806.2428, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(521.6512, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1810.6128, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 60, Loss: 521.6512\n",
      "QUANTILE LOSS:  tensor(789.7790, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1815.5624, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(861.8984, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1820.4948, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(373.7000, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1826.0782, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(762.2582, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1830.0260, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(971.9460, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1835.0245, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1370.4033, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1839.3228, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(947.8826, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1844.1989, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(925.3021, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1849.1957, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1809.5641, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1852.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1304.8856, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1857.6888, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 70, Loss: 1304.8856\n",
      "QUANTILE LOSS:  tensor(1010.6884, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1863.2676, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(889.5203, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1867.3257, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(991.5351, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1873.0132, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(922.9383, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1878.5790, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1530.1603, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1881.5062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1152.7207, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1887.0106, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(977.2176, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1892.1049, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(729.6294, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1896.5972, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(972.4377, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1902.1309, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(828.8404, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1907.2872, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 80, Loss: 828.8404\n",
      "QUANTILE LOSS:  tensor(501.9627, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1912.4935, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(893.3304, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1917.1842, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(866.5041, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1921.6556, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1613.5724, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1925.4847, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1226.9335, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1930.8800, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1018.1973, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1936.2579, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1122.1512, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1940.4286, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(898.7222, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1946.9088, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.2733, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1951.7168, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1483.3295, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1955.0714, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 90, Loss: 1483.3295\n",
      "QUANTILE LOSS:  tensor(1158.8698, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1960.6309, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(975.8624, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1965.8881, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(914.2126, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1970.2607, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(905.6448, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1976.5820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.6812, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1980.9598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1634.2428, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1984.8715, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1202.6031, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1990.3717, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(986.2635, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(1995.8066, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(851.6191, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2000.2135, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(913.5997, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2006.4478, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 100, Loss: 913.5997\n",
      "QUANTILE LOSS:  tensor(841.3235, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2011.3829, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(451.0346, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2017.1652, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(781.8505, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2021.7432, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(917.0391, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2027.1523, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1557.0922, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2029.9738, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1145.5728, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2035.6470, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(926.8647, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2042.7676, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(809.4991, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2045.7578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(904.5187, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2051.6968, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(923.2419, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2057.9309, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 110, Loss: 923.2419\n",
      "QUANTILE LOSS:  tensor(1425.0756, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2060.3071, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1142.5143, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2066.2646, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(994.6440, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2073.1614, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(944.2859, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2076.0566, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(805.8416, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2082.9988, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.3784, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2089.1990, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1052.5745, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2092.5322, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1170.3645, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2096.5466, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(968.1592, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2103.0388, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1137.5691, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2106.4780, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 120, Loss: 1137.5691\n",
      "QUANTILE LOSS:  tensor(970.7172, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2112.4182, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(851.2458, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2119.3757, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(691.1385, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2122.8027, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(700.9150, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2130.2341, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(696.1260, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2136.1238, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1331.9091, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2137.0740, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1069.5287, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2143.1379, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(898.7184, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2149.8250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1037.2939, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2152.9958, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(910.3679, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2159.4424, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 130, Loss: 910.3679\n",
      "QUANTILE LOSS:  tensor(895.7327, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2165.8308, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1177.4196, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2168.2251, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1047.6444, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2174.1873, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(875.8264, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2181.5725, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(959.9099, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2184.3127, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(905.0608, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2191.0925, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(761.4714, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2198.2158, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1030.0116, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2200.5576, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1066.2012, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2205.5457, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.7764, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2214.1326, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 140, Loss: 880.7764\n",
      "QUANTILE LOSS:  tensor(944.3169, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2215.7947, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(913.1841, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2222.6716, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(771.2897, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2229.8748, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(691.7093, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2232.8738, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(648.8645, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2241.4583, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(635.3393, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2252.6436, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1680.5444, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2246.7810, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1311.7985, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2252.4868, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1110.9010, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2258.4143, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1182.2161, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2264.4011, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 150, Loss: 1182.2161\n",
      "QUANTILE LOSS:  tensor(1115.6737, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2270.1838, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(934.9619, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2277.6807, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(937.1821, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2280.2759, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(964.2932, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2287.5337, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(867.9421, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2293.1782, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.7649, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2296.8713, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(940.1086, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2304.3164, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(915.9294, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2312.9734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1426.4644, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2311.2083, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1118.3680, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2317.7527, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 160, Loss: 1118.3680\n",
      "QUANTILE LOSS:  tensor(909.8204, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2328.6199, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(866.6138, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2328.5618, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(950.7737, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2338.9221, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(902.1889, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2345.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1195.1010, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2343.7605, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1053.2545, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2353.8206, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.6097, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2361.3857, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(875.6565, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2361.1743, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(959.9838, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2371.1780, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(846.3598, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2377.7100, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 170, Loss: 846.3598\n",
      "QUANTILE LOSS:  tensor(1109.5704, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2376.2498, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(991.1501, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2386.6956, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(815.7982, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2394.6433, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(858.9800, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2394.0757, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(914.0190, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2403.6072, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(773.4377, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2411.0530, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(814.9824, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2410.7292, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(863.5141, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2422.1018, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(793.9711, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2427.6667, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1087.9716, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2425.4661, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 180, Loss: 1087.9716\n",
      "QUANTILE LOSS:  tensor(937.8445, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2437.2307, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(806.0875, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2443.8530, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(798.4253, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2446.1064, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(846.7944, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2456.7400, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(795.9678, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2459.9231, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(771.6484, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2465.6526, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(818.7320, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2474.6931, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(822.6567, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2476.6746, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(854.2285, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2481.3879, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.7059, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2490.0496, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 190, Loss: 825.7059\n",
      "QUANTILE LOSS:  tensor(751.6891, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2494.6199, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(750.0593, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2504.4192, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(794.2802, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2507.0403, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(744.0203, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2511.5183, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(728.6998, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2521.6174, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(788.0286, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2524.0601, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(707.0707, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2523.5889, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(693.8113, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2539.4885, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(764.0317, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2539.4768, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(728.5198, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2539.3813, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 200, Loss: 728.5198\n",
      "QUANTILE LOSS:  tensor(891.3369, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2551.5662, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(858.2043, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2552.3694, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(759.7603, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2556.0237, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.6179, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2571.5940, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(820.8795, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2568.5654, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(718.8484, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2603.0222, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.8401, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2587.2913, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(804.7822, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2584.0730, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(755.0203, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2607.5583, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(812.8227, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2602.0403, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 210, Loss: 812.8227\n",
      "QUANTILE LOSS:  tensor(835.3392, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2600.6956, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.5708, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2624.9482, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(711.9990, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2616.0913, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(902.6636, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2616.4639, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(959.7022, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2623.5232, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(955.2275, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2634.0090, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(915.4953, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2633.7913, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(933.5755, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2640.7458, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1035.4629, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2648.9924, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1048.9955, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2654.2590, grad_fn=<MeanBackward0>)\n",
      "Epoch 4/25, Batch 220, Loss: 1048.9955\n",
      "QUANTILE LOSS:  tensor(1039.2406, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2657.8928, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1067.9728, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2662.0872, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(742.9312, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2668.2305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.1917, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2673.7734, grad_fn=<MeanBackward0>)\n",
      "Epoch 4, Avg Loss: 1087.6627\n",
      "QUANTILE LOSS:  tensor(11565.1963, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2678.3557, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 0, Loss: 11565.1963\n",
      "QUANTILE LOSS:  tensor(10145.2275, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2684.1951, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8188.8633, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2688.5559, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6121.3828, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2694.3154, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6004.3242, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2700.4700, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1349.4918, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2712.5076, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(693.8586, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2729.5261, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(743.8792, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2720.7686, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(739.7872, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2734.7268, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(811.5927, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2752.1003, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1121.3768, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2739.9065, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 10, Loss: 1121.3768\n",
      "QUANTILE LOSS:  tensor(708.4186, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2757.7317, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(477.2997, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2776.9717, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(541.6849, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2758.6194, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(715.9807, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2774.5576, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(439.2207, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2799.6052, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1481.7213, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2773.5667, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1315.4296, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2775.4932, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(897.1613, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2783.1404, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(912.6460, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2787.2864, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(868.3652, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2794.1565, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 20, Loss: 868.3652\n",
      "QUANTILE LOSS:  tensor(1476.6011, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2799.8416, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1243.4670, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2804.2217, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1093.9896, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2811.0183, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(817.6804, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2819.6394, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(866.0356, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2823.4709, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(853.1602, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2832.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(657.1844, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2841.4128, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(776.9765, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2843.5291, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.6489, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2851.6768, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(589.2233, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2859.9873, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 30, Loss: 589.2233\n",
      "QUANTILE LOSS:  tensor(714.1920, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2859.9270, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.7019, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2868.9871, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(562.9946, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2875.9778, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(690.6171, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2877.5012, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(705.2126, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2886.6289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(773.5754, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2892.0408, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(752.7358, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2895.4685, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(760.4964, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2905.1433, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(589.1008, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2911.7751, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(741.9283, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2913.1680, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 40, Loss: 741.9283\n",
      "QUANTILE LOSS:  tensor(742.4308, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2922.3147, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(686.7464, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2928.3721, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(887.6562, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2929.9817, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.4160, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2940.6436, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(562.3041, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2947.0911, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(727.4659, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2948.6914, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(755.8148, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2958.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(750.7913, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2966.7922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1295.2241, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2965.4387, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(824.6303, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2977.6746, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 50, Loss: 824.6303\n",
      "QUANTILE LOSS:  tensor(1367.0038, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2977.0039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(876.4694, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2985.3386, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(738.1163, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2998.5652, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(374.0147, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(2997.1238, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(732.5840, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3002.5205, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.5793, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3009.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2503.0410, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3012.8936, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1296.3053, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3019.0256, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(912.6746, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3026.9275, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(515.3926, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3032.4910, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 60, Loss: 515.3926\n",
      "QUANTILE LOSS:  tensor(783.5207, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3038.8059, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.6404, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3045.1003, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(367.4424, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3052.2314, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(756.0012, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3057.2478, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(965.6893, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3063.6272, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1364.1469, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3069.0947, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(941.6267, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3075.3076, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(919.0464, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3081.6877, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1803.3090, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3085.8096, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1298.6306, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3092.4802, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 70, Loss: 1298.6306\n",
      "QUANTILE LOSS:  tensor(1004.4338, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3099.6033, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(883.2661, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3104.7529, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(985.2812, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3112.0056, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(916.6849, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3119.1094, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1523.9072, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3122.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1146.4679, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3129.8113, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(970.9651, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3136.2947, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(723.3773, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3142.0022, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(966.1860, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3149.0471, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(822.5891, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3155.6064, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 80, Loss: 822.5891\n",
      "QUANTILE LOSS:  tensor(495.7115, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3162.2275, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(887.0796, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3168.1833, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(860.2537, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3173.8582, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1607.3223, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3178.7034, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1220.6838, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3185.5613, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1011.9478, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3192.3945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1115.9020, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3197.6770, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.4733, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3205.9197, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.0247, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3212.0168, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1477.0814, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3216.2478, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 90, Loss: 1477.0814\n",
      "QUANTILE LOSS:  tensor(1152.6219, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3223.3047, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(969.6149, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3229.9717, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(907.9655, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3235.5056, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(899.3978, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3243.5300, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.4345, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3249.0671, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1627.9966, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3254.0066, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1196.3572, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3260.9744, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(980.0180, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3267.8601, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(845.3737, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3273.4280, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(907.3546, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3281.3281, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 100, Loss: 907.3546\n",
      "QUANTILE LOSS:  tensor(835.0788, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3287.5706, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(444.7902, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3294.8906, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(775.6064, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3300.6711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(910.7952, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3307.5154, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1550.8486, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3311.0442, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1139.3296, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3318.2188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(920.6219, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3327.2402, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(803.2565, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3330.9844, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(898.2766, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3338.4919, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(916.9998, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3346.3762, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 110, Loss: 916.9998\n",
      "QUANTILE LOSS:  tensor(1418.8340, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3349.3337, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1136.2728, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3356.8606, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(988.4029, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3365.5830, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(938.0452, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3369.1980, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(799.6011, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3377.9727, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.1381, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3385.8035, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1046.3344, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3389.9717, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1164.1249, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3395.0129, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(961.9199, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3403.2068, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1131.3301, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3407.5115, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 120, Loss: 1131.3301\n",
      "QUANTILE LOSS:  tensor(964.4785, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3414.9963, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(845.0074, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3423.7754, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(684.9003, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3428.0596, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(694.6772, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3437.4348, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(689.8885, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3444.8516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1325.6718, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3445.9846, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1063.2916, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3453.6174, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.4817, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3462.0388, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1031.0575, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3465.9934, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(904.1317, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3474.1042, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 130, Loss: 904.1317\n",
      "QUANTILE LOSS:  tensor(889.4968, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3482.1445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1171.1840, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3485.1072, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1041.4091, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3492.5977, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(869.5915, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3501.8933, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(953.6750, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3505.2947, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(898.8264, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3513.8164, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(755.2371, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3522.7773, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1023.7778, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3525.6667, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1059.9675, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3531.9138, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.5430, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3542.7166, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 140, Loss: 874.5430\n",
      "QUANTILE LOSS:  tensor(938.0838, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3544.7463, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(906.9514, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3553.3779, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(765.0571, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3562.4219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(685.4771, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3566.1423, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(642.6326, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3576.9285, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(629.1076, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3591.0022, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1674.3131, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3583.5156, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1305.5673, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3590.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1104.6703, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3598.0742, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1175.9855, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3605.5635, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 150, Loss: 1175.9855\n",
      "QUANTILE LOSS:  tensor(1109.4434, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3612.7981, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(928.7318, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3622.1965, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(930.9523, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3625.3928, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(958.0637, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3634.4832, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(861.7128, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3641.5437, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.5359, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3646.1179, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(933.8797, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3655.4368, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(909.7009, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3666.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1420.2362, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3663.9812, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1112.1403, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3672.1589, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 160, Loss: 1112.1403\n",
      "QUANTILE LOSS:  tensor(903.5928, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3685.7957, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(860.3864, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3685.6335, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(944.5466, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3698.6101, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(895.9621, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3706.3308, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1188.8744, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3704.5544, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1047.0283, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3717.1497, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(868.3836, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3726.6289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(869.4307, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3726.2529, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(953.7581, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3738.7637, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(840.1345, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3746.9368, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 170, Loss: 840.1345\n",
      "QUANTILE LOSS:  tensor(1103.3455, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3744.9910, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(984.9252, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3758.0515, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(809.5737, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3767.9951, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(852.7557, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3767.1770, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(907.7949, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3779.0742, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(767.2139, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3788.3855, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(808.7589, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3787.8723, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(857.2908, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3802.0730, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(787.7480, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3809.0149, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1081.7488, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3806.1487, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 180, Loss: 1081.7488\n",
      "QUANTILE LOSS:  tensor(931.6221, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3820.8328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(799.8652, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3829.0950, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(792.2033, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3831.8152, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(840.5726, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3845.0769, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(789.7463, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3849.0212, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(765.4272, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3856.0930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(812.5110, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3867.3555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(816.4359, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3869.7825, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(848.0078, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3875.5784, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(819.4858, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3886.3711, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 190, Loss: 819.4858\n",
      "QUANTILE LOSS:  tensor(745.4692, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3892.0264, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(743.8396, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3904.1731, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(788.0606, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3907.4092, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(737.8011, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3912.9424, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(722.4808, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3925.4539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(781.8098, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3928.4658, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(700.8521, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3927.8196, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(687.5932, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3947.5625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.8137, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3947.5117, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(722.3021, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3947.3313, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 200, Loss: 722.3021\n",
      "QUANTILE LOSS:  tensor(885.1194, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3962.4453, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(851.9870, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3963.4014, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(753.5433, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3967.8933, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.4011, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3987.2227, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(814.6629, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(3983.3955, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(712.6321, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4026.2136, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.6240, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4006.5847, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(798.5662, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4002.5222, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(748.8047, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4031.6697, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(806.6072, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4024.7578, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 210, Loss: 806.6072\n",
      "QUANTILE LOSS:  tensor(829.1240, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4023.0254, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(760.3558, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4053.1096, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(705.7844, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4042.0488, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(896.4490, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4042.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(953.4880, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4051.1667, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(949.0135, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4064.1248, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(909.2814, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4063.8069, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(927.3619, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4072.3840, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1029.2495, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4082.5442, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1042.7823, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4089.0447, grad_fn=<MeanBackward0>)\n",
      "Epoch 5/25, Batch 220, Loss: 1042.7823\n",
      "QUANTILE LOSS:  tensor(1033.0277, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4093.4961, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1061.7601, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4098.6348, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(736.7187, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4106.2036, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(719.9795, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4113.0210, grad_fn=<MeanBackward0>)\n",
      "Epoch 5, Avg Loss: 1081.4246\n",
      "QUANTILE LOSS:  tensor(11558.9844, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4118.6011, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 0, Loss: 11558.9844\n",
      "QUANTILE LOSS:  tensor(10139.0850, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4125.7847, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8182.6523, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4131.1167, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6115.1719, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4138.1851, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5998.1138, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4145.7437, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1343.2812, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4160.6221, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(687.6480, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4181.6255, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(737.6691, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4170.6846, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(733.5773, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4187.9331, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(805.3831, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4209.3433, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1115.1674, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4194.1802, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 10, Loss: 1115.1674\n",
      "QUANTILE LOSS:  tensor(702.2094, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4216.2036, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(471.5934, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4239.9146, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(535.4764, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4217.0913, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(709.7726, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4236.7319, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(434.3310, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4267.5630, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1475.5371, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4235.2495, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1309.2228, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4237.4976, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(890.9551, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4246.8237, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(906.4402, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4251.8306, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(862.1599, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4260.2070, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 20, Loss: 862.1599\n",
      "QUANTILE LOSS:  tensor(1470.3960, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4267.1221, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1237.2625, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4272.4312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1087.7855, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4280.7227, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(811.4764, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4291.2700, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(859.8318, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4295.9082, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(846.9567, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4306.4927, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(650.9814, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4317.8745, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(770.7736, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4320.4014, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.4465, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4330.3726, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(583.0212, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4340.5366, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 30, Loss: 583.0212\n",
      "QUANTILE LOSS:  tensor(707.9902, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4340.3843, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.5003, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4351.4829, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(556.7933, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4360.0181, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(684.4160, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4361.8232, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(699.0117, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4373.0005, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(767.3748, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4379.5918, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(746.5355, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4383.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(754.2962, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4395.5913, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(582.9009, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4403.6831, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(735.7286, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4405.3306, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 40, Loss: 735.7286\n",
      "QUANTILE LOSS:  tensor(736.2313, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4416.5254, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(680.5472, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4423.9087, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(881.4572, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4425.8252, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.2173, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4438.8755, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(556.1055, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4446.7339, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(721.2676, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4448.6382, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(749.6165, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4460.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(744.9796, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4470.7583, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1289.0265, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4469.0342, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(818.4328, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4484.0205, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 50, Loss: 818.4328\n",
      "QUANTILE LOSS:  tensor(1360.8065, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4483.1206, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(870.2723, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4493.2964, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(731.9195, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4509.4766, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(367.9649, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4507.6387, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.3876, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4514.1851, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.3831, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4522.1724, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2496.8447, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4526.7710, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1290.1096, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4534.2207, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(906.4792, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4543.8413, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(509.1973, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4550.5938, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 60, Loss: 509.1973\n",
      "QUANTILE LOSS:  tensor(777.3257, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4558.2661, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.4458, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4565.9165, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(361.2479, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4574.5879, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(749.8068, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4580.6694, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(959.4953, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4588.4224, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1357.9530, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4595.0542, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(935.4329, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4602.5991, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(912.8530, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4610.3521, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1797.1157, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4615.3325, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1292.4376, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4623.4351, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 70, Loss: 1292.4376\n",
      "QUANTILE LOSS:  tensor(998.2410, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4632.0913, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(877.0734, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4638.3286, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(979.0888, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4647.1382, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(910.4925, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4655.7700, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1517.7152, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4660.2183, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1140.2760, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4668.7300, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(964.7734, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4676.5962, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(717.1858, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4683.5112, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(959.9947, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4692.0601, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(816.3979, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4700.0132, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 80, Loss: 816.3979\n",
      "QUANTILE LOSS:  tensor(489.5206, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4708.0396, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.8889, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4715.2549, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(854.0632, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4722.1255, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1601.1317, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4727.9868, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1214.4937, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4736.2949, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1005.7578, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4744.5757, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1109.7123, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4750.9639, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.2837, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4760.9585, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(873.8353, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4768.3389, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1470.8920, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4773.4429, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 90, Loss: 1470.8920\n",
      "QUANTILE LOSS:  tensor(1146.4327, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4781.9868, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(963.4260, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4790.0571, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(901.7768, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4796.7446, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(893.2094, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4806.4634, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.2460, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4813.1538, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1621.8085, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4819.1152, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1190.1692, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4827.5464, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(973.8301, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4835.8716, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(839.1862, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4842.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(901.1672, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4852.1509, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 100, Loss: 901.1672\n",
      "QUANTILE LOSS:  tensor(828.8915, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4859.6929, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(438.6031, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4868.5420, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.4195, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4875.5161, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(904.6085, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4883.7871, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1544.6621, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4888.0239, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1133.1432, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4896.6924, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(914.4357, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4907.6021, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(797.0705, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4912.0967, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.0906, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4921.1636, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(910.8142, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4930.6880, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 110, Loss: 910.8142\n",
      "QUANTILE LOSS:  tensor(1412.6484, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4934.2290, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1130.0875, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4943.3130, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(982.2178, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4953.8516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(931.8602, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4958.1851, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(793.4163, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4968.7808, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(744.9534, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4978.2314, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1040.1500, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4983.2349, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1157.9406, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4989.2964, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(955.7358, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(4999.1821, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1125.1460, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5004.3491, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 120, Loss: 1125.1460\n",
      "QUANTILE LOSS:  tensor(958.2947, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5013.3706, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(838.8238, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5023.9580, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(678.7169, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5029.0991, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(688.4940, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5040.4067, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(683.7054, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5049.3389, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1319.4889, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5050.6636, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1057.1090, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5059.8535, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.2991, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5069.9976, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1024.8751, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5074.7349, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(897.9495, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5084.5000, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 130, Loss: 897.9495\n",
      "QUANTILE LOSS:  tensor(883.3148, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5094.1816, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1165.0021, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5097.7144, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1035.2274, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5106.7241, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(863.4099, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5117.9185, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(947.4937, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5121.9810, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.6452, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5132.2339, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(749.0561, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5143.0200, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1017.5969, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5146.4580, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1053.7867, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5153.9580, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(868.3624, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5166.9663, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 140, Loss: 868.3624\n",
      "QUANTILE LOSS:  tensor(931.9034, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5169.3643, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(900.7710, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5179.7397, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(758.8771, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5190.6147, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(679.2971, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5195.0527, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(636.4529, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5208.0264, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(622.9280, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5224.9692, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1668.1337, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5215.8804, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1299.3881, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5224.4478, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1098.4911, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5233.3442, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1169.8065, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5242.3325, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 150, Loss: 1169.8065\n",
      "QUANTILE LOSS:  tensor(1103.2646, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5251.0103, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(922.5532, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5262.3013, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(924.7739, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5266.0967, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(951.8853, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5277.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.5347, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5285.4771, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.3579, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5290.9302, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(927.7019, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5302.1123, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(903.5231, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5315.1460, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1414.0586, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5312.3071, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1105.9628, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5322.1094, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 160, Loss: 1105.9628\n",
      "QUANTILE LOSS:  tensor(897.4155, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5338.4946, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(854.2093, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5338.2368, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(938.3696, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5353.8130, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(889.7852, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5363.0786, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1182.6979, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5360.8638, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1040.8518, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5375.9790, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(862.2073, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5387.3560, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(863.2545, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5386.8257, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(947.5822, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5401.8276, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(833.9587, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5411.6299, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 170, Loss: 833.9587\n",
      "QUANTILE LOSS:  tensor(1097.1698, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5409.2109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(978.7498, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5424.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(803.3984, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5436.7954, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(846.5805, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5435.7344, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(901.6198, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5449.9844, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(761.0389, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5461.1479, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(802.5843, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5460.4536, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(851.1162, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5477.4644, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(781.5735, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5485.7759, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1075.5746, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5482.2559, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 180, Loss: 1075.5746\n",
      "QUANTILE LOSS:  tensor(925.4479, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5499.8433, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(793.6912, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5509.7349, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(786.0295, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5512.9243, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(834.3989, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5528.7964, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(783.5728, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5533.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(759.2538, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5541.9077, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(806.3377, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5555.3823, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(810.2628, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5558.2554, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(841.8350, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5565.1304, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(813.3128, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5578.0415, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 190, Loss: 813.3128\n",
      "QUANTILE LOSS:  tensor(739.2964, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5584.7759, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(737.6671, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5599.2598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(781.8882, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5603.1069, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(731.6289, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5609.6899, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(716.3088, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5624.6040, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(775.6379, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5628.1851, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(694.6804, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5627.3652, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(681.4215, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5650.9331, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.6421, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5650.8442, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(716.1308, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5650.5859, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 200, Loss: 716.1308\n",
      "QUANTILE LOSS:  tensor(878.9482, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5668.6138, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(845.8159, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5669.7222, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(747.3724, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5675.0503, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(739.2303, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5698.1152, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(808.4922, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5693.4995, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(706.4616, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5744.6196, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.4537, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5721.1270, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(792.3961, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5716.2319, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(742.6346, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5751.0054, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(800.4374, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5742.7163, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 210, Loss: 800.4374\n",
      "QUANTILE LOSS:  tensor(822.9543, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5740.5996, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(754.1862, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5776.4746, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(699.6148, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5763.2324, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(890.2796, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5763.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(947.3188, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5774.0347, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(942.8444, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5789.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(903.1125, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5789.0366, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(921.1932, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5799.2310, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1023.0811, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5811.2944, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1036.6139, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5819.0269, grad_fn=<MeanBackward0>)\n",
      "Epoch 6/25, Batch 220, Loss: 1036.6139\n",
      "QUANTILE LOSS:  tensor(1026.8594, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5824.2910, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1055.5919, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5830.3726, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(730.5506, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5839.3618, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(713.8116, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5847.4497, grad_fn=<MeanBackward0>)\n",
      "Epoch 6, Avg Loss: 1075.2488\n",
      "QUANTILE LOSS:  tensor(11552.8164, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5854.0269, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 0, Loss: 11552.8164\n",
      "QUANTILE LOSS:  tensor(10133.0459, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5862.5493, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8176.4844, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5868.8599, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6109.0039, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5877.2441, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5991.9468, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5886.2109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1337.1140, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5903.9214, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(681.4811, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5928.8940, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(731.5023, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5915.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(727.4106, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5936.3198, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(799.2165, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5961.7476, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1109.0009, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5943.6504, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 10, Loss: 1109.0009\n",
      "QUANTILE LOSS:  tensor(696.0432, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5969.8457, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(466.3622, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5998.0044, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(529.3107, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5970.7192, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(703.6071, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5994.0132, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(429.5747, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6030.5649, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1469.5070, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5992.0259, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1303.0582, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(5994.5981, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(884.7908, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6005.5894, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(900.2764, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6011.4526, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.9962, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6021.3237, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 20, Loss: 855.9962\n",
      "QUANTILE LOSS:  tensor(1464.2328, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6029.4629, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1231.0992, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6035.6934, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1081.6226, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6045.4741, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(805.3138, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6057.9380, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(853.6695, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6063.3804, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(840.7946, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6075.8911, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(644.8194, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6089.3481, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(764.6119, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6092.2856, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(760.2849, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6104.0723, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(576.8598, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6116.0786, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 30, Loss: 576.8598\n",
      "QUANTILE LOSS:  tensor(701.8289, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6115.8384, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.3393, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6128.9644, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(550.6324, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6139.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(678.2552, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6141.1270, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(692.8513, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6154.3438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(761.2144, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6162.1138, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(740.3752, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6166.9844, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(748.1360, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6180.9956, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(576.7409, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6190.5396, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(729.5688, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6192.4453, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 40, Loss: 729.5688\n",
      "QUANTILE LOSS:  tensor(730.0715, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6205.6763, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(674.3876, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6214.3804, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(875.2978, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6216.6050, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.0579, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6232.0337, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(549.9464, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6241.2974, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(715.1086, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6243.5103, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(743.4577, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6257.8105, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(739.2044, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6269.6304, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1282.8678, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6267.5425, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(812.2744, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6285.2612, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 50, Loss: 812.2744\n",
      "QUANTILE LOSS:  tensor(1354.6481, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6284.1401, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(864.1141, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6296.1465, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(725.7614, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6315.2661, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(362.1392, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6313.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(720.2298, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6320.7285, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(739.2256, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6330.1294, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2490.6873, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6335.5103, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1283.9524, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6344.2700, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(900.3221, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6355.5981, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(503.0404, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6363.5312, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 60, Loss: 503.0404\n",
      "QUANTILE LOSS:  tensor(771.1689, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6372.5547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.2890, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6381.5527, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(355.0914, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6391.7554, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(743.6504, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6398.9004, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(953.3391, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6408.0171, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1351.7970, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6415.8071, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(929.2770, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6424.6792, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(906.6972, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6433.8003, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1790.9600, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6439.6353, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1286.2821, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6449.1641, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 70, Loss: 1286.2821\n",
      "QUANTILE LOSS:  tensor(992.0855, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6459.3481, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(870.9182, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6466.6694, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(972.9337, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6477.0288, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(904.3376, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6487.1821, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1511.5602, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6492.3853, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1134.1212, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6502.3940, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(958.6187, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6511.6348, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(711.0312, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6519.7554, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(953.8403, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6529.8013, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(810.2435, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6539.1460, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 80, Loss: 810.2435\n",
      "QUANTILE LOSS:  tensor(483.3665, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6548.5723, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.7348, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6557.0410, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(847.9092, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6565.1069, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1594.9780, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6571.9766, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1208.3400, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6581.7349, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(999.6043, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6591.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1103.5587, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6598.9473, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.1303, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6610.6870, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(867.6819, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6619.3457, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1464.7389, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6625.3203, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 90, Loss: 1464.7389\n",
      "QUANTILE LOSS:  tensor(1140.2799, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6635.3496, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(957.2731, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6644.8149, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(895.6240, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6652.6538, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(887.0566, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6664.0620, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(868.0936, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6671.9019, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1615.6559, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6678.8823, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1184.0170, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6688.7681, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(967.6779, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6698.5312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(833.0341, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6706.4062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(895.0152, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6717.6167, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 100, Loss: 895.0152\n",
      "QUANTILE LOSS:  tensor(822.7397, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6726.4487, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(432.4514, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6736.8237, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.2678, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6744.9897, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(898.4570, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6754.6851, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1538.5106, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6759.6294, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1126.9919, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6769.7817, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(908.2845, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6782.5757, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.9194, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6787.8223, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(885.9397, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6798.4399, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(904.6633, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6809.5991, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 110, Loss: 904.6633\n",
      "QUANTILE LOSS:  tensor(1406.4977, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6813.7231, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1123.9369, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6824.3638, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(976.0673, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6836.7085, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(925.7098, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6841.7603, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(787.2661, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6854.1675, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(738.8033, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6865.2349, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1033.9999, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6871.0684, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1151.7904, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6878.1504, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(949.5859, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6889.7212, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1118.9962, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6895.7520, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 120, Loss: 1118.9962\n",
      "QUANTILE LOSS:  tensor(952.1450, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6906.3052, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(832.6744, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6918.6934, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(672.5674, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6924.6880, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(682.3447, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6937.9233, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(677.5562, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6948.3691, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1313.3397, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6949.8809, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1050.9598, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6960.6294, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.1502, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6972.4902, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1018.7263, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6978.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(891.8008, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(6989.4214, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 130, Loss: 891.8008\n",
      "QUANTILE LOSS:  tensor(877.1663, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7000.7397, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1158.8536, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7004.8442, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1029.0790, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7015.3696, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(857.2617, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7028.4536, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(941.3455, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7033.1743, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.4971, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7045.1538, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(742.9081, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7057.7612, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1011.4490, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7061.7495, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1047.6392, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7070.4995, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(862.2148, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7085.6978, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 140, Loss: 862.2148\n",
      "QUANTILE LOSS:  tensor(925.7559, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7088.4692, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(894.6235, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7100.5825, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(752.7298, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7113.2812, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(673.1500, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7118.4419, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(630.3057, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7133.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(616.7811, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7153.3911, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1661.9867, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7142.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1293.2412, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7152.6987, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1092.3444, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7163.0757, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1163.6600, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7173.5571, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 150, Loss: 1163.6600\n",
      "QUANTILE LOSS:  tensor(1097.1182, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7183.6714, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(916.4067, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7196.8477, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(918.6275, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7201.2446, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(945.7392, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7213.9712, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.3887, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7223.8438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(868.2120, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7230.1772, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(921.5561, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7243.2148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(897.3774, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7258.4219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1407.9130, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7255.0571, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1099.8173, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7266.4785, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 160, Loss: 1099.8173\n",
      "QUANTILE LOSS:  tensor(891.2702, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7285.6035, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(848.0640, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7285.2544, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(932.2244, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7303.4165, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(883.6401, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7314.2266, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1176.5527, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7311.5781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1034.7067, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7329.1992, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(856.0625, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7342.4741, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(857.1099, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7341.7910, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(941.4376, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7359.2749, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(827.8141, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7370.7031, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 170, Loss: 827.8141\n",
      "QUANTILE LOSS:  tensor(1091.0254, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7367.8169, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(972.6055, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7386.0601, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(797.2540, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7399.9634, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(840.4363, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7398.6675, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(895.4758, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7415.2598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(754.8950, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7428.2681, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(796.4404, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7427.4004, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(844.9725, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7447.2109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(775.4300, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7456.8848, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1069.4310, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7452.7202, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 180, Loss: 1069.4310\n",
      "QUANTILE LOSS:  tensor(919.3044, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7473.1968, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(787.5479, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7484.7124, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.8862, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7488.3726, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(828.2557, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7506.8477, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(777.4297, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7512.3071, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(753.1108, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7522.0527, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(800.1949, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7537.7261, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(804.1199, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7541.0474, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(835.6923, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7549.0015, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(807.1702, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7564.0215, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 190, Loss: 807.1702\n",
      "QUANTILE LOSS:  tensor(733.1541, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7571.8335, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(731.5246, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7588.6460, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(775.7460, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7593.1089, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(725.4866, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7600.7441, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(710.1666, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7618.0474, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.4958, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7622.1978, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(688.5385, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7621.2090, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(675.2797, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7648.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.5004, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7648.4644, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(709.9892, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7648.1255, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 200, Loss: 709.9892\n",
      "QUANTILE LOSS:  tensor(872.8067, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7669.0586, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(839.6745, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7670.3228, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(741.2310, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7676.4849, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(733.0891, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7703.2759, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(802.3511, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7697.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(700.3206, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7757.2612, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.3127, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7729.9277, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(786.2551, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7724.2056, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(736.4938, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7764.5796, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(794.2966, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7754.9233, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 210, Loss: 794.2966\n",
      "QUANTILE LOSS:  tensor(816.8137, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7752.4312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(748.0457, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7794.0757, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(693.4744, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7778.6616, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(884.1394, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7779.1362, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(941.1785, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7791.1450, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(936.7043, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7809.0181, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(896.9724, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7808.5054, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(915.0532, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7820.3120, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1016.9412, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7834.2759, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1030.4741, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7843.2319, grad_fn=<MeanBackward0>)\n",
      "Epoch 7/25, Batch 220, Loss: 1030.4741\n",
      "QUANTILE LOSS:  tensor(1020.7197, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7849.3125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1049.4523, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7856.3384, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(724.4111, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7866.7407, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(707.6722, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7876.0938, grad_fn=<MeanBackward0>)\n",
      "Epoch 7, Avg Loss: 1069.1121\n",
      "QUANTILE LOSS:  tensor(11546.6768, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7883.6724, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 0, Loss: 11546.6768\n",
      "QUANTILE LOSS:  tensor(10127.0332, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7893.5298, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8170.3457, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7900.8218, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6102.8647, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7910.5132, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5985.8071, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7920.8809, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1330.9752, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7941.4126, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(675.3423, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7970.3394, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(725.3636, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7955.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(721.2720, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7978.8906, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(793.0779, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8008.3237, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1102.8625, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(7987.3062, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 10, Loss: 1102.8625\n",
      "QUANTILE LOSS:  tensor(689.9048, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8017.6606, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(461.3952, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8050.2476, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(523.1911, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8018.5630, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(697.4692, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8045.5059, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(424.8392, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8087.7700, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1463.7074, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8043.0562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1296.9210, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8045.9331, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(878.6538, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8058.5742, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(894.1396, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8065.2817, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.8597, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8076.6328, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 20, Loss: 849.8597\n",
      "QUANTILE LOSS:  tensor(1458.0963, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8085.9819, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1224.9631, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8093.1304, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1075.4867, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8104.3828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(799.1780, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8118.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(847.5339, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8124.9897, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(834.6592, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8139.4155, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(638.6841, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8154.9380, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(758.4767, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8158.2817, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(754.1498, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8171.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(570.7249, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8185.7163, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 30, Loss: 570.7249\n",
      "QUANTILE LOSS:  tensor(695.6941, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8185.3901, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(739.2046, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8200.5303, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(544.4979, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8212.1406, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(672.1208, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8214.5059, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(686.7170, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8229.7607, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(755.0803, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8238.6982, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(734.2411, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8244.2920, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(742.0022, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8260.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(570.6070, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8271.4502, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(723.4351, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8273.6143, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 40, Loss: 723.4351\n",
      "QUANTILE LOSS:  tensor(723.9380, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8288.8760, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(668.2541, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8298.8965, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(869.1644, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8301.4326, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(738.9246, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8319.2285, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(543.8131, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8329.8955, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(708.9753, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8332.4170, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(737.3245, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8348.9062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(733.5383, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8362.5244, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1276.7350, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8360.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(806.1415, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8380.4834, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 50, Loss: 806.1415\n",
      "QUANTILE LOSS:  tensor(1348.5155, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8379.1260, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(857.9817, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8392.9473, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(719.6290, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8414.9844, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(356.4352, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8412.3584, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(714.0978, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8421.1523, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(733.0938, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8431.9248, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2484.5557, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8438.0576, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1277.8210, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8448.1006, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(894.1909, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8461.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(496.9093, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8470.1982, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 60, Loss: 496.9093\n",
      "QUANTILE LOSS:  tensor(765.0380, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8480.5527, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.1582, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8490.8779, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(349.0685, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8502.5889, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(737.5201, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8510.7178, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(947.2090, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8521.1299, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1345.6671, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8530.0146, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(923.1475, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8540.1553, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(900.5679, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8550.5908, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1784.8309, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8557.2373, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1280.1531, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8568.1475, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 70, Loss: 1280.1531\n",
      "QUANTILE LOSS:  tensor(985.9568, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8579.8174, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(864.7896, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8588.1904, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(966.8052, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8600.0684, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(898.2093, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8611.7080, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1505.4323, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8617.6514, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1127.9933, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8629.1260, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(952.4910, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8639.7236, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(704.9036, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8649.0312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(947.7127, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8660.5547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(804.1160, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8671.2734, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 80, Loss: 804.1160\n",
      "QUANTILE LOSS:  tensor(477.2391, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8682.0850, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(868.6076, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8691.7959, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(841.7820, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8701.0410, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1588.8511, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8708.9111, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1202.2130, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8720.1084, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(993.4775, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8731.2617, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1097.4321, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8739.8496, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.0037, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8753.3193, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(861.5555, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8763.2510, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1458.6127, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8770.0938, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 90, Loss: 1458.6127\n",
      "QUANTILE LOSS:  tensor(1134.1534, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8781.5986, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(951.1468, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8792.4580, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(889.4977, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8801.4424, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.9307, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8814.5322, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(861.9676, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8823.5195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1609.5299, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8831.5156, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1177.8910, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8842.8525, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(961.5522, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8854.0479, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(826.9084, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8863.0713, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(888.8896, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8875.9268, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 100, Loss: 888.8896\n",
      "QUANTILE LOSS:  tensor(816.6141, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8886.0547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(426.3259, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8897.9482, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.1426, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8907.3018, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.3316, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8918.4170, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1532.3854, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8924.0713, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1120.8668, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8935.7080, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(902.1594, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8950.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.7944, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8956.3740, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(879.8148, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8968.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(898.5385, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8981.3330, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 110, Loss: 898.5385\n",
      "QUANTILE LOSS:  tensor(1400.3729, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8986.0420, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1117.8124, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(8998.2275, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(969.9426, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9012.3838, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(919.5853, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9018.1553, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(781.1415, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9032.3691, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(732.6789, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9045.0479, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1027.8755, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9051.7139, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1145.6661, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9059.8174, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(943.4617, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9073.0693, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1112.8724, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9079.9590, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 120, Loss: 1112.8724\n",
      "QUANTILE LOSS:  tensor(946.0210, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9092.0420, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(826.5504, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9106.2354, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(666.4435, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9113.0850, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(676.2208, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9128.2402, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(671.4324, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9140.1924, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1307.2161, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9141.9033, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1044.8362, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9154.1973, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.0267, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9167.7715, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1012.6028, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9174.0723, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(885.6774, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9187.1357, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 130, Loss: 885.6774\n",
      "QUANTILE LOSS:  tensor(871.0427, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9200.0879, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1152.7303, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9204.7617, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1022.9557, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9216.8008, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(851.1385, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9231.7705, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(935.2224, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9237.1572, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.3741, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9250.8633, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(736.7852, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9265.2803, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1005.3260, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9269.8193, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1041.5162, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9279.8135, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(856.0920, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9297.2041, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 140, Loss: 856.0920\n",
      "QUANTILE LOSS:  tensor(919.6332, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9300.3545, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(888.5010, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9314.2021, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(746.6072, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9328.7197, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(667.0274, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9334.5996, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(624.1833, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9351.9268, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(610.6586, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9374.5732, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1655.8643, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9362.3115, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1287.1190, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9373.7236, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1086.2222, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9385.5742, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1157.5378, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9397.5449, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 150, Loss: 1157.5378\n",
      "QUANTILE LOSS:  tensor(1090.9961, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9409.0986, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(910.2847, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9424.1514, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(912.5057, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9429.1553, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(939.6174, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9443.6924, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.2668, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9454.9688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(862.0903, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9462.1826, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(915.4344, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9477.0732, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(891.2558, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9494.4463, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1401.7915, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9490.5635, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1093.6957, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9503.5996, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 160, Loss: 1093.6957\n",
      "QUANTILE LOSS:  tensor(885.1487, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9525.4580, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(841.9427, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9525.0205, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(926.1031, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9545.7666, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(877.5190, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9558.1104, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1170.4316, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9555.0400, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1028.5858, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9575.1660, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.9415, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9590.3271, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(850.9888, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9589.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(935.3167, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9609.4580, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(821.6933, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9622.5068, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 170, Loss: 821.6933\n",
      "QUANTILE LOSS:  tensor(1084.9045, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9619.1650, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(966.4848, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9639.9854, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(791.1335, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9655.8604, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(834.3157, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9654.3350, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(889.3554, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9673.2617, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(748.7745, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9688.1162, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.3201, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9687.0791, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(838.8522, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9709.6777, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.3098, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9720.7139, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1063.3107, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9715.9160, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 180, Loss: 1063.3107\n",
      "QUANTILE LOSS:  tensor(913.1843, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9739.2725, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(781.4279, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9752.4102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(773.7662, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9756.5479, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(822.1358, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9777.6143, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(771.3098, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9783.8330, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(746.9910, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9794.9131, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(794.0752, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9812.7803, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(798.0003, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9816.5518, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(829.5727, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9825.5850, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(801.0508, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9842.7080, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 190, Loss: 801.0508\n",
      "QUANTILE LOSS:  tensor(727.0345, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9851.5986, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(725.4052, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9870.7393, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.6266, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9875.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(719.3674, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9884.4951, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(704.0473, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9904.1885, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.3767, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9908.9072, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(682.4194, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9907.7559, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(669.1606, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9938.9326, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(739.3814, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9938.7803, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(703.8703, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9938.3682, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 200, Loss: 703.8703\n",
      "QUANTILE LOSS:  tensor(866.6878, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9962.2080, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(833.5557, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9963.6240, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(735.1123, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9970.6201, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.9704, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10001.1240, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(796.2325, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(9994.9463, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(694.2020, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10062.5713, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.1943, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10031.4170, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(780.1367, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10024.8701, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(730.3755, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10070.8281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(788.1784, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10059.8213, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 210, Loss: 788.1784\n",
      "QUANTILE LOSS:  tensor(810.6955, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10056.9521, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(741.9275, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10104.3447, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(687.3563, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10086.7803, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(878.0212, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10087.2959, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(935.0606, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10100.9424, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(930.5864, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10121.2705, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(890.8544, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10120.6572, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(908.9354, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10134.0771, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1010.8234, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10149.9424, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1024.3563, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10160.1221, grad_fn=<MeanBackward0>)\n",
      "Epoch 8/25, Batch 220, Loss: 1024.3563\n",
      "QUANTILE LOSS:  tensor(1014.6021, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10167.0117, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1043.3347, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10174.9893, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(718.2936, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10186.8008, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(701.5546, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10197.4209, grad_fn=<MeanBackward0>)\n",
      "Epoch 8, Avg Loss: 1063.0039\n",
      "QUANTILE LOSS:  tensor(11540.5586, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10206.0049, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 0, Loss: 11540.5586\n",
      "QUANTILE LOSS:  tensor(10121.0430, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10217.1943, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8164.2280, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10225.4629, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6096.7480, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10236.4688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5979.6909, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10248.2402, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1324.8580, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10271.5830, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(669.2253, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10304.4492, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(719.2466, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10287.0928, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(715.1550, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10314.1309, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(786.9611, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10347.5518, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1096.7455, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10323.6357, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 10, Loss: 1096.7455\n",
      "QUANTILE LOSS:  tensor(683.7880, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10358.1299, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(456.6148, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10395.1348, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(517.2014, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10359.0723, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(691.3528, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10389.6660, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(420.2351, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10437.6357, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1458.0985, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10386.7607, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1290.8048, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10389.9736, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(872.5378, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10404.2803, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(888.0237, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10411.8447, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.7440, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10424.6934, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 20, Loss: 843.7440\n",
      "QUANTILE LOSS:  tensor(1451.9808, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10435.2705, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1218.8478, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10443.3438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1069.3711, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10456.0889, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(793.0627, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10472.3633, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(841.4187, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10479.4072, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(828.5439, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10495.7588, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(632.5690, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10513.3486, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(752.3619, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10517.1094, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(748.0350, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10532.5107, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(564.6102, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10548.1875, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 30, Loss: 564.6102\n",
      "QUANTILE LOSS:  tensor(689.5793, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10547.7861, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(733.0900, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10564.9492, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(538.3833, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10578.0928, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(666.0063, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10580.7510, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(680.6025, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10598.0312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(748.9658, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10608.1465, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(728.1269, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10614.4639, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(735.8879, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10632.7861, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(564.4929, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10645.2285, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(717.3210, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10647.6514, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 40, Loss: 717.3210\n",
      "QUANTILE LOSS:  tensor(717.8239, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10664.9434, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(662.1401, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10676.2861, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(863.0504, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10679.1338, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(732.8107, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10699.2920, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(537.6993, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10711.3594, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(702.8616, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10714.1924, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(731.2109, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10732.8701, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(727.9319, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10748.2861, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1270.6215, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10745.4678, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(800.0281, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10768.6084, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 50, Loss: 800.0281\n",
      "QUANTILE LOSS:  tensor(1342.4022, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10767.0322, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(851.8683, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10782.6768, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(713.5157, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10807.6396, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(351.0524, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10804.6279, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(707.9847, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10814.5215, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.9808, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10826.6709, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2478.4434, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10833.5576, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1271.7084, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10844.8799, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(888.0784, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10859.5615, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(490.8356, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10869.8076, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 60, Loss: 490.8356\n",
      "QUANTILE LOSS:  tensor(758.9257, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10881.4678, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.0462, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10893.0947, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(343.5179, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10906.3018, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(731.4086, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10915.3018, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(941.0980, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10926.9170, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1339.5565, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10936.8135, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(917.0373, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10948.1406, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(894.4580, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10959.8232, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1778.7213, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10967.2197, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1274.0438, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10979.4531, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 70, Loss: 1274.0438\n",
      "QUANTILE LOSS:  tensor(979.8477, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(10992.5576, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(858.6808, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11001.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(960.6966, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11015.2861, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.1009, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11028.3857, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1499.3239, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11035.0303, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1121.8851, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11047.9463, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(946.3831, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11059.8662, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(698.7958, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11070.3320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(941.6050, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11083.3125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(798.0086, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11095.3838, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 80, Loss: 798.0086\n",
      "QUANTILE LOSS:  tensor(471.1317, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11107.5625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(862.5003, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11118.4990, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(835.6750, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11128.9072, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1582.7438, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11137.7705, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1196.1061, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11150.3936, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(987.3706, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11162.9648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1091.3253, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11172.6416, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(867.8970, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11187.8369, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.4488, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11199.0303, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1452.5059, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11206.7305, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 90, Loss: 1452.5059\n",
      "QUANTILE LOSS:  tensor(1128.0470, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11219.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(945.0405, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11231.9482, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(883.3914, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11242.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.8243, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11256.8389, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.8613, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11266.9678, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1603.4238, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11275.9795, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1171.7850, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11288.7617, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(955.4461, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11301.3857, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(820.8024, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11311.5537, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(882.7837, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11326.0518, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 100, Loss: 882.7837\n",
      "QUANTILE LOSS:  tensor(810.5083, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11337.4697, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(420.3207, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11350.8799, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.0368, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11361.4014, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.2261, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11373.9111, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1526.2798, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11380.2490, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1114.7614, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11393.3506, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(896.0540, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11409.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.6891, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11416.6143, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(873.7095, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11430.3174, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.4333, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11444.7256, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 110, Loss: 892.4333\n",
      "QUANTILE LOSS:  tensor(1394.2677, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11450.0049, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1111.7074, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11463.7305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(963.8378, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11479.6768, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(913.4805, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11486.1611, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(775.0368, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11502.1768, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.5742, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11516.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1021.9204, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11523.9570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1139.5619, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11533., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(937.3576, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11547.8643, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1106.7682, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11555.5576, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 120, Loss: 1106.7682\n",
      "QUANTILE LOSS:  tensor(939.9172, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11569.1104, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(820.4467, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11585.0430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(660.3400, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11592.7080, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(670.1174, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11609.7383, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(665.3292, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11623.1650, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1301.1128, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11625.0361, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1038.7333, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11638.8545, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(867.9238, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11654.1162, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1006.5001, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11661.1768, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(879.5747, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11675.8594, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 130, Loss: 879.5747\n",
      "QUANTILE LOSS:  tensor(864.9402, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11690.4180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1146.6278, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11695.6553, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1016.8533, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11709.1885, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(845.0361, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11726.0312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(929.1202, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11732.0674, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.2718, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11747.4775, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(730.6830, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11763.7021, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(999.2241, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11768.7822, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1035.4142, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11780.0225, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.9901, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11799.5850, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 140, Loss: 849.9901\n",
      "QUANTILE LOSS:  tensor(913.5312, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11803.1084, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(882.3991, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11818.6865, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(740.5054, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11835.0156, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(660.9257, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11841.6133, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(618.0816, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11861.1045, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(604.5570, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11886.5928, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1649.7627, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11872.7549, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1281.0175, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11885.5830, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1080.1207, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11898.9072, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1151.4364, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11912.3643, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 150, Loss: 1151.4364\n",
      "QUANTILE LOSS:  tensor(1084.8948, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11925.3525, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(904.1835, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11942.2852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(906.4043, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11947.8906, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(933.5161, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11964.2344, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.1657, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11976.9150, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.9890, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(11985.0010, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(909.3333, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12001.7490, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(885.1547, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12021.2822, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1395.6906, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12016.8867, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1087.5948, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12031.5420, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 160, Loss: 1087.5948\n",
      "QUANTILE LOSS:  tensor(879.0478, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12056.1201, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(835.8418, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12055.5977, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(920.0023, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12078.9229, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(871.4182, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12092.8018, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1164.3308, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12089.3164, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1022.4851, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12111.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.8408, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12128.9775, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(844.8882, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12128.0225, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(929.2161, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12150.4482, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(815.5928, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12165.1133, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 170, Loss: 815.5928\n",
      "QUANTILE LOSS:  tensor(1078.8040, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12161.3203, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(960.3842, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12184.7178, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(785.0330, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12202.5586, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(828.2153, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12200.8115, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(883.2549, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12222.0742, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(742.6743, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12238.7617, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.2197, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12237.5674, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(832.7520, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12262.9492, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.2095, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12275.3438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1057.2106, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12269.9219, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 180, Loss: 1057.2106\n",
      "QUANTILE LOSS:  tensor(907.0842, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12296.1553, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(775.3277, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12310.9062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(767.6661, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12315.5225, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(816.0358, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12339.1787, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(765.2098, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12346.1553, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(740.8911, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12358.5742, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(787.9752, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12378.6299, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(791.9005, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12382.8525, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(823.4729, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12392.9756, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(794.9509, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12412.1924, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 190, Loss: 794.9509\n",
      "QUANTILE LOSS:  tensor(720.9348, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12422.1572, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(719.3055, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12443.6299, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.5270, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12449.3115, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(713.2676, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12459.0430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(697.9477, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12481.1338, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.2772, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12486.4170, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(676.3198, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12485.1025, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(663.0612, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12520.0820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(733.2819, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12519.8945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(697.7708, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12519.4102, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 200, Loss: 697.7708\n",
      "QUANTILE LOSS:  tensor(860.5884, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12546.1445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(827.4564, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12547.7217, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(729.0129, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12555.5518, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(720.8711, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12589.7656, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.1333, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12582.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(688.1028, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12658.6572, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(739.0952, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12623.7031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(774.0376, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12616.3389, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(724.2764, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12667.8662, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(782.0793, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12655.5117, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 210, Loss: 782.0793\n",
      "QUANTILE LOSS:  tensor(804.5965, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12652.2705, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(735.8286, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12705.4014, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(681.2573, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12685.6963, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(871.9224, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12686.2510, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(928.9617, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12701.5420, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(924.4875, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12724.3213, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(884.7557, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12723.6094, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(902.8366, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12738.6475, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1004.7246, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12756.4219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1018.2578, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12767.8115, grad_fn=<MeanBackward0>)\n",
      "Epoch 9/25, Batch 220, Loss: 1018.2578\n",
      "QUANTILE LOSS:  tensor(1008.5034, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12775.5205, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1037.2362, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12784.4385, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(712.1951, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12797.6611, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(695.4562, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12809.5498, grad_fn=<MeanBackward0>)\n",
      "Epoch 9, Avg Loss: 1056.9230\n",
      "QUANTILE LOSS:  tensor(11534.4609, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12819.1455, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 0, Loss: 11534.4609\n",
      "QUANTILE LOSS:  tensor(10115.0713, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12831.6768, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8158.1294, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12840.9268, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6090.6499, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12853.2383, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5973.5923, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12866.4072, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1318.7598, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12892.5469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(663.1270, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12929.3525, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(713.1484, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12909.8906, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(709.0569, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12940.1670, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(780.8630, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12977.5742, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1090.6476, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12950.7764, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 10, Loss: 1090.6476\n",
      "QUANTILE LOSS:  tensor(677.6900, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12989.3906, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(451.9058, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13030.8076, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(511.3847, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(12990.4111, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(685.2549, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13024.5830, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(415.6526, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13078.1963, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1452.5103, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13021.1553, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1284.7076, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13024.6533, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(866.4407, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13040.5820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(881.9269, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13048.9736, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.6472, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13063.2812, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 20, Loss: 837.6472\n",
      "QUANTILE LOSS:  tensor(1445.8843, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13075.0508, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1212.7512, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13084.0234, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1063.2750, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13098.2275, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(786.9666, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13116.3838, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(835.3227, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13124.2236, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(822.4482, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13142.4697, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(626.4733, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13162.1084, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(746.2661, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13166.2803, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(741.9393, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13183.4717, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(558.5146, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13200.9697, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 30, Loss: 558.5146\n",
      "QUANTILE LOSS:  tensor(683.4839, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13200.4805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.9946, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13219.6484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(532.2879, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13234.3242, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(659.9111, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13237.2656, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(674.5073, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13256.5674, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(742.8707, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13267.8525, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(722.0317, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13274.8906, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(729.7929, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13295.3594, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(558.3979, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13309.2461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(711.2260, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13311.9326, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 40, Loss: 711.2260\n",
      "QUANTILE LOSS:  tensor(711.7290, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13331.2451, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(656.0452, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13343.9014, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(856.9556, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13347.0635, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.7161, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13369.5889, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(531.6046, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13383.0537, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(696.7669, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13386.2021, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(725.1163, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13407.0576, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(722.3418, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13424.2686, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1264.5269, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13421.1025, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(793.9336, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13446.9492, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 50, Loss: 793.9336\n",
      "QUANTILE LOSS:  tensor(1336.3076, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13445.1670, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(845.7737, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13462.6260, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(707.4213, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13490.5049, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(346.2191, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13487.1201, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(701.8907, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13498.0010, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(720.8872, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13511.4150, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2472.3499, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13518.9678, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1265.6151, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13531.4795, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(881.9854, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13547.7549, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(485.1216, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13559.0986, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 60, Loss: 485.1216\n",
      "QUANTILE LOSS:  tensor(752.8336, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13571.9365, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(824.9543, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13584.7656, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(339.1272, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13599.3604, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(725.3178, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13609.0986, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(935.0078, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13621.7803, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1333.4669, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13632.5674, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(910.9481, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13644.9756, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(888.3693, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13657.8018, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1772.6332, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13665.8643, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1267.9559, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13679.3447, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 70, Loss: 1267.9559\n",
      "QUANTILE LOSS:  tensor(973.7602, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13693.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(852.5934, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13704.1357, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(954.6097, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13718.9023, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.0142, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13733.3955, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1493.2374, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13740.7031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1115.7990, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13755.0156, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(940.2969, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13768.2236, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(692.7099, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13779.8213, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(935.5195, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13794.2275, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(791.9230, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13807.6201, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 80, Loss: 791.9230\n",
      "QUANTILE LOSS:  tensor(465.0464, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13821.1367, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(856.4150, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13833.2812, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(829.5898, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13844.8438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1576.6588, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13854.6768, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1190.0211, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13868.7041, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(981.2856, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13882.6797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1085.2404, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13893.4287, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(861.8122, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13910.3350, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.3642, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13922.7900, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1446.4213, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13931.3428, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 90, Loss: 1446.4213\n",
      "QUANTILE LOSS:  tensor(1121.9625, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13945.7744, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(938.9560, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13959.3975, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(877.3069, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13970.6572, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(868.7400, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13987.0889, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.7770, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(13998.3545, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1597.3395, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14008.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1165.7007, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14022.6016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(949.3619, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14036.6475, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(814.7183, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14047.9619, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(876.6995, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14064.0977, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 100, Loss: 876.6995\n",
      "QUANTILE LOSS:  tensor(804.4241, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14076.8018, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(414.3707, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14091.7197, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(744.9529, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14103.4033, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.1423, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14117.2939, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1520.1962, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14124.3213, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1108.6777, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14138.8838, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(889.9705, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14157.2588, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.6055, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14164.7295, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(867.6260, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14179.9639, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.3500, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14195.9844, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 110, Loss: 886.3500\n",
      "QUANTILE LOSS:  tensor(1388.1844, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14201.8428, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1105.6240, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14217.0986, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(957.7545, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14234.8320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(907.3973, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14242.0322, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(768.9536, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14259.8447, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(720.4910, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14275.7275, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1016.4269, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14284.0508, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1133.4791, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14294.0225, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(931.2748, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14310.4775, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1100.6857, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14318.9570, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 120, Loss: 1100.6857\n",
      "QUANTILE LOSS:  tensor(933.8349, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14333.9678, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(814.3646, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14351.6299, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(654.2580, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14360.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(664.0355, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14378.9951, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(659.2474, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14393.8760, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1295.0312, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14395.9170, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1032.6515, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14411.2451, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(861.8423, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14428.1875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1000.4187, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14436.0010, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(873.4933, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14452.3018, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 130, Loss: 873.4933\n",
      "QUANTILE LOSS:  tensor(858.8589, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14468.4688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1140.5465, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14474.2607, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1010.7720, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14489.2881, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(838.9550, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14507.9932, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(923.0391, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14514.6836, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(868.1909, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14531.7959, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(724.6020, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14549.8154, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(993.1431, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14555.4492, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1029.3334, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14567.9326, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.9092, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14589.6611, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 140, Loss: 843.9092\n",
      "QUANTILE LOSS:  tensor(907.4505, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14593.5576, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(876.3183, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14610.8564, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(734.4246, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14628.9990, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(654.8450, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14636.3174, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(612.0009, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14657.9658, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(598.4764, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14686.2861, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1643.6821, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14670.8955, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1274.9369, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14685.1357, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1074.0402, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14699.9297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1145.3560, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14714.8750, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 150, Loss: 1145.3560\n",
      "QUANTILE LOSS:  tensor(1078.8142, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14729.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(898.1030, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14748.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(900.3240, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14754.3115, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(927.4356, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14772.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.0852, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14786.5352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.9089, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14795.5107, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(903.2530, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14814.0986, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(879.0744, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14835.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1389.6102, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14830.8975, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1081.5145, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14847.1650, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 160, Loss: 1081.5145\n",
      "QUANTILE LOSS:  tensor(872.9675, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14874.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(829.7615, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14873.8662, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(913.9221, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14899.7607, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(865.3380, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14915.1670, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1158.2506, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14911.2764, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1016.4050, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14936.3906, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.7607, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14955.3047, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(838.8082, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14954.2295, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(923.1360, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14979.1201, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(809.5128, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14995.3906, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 170, Loss: 809.5128\n",
      "QUANTILE LOSS:  tensor(1072.7240, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(14991.1670, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(954.3043, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15017.1357, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.9531, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15036.9287, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(822.1353, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15034.9775, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(877.1750, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15058.5703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(736.5943, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15077.0820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.1399, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15075.7383, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(826.6721, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15103.9053, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.1297, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15117.6484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1051.1310, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15111.6162, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 180, Loss: 1051.1310\n",
      "QUANTILE LOSS:  tensor(901.0044, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15140.7188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.2480, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15157.0830, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(761.5864, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15162.1885, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(809.9561, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15188.4287, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(759.1302, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15196.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(734.8115, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15209.9258, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(781.8957, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15232.1611, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(785.8209, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15236.8330, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(817.3932, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15248.0508, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(788.8713, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15269.3594, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 190, Loss: 788.8713\n",
      "QUANTILE LOSS:  tensor(714.8552, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15280.3975, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(713.2260, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15304.1992, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.4473, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15310.4893, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(707.1882, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15321.2705, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(691.8682, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15345.7549, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.1978, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15351.6025, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(670.2405, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15350.1240, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(656.9818, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15388.9062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(727.2026, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15388.6865, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(691.6914, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15388.1338, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 200, Loss: 691.6914\n",
      "QUANTILE LOSS:  tensor(854.5092, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15417.7764, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(821.3770, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15419.5010, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(722.9338, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15428.1650, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(714.7918, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15466.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.0540, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15458.3633, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(682.0236, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15542.3975, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(733.0159, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15503.6689, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(767.9584, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15495.4883, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(718.1973, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15552.5732, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(776.0002, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15538.8936, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 210, Loss: 776.0002\n",
      "QUANTILE LOSS:  tensor(798.5173, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15535.2764, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(729.7495, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15594.1299, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(675.1782, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15572.2920, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(865.8433, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15572.8945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(922.8826, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15589.8252, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(918.4085, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15615.0654, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(878.6767, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15614.2588, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(896.7576, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15630.8984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(998.6457, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15650.5850, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1012.1786, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15663.1904, grad_fn=<MeanBackward0>)\n",
      "Epoch 10/25, Batch 220, Loss: 1012.1786\n",
      "QUANTILE LOSS:  tensor(1002.4245, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15671.7080, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1031.1573, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15681.5850, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(706.1161, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15696.2109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(689.3774, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15709.3662, grad_fn=<MeanBackward0>)\n",
      "Epoch 10, Avg Loss: 1050.8745\n",
      "QUANTILE LOSS:  tensor(11528.3828, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15719.9990, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 0, Loss: 11528.3828\n",
      "QUANTILE LOSS:  tensor(10109.1592, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15733.8594, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8152.0508, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15744.0713, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6084.5708, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15757.6680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5967.5137, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15772.2197, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1312.6813, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15801.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(657.0486, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15841.8242, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(707.0700, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15820.2656, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(702.9785, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15853.7490, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(774.7847, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15895.1221, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1084.5692, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15865.4502, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 10, Loss: 1084.5692\n",
      "QUANTILE LOSS:  tensor(671.6117, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15908.1650, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(447.2076, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15953.9658, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(505.8083, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15909.2529, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(679.1768, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15947.0479, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(411.0800, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16006.3408, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1447.1154, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15943.2295, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1278.6296, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15947.0547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(860.3630, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15964.6367, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(875.8491, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15973.8857, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.5696, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(15989.6826, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 20, Loss: 831.5696\n",
      "QUANTILE LOSS:  tensor(1439.8066, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16002.6670, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1206.6736, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16012.5674, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1057.1974, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16028.2510, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(780.8891, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16048.3115, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(829.2452, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16056.9482, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(816.3707, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16077.1162, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(620.3958, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16098.8115, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(740.1887, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16103.4033, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(735.8620, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16122.3984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(552.4374, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16141.7314, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 30, Loss: 552.4374\n",
      "QUANTILE LOSS:  tensor(677.4067, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16141.1719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(720.9174, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16162.3525, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(526.2108, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16178.5576, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(653.8339, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16181.7939, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(668.4301, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16203.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(736.7935, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16215.5830, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(715.9547, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16223.3516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(723.7159, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16245.9658, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(552.3209, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16261.3018, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(705.1490, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16264.2588, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 40, Loss: 705.1490\n",
      "QUANTILE LOSS:  tensor(705.6521, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16285.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(649.9683, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16299.5654, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(850.8787, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16303.0557, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(720.6391, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16327.9346, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(525.5277, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16342.8115, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(690.6901, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16346.2705, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(719.0394, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16369.3115, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(716.7667, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16388.3125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1258.4501, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16384.8027, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(787.8568, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16413.3535, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 50, Loss: 787.8568\n",
      "QUANTILE LOSS:  tensor(1330.2308, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16411.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(839.6971, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16430.6504, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(701.3447, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16461.4395, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(342.4763, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16457.6934, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(695.8145, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16469.4590, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(714.8113, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16484.0605, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2466.2742, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16492.2070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1259.5402, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16505.8438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(875.9108, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16523.6582, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(479.9062, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16536.0312, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 60, Loss: 479.9062\n",
      "QUANTILE LOSS:  tensor(746.7597, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16549.9043, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(818.8810, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16563.7871, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(335.2585, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16579.6543, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(719.2456, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16590.1152, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(928.9360, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16603.8613, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1327.3954, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16615.5410, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(904.8773, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16629.0215, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(882.2988, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16642.9980, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1766.5629, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16651.7285, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1261.8861, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16666.4492, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 70, Loss: 1261.8861\n",
      "QUANTILE LOSS:  tensor(967.6906, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16682.2754, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(846.5242, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16693.5410, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(948.5406, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16709.7207, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(879.9453, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16725.6094, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1487.1688, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16733.5840, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1109.7303, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16749.2871, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(934.2286, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16763.7812, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(686.6416, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16776.5020, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(929.4512, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16792.3340, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(785.8550, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16807.0527, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 80, Loss: 785.8550\n",
      "QUANTILE LOSS:  tensor(458.9784, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16821.9141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(850.3472, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16835.2559, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(823.5222, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16847.9629, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1570.5912, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16858.7715, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1183.9535, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16874.2051, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(975.2183, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16889.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1079.1730, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16901.4121, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.7449, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16920.0195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.2969, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16933.7285, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1440.3541, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16943.1367, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 90, Loss: 1440.3541\n",
      "QUANTILE LOSS:  tensor(1115.8953, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16959.0254, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(932.8889, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16974.0215, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(871.2398, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(16986.4258, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(862.6729, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17004.5098, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.7100, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17016.9160, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1591.2725, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17027.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1159.6337, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17043.6152, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(943.2950, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17059.0918, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(808.6514, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17071.5488, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(870.6326, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17089.3145, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 100, Loss: 870.6326\n",
      "QUANTILE LOSS:  tensor(798.3574, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17103.3027, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(408.6707, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17119.7402, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(738.8862, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17132.5059, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.0757, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17147.7285, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1514.1299, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17155.3945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1102.6115, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17171.3672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(883.9045, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17191.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.5397, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17199.7168, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(861.5603, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17216.4434, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.2842, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17234.0391, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 110, Loss: 880.2842\n",
      "QUANTILE LOSS:  tensor(1382.1188, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17240.4492, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1099.5585, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17257.2188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(951.6890, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17276.7168, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(901.3318, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17284.6133, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(762.8882, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17304.2012, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(714.4258, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17321.6582, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1011.1501, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17330.8027, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1127.4139, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17341.6992, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(925.2100, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17359.7480, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1094.6210, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17369.0176, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 120, Loss: 1094.6210\n",
      "QUANTILE LOSS:  tensor(927.7702, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17385.4785, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(808.2999, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17404.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(648.1935, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17414.1445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(657.9711, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17434.8984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(653.1830, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17451.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1288.9669, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17453.4551, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1026.5875, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17470.2871, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.7781, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17488.9043, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(994.3546, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17497.4805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(867.4294, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17515.3984, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 130, Loss: 867.4294\n",
      "QUANTILE LOSS:  tensor(852.7950, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17533.1660, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1134.4827, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17539.5215, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1004.7083, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17556.0332, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(832.8913, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17576.5957, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(916.9753, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17583.9492, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(862.1272, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17602.7617, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(718.5384, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17622.5723, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(987.0795, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17628.7598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1023.2697, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17642.4785, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.8458, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17666.3770, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 140, Loss: 837.8458\n",
      "QUANTILE LOSS:  tensor(901.3870, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17670.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(870.2549, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17689.6699, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(728.3611, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17709.6230, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(648.7816, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17717.6660, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(605.9376, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17741.4707, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(592.4129, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17772.6074, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1637.6188, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17755.6777, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1268.8735, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17771.3301, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1067.9769, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17787.5957, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1139.2925, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17804.0215, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 150, Loss: 1139.2925\n",
      "QUANTILE LOSS:  tensor(1072.7510, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17819.8730, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.0397, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17840.5449, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(894.2606, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17847.3770, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(921.3724, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17867.3301, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.0220, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17882.7930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.8455, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17892.6602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(897.1898, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17913.0957, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(873.0112, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17936.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1383.5470, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17931.5547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1075.4514, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17949.4277, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 160, Loss: 1075.4514\n",
      "QUANTILE LOSS:  tensor(866.9044, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17979.4277, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(823.6984, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(17978.7715, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(907.8589, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18007.2305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(859.2749, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18024.1582, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1152.1876, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18019.8828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1010.3419, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18047.4785, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.6976, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18068.2578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(832.7451, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18067.0723, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(917.0730, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18094.4160, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(803.4497, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18112.2910, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 170, Loss: 803.4497\n",
      "QUANTILE LOSS:  tensor(1066.6610, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18107.6484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(948.2412, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18136.1777, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.8900, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18157.9277, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(816.0725, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18155.7734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(871.1121, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18181.6973, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(730.5314, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18202.0215, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.0769, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18200.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(820.6092, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18231.4805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.0667, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18246.5801, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1045.0679, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18239.9492, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 180, Loss: 1045.0679\n",
      "QUANTILE LOSS:  tensor(894.9416, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18271.9082, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.1851, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18289.8730, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(755.5235, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18295.4785, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(803.8932, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18324.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(753.0672, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18332.7715, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(728.7487, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18347.8926, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(775.8328, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18372.3066, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.7580, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18377.4180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(811.3304, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18389.7480, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(782.8087, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18413.1348, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 190, Loss: 782.8087\n",
      "QUANTILE LOSS:  tensor(708.7924, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18425.2402, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(707.1631, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18451.3848, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.3847, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18458.2715, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(701.1255, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18470.1016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(685.8055, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18496.9902, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.1349, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18503.3926, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(664.1777, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18501.7598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(650.9191, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18544.3418, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(721.1399, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18544.0820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(685.6288, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18543.4590, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 200, Loss: 685.6288\n",
      "QUANTILE LOSS:  tensor(848.4464, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18576.0117, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(815.3144, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18577.8848, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(716.8710, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18587.3848, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(708.7292, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18629.0215, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(777.9914, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18620.5195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(675.9609, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18712.7285, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.9533, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18670.2383, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(761.8958, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18661.2402, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(712.1346, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18723.8730, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.9376, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18708.8730, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 210, Loss: 769.9376\n",
      "QUANTILE LOSS:  tensor(792.4548, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18704.8848, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(723.6870, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18769.4453, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(669.1157, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18745.4863, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(859.7808, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18746.1445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(916.8201, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18764.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(912.3460, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18792.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(872.6142, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18791.5020, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(890.6951, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18809.7480, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(992.5833, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18831.3652, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1006.1162, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18845.1758, grad_fn=<MeanBackward0>)\n",
      "Epoch 11/25, Batch 220, Loss: 1006.1162\n",
      "QUANTILE LOSS:  tensor(996.3620, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18854.5098, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1025.0948, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18865.3438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(700.0537, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18881.3574, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(683.3149, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18895.7812, grad_fn=<MeanBackward0>)\n",
      "Epoch 11, Avg Loss: 1044.8564\n",
      "QUANTILE LOSS:  tensor(11522.3203, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18907.4590, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 0, Loss: 11522.3203\n",
      "QUANTILE LOSS:  tensor(10103.3477, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18922.6582, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8145.9883, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18933.8457, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6078.5083, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18948.7578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5961.4517, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18964.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1306.6189, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(18996.3652, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(650.9861, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19040.9883, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(701.0076, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19017.3652, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(696.9161, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19054.0547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(768.7222, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19099.3965, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1078.5070, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19066.8867, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 10, Loss: 1078.5070\n",
      "QUANTILE LOSS:  tensor(665.5494, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19113.6816, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(442.5177, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19163.8730, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(500.2488, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19114.8887, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(673.1146, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19156.2520, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(406.5150, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19221.1973, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1441.8019, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19152.0215, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1272.5677, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19156.1855, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(854.3008, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19175.4277, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(869.7871, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19185.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.5076, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19202.8223, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 20, Loss: 825.5076\n",
      "QUANTILE LOSS:  tensor(1433.7448, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19217.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1200.6117, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19227.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1051.1356, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19245.0332, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(774.8273, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19266.9902, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(823.1834, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19276.4473, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(810.3089, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19298.5234, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(614.3342, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19322.2832, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(734.1270, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19327.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(729.8005, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19348.1016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(546.3757, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19369.2676, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 30, Loss: 546.3757\n",
      "QUANTILE LOSS:  tensor(671.3450, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19368.6348, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(714.8558, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19391.8301, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(520.1492, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19409.5645, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(647.7724, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19413.1035, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(662.3687, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19436.4590, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(730.7321, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19450.0918, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(709.8932, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19458.5918, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(717.6545, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19483.3535, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(546.2595, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19500.1367, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(699.0876, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19503.3633, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 40, Loss: 699.0876\n",
      "QUANTILE LOSS:  tensor(699.5906, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19526.7207, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(643.9069, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19542.0195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(844.8172, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19545.8320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(714.5776, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19573.0645, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(519.4663, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19589.3340, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(684.6287, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19593.1230, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(712.9781, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19618.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(711.2048, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19639.1348, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1252.3888, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19635.2598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(781.7956, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19666.4766, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 50, Loss: 781.7956\n",
      "QUANTILE LOSS:  tensor(1324.1697, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19664.2793, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(833.6360, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19685.3535, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(695.2835, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19719.0215, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(339.3799, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19714.8848, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(689.7538, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19727.5957, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(708.7510, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19743.4043, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2460.2144, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19752.1680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1253.4803, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19766.9590, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(869.8513, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19786.3340, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(475.1820, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19799.7578, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 60, Loss: 475.1820\n",
      "QUANTILE LOSS:  tensor(740.7006, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19814.7734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(812.8222, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19829.8145, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(331.7124, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19847.0410, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(713.2064, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19858.1777, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(922.8784, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19872.8965, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1321.3385, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19885.3926, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(898.8206, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19899.8848, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(876.2427, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19914.9355, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1760.5072, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19924.2832, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1255.8304, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19940.1953, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 70, Loss: 1255.8304\n",
      "QUANTILE LOSS:  tensor(961.6355, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19957.3340, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(840.4693, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19969.4980, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(942.4859, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(19987.0527, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(873.8909, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20004.3105, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1481.1146, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20012.9160, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1103.6764, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20029.9785, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(928.1748, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20045.7402, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(680.5879, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20059.5723, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(923.3977, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20076.7930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.8016, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20092.8145, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 80, Loss: 779.8016\n",
      "QUANTILE LOSS:  tensor(452.9251, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20109., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(844.2940, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20123.5332, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(817.4689, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20137.3730, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1564.5382, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20149.1426, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1177.9005, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20165.9766, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(969.1653, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20182.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1073.1202, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20195.6348, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.6922, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20215.9355, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.2443, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20230.8867, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1434.3014, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20241.1465, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 90, Loss: 1434.3014\n",
      "QUANTILE LOSS:  tensor(1109.8427, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20258.4902, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(926.8362, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20274.8535, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(865.1873, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20288.3848, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(856.6204, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20308.1309, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.6575, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20321.6660, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1585.2202, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20333.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1153.5814, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20350.8145, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(937.2426, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20367.6992, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(802.5989, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20381.2949, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(864.5804, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20400.6895, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 100, Loss: 864.5804\n",
      "QUANTILE LOSS:  tensor(792.3052, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20415.9688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(403.3734, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20433.9082, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(732.8342, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20447.6855, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(868.0241, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20464.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1508.0785, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20472.3945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1096.5603, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20489.7168, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(877.8535, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20511.6270, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(760.4890, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20520.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.5096, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20538.6270, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.2338, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20557.7715, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 110, Loss: 874.2338\n",
      "QUANTILE LOSS:  tensor(1376.0685, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20564.6895, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1093.5083, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20582.9297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(945.6389, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20604.1543, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(895.2819, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20612.7305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(756.8384, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20634.0547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(708.3760, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20653.0723, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1006.1082, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20663.0195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1121.3643, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20674.8457, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(919.1605, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20694.4785, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1088.5715, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20704.5430, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 120, Loss: 1088.5715\n",
      "QUANTILE LOSS:  tensor(921.7209, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20722.4668, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(802.2507, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20743.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(642.2076, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20753.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(651.9221, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20776.2480, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(647.1342, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20794.0293, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1282.9181, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20796.3848, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1020.5388, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20814.6973, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.7297, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20834.9668, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(988.3061, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20844.2832, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(861.3809, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20863.7988, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 130, Loss: 861.3809\n",
      "QUANTILE LOSS:  tensor(846.7466, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20883.1445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1128.4344, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20890.0547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(998.6600, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20908.0430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(826.8430, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20930.4590, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(910.9272, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20938.4473, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(856.0790, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20958.9551, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(712.4902, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20980.5508, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(981.0313, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(20987.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1017.2219, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21002.2285, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.7977, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21028.2891, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 140, Loss: 831.7977\n",
      "QUANTILE LOSS:  tensor(895.3391, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21032.9395, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(864.2068, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21053.6758, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(722.3133, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21075.4082, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(642.7336, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21084.1758, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(599.8896, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21110.1348, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(586.3651, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21144.0957, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1631.5712, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21125.6035, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1262.8256, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21142.6777, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1061.9291, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21160.4082, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1133.2448, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21178.3125, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 150, Loss: 1133.2448\n",
      "QUANTILE LOSS:  tensor(1066.7031, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21195.5840, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(885.9919, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21218.1230, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(888.2129, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21225.5645, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(915.3247, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21247.3105, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(818.9743, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21264.1758, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.7978, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21274.9258, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(891.1421, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21297.2070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(866.9634, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21323.1895, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1377.4994, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21317.3145, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1069.4037, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21336.8008, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 160, Loss: 1069.4037\n",
      "QUANTILE LOSS:  tensor(860.8568, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21369.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(817.6507, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21368.7812, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(901.8113, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21399.7949, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(853.2273, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21418.2285, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1146.1400, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21413.5723, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1004.2941, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21443.6484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.6500, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21466.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(826.6975, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21464.9980, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(911.0254, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21494.8008, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(797.4021, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21514.2617, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 170, Loss: 797.4021\n",
      "QUANTILE LOSS:  tensor(1060.6134, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21509.2090, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(942.1936, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21540.3008, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.8425, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21563.9883, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(810.0249, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21561.6465, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(865.0645, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21589.8848, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(724.4839, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21612.0215, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.0293, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21610.4258, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(814.5616, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21644.1230, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.0193, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21660.5547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1039.0204, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21653.3418, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 180, Loss: 1039.0204\n",
      "QUANTILE LOSS:  tensor(888.8940, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21688.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.1376, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21707.7168, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(749.4760, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21713.8320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(797.8457, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21745.2090, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(747.0198, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21754.4395, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(722.7012, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21770.9141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.7854, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21797.4883, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(773.7105, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21803.0508, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(805.2830, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21816.4902, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(776.7612, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21841.9395, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 190, Loss: 776.7612\n",
      "QUANTILE LOSS:  tensor(702.7449, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21855.1230, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(701.1157, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21883.6016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.3372, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21891.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(695.0781, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21903.9551, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(679.7581, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21933.2578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(739.0876, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21940.2051, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(658.1303, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21938.4062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(644.8716, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21984.7910, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(715.0925, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21984.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(679.5814, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(21983.7949, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 200, Loss: 679.5814\n",
      "QUANTILE LOSS:  tensor(842.3991, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22019.2734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(809.2669, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22021.2715, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(710.8237, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22031.6074, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(702.6818, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22076.9551, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(771.9440, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22067.6738, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(669.9136, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22168.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(720.9059, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22121.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(755.8484, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22112., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(706.0872, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22180.1484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.8903, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22163.8516, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 210, Loss: 763.8903\n",
      "QUANTILE LOSS:  tensor(786.4075, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22159.4824, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(717.6396, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22229.7441, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(663.0684, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22203.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(853.7335, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22204.3730, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(910.7729, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22224.5840, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(906.2988, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22254.7578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(866.5670, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22253.7266, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(884.6479, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22273.5801, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(986.5359, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22297.1270, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1000.0690, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22312.1250, grad_fn=<MeanBackward0>)\n",
      "Epoch 12/25, Batch 220, Loss: 1000.0690\n",
      "QUANTILE LOSS:  tensor(990.3149, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22322.2715, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1019.0475, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22334.0645, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(694.0065, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22351.4785, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(677.2677, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22367.1602, grad_fn=<MeanBackward0>)\n",
      "Epoch 12, Avg Loss: 1038.8637\n",
      "QUANTILE LOSS:  tensor(11516.2734, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22379.9141, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 0, Loss: 11516.2734\n",
      "QUANTILE LOSS:  tensor(10097.5518, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22396.4355, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8139.9414, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22408.6035, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6072.4614, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22424.8145, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5955.4043, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22442.1660, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1300.5717, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22476.5723, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(644.9390, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22525.1035, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(694.9604, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22499.4277, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(690.8691, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22539.3125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(762.6750, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22588.6152, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1072.4598, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22553.2812, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 10, Loss: 1072.4598\n",
      "QUANTILE LOSS:  tensor(659.5022, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22604.1426, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(437.9530, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22658.7051, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(495.0688, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22605.4395, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(667.0677, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22650.2520, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(402.0756, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22720.7402, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1436.4995, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22645.4160, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1266.5215, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22649.8223, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(848.2551, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22670.6113, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(863.7414, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22681.5098, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(819.4620, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22700.2031, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 20, Loss: 819.4620\n",
      "QUANTILE LOSS:  tensor(1427.6992, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22715.5645, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1194.5665, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22727.2734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1045.0903, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22745.8574, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(768.7822, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22769.6660, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(817.1384, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22779.8848, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(804.2640, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22803.8262, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(608.2893, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22829.6016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(728.0823, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22835.0020, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(723.7557, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22857.5801, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(540.3311, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22880.5449, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 30, Loss: 540.3311\n",
      "QUANTILE LOSS:  tensor(665.3005, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22879.8340, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(708.8113, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22905.0176, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(514.1047, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22924.2598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(641.7280, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22928.0840, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(656.3242, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22953.4355, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(724.6877, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22968.2480, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(703.8488, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(22977.4551, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(711.6100, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23004.3438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(540.2151, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23022.5625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(693.0433, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23026.0605, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 40, Loss: 693.0433\n",
      "QUANTILE LOSS:  tensor(693.5464, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23051.4277, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(637.8626, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23068.0215, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(838.7730, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23072.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(708.5336, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23101.7363, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(513.4222, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23119.3984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(678.5845, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23123.5098, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(706.9339, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23150.8965, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(705.7800, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23173.4766, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1246.3446, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23169.2598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(775.7515, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23203.1484, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 50, Loss: 775.7515\n",
      "QUANTILE LOSS:  tensor(1318.1256, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23200.7676, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(827.5919, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23223.6465, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(689.2394, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23260.2051, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(336.7287, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23255.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(683.7505, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23269.4395, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(702.7072, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23286.5020, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2454.2415, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23295.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1247.4370, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23311.8828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(863.8079, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23332.8027, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(470.7544, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23347.2910, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 60, Loss: 470.7544\n",
      "QUANTILE LOSS:  tensor(734.6995, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23363.3730, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(806.7798, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23379.4688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(328.9910, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23397.9473, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(707.2886, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23409.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(916.8376, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23425.3926, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1315.2981, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23438.6855, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.7805, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23454.1777, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(870.2029, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23470.2988, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1754.4677, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23480.2480, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1249.7914, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23497.3438, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 70, Loss: 1249.7914\n",
      "QUANTILE LOSS:  tensor(955.5966, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23515.7793, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(834.4308, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23528.8516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(936.4476, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23547.7715, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(867.8527, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23566.3730, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1475.0765, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23575.6113, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1097.6385, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23594.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(922.1371, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23611.0449, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(674.5640, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23625.9863, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(917.3603, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23644.5723, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(773.7643, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23661.8652, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 80, Loss: 773.7643\n",
      "QUANTILE LOSS:  tensor(446.9431, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23679.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(838.2571, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23694.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(811.4324, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23709.8516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1558.5016, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23722.4980, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1171.8643, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23740.6465, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(963.1293, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23758.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1067.0842, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23772.6465, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.6562, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23794.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.2085, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23810.7402, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1428.2657, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23821.8223, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 90, Loss: 1428.2657\n",
      "QUANTILE LOSS:  tensor(1103.8070, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23840.5605, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(920.8008, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23858.2520, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(859.1520, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23872.8926, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(850.5851, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23894.2598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.6223, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23908.9141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1579.1851, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23921.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1147.5463, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23940.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(931.2075, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23958.7344, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(796.5640, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23973.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(858.5454, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(23994.4766, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 100, Loss: 858.5454\n",
      "QUANTILE LOSS:  tensor(786.2701, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24011.0176, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(398.8094, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24030.4453, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.7996, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24045.2402, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(861.9896, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24062.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1502.0443, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24071.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1090.5264, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24090.4160, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(871.8196, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24114.0840, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(754.4551, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24123.5547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.4761, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24143.1699, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(868.2004, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24163.8359, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 110, Loss: 868.2004\n",
      "QUANTILE LOSS:  tensor(1370.0352, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24171.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1087.4751, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24190.9863, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(939.6058, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24213.9473, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(889.2488, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24223.1875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(750.8054, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24246.2520, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(702.3430, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24266.8223, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1001.4196, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24277.5723, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1115.3317, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24290.2910, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(913.1279, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24311.4707, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1082.5392, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24322.3027, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 120, Loss: 1082.5392\n",
      "QUANTILE LOSS:  tensor(915.6885, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24341.6484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(796.2185, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24364.4668, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(636.2988, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24375.3223, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(645.8901, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24399.7480, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(641.1023, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24418.9551, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1276.8864, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24421.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1014.5070, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24441.2793, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.6979, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24463.1895, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(982.2744, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24473.2676, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.3494, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24494.3730, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 130, Loss: 855.3494\n",
      "QUANTILE LOSS:  tensor(840.7151, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24515.3125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1122.4030, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24522.7676, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(992.6286, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24542.2324, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(820.8117, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24566.4980, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(904.8958, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24575.1465, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(850.0477, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24597.3262, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(706.4589, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24620.7090, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(975.0001, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24627.9980, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1011.1904, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24644.1660, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.7664, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24672.3750, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 140, Loss: 825.7664\n",
      "QUANTILE LOSS:  tensor(889.3077, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24677.4160, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(858.1756, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24699.8516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(716.2820, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24723.3926, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(636.7025, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24732.8730, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(593.8585, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24760.9902, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(580.3339, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24797.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1625.5399, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24777.7480, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1256.7946, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24796.2188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1055.8978, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24815.3984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1127.2618, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24834.7910, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 150, Loss: 1127.2618\n",
      "QUANTILE LOSS:  tensor(1060.6721, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24853.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(879.9609, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24877.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(882.1818, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24885.8516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(909.2937, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24909.3691, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(812.9434, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24927.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.7669, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24939.2090, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(885.1113, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24963.3027, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(860.9328, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24991.4238, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1371.4686, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(24985.0527, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1063.3729, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25006.1230, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 160, Loss: 1063.3729\n",
      "QUANTILE LOSS:  tensor(854.8260, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25041.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(811.6199, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25040.7188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(895.7806, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25074.2734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(847.1966, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25094.2051, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1140.1093, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25089.1777, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(998.2637, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25121.7168, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(819.6194, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25146.1855, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(820.6669, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25144.8047, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(904.9948, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25177.0527, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(791.3715, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25198.0918, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 170, Loss: 791.3715\n",
      "QUANTILE LOSS:  tensor(1054.5829, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25192.6484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(936.1631, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25226.2812, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(760.8118, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25251.8848, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(803.9944, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25249.3730, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(859.0340, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25279.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(718.4533, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25303.8535, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(759.9988, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25302.1348, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(808.5311, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25338.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(738.9888, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25356.3574, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1032.9900, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25348.5645, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 180, Loss: 1032.9900\n",
      "QUANTILE LOSS:  tensor(882.8635, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25386.2266, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.1070, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25407.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(743.4456, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25413.9980, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(791.8152, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25447.9355, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(740.9893, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25457.9141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(716.6706, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25475.7344, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.7548, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25504.4785, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(767.6801, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25510.4785, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(799.2526, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25525.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(770.7307, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25552.5449, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 190, Loss: 770.7307\n",
      "QUANTILE LOSS:  tensor(696.7145, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25566.7832, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(695.0852, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25597.6152, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(739.3067, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25605.6895, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(689.0476, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25619.6016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(673.7276, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25651.3125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(733.0571, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25658.8105, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(652.0999, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25656.8359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(638.8412, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25707.0449, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(709.0620, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25706.7012, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(673.5508, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25705.9199, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 200, Loss: 673.5508\n",
      "QUANTILE LOSS:  tensor(836.3686, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25744.3125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(803.2366, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25746.4473, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(704.7933, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25757.6172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(696.6514, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25806.6699, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(765.9136, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25796.6035, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(663.8831, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25905.0996, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(714.8755, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25855.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(749.8181, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25844.5234, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(700.0569, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25918.1895, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.8599, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25900.5996, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 210, Loss: 757.8599\n",
      "QUANTILE LOSS:  tensor(780.3770, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25895.8574, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(711.6092, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25971.7910, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(657.0380, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25943.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(847.7029, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25944.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(904.7424, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25966.2188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(900.2683, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25998.8613, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(860.5366, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(25997.7207, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(878.6174, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26019.1855, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(980.5057, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26044.6738, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(994.0386, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26060.8574, grad_fn=<MeanBackward0>)\n",
      "Epoch 13/25, Batch 220, Loss: 994.0386\n",
      "QUANTILE LOSS:  tensor(984.2844, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26071.7949, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1013.0172, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26084.5723, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(687.9761, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26103.3574, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(671.2373, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26120.3145, grad_fn=<MeanBackward0>)\n",
      "Epoch 13, Avg Loss: 1032.9041\n",
      "QUANTILE LOSS:  tensor(11510.2422, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26134.1387, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 0, Loss: 11510.2422\n",
      "QUANTILE LOSS:  tensor(10091.7705, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26151.9863, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8133.9106, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26165.1465, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6066.4312, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26182.6660, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5949.3735, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26201.3984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1294.5414, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26238.5410, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(638.9086, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26290.9590, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(688.9300, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26263.2734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(684.8386, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26306.3340, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(756.6447, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26359.5840, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1066.4293, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26321.4590, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 10, Loss: 1066.4293\n",
      "QUANTILE LOSS:  tensor(653.4719, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26376.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(433.4006, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26435.2734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(490.2428, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26377.7812, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(661.0375, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26426.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(397.6463, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26502.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1431.4137, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26420.6699, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1260.4918, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26425.2715, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(842.2255, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26447.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(857.7121, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26459.2324, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(813.4329, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26479.3125, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 20, Loss: 813.4329\n",
      "QUANTILE LOSS:  tensor(1421.6703, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26495.8027, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1188.5376, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26508.3516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1039.0616, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26528.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(762.7536, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26553.9688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(811.1100, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26564.9473, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(798.2358, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26590.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(602.2610, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26618.5020, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(722.0540, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26624.3027, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(717.7275, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26648.6426, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(534.3030, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26673.3848, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 30, Loss: 534.3030\n",
      "QUANTILE LOSS:  tensor(659.2725, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26672.5918, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(702.7833, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26699.7324, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(508.0912, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26720.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(635.7001, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26724.5449, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(650.2966, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26751.8652, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(718.6601, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26767.8027, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(697.8212, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26777.6973, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(705.5826, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26806.6777, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(534.1877, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26826.3145, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(687.0159, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26830.0449, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 40, Loss: 687.0159\n",
      "QUANTILE LOSS:  tensor(687.5190, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26857.3984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(631.8353, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26875.2969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(832.7458, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26879.7324, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(702.5063, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26911.6270, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(507.3949, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26930.6875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(672.5573, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26935.1016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(700.9067, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26964.6270, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(700.3677, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26988.9844, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1240.3175, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(26984.4375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.7244, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27020.9785, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 50, Loss: 769.7244\n",
      "QUANTILE LOSS:  tensor(1312.0985, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27018.4199, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(821.5648, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27043.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(683.2124, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27082.5176, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(334.1451, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27077.6777, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(677.9026, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27092.4785, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(696.6868, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27110.8457, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2448.3379, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27120.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1241.4102, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27138.1016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(857.7812, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27160.6074, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(466.9456, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27176.1895, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 60, Loss: 466.9456\n",
      "QUANTILE LOSS:  tensor(728.7962, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27193.2910, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(800.7539, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27210.4395, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(326.6327, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27230.1973, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(701.3863, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27242.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(910.8126, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27259.3145, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1309.2736, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27273.4688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.7563, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27289.9902, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(864.1790, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27307.2109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1748.4438, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27317.8105, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1243.7678, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27336.1152, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 70, Loss: 1243.7678\n",
      "QUANTILE LOSS:  tensor(949.5732, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27355.8770, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(828.4074, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27369.8652, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(930.4245, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27390.1777, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(861.8297, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27410.1484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1469.0537, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27420.0410, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1091.6158, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27439.8438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(916.1144, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27458.1309, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(668.6641, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27474.1777, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(911.3378, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27494.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(767.7420, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27512.7637, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 80, Loss: 767.7420\n",
      "QUANTILE LOSS:  tensor(441.4784, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27531.5645, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(832.2349, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27548.2598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(805.4103, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27564.1777, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1552.4800, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27577.7051, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1165.8427, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27597.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(957.1079, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27616.5801, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1061.0630, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27631.4766, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.6351, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27655.0469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.1875, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27672.3848, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1422.2448, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27684.2715, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 90, Loss: 1422.2448\n",
      "QUANTILE LOSS:  tensor(1097.7863, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27704.4160, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(914.7800, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27723.4375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(853.1313, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27739.1738, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(844.5645, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27762.1582, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.6018, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27777.9141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1573.1642, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27791.9355, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1141.5258, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27811.8574, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(925.1871, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27831.5215, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.5436, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27847.3770, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(852.5250, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27869.9922, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 100, Loss: 852.5250\n",
      "QUANTILE LOSS:  tensor(780.2497, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27887.8027, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(394.5228, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27908.7266, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(720.7794, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27924.6074, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.9695, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27943.6484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1496.0240, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27953.1035, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1084.5062, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27973.1699, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(865.7996, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(27998.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(748.5394, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28008.8145, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.4562, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28029.8770, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(862.1805, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28052.0938, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 110, Loss: 862.1805\n",
      "QUANTILE LOSS:  tensor(1364.0155, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28060.0645, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1081.4553, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28081.2520, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(933.5862, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28105.9277, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(883.2293, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28115.8516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(744.7859, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28140.6699, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(696.3237, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28162.7949, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(996.7480, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28174.3516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1109.3124, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28188.0312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(907.1086, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28210.8203, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1076.5199, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28222.4609, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 120, Loss: 1076.5199\n",
      "QUANTILE LOSS:  tensor(909.6693, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28243.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.1994, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28267.8516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(630.4020, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28279.5312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(639.8710, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28305.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(635.0845, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28326.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1270.8673, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28329.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1008.4880, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28350.4473, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.6790, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28374.0176, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(976.2554, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28384.8340, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.3306, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28407.5215, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 130, Loss: 849.3306\n",
      "QUANTILE LOSS:  tensor(834.6963, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28430.0410, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1116.3842, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28438.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(986.6099, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28459.0020, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(814.7929, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28485.0957, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(898.8770, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28494.3965, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(844.0289, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28518.2715, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(700.4402, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28543.4277, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(968.9814, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28551.2598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1005.1719, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28568.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(819.7478, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28599.0215, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 140, Loss: 819.7478\n",
      "QUANTILE LOSS:  tensor(883.2891, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28604.4375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(852.1570, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28628.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(710.2635, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28653.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(630.6839, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28664.1270, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(587.8399, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28694.3770, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(574.3154, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28733.9473, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1619.5214, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28712.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1250.7760, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28732.3027, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1049.8794, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28752.9551, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1121.3658, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28773.8223, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 150, Loss: 1121.3658\n",
      "QUANTILE LOSS:  tensor(1054.6534, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28793.9004, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(873.9424, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28820.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(876.1634, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28828.7715, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(903.2752, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28854.0762, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(806.9249, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28873.6777, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.7484, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28886.2051, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(879.0927, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28912.1270, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(854.9142, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28942.3730, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1365.4501, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28935.5332, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1057.3544, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28958.2090, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 160, Loss: 1057.3544\n",
      "QUANTILE LOSS:  tensor(848.8074, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28996.2676, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(805.6016, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(28995.4355, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(889.7620, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29031.5449, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(841.1780, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29052.9688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1134.0908, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29047.5840, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(992.2451, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29082.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(813.6008, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29108.9004, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(814.6483, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29107.4395, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(898.9761, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29142.1309, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(785.3530, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29164.7402, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 170, Loss: 785.3530\n",
      "QUANTILE LOSS:  tensor(1048.5643, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29158.9160, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(930.1446, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29195.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(754.7934, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29222.6270, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(797.9758, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29219.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(853.0154, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29252.7949, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(712.4346, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29278.5254, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(753.9802, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29276.7051, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(802.5125, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29315.9199, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(732.9702, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29335.0098, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1026.9713, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29326.6562, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 180, Loss: 1026.9713\n",
      "QUANTILE LOSS:  tensor(876.8449, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29367.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.0886, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29389.8848, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(737.4270, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29397.0254, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(785.7967, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29433.5254, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(734.9707, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29444.2344, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(710.6520, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29463.4375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.7363, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29494.3223, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(761.6614, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29500.7676, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(793.2339, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29516.4551, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(764.7120, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29546.0156, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 190, Loss: 764.7120\n",
      "QUANTILE LOSS:  tensor(690.6959, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29561.3125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(689.0667, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29594.4902, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(733.2881, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29603.1484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(683.0290, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29618.1074, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(667.7090, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29652.2324, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(727.0385, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29660.2754, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(646.0812, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29658.1348, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(632.8226, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29712.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(703.0435, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29711.7598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(667.5322, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29710.9160, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 200, Loss: 667.5322\n",
      "QUANTILE LOSS:  tensor(830.3499, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29752.2324, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(797.2180, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29754.4902, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(698.7746, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29766.4824, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(690.6328, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29819.2598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(759.8950, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29808.3965, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(657.8646, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29925.0176, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(708.8569, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29871.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(743.7993, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29859.9141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(694.0383, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29939.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.8411, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29920.2285, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 210, Loss: 751.8411\n",
      "QUANTILE LOSS:  tensor(774.3584, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29915.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(705.5905, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29996.7090, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(651.0193, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29966.4590, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(841.6844, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29967.2441, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(898.7237, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(29990.7285, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(894.2497, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30025.8574, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(854.5178, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30024.5840, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(872.5988, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30047.6465, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(974.4868, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30075.1016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(988.0198, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30092.4551, grad_fn=<MeanBackward0>)\n",
      "Epoch 14/25, Batch 220, Loss: 988.0198\n",
      "QUANTILE LOSS:  tensor(978.2656, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30104.1973, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1006.9985, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30117.9395, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(681.9575, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30138.1016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(665.2186, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30156.3203, grad_fn=<MeanBackward0>)\n",
      "Epoch 14, Avg Loss: 1026.9714\n",
      "QUANTILE LOSS:  tensor(11504.2236, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30171.2520, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 0, Loss: 11504.2236\n",
      "QUANTILE LOSS:  tensor(10086.0010, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30190.4355, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8127.8926, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30204.5645, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6060.4116, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30223.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5943.3555, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30243.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1288.5227, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30283.3770, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(632.8899, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30339.6855, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(682.9113, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30310.0098, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(678.8198, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30356.2188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(750.6259, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30413.4121, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1060.4106, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30372.5020, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 10, Loss: 1060.4106\n",
      "QUANTILE LOSS:  tensor(647.4531, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30431.4062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(428.8526, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30494.7090, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(485.5824, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30433.0176, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(655.0189, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30484.7871, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(393.2209, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30566.3691, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1426.3755, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30478.9785, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1254.4733, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30483.9004, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(836.2070, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30507.8359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(851.6935, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30520.3340, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(807.4145, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30541.8750, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 20, Loss: 807.4145\n",
      "QUANTILE LOSS:  tensor(1415.6519, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30559.5781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1182.5192, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30573.0312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1033.0433, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30594.4785, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(756.7353, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30621.9844, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(805.0916, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30633.7598, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(792.2175, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30661.4551, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(596.3387, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30691.2676, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(716.0358, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30697.4375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(711.7093, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30723.5215, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(528.2849, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30750.0625, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 30, Loss: 528.2849\n",
      "QUANTILE LOSS:  tensor(653.2543, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30749.1660, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(696.7652, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30778.2910, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(502.2882, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30800.5449, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(629.6823, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30804.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(644.2787, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30834.0312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(712.6424, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30851.0605, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(691.8038, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30861.6035, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(699.5651, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30892.6484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(528.1703, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30913.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(680.9986, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30917.6426, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 40, Loss: 680.9986\n",
      "QUANTILE LOSS:  tensor(681.5018, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30946.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(625.8182, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30966.1074, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(826.7287, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(30970.8438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(696.4893, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31005.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(501.4547, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31025.4551, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(666.5406, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31030.0840, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(694.8901, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31061.6973, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(694.9634, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31087.7344, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1234.3010, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31082.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.7079, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31121.9355, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 50, Loss: 763.7079\n",
      "QUANTILE LOSS:  tensor(1306.0823, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31119.1230, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(815.5486, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31145.5410, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(677.1963, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31187.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(331.5568, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31182.5645, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(672.1315, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31198.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(690.7933, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31218.0215, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2442.4446, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31228.8047, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1235.3945, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31247.1426, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(851.7656, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31271.2324, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(463.5695, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31287.8984, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 60, Loss: 463.5695\n",
      "QUANTILE LOSS:  tensor(722.9661, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31306.1777, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(794.7388, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31324.4746, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(324.3890, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31345.5527, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(695.4938, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31358.7656, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(904.7979, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31376.5527, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1303.2589, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31391.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.7419, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31409.2559, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(858.1646, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31427.6465, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1742.4297, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31438.9355, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1237.7538, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31458.5020, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 70, Loss: 1237.7538\n",
      "QUANTILE LOSS:  tensor(943.5593, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31479.6465, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(822.3935, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31494.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(924.4106, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31516.3301, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.8160, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31537.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1463.0400, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31548.2910, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1085.6021, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31569.4941, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(910.1008, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31589.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(662.8217, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31606.2520, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(905.3244, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31627.5879, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(761.7286, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31647.4395, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 80, Loss: 761.7286\n",
      "QUANTILE LOSS:  tensor(436.7173, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31667.5176, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(826.2225, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31684.9824, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(799.3984, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31701.6660, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1546.4686, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31715.8145, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1159.8317, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31736.3730, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(951.0972, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31756.8926, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1055.0527, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31772.6191, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.6252, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31797.6660, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(819.1777, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31816.0547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1416.2355, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31828.6074, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 90, Loss: 1416.2355\n",
      "QUANTILE LOSS:  tensor(1091.7770, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31850.0449, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(908.7711, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31870.2910, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(847.1225, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31887.0215, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(838.5558, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31911.5469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(819.5933, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31928.3223, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1567.1562, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31943.2520, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1135.5177, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31964.5312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(919.1790, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(31985.5410, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.5356, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32002.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(846.5171, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32026.6621, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 100, Loss: 846.5171\n",
      "QUANTILE LOSS:  tensor(774.2419, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32045.6895, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(390.5345, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32068.0605, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(714.7720, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32084.8105, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.9626, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32104.9590, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1490.0175, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32114.8574, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1078.4999, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32136.1484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(859.7935, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32163.2441, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(742.6553, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32174.0059, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.4505, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32196.4316, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(856.1752, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32220.0840, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 110, Loss: 856.1752\n",
      "QUANTILE LOSS:  tensor(1358.0101, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32228.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1075.4503, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32251.1152, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(927.5813, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32277.4551, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(877.2244, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32288.0254, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(738.7812, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32314.5234, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(690.3190, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32338.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(992.0826, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32350.4941, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1103.3080, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32365.0879, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(901.1042, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32389.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1070.5155, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32401.9062, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 120, Loss: 1070.5155\n",
      "QUANTILE LOSS:  tensor(903.6650, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32424.1660, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.1951, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32450.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(624.5196, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32462.9355, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(633.8668, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32491.0645, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(629.2021, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32513.2051, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1264.8633, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32516.0410, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1002.4839, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32538.8223, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.6749, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32564.0566, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(970.2515, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32575.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.3264, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32599.9277, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 130, Loss: 843.3264\n",
      "QUANTILE LOSS:  tensor(828.6922, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32624.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1110.3800, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32632.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(980.6059, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32655.0410, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(808.7888, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32682.9863, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.8730, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32692.9434, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(838.0250, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32718.5215, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(694.4363, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32745.4355, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(962.9775, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32753.8359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(999.1679, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32772.4883, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(813.7439, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32804.9883, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 140, Loss: 813.7439\n",
      "QUANTILE LOSS:  tensor(877.2852, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32810.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(846.1531, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32836.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(704.2595, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32863.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(624.6799, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32874.7305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(581.8360, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32907.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(568.4276, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32949.4805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1613.5175, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32926.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1244.7721, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32947.6484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1043.8756, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32969.7383, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1115.4838, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(32992.0664, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 150, Loss: 1115.4838\n",
      "QUANTILE LOSS:  tensor(1048.6498, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33013.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(867.9387, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33041.5781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(870.1596, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33050.8320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(897.2715, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33077.9141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(800.9211, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33098.8867, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(819.7448, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33112.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(873.0891, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33140.0234, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(848.9106, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33172.3945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1359.4465, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33165.0820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1051.3510, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33189.3438, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 160, Loss: 1051.3510\n",
      "QUANTILE LOSS:  tensor(842.8039, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33230.0742, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(799.5979, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33229.1836, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(883.7585, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33267.8320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(835.1746, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33290.7344, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1128.0872, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33285.0195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(986.2415, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33322.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(807.5972, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33350.6016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(808.6448, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33349.0820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.9727, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33386.1992, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.3494, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33410.3711, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 170, Loss: 779.3494\n",
      "QUANTILE LOSS:  tensor(1042.5607, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33404.1680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(924.1409, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33442.8945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(748.7897, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33472.3438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(791.9722, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33469.4805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(847.0118, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33504.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(706.4312, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33532.1758, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(747.9767, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33530.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(796.5090, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33572.2070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.9666, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33592.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1020.9676, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33583.7188, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 180, Loss: 1020.9676\n",
      "QUANTILE LOSS:  tensor(870.8413, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33627.0547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(739.0849, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33651.3516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(731.4233, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33659.0352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.7930, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33698.0820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(728.9671, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33709.5117, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(704.6484, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33730.0977, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.7326, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33763.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(755.6578, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33770.0039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(787.2303, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33786.8320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(758.7084, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33818.4375, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 190, Loss: 758.7084\n",
      "QUANTILE LOSS:  tensor(684.6922, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33834.7773, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(683.0629, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33870.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(727.2845, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33879.5625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(677.0253, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33895.5664, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(661.7054, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33932.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(721.0349, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33940.6836, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(640.0776, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33938.3789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(626.8188, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33996.2188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(697.0397, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33995.7656, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(661.5286, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(33994.8555, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 200, Loss: 661.5286\n",
      "QUANTILE LOSS:  tensor(824.3462, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34039.1016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(791.2142, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34041.4688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(692.7709, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34054.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(684.6290, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34110.7773, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(753.8911, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34099.1484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(651.8608, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34223.8516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(702.8530, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34166.5195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(737.7956, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34154.2617, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(688.0345, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34238.9180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.8374, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34218.7852, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 210, Loss: 745.8374\n",
      "QUANTILE LOSS:  tensor(768.3545, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34213.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(699.5867, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34300.5508, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(645.0154, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34268.2266, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(835.6805, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34269.0586, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.7199, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34294.1836, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(888.2458, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34331.7930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(848.5139, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34330.3867, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(866.5950, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34355.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(968.4830, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34384.4766, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(982.0159, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34402.9883, grad_fn=<MeanBackward0>)\n",
      "Epoch 15/25, Batch 220, Loss: 982.0159\n",
      "QUANTILE LOSS:  tensor(972.2619, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34415.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1000.9946, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34430.2656, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(675.9536, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34451.7930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(659.2147, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34471.2773, grad_fn=<MeanBackward0>)\n",
      "Epoch 15, Avg Loss: 1021.0630\n",
      "QUANTILE LOSS:  tensor(11498.2188, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34487.3320, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 0, Loss: 11498.2188\n",
      "QUANTILE LOSS:  tensor(10080.2461, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34507.8359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8121.8882, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34522.9688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6054.4082, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34543.1016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5937.3501, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34564.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1282.5187, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34607.1680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(626.8859, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34667.3477, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(676.9073, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34635.6992, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(672.8159, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34685.0508, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(744.6220, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34746.1758, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1054.4067, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34702.5117, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 10, Loss: 1054.4067\n",
      "QUANTILE LOSS:  tensor(641.4491, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34765.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(424.3095, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34833.0742, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(481.0812, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34767.1992, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(649.0150, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34822.4414, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(388.9305, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34909.6016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1421.3456, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34816.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1248.4696, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34821.3164, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(830.2034, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34846.8086, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(845.6902, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34860.0742, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(801.4112, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34883.0195, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 20, Loss: 801.4112\n",
      "QUANTILE LOSS:  tensor(1409.6488, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34901.8594, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1176.5161, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34916.1992, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1027.0402, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34939.0742, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(750.7322, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34968.3984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(799.0887, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(34980.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(786.2145, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35010.5039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(590.4575, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35042.3164, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(710.0331, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35048.8984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(705.7066, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35076.7266, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(522.2969, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35105.0664, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 30, Loss: 522.2969\n",
      "QUANTILE LOSS:  tensor(647.2518, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35104.0664, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(690.7627, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35135.1211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(496.8561, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35158.8320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(623.6799, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35163.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(638.2766, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35194.3945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(706.6404, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35212.4648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(685.8018, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35223.6445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(693.5634, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35256.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(522.2353, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35279.1094, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(674.9971, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35283.2461, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 40, Loss: 674.9971\n",
      "QUANTILE LOSS:  tensor(675.5005, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35314.4375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(620.0184, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35334.8320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(820.7277, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35339.6055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(690.4887, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35375.8711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(495.7000, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35397.4336, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(660.5407, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35402.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(688.8906, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35435.6016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(689.5715, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35463.1836, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1228.3021, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35457.6836, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.7094, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35499.2852, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 50, Loss: 757.7094\n",
      "QUANTILE LOSS:  tensor(1300.0839, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35496.1406, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(809.5504, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35524.2031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(671.1982, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35569.1836, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(329.0789, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35563.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(666.4255, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35580.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(684.9172, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35601.0820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2436.5686, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35612.4961, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1229.3973, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35631.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(845.7686, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35657.5820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(460.2417, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35675.3086, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 60, Loss: 460.2417\n",
      "QUANTILE LOSS:  tensor(717.2119, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35694.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(788.7420, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35714.1836, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(322.1528, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35736.6445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(689.7227, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35750.6914, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(898.8015, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35769.5898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1297.2626, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35785.6094, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.7456, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35804.3555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(852.1685, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35823.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1736.4336, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35835.9336, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1231.7578, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35856.7695, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 70, Loss: 1231.7578\n",
      "QUANTILE LOSS:  tensor(937.5632, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35879.2969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(816.3976, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35895.2305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(918.4147, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35918.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.8203, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35941.1680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1457.0444, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35952.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1079.6063, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35975.0664, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(904.1050, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(35995.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(657.2747, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36014.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(899.3288, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36036.9805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(755.7331, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36058.1250, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 80, Loss: 755.7331\n",
      "QUANTILE LOSS:  tensor(432.6661, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36079.5039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(820.2271, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36097.9883, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(793.4033, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36115.6289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1540.4736, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36130.6055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1153.8369, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36152.4180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(945.1027, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36174.2148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1049.0582, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36190.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.6309, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36217.5625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(813.1835, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36237.0898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1410.2413, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36250.4375, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 90, Loss: 1410.2413\n",
      "QUANTILE LOSS:  tensor(1085.7830, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36273.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(902.7771, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36294.8086, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(841.1286, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36312.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(832.5620, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36338.7305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(813.5994, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36356.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1561.1625, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36372.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1129.5238, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36395.1836, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(913.1853, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36417.5898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.5418, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36435.6289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(840.5235, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36461.4180, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 100, Loss: 840.5235\n",
      "QUANTILE LOSS:  tensor(768.2482, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36481.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(387.2149, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36505.5625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(708.7785, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36523.3320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.9691, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36544.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1484.0240, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36555.2461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1072.5067, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36577.8789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(853.8003, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36606.7305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(736.8995, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36618.1602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.4578, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36641.8984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(850.1826, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36666.9805, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 110, Loss: 850.1826\n",
      "QUANTILE LOSS:  tensor(1352.0177, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36675.8516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1069.4579, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36699.8164, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(921.5891, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36727.7930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(871.2325, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36738.9570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(732.7893, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36767.1484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(684.3273, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36792.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(987.4221, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36805.3555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1097.3163, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36820.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(895.1128, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36846.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1064.5240, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36860.0352, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 120, Loss: 1064.5240\n",
      "QUANTILE LOSS:  tensor(897.6736, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36883.7305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.2038, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36911.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(618.6494, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36925.0117, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(627.8755, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36954.9805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(623.3568, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36978.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1258.8719, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(36981.5352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(996.4930, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37005.7695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.6839, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37032.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(964.2605, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37044.9336, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.3356, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37070.7891, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 130, Loss: 837.3356\n",
      "QUANTILE LOSS:  tensor(822.7015, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37096.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1104.3893, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37105.6055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(974.6151, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37129.4727, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(802.7981, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37159.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.8824, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37169.8555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(832.0342, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37197.0898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(688.4457, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37225.7734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(956.9868, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37234.7461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(993.1772, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37254.6133, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(807.7532, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37289.2461, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 140, Loss: 807.7532\n",
      "QUANTILE LOSS:  tensor(871.2946, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37295.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(840.1625, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37323.0156, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(698.2690, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37351.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(618.6893, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37363.5898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(575.8453, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37398.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(562.6801, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37443.2773, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1607.5269, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37418.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1238.7817, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37441.1836, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1037.8853, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37464.6602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1109.6144, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37488.3867, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 150, Loss: 1109.6144\n",
      "QUANTILE LOSS:  tensor(1042.6595, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37511.2461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(861.9485, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37541.0664, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(864.1697, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37550.8945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(891.2814, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37579.7305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(794.9313, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37602.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(813.7548, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37616.3086, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(867.0992, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37645.8398, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(842.9207, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37680.3086, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1353.4564, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37672.5352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1045.3610, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37698.3711, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 160, Loss: 1045.3610\n",
      "QUANTILE LOSS:  tensor(836.8140, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37741.7266, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(793.6081, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37740.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(877.7687, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37781.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(829.1846, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37806.3516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1122.0974, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37800.2852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(980.2516, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37840.1875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(801.6074, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37870.1484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(802.6550, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37868.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.9829, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37908.0977, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(773.3596, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37933.8320, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 170, Loss: 773.3596\n",
      "QUANTILE LOSS:  tensor(1036.5709, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37927.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(918.1511, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37968.5234, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(742.8000, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37999.8711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(785.9822, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(37996.8789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(841.0220, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38034.3477, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(700.4412, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38063.6289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(741.9868, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38061.6445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.5190, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38106.3398, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(720.9766, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38128.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1014.9778, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38118.6445, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 180, Loss: 1014.9778\n",
      "QUANTILE LOSS:  tensor(864.8514, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38164.7773, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(733.0950, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38190.6602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(725.4333, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38198.8945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(773.8030, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38240.4766, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(722.9771, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38252.6484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(698.6584, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38274.6055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.7427, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38309.7852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(749.6678, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38317.0898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(781.2402, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38335.0664, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(752.7183, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38368.7109, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 190, Loss: 752.7183\n",
      "QUANTILE LOSS:  tensor(678.7021, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38386.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(677.0729, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38424.0234, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(721.2944, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38433.8516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(671.0352, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38450.8789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(655.7152, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38489.8789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(715.0447, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38498.9805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(634.0874, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38496.4961, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(620.8286, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38558.1680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(691.0495, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38557.6680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(655.5384, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38556.6758, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 200, Loss: 655.5384\n",
      "QUANTILE LOSS:  tensor(818.3560, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38603.8555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(785.2239, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38606.3555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(686.7807, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38620.0234, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(678.6387, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38680.2109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(747.9009, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38667.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(645.8704, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38800.5820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(696.8628, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38739.6055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(731.8052, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38726.5117, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(682.0441, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38816.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(739.8470, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38795.2930, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 210, Loss: 739.8470\n",
      "QUANTILE LOSS:  tensor(762.3642, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38789.4102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(693.5964, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38882.3125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(639.0251, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38847.9336, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(829.6902, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38848.8086, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.7295, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38875.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(882.2554, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38915.7031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(842.5236, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38914.1406, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(860.6044, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38940.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(962.4927, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38971.8320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(976.0256, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(38991.5039, grad_fn=<MeanBackward0>)\n",
      "Epoch 16/25, Batch 220, Loss: 976.0256\n",
      "QUANTILE LOSS:  tensor(966.2714, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39004.8633, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(995.0042, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39020.5625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(669.9630, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39043.4805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(653.2241, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39064.2148, grad_fn=<MeanBackward0>)\n",
      "Epoch 16, Avg Loss: 1015.1838\n",
      "QUANTILE LOSS:  tensor(11492.2295, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39081.4141, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 0, Loss: 11492.2295\n",
      "QUANTILE LOSS:  tensor(10074.5039, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39103.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8115.8979, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39119.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6048.4180, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39140.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5931.3608, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39163.7461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1276.5444, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39208.9727, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(620.8953, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39272.9805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(670.9168, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39239.3164, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(666.8254, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39291.7852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(738.6314, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39356.8164, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1048.4161, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39310.3789, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 10, Loss: 1048.4161\n",
      "QUANTILE LOSS:  tensor(635.4587, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39377.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(419.8804, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39449.2148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(476.6635, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39379.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(643.0247, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39437.7461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(384.8377, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39530.3789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1416.3232, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39430.8789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1242.4799, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39436.2305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(824.2141, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39463.2266, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(839.7009, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39477.2305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(795.4219, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39501.5273, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 20, Loss: 795.4219\n",
      "QUANTILE LOSS:  tensor(1403.6595, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39521.5039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1170.5271, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39536.6836, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1021.0513, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39560.9336, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(744.7435, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39592.0898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(793.0999, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39605.3945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(780.2258, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39636.7773, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(584.5897, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39670.5820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(704.0444, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39677.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(699.7181, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39707.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(516.5015, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39737.2461, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 30, Loss: 516.5015\n",
      "QUANTILE LOSS:  tensor(641.2634, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39736.0586, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(684.7744, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39769., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(491.6887, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39794.1445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(617.6933, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39798.5195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(632.2894, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39831.2539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(700.6536, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39850.1602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(679.8154, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39861.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(687.5772, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39896.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(516.3702, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39920.2773, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(669.0115, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39924.4766, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 40, Loss: 669.0115\n",
      "QUANTILE LOSS:  tensor(669.5151, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39957.4766, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(614.6862, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39979.0039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(814.7429, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(39983.8789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(684.5040, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40022.2852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(490.5713, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40045.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(654.5872, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40049.6680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(682.9070, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40084.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(684.1910, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40113.9258, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1222.3195, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40107.7930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.7270, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40151.7461, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 50, Loss: 751.7270\n",
      "QUANTILE LOSS:  tensor(1294.1017, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40148.1875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(803.5685, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40177.8164, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(665.3338, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40225.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(326.7392, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40219.1992, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(660.8057, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40236.7461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(679.0569, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40258.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2430.7085, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40270.5469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1223.4171, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40291.0352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(839.7888, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40318.1016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(457.0244, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40336.7695, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 60, Loss: 457.0244\n",
      "QUANTILE LOSS:  tensor(711.4731, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40357.2383, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(782.7627, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40377.7695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(319.9024, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40401.4766, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(683.9844, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40416.2734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.8228, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40436.2656, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1291.2841, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40453.1719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(868.7673, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40473.0234, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(846.1901, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40493.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1730.4554, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40506.4336, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1225.7797, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40528.5352, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 70, Loss: 1225.7797\n",
      "QUANTILE LOSS:  tensor(931.5852, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40552.3789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(810.4197, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40569.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(912.4368, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40593.8711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.8423, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40618.0352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1451.0665, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40630.0234, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1073.6285, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40654.0195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(898.1272, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40676.1719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(651.7776, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40695.6602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(893.3511, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40719.7539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(749.7554, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40742.2305, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 80, Loss: 749.7554\n",
      "QUANTILE LOSS:  tensor(428.9529, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40764.9570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(814.2497, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40784.4805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(787.4258, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40803.1719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1534.4965, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40819.0039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1147.8596, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40842.1289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(939.1255, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40865.2305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1043.0809, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40882.9180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(819.6538, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40911.1875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(807.2065, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40931.9023, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1404.2643, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40946.0586, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 90, Loss: 1404.2643\n",
      "QUANTILE LOSS:  tensor(1079.8060, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40970.2734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(896.8002, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(40993.1484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(835.1518, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41012.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(826.5851, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41039.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(807.6226, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41058.7852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1555.1855, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41075.7031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1123.5470, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41099.7461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(907.2085, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41123.5586, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.6059, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41142.7227, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(834.5467, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41170.0625, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 100, Loss: 834.5467\n",
      "QUANTILE LOSS:  tensor(762.2717, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41191.5898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(384.1327, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41216.8945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(702.8021, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41235.6484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.9927, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41258.2852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1478.0479, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41269.3555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1066.5306, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41293.3320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(847.8244, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41323.8789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(731.5593, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41335.9727, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.4821, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41361.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(844.2068, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41387.5430, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 110, Loss: 844.2068\n",
      "QUANTILE LOSS:  tensor(1346.0424, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41396.8516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1063.4827, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41422.2070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(915.6140, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41451.8281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(865.2574, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41463.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.8144, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41493.4805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(678.4126, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41520.1094, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(982.7963, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41533.9102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1091.3418, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41550.2109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(889.1384, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41577.5664, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1058.5500, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41591.4961, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 120, Loss: 1058.5500\n",
      "QUANTILE LOSS:  tensor(891.6997, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41616.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.2299, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41646.1289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(612.9016, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41660.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(621.9022, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41691.7930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(617.6234, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41716.6680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1252.8990, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41719.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(990.5200, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41745.3125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(819.7112, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41773.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(958.2879, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41786.6914, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.3631, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41814.0625, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 130, Loss: 831.3631\n",
      "QUANTILE LOSS:  tensor(816.7290, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41841.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1098.4170, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41850.8984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(968.6428, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41876.1836, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(796.8260, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41907.7461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.9102, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41918.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(826.0623, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41947.8516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(682.4736, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41978.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(951.0148, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(41987.7695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(987.2053, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42008.8516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(801.7812, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42045.5820, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 140, Loss: 801.7812\n",
      "QUANTILE LOSS:  tensor(865.3226, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42052.1719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(834.1905, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42081.4219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(692.2971, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42112.1016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(612.7173, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42124.4883, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(569.8735, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42161.1289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(557.0679, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42209.0586, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1601.5551, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42182.9141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1232.8098, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42206.8594, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1031.9133, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42231.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1103.8502, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42257., grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 150, Loss: 1103.8502\n",
      "QUANTILE LOSS:  tensor(1036.6877, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42281.2031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.9767, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42312.8164, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(858.1978, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42323.2461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(885.3098, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42353.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(788.9595, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42377.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(807.7830, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42392.6016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(861.1275, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42423.9453, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(836.9490, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42460.5234, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1347.4850, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42452.2539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1039.3893, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42479.6875, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 160, Loss: 1039.3893\n",
      "QUANTILE LOSS:  tensor(830.8425, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42525.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(787.6364, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42524.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(871.7971, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42568.4102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(823.2131, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42594.2461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1116.1257, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42587.8789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(974.2800, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42630.2070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(795.6357, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42661.9688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(796.6833, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42660.3398, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(881.0112, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42702.3047, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(767.3879, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42729.5820, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 170, Loss: 767.3879\n",
      "QUANTILE LOSS:  tensor(1030.5991, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42722.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(912.1794, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42766.4648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(736.8282, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42799.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(780.0107, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42796.5820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(835.0502, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42836.3711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(694.4695, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42867.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(736.0150, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42865.3555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.5472, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42912.7930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(715.0049, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42935.8164, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1009.0060, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42925.8945, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 180, Loss: 1009.0060\n",
      "QUANTILE LOSS:  tensor(858.8796, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(42974.8711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(727.1230, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43002.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(719.4615, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43011.0664, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(767.8311, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43055.2070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(717.0052, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43068.1016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(692.6865, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43091.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(739.7707, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43128.7539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(743.6959, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43136.5156, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(775.2682, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43155.6680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(746.7463, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43191.3164, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 190, Loss: 746.7463\n",
      "QUANTILE LOSS:  tensor(672.7301, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43209.7734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(671.1008, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43250.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(715.3223, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43260.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(665.0632, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43278.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(649.7431, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43319.9727, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(709.0726, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43329.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(628.1152, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43326.9570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(614.8566, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43392.4766, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(685.0773, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43391.9180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(649.5662, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43390.8359, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 200, Loss: 649.5662\n",
      "QUANTILE LOSS:  tensor(812.3838, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43440.9805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.2516, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43443.5820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(680.8084, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43458.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(672.6664, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43522.0039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(741.9286, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43508.7930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(639.8981, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43649.6602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(690.8904, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43585.0430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(725.8329, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43571.1211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(676.0717, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43666.7656, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(733.8746, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43644.1445, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 210, Loss: 733.8746\n",
      "QUANTILE LOSS:  tensor(756.3917, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43637.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(687.6240, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43736.4336, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(633.0527, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43700.0039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(823.7177, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43700.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.7570, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43729.3477, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(876.2829, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43771.9805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(836.5510, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43770.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(854.6320, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43798.1875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(956.5200, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43831.5781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(970.0530, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43852.3945, grad_fn=<MeanBackward0>)\n",
      "Epoch 17/25, Batch 220, Loss: 970.0530\n",
      "QUANTILE LOSS:  tensor(960.2986, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43866.5469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(989.0314, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43883.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(663.9903, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43907.5195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(647.2515, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43929.5469, grad_fn=<MeanBackward0>)\n",
      "Epoch 17, Avg Loss: 1009.3386\n",
      "QUANTILE LOSS:  tensor(11486.2568, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43947.9102, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 0, Loss: 11486.2568\n",
      "QUANTILE LOSS:  tensor(10068.7764, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43971.0898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8109.9253, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(43988.2031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6042.4448, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44010.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5925.3882, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44035.2852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1270.6915, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44083.1602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(614.9492, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44151.0430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(664.9441, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44115.4102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(660.8526, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44170.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(732.6588, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44239.8711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1042.4435, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44190.6719, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 10, Loss: 1042.4435\n",
      "QUANTILE LOSS:  tensor(629.4860, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44261.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(415.6307, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44337.7461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(472.2885, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44263.4492, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(637.0522, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44325.4375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(381.0456, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44423.5469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1411.3104, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44317.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1236.5082, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44323.4180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(818.2424, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44351.8477, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(833.7293, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44366.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(789.4506, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44392.1680, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 20, Loss: 789.4506\n",
      "QUANTILE LOSS:  tensor(1397.6885, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44413.2227, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1164.5562, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44429.2031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1015.0805, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44454.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(738.7727, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44487.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(787.1293, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44501.7539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(774.2553, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44534.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(578.8171, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44570.7305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(698.0742, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44578.0156, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(693.7477, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44609.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(510.9254, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44641.1289, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 30, Loss: 510.9254\n",
      "QUANTILE LOSS:  tensor(635.2934, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44639.7188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(678.8046, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44674.4688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(487.1899, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44701.0039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(611.9334, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44705.4180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(626.3203, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44739.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(694.6846, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44759.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(673.8467, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44771.8281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(681.6088, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44808.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(510.6236, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44833.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(663.0435, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44837.7305, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 40, Loss: 663.0435\n",
      "QUANTILE LOSS:  tensor(663.5474, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44872.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(609.6135, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44895.1211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(808.7756, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44900.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(678.5371, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44940.4961, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(485.9033, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44964.4375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(648.7405, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(44968.9023, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(676.9413, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45005.8711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(678.8231, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45036.2539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1216.3545, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45029.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.7624, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45075.7305, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 50, Loss: 745.7624\n",
      "QUANTILE LOSS:  tensor(1288.1373, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45071.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(797.6043, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45102.9102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(659.4894, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45153.1602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(324.5654, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45146.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(655.2004, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45164.7695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(673.2944, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45187.7773, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2424.8647, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45200.2031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1217.4545, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45221.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(833.8262, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45250.2148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(453.8058, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45269.8477, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 60, Loss: 453.8058\n",
      "QUANTILE LOSS:  tensor(705.7495, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45291.3945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(776.8007, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45313.0195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(317.6378, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45338.0352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(678.2612, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45353.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.8611, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45374.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1285.3225, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45392.5625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(862.8057, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45413.5039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(840.2287, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45435.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1724.4941, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45448.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1219.8184, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45472.1680, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 70, Loss: 1219.8184\n",
      "QUANTILE LOSS:  tensor(925.6240, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45497.3945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(804.4585, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45515.2852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(906.4756, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45541.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.8811, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45566.8555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1445.1051, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45579.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1067.6674, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45604.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(892.1662, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45628.4023, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(646.3237, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45649.0352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(887.3900, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45674.5039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(743.8975, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45698.2500, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 80, Loss: 743.8975\n",
      "QUANTILE LOSS:  tensor(425.4484, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45722.2305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(808.2890, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45742.6875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(781.4655, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45762.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1528.5361, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45778.8906, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1141.8997, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45803.2031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(933.1656, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45827.5195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1037.1215, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45846.1289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(813.6943, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45875.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(801.2471, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45897.7930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1398.3051, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45912.7070, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 90, Loss: 1398.3051\n",
      "QUANTILE LOSS:  tensor(1073.8470, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45938.2656, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(890.8411, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45962.4180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(829.1927, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(45982.3945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(820.6260, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46011.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(801.6636, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46031.7539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1549.2266, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46049.6289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1117.5881, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46075.0898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(901.2498, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46100.2461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.7915, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46120.5352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(828.5880, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46149.3711, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 100, Loss: 828.5880\n",
      "QUANTILE LOSS:  tensor(756.3130, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46172.0469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(381.3672, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46198.7539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(696.8438, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46218.4219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(832.0347, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46242.2305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1472.0900, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46253.8281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1060.5729, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46279.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(841.8668, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46311.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.3159, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46323.9961, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(819.5246, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46350.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(838.3537, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46378.4375, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 110, Loss: 838.3537\n",
      "QUANTILE LOSS:  tensor(1340.0853, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46388.1914, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1057.5256, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46414.9180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(909.6570, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46446.1680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(859.3187, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46458.5820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(720.8577, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46490.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(672.5749, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46518.1445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(978.4363, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46532.6680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1085.3854, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46549.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(883.1821, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46578.6406, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1052.5939, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46593.2852, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 120, Loss: 1052.5939\n",
      "QUANTILE LOSS:  tensor(885.7437, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46619.6875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.2741, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46650.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(607.5054, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46665.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(615.9465, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46698.9961, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(611.9057, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46725.1289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1246.9438, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46728.2031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(984.5651, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46755.1445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(813.7563, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46785.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(952.3333, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46798.6914, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.4085, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46827.5625, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 130, Loss: 825.4085\n",
      "QUANTILE LOSS:  tensor(810.7745, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46856.2227, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1092.4625, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46866.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(962.6885, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46893.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.8718, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46926.3789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.9561, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46938.2461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(820.1082, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(46968.7344, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(676.5195, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47000.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(945.0607, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47010.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(981.2513, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47033.1914, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(795.8272, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47072.0352, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 140, Loss: 795.8272\n",
      "QUANTILE LOSS:  tensor(859.3687, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47079., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(828.2366, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47109.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(686.3430, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47142.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(606.7635, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47155.4805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(563.9194, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47194.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(551.4702, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47244.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1595.6011, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47217.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1226.8558, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47242.6523, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1025.9594, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47269.0273, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1098.1339, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47295.7031, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 150, Loss: 1098.1339\n",
      "QUANTILE LOSS:  tensor(1030.7338, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47321.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(850.0227, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47354.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(852.2439, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47365.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(879.3557, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47398.1289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(783.0056, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47423.1602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(801.8290, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47439.2070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.1734, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47472.3711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(830.9949, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47511.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1341.5309, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47502.3711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1033.4352, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47531.3945, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 160, Loss: 1033.4352\n",
      "QUANTILE LOSS:  tensor(824.8882, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47580.0430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(781.6823, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47579.0664, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(865.8430, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47625.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(817.2588, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47652.5820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1110.1715, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47645.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(968.3259, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47690.7031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(789.6815, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47724.2539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.7290, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47722.6094, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(875.0569, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47767.0195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(761.4335, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47795.8281, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 170, Loss: 761.4335\n",
      "QUANTILE LOSS:  tensor(1024.6449, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47788.6289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(906.2250, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47834.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(730.8738, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47870.0469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(774.0562, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47866.8555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(829.0957, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47908.9102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(688.5150, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47941.7188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(730.0605, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47939.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.5927, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(47989.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(709.0503, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48014.1289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1003.0515, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48003.7070, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 180, Loss: 1003.0515\n",
      "QUANTILE LOSS:  tensor(852.9249, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48055.5195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(721.1685, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48084.4805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(713.5068, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48093.8281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(761.8765, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48140.5039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(711.0505, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48154.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(686.7318, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48178.8945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(733.8159, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48218.3164, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(737.7410, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48226.5039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.3134, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48246.8477, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(740.7915, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48284.5039, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 190, Loss: 740.7915\n",
      "QUANTILE LOSS:  tensor(666.7753, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48304.0195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(665.1459, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48346.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(709.3675, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48357.6680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(659.1082, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48376.7930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(643.7881, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48420.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(703.1176, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48430.8477, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(622.1602, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48428.0195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(608.9015, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48497.4023, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(679.1223, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48496.7656, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(643.6111, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48495.6250, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 200, Loss: 643.6111\n",
      "QUANTILE LOSS:  tensor(806.4286, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48548.7461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(773.2966, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48551.4375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(674.8532, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48566.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(666.7112, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48634.4180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(735.9734, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48620.4180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(633.9429, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48769.3320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(684.9351, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48701.1211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(719.8776, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48686.3711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(670.1164, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48787.4648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(727.9193, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48763.6445, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 210, Loss: 727.9193\n",
      "QUANTILE LOSS:  tensor(750.4363, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48757., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(681.6685, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48861.1836, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(627.0972, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48822.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(817.7622, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48823.7031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.8015, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48853.7656, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(870.3274, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48898.9180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(830.5955, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48897.0469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(848.6765, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48926.5781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(950.5644, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48961.9805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(964.0974, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48983.9531, grad_fn=<MeanBackward0>)\n",
      "Epoch 18/25, Batch 220, Loss: 964.0974\n",
      "QUANTILE LOSS:  tensor(954.3432, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(48998.9023, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(983.0759, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49016.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(658.0346, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49042.2461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(641.2958, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49065.5234, grad_fn=<MeanBackward0>)\n",
      "Epoch 18, Avg Loss: 1003.5304\n",
      "QUANTILE LOSS:  tensor(11480.3008, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49085.1055, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 0, Loss: 11480.3008\n",
      "QUANTILE LOSS:  tensor(10063.0674, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49109.6055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8103.9688, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49127.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6036.4888, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49151.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5919.4316, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49177.5156, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1264.8547, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49228.0352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(609.3242, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49299.7539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(658.9884, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49262.0430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(654.8972, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49320.4688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.7409, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49393.1602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1036.4884, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49341.1094, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 10, Loss: 1036.4884\n",
      "QUANTILE LOSS:  tensor(623.5311, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49415.6289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(411.4587, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49496.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(468.0236, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49417.5820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(631.0978, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49482.8281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(377.4677, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49586.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1406.3068, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49474.7305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1230.5541, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49480.4102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(812.2885, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49510.3320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(827.7757, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49525.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(783.4970, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49552.7852, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 20, Loss: 783.4970\n",
      "QUANTILE LOSS:  tensor(1391.7349, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49574.9648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1158.6027, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49591.7930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1009.1270, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49618.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(732.8193, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49653.5352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(781.1760, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49668.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(768.3018, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49703.3555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(573.1181, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49741.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(692.1208, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49748.7461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(687.7946, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49781.7031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(505.6067, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49815.2969, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 30, Loss: 505.6067\n",
      "QUANTILE LOSS:  tensor(629.3403, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49813.7227, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(672.8517, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49850.3555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(482.9460, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49878.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(606.3230, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49882.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(620.3679, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49918.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(688.7326, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49939.6875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(667.8948, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49952.2969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(675.6571, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(49991.0195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(505.1410, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50017.0898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(657.0923, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50021.2773, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 40, Loss: 657.0923\n",
      "QUANTILE LOSS:  tensor(657.5964, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50057.7461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(604.6971, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50081.4062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(802.8255, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50086.2305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(672.5873, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50128.5469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(481.7751, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50153.5273, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(642.9457, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50157.7461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(670.9931, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50196.2656, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(673.4674, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50227.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1210.4072, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50220.3711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(739.8154, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50268.8320, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 50, Loss: 739.8154\n",
      "QUANTILE LOSS:  tensor(1282.1908, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50264.3711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(791.6581, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50296.9648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(653.6618, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50349.7695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(322.4049, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50342.3945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(649.6097, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50361.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(667.5856, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50385.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2419.0378, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50398.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1211.5096, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50421.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(827.8815, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50451.2461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(450.6881, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50471.8555, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 60, Loss: 450.6881\n",
      "QUANTILE LOSS:  tensor(700.1732, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50494.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(770.8566, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50517.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(315.3575, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50543.2656, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(672.5538, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50559.5469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.9177, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50581.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1279.3792, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50600.3398, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(856.8625, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50622.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(834.2856, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50645.2852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1718.5511, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50659.3789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1213.8754, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50683.9570, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 70, Loss: 1213.8754\n",
      "QUANTILE LOSS:  tensor(919.6812, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50710.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(798.5161, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50729.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(900.5331, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50756.6602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.9387, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50783.5625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1439.1630, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50796.8945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1061.7252, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50823.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.2239, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50848.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(641.1981, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50870.0352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(881.4482, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50896.6875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(738.0740, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50921.5352, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 80, Loss: 738.0740\n",
      "QUANTILE LOSS:  tensor(422.2535, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50946.6289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(802.3480, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50967.9570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(775.5248, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(50988.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1522.5957, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51005.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1135.9594, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51031.1445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(927.2256, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51056.6445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1031.1816, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51076.1289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(807.7545, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51107.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(795.3074, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51130.4023, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1392.3656, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51146.0352, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 90, Loss: 1392.3656\n",
      "QUANTILE LOSS:  tensor(1067.9075, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51172.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(884.9018, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51198.3320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(823.2534, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51219.3320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(814.6870, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51250.2070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(795.7244, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51271.3398, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1543.2876, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51290.1602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1111.6492, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51316.9805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(895.3107, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51343.4961, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(761.2158, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51364.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(822.6492, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51395.2070, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 100, Loss: 822.6492\n",
      "QUANTILE LOSS:  tensor(750.3743, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51419.0898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(378.7276, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51447.2188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(690.9052, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51467.8281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(826.0964, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51492.8164, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1466.1517, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51504.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1054.6346, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51531.4805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(835.9287, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51565.3789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(721.2500, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51578.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(813.5869, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51606.3906, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(832.5341, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51635.7031, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 110, Loss: 832.5341\n",
      "QUANTILE LOSS:  tensor(1334.1478, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51645.8398, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1051.5885, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51673.8867, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(903.7200, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51706.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(853.5861, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51719.7031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(714.9211, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51752.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(666.7562, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51782.2305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(974.1970, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51797.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1079.4492, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51815.3594, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(877.2463, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51845.6367, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1046.6581, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51860.9648, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 120, Loss: 1046.6581\n",
      "QUANTILE LOSS:  tensor(879.8079, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51888.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(760.3386, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51921.5820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(602.3690, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51937.0898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(610.0112, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51971.9961, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(606.2224, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(51999.4102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1241.0092, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52002.4219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(978.6304, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52030.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(807.8218, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52061.9961, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(946.3991, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52076.2070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(819.4744, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52106.4961, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 130, Loss: 819.4744\n",
      "QUANTILE LOSS:  tensor(804.8406, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52136.6055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1086.5288, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52147.2148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(956.7548, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52175.2539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.9382, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52210.2969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(869.0225, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52222.7305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(814.1747, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52254.8320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(670.5862, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52288.6602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(939.1274, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52299.2148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(975.3181, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52322.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(789.8940, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52363.5898, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 140, Loss: 789.8940\n",
      "QUANTILE LOSS:  tensor(853.4355, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52370.9180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(822.3035, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52403.5195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(680.4099, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52437.7031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(600.8304, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52451.5352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(558.0402, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52492.3945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(546.0604, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52545.7773, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1589.6680, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52516.5039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1220.9232, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52543.1211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1020.0268, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52570.7930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1092.4364, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52598.8008, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 150, Loss: 1092.4364\n",
      "QUANTILE LOSS:  tensor(1024.8014, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52625.7305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(844.0904, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52660.9102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(846.3116, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52672.5039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(873.4235, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52706.5586, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(777.0734, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52732.8711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(795.8970, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52749.7773, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.2415, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52784.6836, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.0630, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52825.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1335.5988, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52816.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1027.5033, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52846.8594, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 160, Loss: 1027.5033\n",
      "QUANTILE LOSS:  tensor(818.9564, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52898.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(775.7503, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52897.0898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(859.9109, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52945.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(811.3269, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52974.5469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1104.2396, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(52967.5977, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(962.3940, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53014.7695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(783.7497, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53050.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.7971, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53048.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(869.1250, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53095.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(755.5015, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53125.5938, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 170, Loss: 755.5015\n",
      "QUANTILE LOSS:  tensor(1018.7128, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53118.1289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(900.2930, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53166.9180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(724.9419, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53203.8945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(768.1242, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53200.5977, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(823.1638, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53244.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(682.5829, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53279.5039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(724.1284, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53277.3789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.6606, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53330.2852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(703.1182, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53355.8945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(997.1192, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53345., grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 180, Loss: 997.1192\n",
      "QUANTILE LOSS:  tensor(846.9929, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53399.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(715.2363, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53430.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(707.5746, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53440.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(755.9443, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53489.2539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(705.1182, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53503.5977, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(680.7995, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53529.7930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(727.8836, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53571.3320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(731.8088, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53579.9570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.3811, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53601.4961, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(734.8591, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53641.1602, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 190, Loss: 734.8591\n",
      "QUANTILE LOSS:  tensor(660.8429, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53661.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(659.2134, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53706.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(703.4350, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53718.3086, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(653.1756, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53738.4805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(637.8557, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53784.8438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(697.1850, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53795.5352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(616.2276, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53792.5195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(602.9689, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53865.7539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(673.1896, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53865.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(637.6784, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53863.8320, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 200, Loss: 637.6784\n",
      "QUANTILE LOSS:  tensor(800.4958, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53919.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(767.3638, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53922.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(668.9204, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53938.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(660.7784, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54010.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(730.0406, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(53995.4648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(628.0101, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54152.3945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(679.0023, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54080.6094, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(713.9446, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54065.0508, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(664.1834, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54171.6055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(721.9863, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54146.6055, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 210, Loss: 721.9863\n",
      "QUANTILE LOSS:  tensor(744.5034, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54139.5586, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(675.7355, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54249.3398, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(621.1642, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54208.8906, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(811.8292, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54209.9102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(868.8684, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54241.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(864.3942, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54289.3125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(824.6624, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54287.2852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(842.7432, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54318.4492, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(944.6312, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54355.8789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(958.1641, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54378.9727, grad_fn=<MeanBackward0>)\n",
      "Epoch 19/25, Batch 220, Loss: 958.1641\n",
      "QUANTILE LOSS:  tensor(948.4099, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54394.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(977.1424, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54413.4727, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(652.1013, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54440.4102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(635.3624, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54464.9766, grad_fn=<MeanBackward0>)\n",
      "Epoch 19, Avg Loss: 997.7615\n",
      "QUANTILE LOSS:  tensor(11474.3672, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54485.7852, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 0, Loss: 11474.3672\n",
      "QUANTILE LOSS:  tensor(10057.3779, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54511.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8098.0352, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54530.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6030.5552, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54556.1211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5913.4985, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54583.2539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1259.1508, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54636.3711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(604.1420, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54711.8867, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(653.0550, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54672.1602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(648.9639, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54733.4648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(720.9253, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54809.9570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1030.5553, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54755.1406, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 10, Loss: 1030.5553\n",
      "QUANTILE LOSS:  tensor(617.5981, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54833.4375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(407.5211, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54918.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(463.8910, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54835.3320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(625.1654, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54903.6289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(373.8853, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55012.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1401.3146, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54894.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1224.6226, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54900.5039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(806.3571, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54931.8398, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(821.8444, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54947.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(777.5661, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54976.2148, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 20, Loss: 777.5661\n",
      "QUANTILE LOSS:  tensor(1385.8041, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(54999.4375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1152.6720, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55017.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1003.1965, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55045.3789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.8887, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55081.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(775.2455, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55097.3789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(762.3716, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55134.2070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(567.6754, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55173.9062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(686.1907, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55181.7461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(681.8648, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55216.2539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(500.4663, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55251.4688, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 30, Loss: 500.4663\n",
      "QUANTILE LOSS:  tensor(623.4111, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55249.4648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(666.9229, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55287.7031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(478.7907, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55316.8555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(600.9902, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55321.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(614.4401, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55358.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(682.8433, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55380.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(661.9678, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55393.3594, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(669.7304, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55433.8281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(500.1307, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55461.0039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(651.1664, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55464.9961, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 40, Loss: 651.1664\n",
      "QUANTILE LOSS:  tensor(651.6709, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55503., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(600.1767, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55527.5898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(796.9006, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55532.3789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(666.6628, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55576.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(478.1360, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55602.7305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(637.2558, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55606.8711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(665.0694, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55647.1445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(668.1942, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55680.1914, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1204.4840, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55672.0039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(733.8926, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55722.7461, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 50, Loss: 733.8926\n",
      "QUANTILE LOSS:  tensor(1276.2682, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55717.8711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(785.7358, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55751.9648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(647.8565, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55807.3555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(320.2232, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55799.4844, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(644.0385, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55819.5781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(661.9534, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55844.8281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2413.2332, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55858.3711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1205.5881, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55881.9883, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(821.9602, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55913.3945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(447.6843, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55934.9961, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 60, Loss: 447.6843\n",
      "QUANTILE LOSS:  tensor(694.7195, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55958.6367, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(764.9357, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(55982.2773, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(313.0580, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56009.7539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(666.9324, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56026.7852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.9973, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56049.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1273.5441, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56069.4805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(850.9424, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56092.4258, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(828.3657, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56116.4375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1712.6313, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56131.1680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1207.9558, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56156.9102, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 70, Loss: 1207.9558\n",
      "QUANTILE LOSS:  tensor(913.7617, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56184.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(792.7687, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56204.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(894.6138, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56233.1445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(826.1240, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56261.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1433.2438, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56275.1680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1055.8064, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56303.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.3054, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56328.9648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(636.3298, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56351.6602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(875.5298, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56379.6055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(732.4045, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56405.6211, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 80, Loss: 732.4045\n",
      "QUANTILE LOSS:  tensor(419.1936, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56431.8438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(796.4302, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56454., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.6074, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56475.2305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1516.6786, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56493.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1130.0426, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56519.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(921.3088, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56546.4727, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1025.2650, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56566.8320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(801.8381, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56599.6406, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(789.3913, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56623.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1386.4493, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56640.0430, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 90, Loss: 1386.4493\n",
      "QUANTILE LOSS:  tensor(1061.9916, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56668.2227, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(878.9858, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56694.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(817.4567, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56716.9336, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(808.7712, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56749.2344, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(789.8089, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56771.3320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1537.3721, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56791.0039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1105.7338, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56819.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(889.3955, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56846.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(755.7957, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56869.3555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(816.7341, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56901.1445, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 100, Loss: 816.7341\n",
      "QUANTILE LOSS:  tensor(744.5103, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56926.1602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(376.3077, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56955.6055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(684.9908, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(56977.0430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(820.1821, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57003.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1460.2377, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57015.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1048.7208, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57043.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(830.0150, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57078.9492, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(716.6011, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57092.8555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(807.6738, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57121.6289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(826.7374, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57152.1836, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 110, Loss: 826.7374\n",
      "QUANTILE LOSS:  tensor(1328.2352, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57162.5898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1045.6763, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57191.8477, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(897.8079, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57226.1680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(847.9067, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57239.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(709.0093, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57274.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(660.9609, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57305.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(970.0293, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57321.0039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1073.5378, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57339.7539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(871.3350, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57371.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1040.7471, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57387.5625, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 120, Loss: 1040.7471\n",
      "QUANTILE LOSS:  tensor(873.8969, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57416.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(754.4275, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57451.1602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(597.4771, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57467.4375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(604.1004, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57504.0430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(600.7764, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57532.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1235.0983, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57535.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(972.7198, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57565.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(801.9114, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57598.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(940.4885, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57613.3789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(813.5640, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57645.2070, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 130, Loss: 813.5640\n",
      "QUANTILE LOSS:  tensor(798.9302, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57676.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1080.6183, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57687.9805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(950.8445, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57717.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.0278, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57754.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(863.1123, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57767.3711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(808.2643, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57801.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(664.6757, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57836.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(933.2171, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57847.8398, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(969.4075, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57872.4961, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(783.9837, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57915.5195, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 140, Loss: 783.9837\n",
      "QUANTILE LOSS:  tensor(847.5251, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57923.2852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(816.3930, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57957.5625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(674.4995, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(57993.5352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(594.9199, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58008.0820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(552.2459, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58051.0664, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(540.7308, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58107.2148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1583.7574, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58076.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1215.0126, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58104.5039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1014.1163, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58133.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1086.7582, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58163.1250, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 150, Loss: 1086.7582\n",
      "QUANTILE LOSS:  tensor(1018.8908, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58191.4492, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(838.1799, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58228.4375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(840.4009, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58240.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(867.5129, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58276.5195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(771.1626, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58304.1992, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(789.9861, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58322.0117, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(843.3306, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58358.7695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(819.1521, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58401.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1329.6880, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58392.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1021.5923, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58424.2383, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 160, Loss: 1021.5923\n",
      "QUANTILE LOSS:  tensor(813.0454, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58478.1211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.8394, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58477.1445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(853.9998, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58528.3555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(805.4158, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58558.5469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1098.3285, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58551.3555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(956.4827, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58600.9805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(777.8384, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58638.0898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.8857, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58636.4688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(863.2136, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58685.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(749.5903, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58717.5469, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 170, Loss: 749.5903\n",
      "QUANTILE LOSS:  tensor(1012.8015, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58709.8242, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(894.3817, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58761.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(719.0305, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58800., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(762.2127, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58796.6289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(817.2522, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58843.2852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(676.6714, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58879.5508, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(718.2168, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58877.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.7490, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58933.0820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(697.2065, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58959.9688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(991.2077, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(58948.6562, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 180, Loss: 991.2077\n",
      "QUANTILE LOSS:  tensor(841.0810, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59006.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(709.3245, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59038.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(701.6628, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59048.6602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(750.0324, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59100.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(699.2063, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59115.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(674.8875, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59143.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(721.9716, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59186.7148, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(725.8967, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59195.7539, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.4690, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59218.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(728.9470, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59260.1875, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 190, Loss: 728.9470\n",
      "QUANTILE LOSS:  tensor(654.9307, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59281.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(653.3013, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59329.3242, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(697.5228, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59341.3789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(647.2634, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59362.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(631.9434, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59411.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(691.2727, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59422.6445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(610.3152, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59419.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(597.0565, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59496.5898, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(667.2772, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59495.8242, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(631.7930, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59494.5195, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 200, Loss: 631.7930\n",
      "QUANTILE LOSS:  tensor(794.5835, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59553.5820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(761.4514, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59556.3945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(663.0080, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59573.3320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(654.8660, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59648.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(724.1281, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59632.8281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(622.0977, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59797.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(673.0898, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59722.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(708.0322, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59705.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(658.2710, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59817.9258, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(716.0738, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59791.7617, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 210, Loss: 716.0738\n",
      "QUANTILE LOSS:  tensor(738.5909, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59784.2969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(669.8229, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59899.6758, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(615.2516, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59857.1914, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(805.9165, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59858.2852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(862.9559, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59891.6289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(858.4816, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59941.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(818.7497, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59939.6680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(836.8306, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(59972.4180, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(938.7186, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60011.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(952.2514, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60036.0820, grad_fn=<MeanBackward0>)\n",
      "Epoch 20/25, Batch 220, Loss: 952.2514\n",
      "QUANTILE LOSS:  tensor(942.4971, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60052.6602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(971.2297, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60072.4219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(646.1885, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60100.7188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(629.4496, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60126.5391, grad_fn=<MeanBackward0>)\n",
      "Epoch 20, Avg Loss: 992.0402\n",
      "QUANTILE LOSS:  tensor(11468.4541, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60148.6289, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 0, Loss: 11468.4541\n",
      "QUANTILE LOSS:  tensor(10051.7070, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60175.8281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8092.1230, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60195.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6024.6426, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60222.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5907.5845, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60251.1680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1253.4703, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60306.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(599.1888, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60386.2070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(647.1420, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60344.5469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(643.0510, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60408.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(715.3933, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60489.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1024.6427, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60431.3867, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 10, Loss: 1024.6427\n",
      "QUANTILE LOSS:  tensor(611.6857, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60513.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(403.7023, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60602.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(460.0621, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60515.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(619.2536, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60586.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(370.2990, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60700.5820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1396.3330, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60577., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1218.7111, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60582.9648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(800.4458, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60615.7383, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(815.9333, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60632.5625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(771.6548, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60662.1602, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 20, Loss: 771.6548\n",
      "QUANTILE LOSS:  tensor(1379.8931, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60686.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1146.7609, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60704.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(997.2856, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60734.6406, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(720.9780, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60772.9102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(769.3347, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60789.1602, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(756.4608, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60827.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(562.6347, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60869.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(680.2802, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60877.5195, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(675.9544, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60913.6055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(495.7178, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60950.4570, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 30, Loss: 495.7178\n",
      "QUANTILE LOSS:  tensor(617.7805, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60948.2305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(661.0131, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(60988.0820, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(475.0090, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61018.4648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(595.7823, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61022.6758, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(608.5314, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61061.7695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(677.0513, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61083.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(656.0600, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61097.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(663.8229, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61139.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(495.4963, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61167.5625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(645.2597, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61171.3398, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 40, Loss: 645.2597\n",
      "QUANTILE LOSS:  tensor(645.7645, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61210.8555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(595.6893, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61236.3555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.9951, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61240.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(660.7576, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61287.1055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(474.5937, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61314.1367, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(631.5829, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61318.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(659.1652, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61360.1211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(662.9959, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61394.5352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1198.5804, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61385.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(727.9892, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61438.7305, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 50, Loss: 727.9892\n",
      "QUANTILE LOSS:  tensor(1270.3651, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61433.4492, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.8328, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61469.0742, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(642.0693, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61527.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(318.0231, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61518.7031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(638.4831, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61539.6992, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(656.3980, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61566.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2407.4465, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61580.2305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1199.6859, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61604.9648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(816.0579, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61637.8398, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(444.6740, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61660.4688, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 60, Loss: 444.6740\n",
      "QUANTILE LOSS:  tensor(689.2801, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61685.2461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(759.0337, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61710.0352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(310.7416, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61738.8242, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(661.3774, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61756.6758, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(869.0953, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61780.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1267.7578, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61801.4688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(845.0405, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61825.5742, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(822.4638, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61850.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1706.7294, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61866.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1202.0540, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61893.2500, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 70, Loss: 1202.0540\n",
      "QUANTILE LOSS:  tensor(907.8597, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61922.4805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(787.0982, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61943.2383, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(888.7119, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(61973.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(820.4533, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62002.9102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1427.3419, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62017.4492, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1049.9044, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62046.8242, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.4035, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62073.9648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(631.6092, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62097.8398, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(869.6280, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62127.0117, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.8498, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62154.2031, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 80, Loss: 726.8498\n",
      "QUANTILE LOSS:  tensor(416.2986, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62181.6211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.5288, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62204.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.7059, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62226.9570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1510.7772, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62245.7695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1124.1412, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62273.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(915.4075, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62301.5312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1019.4044, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62322.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(795.9370, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62357.2305, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(783.4902, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62382.3438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1380.5485, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62399.4805, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 90, Loss: 1380.5485\n",
      "QUANTILE LOSS:  tensor(1056.0907, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62428.9883, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(873.0850, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62456.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(811.7873, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62480.0430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(802.8705, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62513.9062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(783.9084, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62537.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1531.4716, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62557.7070, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1099.8331, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62587.2031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(883.4949, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62616.3711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(750.4730, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62639.9102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(810.8336, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62673.2461, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 100, Loss: 810.8336\n",
      "QUANTILE LOSS:  tensor(738.7253, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62699.5352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(374.0626, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62730.4102, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(679.1033, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62752.8789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(814.2816, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62780.1289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1454.3373, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62793.2852, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1042.8207, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62822.2969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(824.1147, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62859.4648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(712.2278, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62874.0352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(801.7737, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62904.1367, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(820.9531, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62936.0938, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 110, Loss: 820.9531\n",
      "QUANTILE LOSS:  tensor(1322.3353, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62946.9570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1039.7764, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(62977.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(891.9082, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63013.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(842.2524, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63027.6992, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(703.1096, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63063.8867, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(655.1928, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63096.1289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(965.8625, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63112.6875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1067.6388, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63132.1758, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(865.4360, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63165.3438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1034.8480, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63182.1133, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 120, Loss: 1034.8480\n",
      "QUANTILE LOSS:  tensor(867.9980, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63212.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(748.5287, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63248.6133, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(592.6293, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63265.6133, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(598.2842, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63303.8789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(595.3398, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63333.8945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1229.2001, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63337.0312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(966.8218, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63367.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(796.0134, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63402.3711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(934.5906, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63417.9648, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(807.6662, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63451.2500, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 130, Loss: 807.6662\n",
      "QUANTILE LOSS:  tensor(793.0325, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63484.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1074.7207, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63495.9961, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(944.9468, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63526.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(773.1302, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63565.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(857.2148, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63579.1719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(802.3668, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63614.5625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(658.7784, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63651.8281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(927.3196, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63663.5117, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(963.5103, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63689.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.0862, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63734.5000, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 140, Loss: 778.0862\n",
      "QUANTILE LOSS:  tensor(841.6276, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63742.6680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(810.4956, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63778.6133, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(668.6021, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63816.3320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(589.0224, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63831.6289, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(546.4638, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63876.7461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(535.4255, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63935.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1577.8602, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63903.4062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1209.1151, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63932.7461, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1008.2188, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63963.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1081.0916, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(63994.1875, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 150, Loss: 1081.0916\n",
      "QUANTILE LOSS:  tensor(1012.9933, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64023.8789, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(832.2823, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64062.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(834.5036, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64075.5352, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(861.6155, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64113.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(765.2653, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64142.1133, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.0889, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64160.8555, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(837.4332, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64199.3945, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(813.2547, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64244.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1323.7904, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64234.3594, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1015.6949, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64268.1211, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 160, Loss: 1015.6949\n",
      "QUANTILE LOSS:  tensor(807.1479, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64324.6133, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.9418, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64323.6445, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(848.1024, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64377.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(799.5183, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64408.9570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1092.4309, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64401.5625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(950.5850, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64453.6055, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(771.9407, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64492.4961, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.9881, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64490.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(857.3160, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64542.5430, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(743.6926, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64575.8750, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 170, Loss: 743.6926\n",
      "QUANTILE LOSS:  tensor(1006.9037, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64567.9375, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(888.4839, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64621.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(713.1326, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64662.4883, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(756.3149, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64659.0742, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(811.3543, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64708., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(670.7735, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64746.0312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(712.3189, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64743.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(760.8510, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64802.2695, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(691.3085, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64830.4570, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(985.3094, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64818.6875, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 180, Loss: 985.3094\n",
      "QUANTILE LOSS:  tensor(835.1829, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64878.9258, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(703.4263, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64912.4805, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(695.7646, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64923.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(744.1341, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64977.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(693.3081, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(64993.6992, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(668.9892, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65022.7812, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(716.0732, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65068.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(719.9983, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65077.9961, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.5706, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65102.0312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(723.0485, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65145.6367, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 190, Loss: 723.0485\n",
      "QUANTILE LOSS:  tensor(649.0322, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65168.2930, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(647.4028, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65218.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(691.6241, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65230.8711, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(641.3648, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65253.1211, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(626.0447, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65304.4883, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(685.3740, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65316.1680, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(604.4165, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65312.8242, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(591.1577, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65393.8320, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(661.3784, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65393.0039, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(626.0096, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65391.6562, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 200, Loss: 626.0096\n",
      "QUANTILE LOSS:  tensor(788.6846, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65453.6875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(755.5524, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65456.5781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(657.1091, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65474.3398, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(648.9670, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65553.2109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(718.2290, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65536.8047, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(616.1985, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65709.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(667.1907, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65630.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(702.1331, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65613.5703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(652.3718, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65730.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(710.1746, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65703.6875, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 210, Loss: 710.1746\n",
      "QUANTILE LOSS:  tensor(732.6916, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65695.8359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(663.9236, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65816.7578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(609.3522, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65772.3438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(800.0173, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65773.4766, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(857.0564, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65808.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(852.5822, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65861.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(812.8502, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65858.8828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(830.9310, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65893.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(932.8188, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65934.8359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(946.3518, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65960.1328, grad_fn=<MeanBackward0>)\n",
      "Epoch 21/25, Batch 220, Loss: 946.3518\n",
      "QUANTILE LOSS:  tensor(936.5974, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65977.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(965.3299, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(65998.3203, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(640.2888, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66027.9141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(623.5497, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66055.0469, grad_fn=<MeanBackward0>)\n",
      "Epoch 21, Avg Loss: 986.3543\n",
      "QUANTILE LOSS:  tensor(11462.5547, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66078.4531, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 0, Loss: 11462.5547\n",
      "QUANTILE LOSS:  tensor(10046.0508, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66106.9688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8086.2227, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66128.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6018.7427, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66156.1016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5901.6855, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66186.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1247.8867, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66244.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(594.3550, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66327.4453, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(641.2938, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66283.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(637.1511, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66350.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(710.0821, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66434.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1018.7434, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66374.3359, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 10, Loss: 1018.7434\n",
      "QUANTILE LOSS:  tensor(605.7865, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66459.8047, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(399.8877, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66552.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(456.3645, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66461.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(613.3552, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66535.8047, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(366.7514, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66654.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1391.3584, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66525.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1212.8136, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66531.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(794.5486, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66565.3516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(810.0364, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66582.7578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(765.7582, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66613.5781, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 20, Loss: 765.7582\n",
      "QUANTILE LOSS:  tensor(1373.9965, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66638.8984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1140.8646, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66658.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(991.3892, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66689.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(715.0817, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66729., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(763.4387, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66745.9453, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(750.5648, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66786.3203, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(557.7764, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66829.9141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(674.3844, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66838.2969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(670.0587, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66876.0234, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(491.1574, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66914.6172, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 30, Loss: 491.1574\n",
      "QUANTILE LOSS:  tensor(612.4375, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66912.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(655.1179, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66953.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(471.4186, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66984.9141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(590.7689, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(66989.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(602.6372, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67029.6797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(671.2841, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67052.6484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(650.1664, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67066.1875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(657.9296, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67110.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(491.2527, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67139.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(639.3682, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67142.9141, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 40, Loss: 639.3682\n",
      "QUANTILE LOSS:  tensor(639.8724, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67183.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(591.4079, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67210.1328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(785.1037, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67214.6328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(654.8665, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67262.6172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(471.3138, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67290.6484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(625.9224, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67294.4688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(653.2752, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67338.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(657.8836, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67373.8359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1192.6910, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67364.2578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(722.1001, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67419.4609, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 50, Loss: 722.1001\n",
      "QUANTILE LOSS:  tensor(1264.4762, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67413.7266, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(773.9443, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67450.8359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(636.2957, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67511.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(315.8087, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67502.4297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(632.9393, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67524.2734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(650.8544, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67551.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2401.6736, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67566.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1193.7982, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67592.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(810.1703, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67626.6172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(441.7383, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67650.2500, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 60, Loss: 441.7383\n",
      "QUANTILE LOSS:  tensor(683.8517, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67676.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(753.1463, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67701.9141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(308.4115, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67731.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(655.8345, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67750.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(863.2083, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67775.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1261.9924, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67797.3516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(839.1537, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67822.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(816.5771, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67848.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1700.8428, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67864.8984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1196.1674, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67893.1172, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 70, Loss: 1196.1674\n",
      "QUANTILE LOSS:  tensor(901.9733, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67923.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(781.5681, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67945.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(882.8256, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(67976.6328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(814.7967, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68007.3828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1421.4561, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68022.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1044.0187, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68053.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(868.5178, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68081.4062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(627.2470, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68106.3047, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(863.7427, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68136.6016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(721.3085, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68164.9062, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 80, Loss: 721.3085\n",
      "QUANTILE LOSS:  tensor(413.4880, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68193.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.6441, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68217.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.8215, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68240.4922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1504.8927, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68260.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1118.2570, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68289.0703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(909.5236, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68318.1719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1013.6348, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68340.4297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.0532, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68376.2734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(777.6064, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68402.5312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1374.6649, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68420.3672, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 90, Loss: 1374.6649\n",
      "QUANTILE LOSS:  tensor(1050.2069, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68451.2422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(867.2014, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68480.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(806.1325, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68504.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(796.9870, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68540.0234, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.0947, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68564.2266, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1525.5880, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68585.8125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1093.9498, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68616.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(877.6115, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68647.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.1653, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68671.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(804.9504, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68706.5078, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 100, Loss: 804.9504\n",
      "QUANTILE LOSS:  tensor(732.9566, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68733.9297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(371.8430, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68766.1875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(673.3638, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68789.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(808.3987, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68818.0469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1448.4545, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68831.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1036.9379, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68862.0547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(818.2324, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68900.8828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(707.9492, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68916.1016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(795.8913, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68947.4922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(815.1851, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68980.8359, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 110, Loss: 815.1851\n",
      "QUANTILE LOSS:  tensor(1316.4532, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(68992.1719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1033.8942, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69024.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(886.0259, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69061.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(836.7136, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69076.4766, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(697.2277, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69114.3125, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(649.7493, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69148.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(961.6966, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69165.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1061.7568, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69185.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(859.5542, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69220.2969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1028.9663, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69237.7891, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 120, Loss: 1028.9663\n",
      "QUANTILE LOSS:  tensor(862.1164, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69269.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(742.6472, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69307.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(587.8916, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69325.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(592.5367, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69365.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(589.9244, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69396.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1223.3187, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69399.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(960.9404, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69431.8984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.1321, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69467.8359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(928.7095, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69484.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(801.7852, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69518.8672, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 130, Loss: 801.7852\n",
      "QUANTILE LOSS:  tensor(787.1514, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69553.4062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1068.8398, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69565.6016, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(939.0660, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69597.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(767.2493, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69638.2188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(851.3338, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69652.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(796.4859, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69689.5781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(652.8975, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69728.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(921.4388, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69740.8203, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(957.6294, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69767.8828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.2054, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69815.0859, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 140, Loss: 772.2054\n",
      "QUANTILE LOSS:  tensor(835.7468, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69823.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(804.6147, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69861.2969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(662.7211, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69900.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(583.1415, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69916.8359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(540.6972, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69964.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(530.2660, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70025.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1571.9791, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(69991.9453, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1203.2343, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70022.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1002.3379, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70054.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1075.4393, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70086.9062, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 150, Loss: 1075.4393\n",
      "QUANTILE LOSS:  tensor(1007.1125, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70117.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(826.4016, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70158.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(828.6226, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70172.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.7346, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70211.2969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(759.3845, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70241.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.2079, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70261.2969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.5522, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70301.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(807.3738, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70348.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1317.9095, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70338.3047, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1009.8138, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70373.6641, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 160, Loss: 1009.8138\n",
      "QUANTILE LOSS:  tensor(801.2669, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70432.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(758.0609, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70431.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(842.2214, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70488., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(793.6372, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70521.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1086.5498, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70513.4219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(944.7040, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70567.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.0596, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70608.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(767.1070, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70607.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(851.4348, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70661.0469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(737.8113, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70695.8828, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 170, Loss: 737.8113\n",
      "QUANTILE LOSS:  tensor(1001.0225, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70687.7031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(882.6025, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70744.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(707.2512, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70786.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(750.4334, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70783.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(805.4728, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70834.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(664.8920, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70874.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(706.4373, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70872.0703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(754.9694, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70933.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(685.4268, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70962.5703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(979.4278, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(70950.4453, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 180, Loss: 979.4278\n",
      "QUANTILE LOSS:  tensor(829.3012, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71013.4297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(697.5445, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71048.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(689.8829, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71060.3438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(738.2524, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71117.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(687.4262, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71133.6172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(663.1074, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71164.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(710.1912, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71211.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(714.1164, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71221.8828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.6887, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71247.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(717.1665, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71292.7422, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 190, Loss: 717.1665\n",
      "QUANTILE LOSS:  tensor(643.1501, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71316.4453, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(641.5206, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71368.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(685.7419, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71381.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(635.4826, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71405.2969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(620.1624, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71459.1719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(679.4917, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71471.3672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(598.5342, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71467.8359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(585.2753, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71552.7578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(655.4959, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71551.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(620.3393, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71550.4141, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 200, Loss: 620.3393\n",
      "QUANTILE LOSS:  tensor(782.8022, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71615.4219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(749.6700, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71618.3203, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(651.2266, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71636.8828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(643.0847, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71719.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(712.3467, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71702.2031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(610.3161, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71883., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(661.3083, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71800.6328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(696.2507, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71782.5312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(646.4893, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71905.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(704.2922, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71876.8750, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 210, Loss: 704.2922\n",
      "QUANTILE LOSS:  tensor(726.8091, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71868.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(658.1475, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71995.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(603.4698, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71948.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(794.1348, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71949.7578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(851.1740, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(71986.3828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(846.6997, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72041.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(806.9677, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72039.0469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.0485, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72075., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(926.9365, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72118.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(940.4694, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72145.0078, grad_fn=<MeanBackward0>)\n",
      "Epoch 22/25, Batch 220, Loss: 940.4694\n",
      "QUANTILE LOSS:  tensor(930.7150, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72163.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(959.4476, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72184.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(634.4063, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72215.8438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(617.6672, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72244.2422, grad_fn=<MeanBackward0>)\n",
      "Epoch 22, Avg Loss: 980.7029\n",
      "QUANTILE LOSS:  tensor(11456.6719, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72268.9141, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 0, Loss: 11456.6719\n",
      "QUANTILE LOSS:  tensor(10040.4092, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72298.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8080.3403, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72320.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6012.8608, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72350.2422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5895.8032, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72381.5781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1242.7020, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72442.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(589.7050, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72529.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(635.6296, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72483.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(631.2693, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72553.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(705.1179, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72640.6953, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1012.8620, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72577.1016, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 10, Loss: 1012.8620\n",
      "QUANTILE LOSS:  tensor(599.9055, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72665.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(396.2025, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72762.7188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(452.7705, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72667.2578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(607.4750, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72744.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(363.3009, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72868.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1386.3929, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72733.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1206.9342, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72739.2969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(788.6694, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72774.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(804.1573, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72792.7031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(759.8793, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72824.7500, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 20, Loss: 759.8793\n",
      "QUANTILE LOSS:  tensor(1368.1177, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72851.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1134.9860, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72871.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(985.5107, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72903.2578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(709.2034, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72944.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(757.5603, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(72962.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(744.6865, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73004.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(552.9258, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73050.2422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(668.5063, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73058.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(664.1805, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73098.3672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(486.9709, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73138.6562, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 30, Loss: 486.9709\n",
      "QUANTILE LOSS:  tensor(607.2447, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73135.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(649.2404, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73178.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(467.8767, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73211.2578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(585.9857, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73215.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(596.7607, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73257.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(665.6362, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73280.9297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(644.2906, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73294.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(652.0541, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73340.2578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(487.3153, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73370.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(633.6072, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73374.0234, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 40, Loss: 633.6072\n",
      "QUANTILE LOSS:  tensor(633.9977, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73416.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(587.2057, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73443.7578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.2295, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73448.1719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(648.9926, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73498.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(468.0836, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73527.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(620.2903, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73530.9062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(647.4020, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73576.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(652.8990, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73613.2578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1186.8184, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73603.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(716.2277, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73660.3828, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 50, Loss: 716.2277\n",
      "QUANTILE LOSS:  tensor(1258.6041, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73654.2422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(768.0723, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73692.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(630.5535, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73755.8203, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(313.5779, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73746.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(627.4966, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73768.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(645.3245, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73797.4453, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2395.9165, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73812.6172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1187.9274, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73839.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(804.2997, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73875.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(438.8241, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73899.7109, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 60, Loss: 438.8241\n",
      "QUANTILE LOSS:  tensor(678.4361, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73926.5547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(747.3147, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73953.4922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(306.0654, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(73984.7578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(650.3401, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74004.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(857.3384, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74030.2969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1256.3500, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74052.5781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(833.2842, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74078.6953, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(810.7114, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74106.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1694.9736, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74122.7578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1190.2983, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74152.1250, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 70, Loss: 1190.2983\n",
      "QUANTILE LOSS:  tensor(896.1043, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74183.8984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(776.3633, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74206.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(876.9570, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74238.8438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(809.1555, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74270.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1415.5883, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74286.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1038.1508, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74318.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(862.6501, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74347.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(622.9688, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74373.3828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(857.8752, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74404.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(715.8917, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74434.3672, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 80, Loss: 715.8917\n",
      "QUANTILE LOSS:  tensor(410.6853, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74464.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.7770, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74488.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.9545, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74512.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1499.0259, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74533.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1112.3903, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74563.5312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(903.6568, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74593.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1007.8818, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74617.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.1867, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74654.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(771.7400, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74681.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1368.7983, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74700.4297, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 90, Loss: 1368.7983\n",
      "QUANTILE LOSS:  tensor(1044.3405, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74732.6328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(861.3350, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74763.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(800.5106, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74788.3672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(791.1206, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74825.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.3417, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74850.5781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1519.7217, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74873.0469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1088.0836, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74905.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(871.7454, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74936.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(740.0892, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74962.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(799.0843, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(74998.9609, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 100, Loss: 799.0843\n",
      "QUANTILE LOSS:  tensor(727.2551, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75027.5469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(369.6067, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75061.1719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(667.7249, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75085.5469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(802.5330, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75115.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1442.5889, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75129.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1031.0723, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75160.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(812.3667, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75201.4766, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(703.7845, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75217.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.0258, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75250.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(809.4330, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75284.9062, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 110, Loss: 809.4330\n",
      "QUANTILE LOSS:  tensor(1310.5878, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75296.7188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1028.0288, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75330.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(880.1607, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75369.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(831.1883, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75384.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(691.3622, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75424.3203, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(644.4515, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75459.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(957.5322, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75477.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1055.8917, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75498.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(853.6890, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75534.7188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1023.1013, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75552.9141, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 120, Loss: 1023.1013\n",
      "QUANTILE LOSS:  tensor(856.2515, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75586.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(736.7822, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75625.4219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(583.1603, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75643.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(586.9083, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75685.7578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(584.6263, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75718.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1217.4540, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75721.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(955.0757, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75755.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.2676, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75792.7578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(922.8450, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75809.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(795.9206, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75845.9609, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 130, Loss: 795.9206\n",
      "QUANTILE LOSS:  tensor(781.2869, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75882.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1062.9752, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75894.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(933.2015, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75928.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(761.3848, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75970.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(845.4693, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(75985.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.6215, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76024.2422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(647.0330, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76064.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(915.5743, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76077.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(951.7648, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76106.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.3408, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76155.4062, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 140, Loss: 766.3408\n",
      "QUANTILE LOSS:  tensor(829.8823, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76164.4062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(798.7502, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76203.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(656.8565, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76244.9688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(577.2769, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76261.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(534.9532, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76311.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(525.2969, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76375.4062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1566.1146, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76340.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1197.3698, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76372., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(996.4736, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76405.2422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1069.8019, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76438.9141, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 150, Loss: 1069.8019\n",
      "QUANTILE LOSS:  tensor(1001.2484, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76471.2422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(820.5374, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76513.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(822.7586, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76527.5781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.8706, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76568.5469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(753.5205, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76600.1875, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.3440, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76620.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.6885, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76662.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(801.5099, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76711.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1312.0455, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76701.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1003.9501, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76737.9609, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 160, Loss: 1003.9501\n",
      "QUANTILE LOSS:  tensor(795.4031, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76799.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(752.1970, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76798.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(836.3574, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76857.3828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(787.7734, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76891.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1080.6859, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76883.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(938.8401, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76940.8438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(760.1956, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76983.2109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(761.2430, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(76981.7578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(845.5706, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77038.2031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(731.9473, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77074.5078, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 170, Loss: 731.9473\n",
      "QUANTILE LOSS:  tensor(995.1584, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77066.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(876.7385, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77124.9219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(701.3870, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77169.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(744.5693, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77165.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(799.6087, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77219.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(659.0277, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77260.8203, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(700.5731, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77258.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(749.1050, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77322.5703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(679.5625, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77353.2578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(973.5635, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77340.7578, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 180, Loss: 973.5635\n",
      "QUANTILE LOSS:  tensor(823.4368, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77406.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(691.6801, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77443.1328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(684.0183, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77455.6172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(732.3879, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77514.8438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(681.5617, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77532.0703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(657.2427, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77564.0625, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(704.3267, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77613.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(708.2516, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77624.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(739.8238, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77650.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(711.3017, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77698.4219, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 190, Loss: 711.3017\n",
      "QUANTILE LOSS:  tensor(637.2853, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77723.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(635.6558, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77778.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(679.8770, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77791.6797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(629.6177, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77816.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(614.2974, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77872.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(673.6266, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77885.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(592.7147, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77881.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(579.5217, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77970.1797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(649.6310, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77969., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(614.7012, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(77967.3203, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 200, Loss: 614.7012\n",
      "QUANTILE LOSS:  tensor(776.9376, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78035.2031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(743.8054, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78038.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(645.3621, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78057.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(637.2201, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78143.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(706.4822, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78125.3438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(604.5317, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78314.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(655.4439, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78228.0469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(690.3864, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78208.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(640.6252, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78337.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(698.4280, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78307.4141, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 210, Loss: 698.4280\n",
      "QUANTILE LOSS:  tensor(720.9450, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78298.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(652.3969, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78430.6406, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(597.6057, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78382.1797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(788.2707, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78383.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(845.3099, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78421.4922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(840.8357, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78479.3672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(801.1038, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78476.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(819.1845, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78514., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(921.0723, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78559.7188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(934.6054, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78587.1250, grad_fn=<MeanBackward0>)\n",
      "Epoch 23/25, Batch 220, Loss: 934.6054\n",
      "QUANTILE LOSS:  tensor(924.8510, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78605.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(953.5835, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78628.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(628.5422, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78661.0547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(611.8033, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78690.6953, grad_fn=<MeanBackward0>)\n",
      "Epoch 23, Avg Loss: 975.0894\n",
      "QUANTILE LOSS:  tensor(11450.8076, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78716.7188, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 0, Loss: 11450.8076\n",
      "QUANTILE LOSS:  tensor(10034.7842, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78747.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8074.4761, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78771.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6006.9961, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78801.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5889.9824, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78834.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1237.6587, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78897.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(585.1469, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78987.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(630.0839, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(78940.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(625.4333, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79012.8359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(700.4731, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79104.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1006.9986, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79037.4922, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 10, Loss: 1006.9986\n",
      "QUANTILE LOSS:  tensor(594.0424, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79129.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(392.6049, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79230.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(449.1729, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79130.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(601.6125, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79211.1328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(359.9879, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79340.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1381.4366, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79199.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1201.0724, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79205.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(782.8079, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79241.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(798.2959, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79260.3672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(754.0180, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79293.6172, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 20, Loss: 754.0180\n",
      "QUANTILE LOSS:  tensor(1362.2565, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79320.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1129.1249, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79341.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(979.6497, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79375.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(703.3925, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79418.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(751.6996, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79436.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(738.8260, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79480.6328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(548.1816, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79527.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(662.6461, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79536.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(658.3205, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79577.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(483.3187, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79619.6719, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 30, Loss: 483.3187\n",
      "QUANTILE LOSS:  tensor(602.0638, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79616.1406, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(643.3813, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79660.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(464.5348, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79694.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(581.5601, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79698.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(590.9028, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79741.4453, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(660.0682, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79765.6328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(638.4334, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79779.5703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(646.1973, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79826.6328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(483.7935, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79858., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(627.8978, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79860.8359, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 40, Loss: 627.8978\n",
      "QUANTILE LOSS:  tensor(628.1425, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79904.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(583.2070, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79932.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(773.3755, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79936.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(643.1391, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(79987.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(464.9779, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80017.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(614.8351, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80020.7969, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(641.5501, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80067.4219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(647.9622, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80105.6172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1180.9674, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80094.4922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(710.3770, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80153.8359, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 50, Loss: 710.3770\n",
      "QUANTILE LOSS:  tensor(1252.7538, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80147.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(762.2222, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80187.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(624.9288, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80252.4531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(311.3276, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80242.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(622.1832, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80265.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(639.8127, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80295.1328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2390.1799, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80310.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1182.0789, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80338.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(798.4512, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80375.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(435.8982, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80401.0703, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 60, Loss: 435.8982\n",
      "QUANTILE LOSS:  tensor(673.1152, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80428.9453, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(741.5790, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80456.8047, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(303.7923, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80489.2578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(644.9419, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80509.1797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(851.4910, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80536.3672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1250.7274, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80559.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(827.4370, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80586.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(804.9767, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80614.9062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1689.1266, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80632.3203, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1184.4514, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80662.8281, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 70, Loss: 1184.4514\n",
      "QUANTILE LOSS:  tensor(890.2575, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80695.8359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(771.4868, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80719.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(871.1104, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80752.9688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(803.5335, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80786.1328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1409.7416, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80802.3672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1032.3044, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80835.4062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(856.8037, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80866., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(618.7751, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80892.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(852.0291, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80925.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(710.4946, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80956.3281, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 80, Loss: 710.4946\n",
      "QUANTILE LOSS:  tensor(407.9290, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(80987.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.9311, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81013.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(746.1402, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81037.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1493.1805, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81059.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1106.5447, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81090.4062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(897.8115, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81121.9062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1002.1486, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81145.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.3415, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81184.8438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(765.8948, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81213.3281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1362.9532, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81232.7422, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 90, Loss: 1362.9532\n",
      "QUANTILE LOSS:  tensor(1038.4955, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81266.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(855.4899, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81297.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(795.2708, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81324.3203, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(785.2759, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81362.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.6094, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81388.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1513.8774, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81411.8047, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1082.2395, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81445.1328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(865.9014, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81478.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(735.2910, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81504.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(793.2407, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81542.2891, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 100, Loss: 793.2407\n",
      "QUANTILE LOSS:  tensor(721.7001, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81571.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(367.3496, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81606.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(662.1062, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81631.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(796.6906, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81662.2109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1436.7466, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81676.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1025.2301, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81709.6172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(806.5247, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81751.6797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(699.6226, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81768.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.1841, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81802.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(803.7034, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81838.2422, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 110, Loss: 803.7034\n",
      "QUANTILE LOSS:  tensor(1304.7462, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81850.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1022.1874, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81885.2422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(874.3193, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81926.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(825.6829, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81942.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(685.5210, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(81983.2422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(639.2975, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82019.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(953.3703, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82038.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1050.0507, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82060.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(847.8481, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82098.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1017.2603, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82116.9922, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 120, Loss: 1017.2603\n",
      "QUANTILE LOSS:  tensor(850.4106, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82151.4922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(730.9414, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82192.4766, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(578.6175, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82211.8203, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(581.4033, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82255.2109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(579.4163, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82289.0547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1211.6136, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82292.3438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(949.2355, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82327.2422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.4275, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82366.1250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(917.0049, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82383.7031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.0806, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82421.4062, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 130, Loss: 790.0806\n",
      "QUANTILE LOSS:  tensor(775.4472, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82458.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1057.1355, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82472.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(927.3618, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82507.1797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(755.5452, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82551.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(839.6298, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82566.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.7819, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82606.8828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(641.1934, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82649.2578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(909.7347, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82662.6797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(945.9252, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82692.1797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(760.5013, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82743.5391, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 140, Loss: 760.5013\n",
      "QUANTILE LOSS:  tensor(824.0427, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82752.9688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(792.9105, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82793.9453, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(651.0169, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82836.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(571.4372, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82854.4922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(529.3848, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82905.8828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(520.5666, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82972.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1560.2753, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82935.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1191.5305, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(82969.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(990.6345, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83003.5703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1064.1863, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83038.5703, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 150, Loss: 1064.1863\n",
      "QUANTILE LOSS:  tensor(995.4094, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83072.1328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(814.6985, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83116.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(816.9197, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83130.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(844.0318, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83173.3828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(747.6816, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83206.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.5051, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83227.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(819.8496, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83271.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(795.6711, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83322.5938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1306.2069, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83311.4297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(998.1113, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83349.8281, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 160, Loss: 998.1113\n",
      "QUANTILE LOSS:  tensor(789.5641, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83414.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(746.3582, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83413.1328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(830.5186, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83474.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(781.9344, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83510.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1074.8470, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83502.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(933.0012, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83561.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(754.3568, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83605.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(755.4041, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83604.0703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(839.7318, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83662.8828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.1082, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83700.6328, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 170, Loss: 726.1082\n",
      "QUANTILE LOSS:  tensor(989.3193, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83692.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(870.8994, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83753.4062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(695.5479, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83799.6172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(738.7302, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83796.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(793.7695, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83851.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(653.1885, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83895.0703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(694.7337, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83893.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(743.2658, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83959.6172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(673.7231, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83991.5703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(967.7241, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(83978.7109, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 180, Loss: 967.7241\n",
      "QUANTILE LOSS:  tensor(817.5972, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84047.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(685.8406, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84085.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(678.1788, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84098.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(726.5483, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84160.2422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(675.7220, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84178.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(651.4030, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84211.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(698.4869, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84263.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(702.4118, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84274.4297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(733.9839, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84302.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(705.4619, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84351.7500, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 190, Loss: 705.4619\n",
      "QUANTILE LOSS:  tensor(631.4454, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84377.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(629.8159, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84434.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(674.0370, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84449.0781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(623.7775, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84474.4922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(608.4573, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84533.4297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(667.7864, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84546.6172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(587.1048, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84542.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(574.0020, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84635.1719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(643.7913, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84633.4219, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(609.1769, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84631.2422, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 200, Loss: 609.1769\n",
      "QUANTILE LOSS:  tensor(771.0984, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84701.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(737.9667, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84704.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(639.5236, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84723.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(631.3819, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84813.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(700.6443, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84794.2031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(598.8057, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84990.4922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(649.6064, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84900.8281, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(684.5490, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84880.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(634.8696, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85013.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(692.5910, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84982.9609, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 210, Loss: 692.5910\n",
      "QUANTILE LOSS:  tensor(715.1082, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(84973.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(646.6975, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85110.7578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(591.7693, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85060.0547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(782.4344, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85060.9531, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(839.4739, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85100.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(834.9998, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85160.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(795.2679, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85157.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(813.3488, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85196.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(915.2368, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85244.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(928.7698, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85272.5703, grad_fn=<MeanBackward0>)\n",
      "Epoch 24/25, Batch 220, Loss: 928.7698\n",
      "QUANTILE LOSS:  tensor(919.0154, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85292.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(947.7480, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85315.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(622.7068, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85349.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(605.9679, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85380.1641, grad_fn=<MeanBackward0>)\n",
      "Epoch 24, Avg Loss: 969.5254\n",
      "QUANTILE LOSS:  tensor(11444.9727, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85407.5703, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 0, Loss: 11444.9727\n",
      "QUANTILE LOSS:  tensor(10029.1875, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85440., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(8068.6406, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85464.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(6001.1606, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85496.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(5884.2734, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85530.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1232.7159, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85595.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(580.6503, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85689.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(624.6360, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85640.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(619.7097, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85715.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(695.8649, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85810.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1001.1634, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85741.5078, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 10, Loss: 1001.1634\n",
      "QUANTILE LOSS:  tensor(588.2072, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85837.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(389.0248, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85942.1719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(445.5688, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85838.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(595.7774, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85921.9688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(356.9579, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86056.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1376.4943, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85909.3672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1195.2377, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85915.5781, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(776.9731, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85953.6797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(792.4612, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(85973.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(748.1834, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86007.5859, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 20, Loss: 748.1834\n",
      "QUANTILE LOSS:  tensor(1356.4220, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86035.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1123.2904, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86057.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(973.8154, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86092.3828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(697.6695, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86137.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(745.8652, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86156.5469, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(732.9915, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86202.1797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(543.5742, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86251.4922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(656.8806, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86260.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(652.4861, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86303.3203, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(479.9794, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86346.8672, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 30, Loss: 479.9794\n",
      "QUANTILE LOSS:  tensor(597.0206, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86343.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(637.5473, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86389.0703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(461.3408, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86423.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(577.2850, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86427.6172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(585.1031, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86472.3516, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(654.5696, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86497.2031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(632.6009, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86511.3828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(640.3650, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86560.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(480.4298, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86592.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(622.2887, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86595.1562, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 40, Loss: 622.2887\n",
      "QUANTILE LOSS:  tensor(622.3110, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86640.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(579.3799, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86669.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(767.5445, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86672.8203, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(637.3083, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86726.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(462.0040, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86756.9297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(609.4499, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86760.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(635.7199, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86808.4922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(643.2051, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86848.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1175.1377, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86836.1484, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(704.5477, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86897.5391, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 50, Loss: 704.5477\n",
      "QUANTILE LOSS:  tensor(1246.9244, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86890.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(756.3933, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86931.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(619.3224, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86999.4297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(309.0567, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(86988.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(616.9107, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87012.9297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(634.3179, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87043.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(2384.4631, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87059.6172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1176.2509, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87088.3672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(792.6233, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87126.8672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(432.9599, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87153.2891, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 60, Loss: 432.9599\n",
      "QUANTILE LOSS:  tensor(667.8431, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87182.2422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(735.8981, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87211.1797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(301.5205, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87244.8438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(639.5589, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87265.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(845.6635, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87293.7031, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1245.2000, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87317.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(821.6099, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87345.6719, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(799.2607, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87374.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1683.2998, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87392.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1178.6246, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87424.5938, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 70, Loss: 1178.6246\n",
      "QUANTILE LOSS:  tensor(884.4309, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87458.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(766.6597, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87483.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(865.2840, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87518.1797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(798.1045, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87552.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1403.9154, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87569.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1026.4783, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87603.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(850.9778, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87635.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(614.6143, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87663.1797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(846.2033, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87697.1953, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(705.2212, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87728.9688, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 80, Loss: 705.2212\n",
      "QUANTILE LOSS:  tensor(405.2486, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87760.9297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(767.1058, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87787.6797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(740.4832, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87813.4297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1487.3553, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87835.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1100.7200, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87867.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(891.9868, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87900.2188, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(996.4453, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87925.1328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.5171, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87965.4141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(760.0964, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(87994.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1357.1292, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88014.9062, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 90, Loss: 1357.1292\n",
      "QUANTILE LOSS:  tensor(1032.6716, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88049.5547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(849.6663, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88082.4297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.2234, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88109.6797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(779.4524, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88149.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(760.8969, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88176.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1508.0541, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88200.4922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1076.4161, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88235.0703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(860.0781, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88269.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(730.6519, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88297.0703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(787.4177, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88335.9609, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 100, Loss: 787.4177\n",
      "QUANTILE LOSS:  tensor(716.2095, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88366.5859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(365.0734, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88402.6172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(656.5051, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88428.7500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(790.8679, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88460.4922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1430.9240, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88475.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1019.4074, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88509.7422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(800.7020, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88553.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(695.4622, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88570.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.4150, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88605.8828, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(797.9917, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88643.4453, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 110, Loss: 797.9917\n",
      "QUANTILE LOSS:  tensor(1298.9238, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88656.1797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1016.3651, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88692.2422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(868.4969, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88734.6328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(820.3054, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88751.3438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(679.6987, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88794., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(634.2510, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88832.0312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(949.2100, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88851.2734, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1044.2286, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88874.0703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(842.0263, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88913.0078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1011.4385, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88932.7109, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 120, Loss: 1011.4385\n",
      "QUANTILE LOSS:  tensor(844.5889, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(88968.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(725.1197, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89011.0859, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(574.1249, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89031.1953, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(575.9140, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89076.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(574.2593, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89111.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1205.7920, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89114.9297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(943.4141, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89151.2109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(772.6060, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89191.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(911.1834, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89209.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(784.2591, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89249.2109, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 130, Loss: 784.2591\n",
      "QUANTILE LOSS:  tensor(769.6257, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89288.2109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1051.3140, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89302.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(921.5402, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89338.5312, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(749.7236, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89384.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(833.8082, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89400.4688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(778.9602, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89442.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(635.3716, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89486.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(903.9129, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89500.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(940.1035, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89531.1797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(754.6794, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89584.6562, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 140, Loss: 754.6794\n",
      "QUANTILE LOSS:  tensor(818.2208, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89594.5391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(787.0887, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89637.1953, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(645.1950, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89681.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(565.6152, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89700.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(523.8951, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89753.8203, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(516.0148, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89823.5078, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1554.4531, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89785.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1185.7086, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89819.5703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(984.8124, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89855.4453, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1058.5859, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89891.8438, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 150, Loss: 1058.5859\n",
      "QUANTILE LOSS:  tensor(989.5875, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89926.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(808.8765, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89972.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(811.0977, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(89987.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(838.2098, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90032.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(741.8594, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90066.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(760.6832, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90088.6797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(814.0275, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90134.2891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(789.8491, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90187.4922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1300.3846, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90175.9297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(992.2891, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90215.9141, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 160, Loss: 992.2891\n",
      "QUANTILE LOSS:  tensor(783.7421, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90282.6562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(740.5359, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90281.8359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(824.6964, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90345.4922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(776.1121, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90382.6797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(1069.0248, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90374.5000, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(927.1787, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90436.1328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(748.5344, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90481.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(749.5816, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90480.7109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(833.9092, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90541.9297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(720.2857, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90581.1562, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 170, Loss: 720.2857\n",
      "QUANTILE LOSS:  tensor(983.4968, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90572.4688, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(865.0767, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90636.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(689.7253, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90684.2578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(732.9074, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90680.8438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(787.9467, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90738.9141, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(647.3657, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90783.6797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(688.9109, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90781.8750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(737.4429, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90851., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(667.9002, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90884.2422, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(961.9011, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90871.0391, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 180, Loss: 961.9011\n",
      "QUANTILE LOSS:  tensor(811.7744, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90942.3750, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(680.0175, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90981.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(672.3558, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(90995.7578, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(720.7250, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91060.0391, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(669.8988, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91078.6172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(645.5797, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91113.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(692.6636, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91167.6797, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(696.5884, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91178.8984, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(728.1605, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91208.0547, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(699.6384, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91259.4375, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 190, Loss: 699.6384\n",
      "QUANTILE LOSS:  tensor(625.6218, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91286.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(623.9922, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91346.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(668.2134, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91360.7891, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(618.1352, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91387.2500, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(602.6337, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91448.6250, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(661.9630, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91462.1328, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(581.8500, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91457.9609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(568.7331, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91554.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(637.9680, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91552.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(603.8978, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91549.7422, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 200, Loss: 603.8978\n",
      "QUANTILE LOSS:  tensor(765.2754, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91623., grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(732.1439, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91625.1562, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(633.7010, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91645.4453, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(625.5594, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91738.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(694.8218, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91718.4062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(593.1787, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91922.3672, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(643.7842, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91829.1172, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(678.7269, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91807.9297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(629.3069, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91946.4297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(686.7691, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91913.9922, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 210, Loss: 686.7691\n",
      "QUANTILE LOSS:  tensor(709.2866, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91903.8438, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(641.0975, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(92046.4062, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(585.9479, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91993.5703, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(776.6132, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(91994.3359, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(833.6528, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(92035.4297, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(829.1787, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(92098.1641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(789.4468, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(92094.4609, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(807.5278, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(92134.9922, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(909.4160, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(92184.7266, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(922.9489, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(92213.9609, grad_fn=<MeanBackward0>)\n",
      "Epoch 25/25, Batch 220, Loss: 922.9489\n",
      "QUANTILE LOSS:  tensor(913.1946, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(92234.2109, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(941.9272, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(92259.0938, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(616.8860, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(92293.6641, grad_fn=<MeanBackward0>)\n",
      "QUANTILE LOSS:  tensor(600.1470, grad_fn=<MeanBackward0>)\n",
      "DISTILLATION LOSS:  tensor(92325.7109, grad_fn=<MeanBackward0>)\n",
      "Epoch 25, Avg Loss: 964.0030\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load teacher predictions\n",
    "timeframe = '5min'\n",
    "teacher_5min_preds = mv_dict[timeframe]\n",
    "teacher_5min_preds_df = pd.DataFrame(\n",
    "    teacher_5min_preds,\n",
    "    columns=[\n",
    "        \"power_pred_med_\" + timeframe,\n",
    "        \"voltage_pred_med_\" + timeframe,\n",
    "        \"current_pred_med_\" + timeframe\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Normalize the data\n",
    "X_normalized = (X - X.min()) / (X.max() - X.min())\n",
    "\n",
    "# Split train and test sets\n",
    "X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
    "y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Split the testing set into validation and final test sets (50/50)\n",
    "X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Resample data to 15-minute intervals\n",
    "X_train = X_train.resample('15min').mean().dropna()\n",
    "y_train = y_train.resample('15min').mean().dropna()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "X_valid = X_valid.resample('15min').mean().dropna()\n",
    "y_valid = y_valid.resample('15min').mean().dropna()\n",
    "\n",
    "# Align predictions to data\n",
    "# Filter 5-minute predictions to keep those 5 minutes before 15-minute intervals\n",
    "valid_timestamps = X_train.index\n",
    "teacher_5min_preds_df = teacher_5min_preds_df.loc[teacher_5min_preds_df.index.isin(valid_timestamps - pd.Timedelta(minutes=5))]\n",
    "\n",
    "# Reassign the prediction timestamps to align with the current data timestamp\n",
    "teacher_5min_preds_df.index = teacher_5min_preds_df.index + pd.Timedelta(minutes=5)\n",
    "\n",
    "nan_entries = teacher_5min_preds_df.index[pd.isnull(teacher_5min_preds_df.index)]\n",
    "# Print all NaN entries\n",
    "print(\"NaN Entries in teacher_5min_preds_df.index:\")\n",
    "print(nan_entries)\n",
    "\n",
    "# Concatenate predictions with training data\n",
    "X_with_teacher = pd.concat([X_train, teacher_5min_preds_df], axis=1)\n",
    "\n",
    "# Drop rows with missing values caused by timestamp mismatches\n",
    "# X_with_teacher = X_with_teacher.dropna()\n",
    "# Verify the updated shape\n",
    "print(\"Updated X Shape: Rows \" + str(X_with_teacher.shape[0]) + \" Columns \" + str(X_with_teacher.shape[1]))\n",
    "print(X_with_teacher.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "def quantile_loss(y_true, y_pred, quantile=0.5):\n",
    "    error = y_true - y_pred\n",
    "    loss = torch.mean(torch.max(quantile * error, (quantile - 1) * error))\n",
    "    print(\"QUANTILE LOSS: \", loss)\n",
    "    return loss\n",
    "\n",
    "def distillation_loss(y_teacher, y_pred):\n",
    "  loss = torch.mean((y_teacher - y_pred) ** 2)\n",
    "  print(\"DISTILLATION LOSS: \", loss)\n",
    "\n",
    "  return loss\n",
    "\n",
    "def combined_loss(y_true, y_pred, y_teacher, quantile=0.5, alpha=0.5):\n",
    "  return ((alpha) * quantile_loss(y_true, y_pred, quantile)) + ((1-alpha) * distillation_loss(y_teacher, y_pred))\n",
    "\n",
    "# Define the StudentModel (as provided earlier)\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, num_inputs, num_steps, output_size):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_steps = num_steps\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=num_inputs, hidden_size=200, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(200, 100)\n",
    "        self.fc2 = nn.Linear(100, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        fc1_out = self.relu(self.fc1(lstm_out))\n",
    "        output = self.fc2(fc1_out)\n",
    "        return output\n",
    "\n",
    "# Set parameters\n",
    "batch_size = 32  # Adjust as needed\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-3\n",
    "beta = 0.5  # For quantile loss\n",
    "\n",
    "X_teacher_normalized = (X_with_teacher - X_with_teacher.min()) / (X_with_teacher.max() - X_with_teacher.min())\n",
    "\n",
    "X_train_final, X_test_final = train_test_split(X_teacher_normalized, test_size=0.3, shuffle=False)\n",
    "y_train_final, y_test_final = train_test_split(y_train, test_size=0.3, shuffle=False)\n",
    "\n",
    "print(\"Training labels shape (X_with_teacher):\", X_teacher_normalized.shape)\n",
    "print(\"Training set shape (X_train_final):\", X_train_final.shape)\n",
    "print(\"Training labels shape (y_train_final):\", y_train_final.shape)\n",
    "\n",
    "# Split the test set further into validation and test sets\n",
    "X_valid_final, X_test_final = train_test_split(X_test_final, test_size=0.5, shuffle=False)\n",
    "y_valid_final, y_test_final = train_test_split(y_test_final, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Reshape data for LSTM input\n",
    "X_train_final = X_train_final.values.reshape((X_train_final.shape[0], 1, X_train_final.shape[1]))\n",
    "X_valid_final = X_valid_final.values.reshape((X_valid_final.shape[0], 1, X_valid_final.shape[1]))\n",
    "X_test_final = X_test_final.values.reshape((X_test_final.shape[0], 1, X_test_final.shape[1]))\n",
    "\n",
    "# X_train_teacher = X_train_teacher.values.reshape((X_train_teacher.shape[0], 1, X_train_teacher.shape[1]))\n",
    "#         X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
    "#         X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "#         # convert to tensor\n",
    "#         X_train_teacher = torch.tensor(X_train_teacher)\n",
    "#         y_train_teacher = torch.tensor(y_train_teacher.values)\n",
    "#         X_test = torch.tensor(X_test)\n",
    "#         y_test = torch.tensor(y_test.values)\n",
    "# Convert to tensor\n",
    "\n",
    "\n",
    "X_train_final = torch.tensor(X_train_final).float()\n",
    "y_train_final = torch.tensor(y_train_final.values).float()\n",
    "X_valid_final = torch.tensor(X_valid_final).float()\n",
    "y_valid_final = torch.tensor(y_valid_final.values).float()\n",
    "X_test_final = torch.tensor(X_test_final).float()\n",
    "y_test_final = torch.tensor(y_test_final.values).float()\n",
    "\n",
    "# Clean X_train_final tensor\n",
    "nan_mask = torch.isnan(X_train_final)  # Identify NaN values\n",
    "non_nan_values = X_train_final[~nan_mask]  # Filter out NaN values for mean calculation\n",
    "mean_value = torch.mean(non_nan_values)  # Compute mean of valid values\n",
    "\n",
    "# Replace NaNs with the computed mean\n",
    "tensor_cleaned = X_train_final.clone()\n",
    "tensor_cleaned[nan_mask] = mean_value\n",
    "\n",
    "# Reassign cleaned tensor back if desired\n",
    "X_train_final = tensor_cleaned\n",
    "\n",
    "# Create datasets\n",
    "train_dataset_final = TensorDataset(X_train_final, y_train_final)\n",
    "valid_dataset_final = TensorDataset(X_valid_final, y_valid_final)\n",
    "test_dataset_final = TensorDataset(X_test_final, y_test_final)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset_final, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset_final, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset_final, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Define model, optimizer, and loss function\n",
    "num_steps = 1  # Since you're using LSTM for time series data with one step\n",
    "num_inputs = X_train_final.shape[2]\n",
    "output_size = y_train_final.shape[1]\n",
    "model = Net(num_inputs, num_steps).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device).float(), targets.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)  # Ensure your model's forward method aligns\n",
    "        y_teacher = data[:, :, [20, 21, 22]].squeeze(-1)\n",
    "        y_pred = outputs[-1]\n",
    "        y_true = targets\n",
    "        # y_pred = y_pred.permute(1, 0, 2)\n",
    "\n",
    "        # print(\"teacher preds: \", y_teacher)\n",
    "        # print(\"student preds: \", y_pred)\n",
    "        # print(\"outputs: \", outputs)\n",
    "        # print(\"targets: \", targets)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = combined_loss(y_true, y_pred, y_teacher, quantile=0.5, alpha=1.0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Avg Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actuals:  [[ 241.8258  1189.6141  2090.2522 ]\n",
      " [ 313.35068 1186.8251  2711.1724 ]\n",
      " [ 302.72406 1164.8369  2670.9026 ]\n",
      " ...\n",
      " [ 732.9445  3099.3567  2410.5564 ]\n",
      " [ 922.6488  3775.9644  2457.9927 ]\n",
      " [ 872.704   4119.1685  2137.6677 ]]\n",
      "predictions:  [[-0.01992282 -0.03442565  0.08372541]\n",
      " [-0.01992282 -0.03442565  0.08372541]\n",
      " [-0.01992282 -0.03442565  0.08372541]\n",
      " ...\n",
      " [-0.01992282 -0.03442565  0.08372541]\n",
      " [-0.01992282 -0.03442565  0.08372541]\n",
      " [-0.01992282 -0.03442565  0.08372541]]\n",
      "Actual values shape: (1543, 3)\n",
      "Predictions shape: (1543, 3)\n",
      "Test MAPE power: 100.007%\n",
      "Test MAPE voltage: 100.003%\n",
      "Test MAPE current: 99.997%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# Define model, optimizer, and loss function\n",
    "num_steps = 1  # Since you're using LSTM for time series data with one step\n",
    "num_inputs = X_train_final.shape[2]\n",
    "output_size = y_train_final.shape[1]\n",
    "model = Net(num_inputs, num_steps).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Assuming the model has already been trained\n",
    "# Now let's test the model on the test set\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "actuals = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = model(data)\n",
    "        y_pred = outputs[-1]  # Last time-step prediction\n",
    "        y_pred = y_pred.permute(1, 0, 2)  # Reorganize to match ground truth shape\n",
    "\n",
    "        # Collect actual and predicted values\n",
    "        actuals.append(targets.cpu().numpy())  # Convert to numpy for easier comparison\n",
    "        predictions.append(y_pred.cpu().numpy())  # Convert to numpy for easier comparison\n",
    "\n",
    "# Concatenate all batches\n",
    "actuals = np.concatenate(actuals, axis=0)\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "predictions = predictions.squeeze(1)\n",
    "print(\"Actuals: \", actuals)\n",
    "print(\"predictions: \", predictions)\n",
    "\n",
    "# Print the shapes of the actuals and predictions\n",
    "print(f\"Actual values shape: {actuals.shape}\")\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Compute MAPE for each output (e.g., power, voltage, current)\n",
    "mape_power = MAPE(actuals[:, 0], predictions[:, 0])\n",
    "mape_voltage = MAPE(actuals[:, 1], predictions[:, 1])\n",
    "mape_current = MAPE(actuals[:, 2], predictions[:, 2])\n",
    "\n",
    "# Print results\n",
    "print(f\"Test MAPE power: {mape_power:.3f}%\")\n",
    "print(f\"Test MAPE voltage: {mape_voltage:.3f}%\")\n",
    "print(f\"Test MAPE current: {mape_current:.3f}%\")\n",
    "\n",
    "# Optionally, you can store the results in a dictionary or log them for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "xE1Qx0sDuEaR",
    "outputId": "c7238a57-0c15-44d3-e141-c4da226cc0fe"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input has inconsistent input_size: got 23 expected 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m quantile_loss(targets, outputs, quantile\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     14\u001b[0m valid_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[18], line 48\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m mem_rec \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_steps):\n\u001b[0;32m---> 48\u001b[0m     spk1, syn1, mem1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslstm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msyn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     spk2, mem2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(spk1), mem2)\n\u001b[1;32m     50\u001b[0m     spk3, mem3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(spk2), mem3)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/snntorch/_neurons/slstm.py:243\u001b[0m, in \u001b[0;36mSLSTM.forward\u001b[0;34m(self, input_, syn, mem)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(correct_shape, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem_reset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem)\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msyn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_quant:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msyn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_quant(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msyn)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/snntorch/_neurons/slstm.py:281\u001b[0m, in \u001b[0;36mSLSTM._base_int\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_base_int\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_):\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_state_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/snntorch/_neurons/slstm.py:259\u001b[0m, in \u001b[0;36mSLSTM._base_state_function\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_base_state_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_):\n\u001b[0;32m--> 259\u001b[0m     base_fn_mem, base_fn_syn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msyn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base_fn_syn, base_fn_mem\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1705\u001b[0m, in \u001b[0;36mLSTMCell.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1703\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched \u001b[38;5;28;01melse\u001b[39;00m hx\n\u001b[0;32m-> 1705\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_cell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   1715\u001b[0m     ret \u001b[38;5;241m=\u001b[39m (ret[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), ret[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input has inconsistent input_size: got 23 expected 20"
     ]
    }
   ],
   "source": [
    "# Validation loop\n",
    "model.eval()\n",
    "valid_loss = 0.0\n",
    "valid_mape = 0.0\n",
    "valid_mse = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in valid_loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = quantile_loss(targets, outputs, quantile=0.5)\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "        # Calculate MAPE\n",
    "        mape = mean_absolute_percentage_error(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "        valid_mape += mape\n",
    "\n",
    "        # Calculate MSE\n",
    "        mse = mean_squared_error(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "        valid_mse += mse\n",
    "\n",
    "print(f\"Validation Loss: {valid_loss / len(valid_loader):.4f}\")\n",
    "print(f\"Validation MAPE: {valid_mape / len(valid_loader):.4f}\")\n",
    "print(f\"Validation MSE: {valid_mse / len(valid_loader):.4f}\")\n",
    "\n",
    "# Test loop\n",
    "test_loss = 0.0\n",
    "test_mape = 0.0\n",
    "test_mse = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = quantile_loss(targets, outputs, quantile=0.5)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Calculate MAPE\n",
    "        mape = mean_absolute_percentage_error(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "        test_mape += mape\n",
    "\n",
    "        # Calculate MSE\n",
    "        mse = mean_squared_error(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "        test_mse += mse\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader):.4f}\")\n",
    "print(f\"Test MAPE: {test_mape / len(test_loader):.4f}\")\n",
    "print(f\"Test MSE: {test_mse / len(test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0KJvECeCuEaS"
   },
   "outputs": [],
   "source": [
    "\n",
    "batchsize_list = [300, 150, 50, 20, 8]\n",
    "time_frame_list = ['3min', '5min', '15min', '30min', '60min']\n",
    "time_frame_seconds_list = [180, 300, 900, 1800, 3600]\n",
    "n = 0\n",
    "\n",
    "snn_power_mape_list = []\n",
    "snn_volt_mape_list = []\n",
    "snn_curr_mape_list = []\n",
    "\n",
    "# Dictionary to store mv variables\n",
    "mv_dict = {}\n",
    "\n",
    "for j in range(len(batchsize_list)):\n",
    "    batchsize = batchsize_list[j]\n",
    "    time_frame = time_frame_list[j]\n",
    "    time_frame_seconds = time_frame_seconds_list[j]\n",
    "\n",
    "    X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"],\n",
    "                   df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"],\n",
    "                   df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"],\n",
    "                   df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"],\n",
    "                   df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"],\n",
    "                   df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"],\n",
    "                   df[\"tsd\"], df[\"hour\"]], axis=1)\n",
    "    y = pd.concat([df[\"Power (uW)\"], df['Voltage (mV)'], df['Current (uA)']], axis=1)\n",
    "\n",
    "    # Normalize Data\n",
    "    X_normalized = ((X - X.min()) / (X.max() - X.min()))\n",
    "\n",
    "    # Split train and test sets\n",
    "    X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
    "    y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
    "\n",
    "    X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
    "    y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
    "\n",
    "    # Calculate actual energy generated in test set\n",
    "    E_actual = 0\n",
    "    for i in range(len(y_test) - 1):\n",
    "        t = (y_test.index[i+1] - y_test.index[i]).total_seconds()\n",
    "        if t < 180:\n",
    "            E_actual += y_test['Power (uW)'][i] * t\n",
    "\n",
    "    # Resample data\n",
    "    X_valid = X_valid.resample(time_frame).mean().dropna()\n",
    "    y_valid = y_valid.resample(time_frame).mean().dropna()\n",
    "\n",
    "    X_test = X_test.resample(time_frame).mean().dropna()\n",
    "    y_test = y_test.resample(time_frame).mean().dropna()\n",
    "\n",
    "    # Define mv variable for the current time frame\n",
    "    mv_dict[time_frame] = y_test\n",
    "\n",
    "    # Reshape data\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
    "    X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "    # Convert to tensor\n",
    "    X_train = torch.tensor(X_train)\n",
    "    y_train = torch.tensor(y_train.values)\n",
    "    X_valid = torch.tensor(X_valid)\n",
    "    y_valid = torch.tensor(y_valid.values)\n",
    "    X_test = torch.tensor(X_test)\n",
    "    y_test = torch.tensor(y_test.values)\n",
    "\n",
    "    # Make datasets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=False)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batchsize, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
    "\n",
    "    num_steps = 50\n",
    "    num_inputs = X_train.shape[2]\n",
    "\n",
    "    # Create new instance of the SNN Class\n",
    "    model = Net(num_inputs, num_steps).to(device)\n",
    "\n",
    "    file = 'trained_models/snn_' + time_frame + '_quant50.pth'\n",
    "    print(file)\n",
    "\n",
    "    checkpoint = torch.load(file, map_location=torch.device('cpu'), weights_only=True)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            # Prepare data\n",
    "            data = data.to(device).float()\n",
    "            targets = targets.to(device).float()\n",
    "\n",
    "            _, _, _, output = model(data)\n",
    "\n",
    "            output = output.cpu().squeeze(1).detach()\n",
    "            actuals.append(targets)\n",
    "            predictions.append(output[-1])\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    actuals = torch.cat(actuals, dim=0)\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "\n",
    "    mv = mv_dict[time_frame]\n",
    "    mv[\"power_pred_med_\" + time_frame] = predictions[:, 0].numpy()\n",
    "    mv[\"voltage_pred_med_\" + time_frame] = predictions[:, 1].numpy()\n",
    "    mv[\"current_pred_med_\" + time_frame] = predictions[:, 2].numpy()\n",
    "\n",
    "    print(f'Voltage overestimation rate for {time_frame}: %.3f%%' % (\n",
    "        (mv['Voltage (mV)'].values <= mv[\"voltage_pred_med_\" + time_frame]).mean() * 100))\n",
    "    print(f\"Test MAPE power ({time_frame}): %3f\" % MAPE(mv['Power (uW)'].values.ravel(), mv[\"power_pred_med_\" + time_frame]))\n",
    "    print(f\"Test MAPE voltage ({time_frame}): %3f\" % MAPE(mv['Voltage (mV)'], mv[\"voltage_pred_med_\" + time_frame]))\n",
    "    print(f\"Test MAPE current ({time_frame}): %3f\" % MAPE(mv['Current (uA)'], mv[\"current_pred_med_\" + time_frame]))\n",
    "\n",
    "    E_pred = 0\n",
    "    for i in range(len(mv) - 1):\n",
    "        t = (mv.index[i+1] - mv.index[i]).total_seconds()\n",
    "        if t <= time_frame_seconds + 50:\n",
    "            E_pred += mv[\"power_pred_med_\" + time_frame][i] * t\n",
    "\n",
    "    print(f'Predicted vs. Actual Total Energy Percent Difference ({time_frame}): %.3f%%' % (\n",
    "        (E_pred - E_actual) * 100 / E_actual))\n",
    "\n",
    "    V_actual = mv['Voltage (mV)'].mean()\n",
    "    V_pred = mv[\"voltage_pred_med_\" + time_frame].mean()\n",
    "    print(f'Predicted vs. Actual Total Voltage Percent Difference ({time_frame}): %.3f%%' % (\n",
    "        (V_pred - V_actual) * 100 / V_actual))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "cObaSZAexrX-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
